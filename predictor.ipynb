{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PD结合的思路"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分别替换gate/up层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先输入一句话，按照70%稀疏去记录prefill阶段激活的神经元，最后统计这个输入prompt对应的最高激活次数的70%的神经元。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lz/anaconda3/envs/moe/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.03s/it]\n",
      "Convert Llama Models: 452it [00:00, 167742.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Model Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "import convert_llama\n",
    "from transformers import GenerationConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "### from path.json read paths of model and dataset\n",
    "model_name = \"Llama3-8b\"\n",
    "dataset_name = \"c4\"\n",
    "with open('path.json', 'r') as file:\n",
    "    paths = json.load(file)\n",
    "    model_path = paths.get(model_name, '')\n",
    "    dataset_path = paths.get(dataset_name, '')\n",
    "\n",
    "c4 = load_dataset(dataset_path)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map='auto',\n",
    "    use_cache=True,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "convert_llama.convert_llama_model(model, sparsity=0.1, start_num=14, end_num=16, )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[prefill] in gate layer: 15\n",
      "[prefill] in up layer: 15\n",
      "in decode, gate layer 15\n",
      "Overlap count: 1243.8621, Overlap ratio: 0.8680\n",
      "in decode, up layer 15\n",
      "Overlap count: 1080.8966, Overlap ratio: 0.7543\n"
     ]
    }
   ],
   "source": [
    "for c4_demo in c4['validation']['text'][:1]:\n",
    "    input_demo = tokenizer(c4_demo, padding=\"max_length\", truncation=True, max_length=200, return_tensors=\"pt\")\n",
    "    generate_ids = model.generate(input_demo.input_ids.to('cuda:0'), max_length=230, generation_config=GenerationConfig(do_sample=False), pad_token_id=tokenizer.eos_token_id)\n",
    "    tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    model.model.layers[15].mlp.gate_proj.coreinfer_recall()\n",
    "    model.model.layers[15].mlp.up_proj.coreinfer_recall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[prefill] in gate layer: 15\n",
      "[prefill] in up layer: 15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The woman who died after falling from a bridge over the A21 has been identified as a Sevenoaks mum.\\nMarta Kendle, 37, fell from the Gracious Lane bridge on the morning of February 19.\\nPolice were called to the carriageway around 6.10am and the road was promptly closed in both directions.\\nDespite paramedics best efforts, Marta, who was originally from Poland, was pronounced dead at the scene.\\nKent and Medway Coroners office have confirmed an inquest into her death will open on Wednesday (February 27).\\nTributes to the mum were left at the scene and on social media.\\nFriend, Jodi Cahill posted on Facebook: \"I will certainly remember you. I am sorry we did not see how lost and alone you felt.\\n\"Be at peace dear Marta.\"\\nA floral tribute left at the scene said goodbye to the \"beautiful and kind soul\".\\nIt read: \"To a beautiful and kind soul. You will be missed. Rest in peace.\"\\nA spokesman for Kent Police said: \"Officers were called to the A21 at Gracious Lane,'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 使用greedy decode\n",
    "generate_ids = model.generate(input_demo.input_ids.to('cuda:0'), max_length=230, generation_config=GenerationConfig(do_sample=False), pad_token_id=tokenizer.eos_token_id)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in decode, gate layer 15\n",
      "Overlap count: 1243.8621, Overlap ratio: 0.8680\n",
      "in decode, up layer 15\n",
      "Overlap count: 1080.8966, Overlap ratio: 0.7543\n"
     ]
    }
   ],
   "source": [
    "model.model.layers[15].mlp.gate_proj.coreinfer_recall()\n",
    "model.model.layers[15].mlp.up_proj.coreinfer_recall()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_factory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
