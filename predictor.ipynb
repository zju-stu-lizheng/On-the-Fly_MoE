{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PD结合的思路"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分别替换gate/up层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先输入一句话，按照70%稀疏去记录prefill阶段激活的神经元，最后统计这个输入prompt对应的最高激活次数的70%的神经元。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07910100ce974928b24c9d5ed092a9ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "from transformers import GenerationConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "### from path.json read paths of model and dataset\n",
    "model_name = \"Llama3-8b\"\n",
    "dataset_name = \"c4\"\n",
    "with open('path.json', 'r') as file:\n",
    "    paths = json.load(file)\n",
    "    model_path = paths.get(model_name, '')\n",
    "    dataset_path = paths.get(dataset_name, '')\n",
    "\n",
    "c4 = load_dataset(dataset_path)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map='auto',\n",
    "    use_cache=True,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.2, gamma: 0.3, beta: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convert Llama Models: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting Layer model.layers.21.mlp with CoreInfer\n",
      "Converting Layer model.layers.22.mlp with CoreInfer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convert Llama Models: 318it [00:00, 864.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting Layer model.layers.23.mlp with CoreInfer\n",
      "Converting Layer model.layers.24.mlp with CoreInfer\n",
      "Converting Layer model.layers.25.mlp with CoreInfer\n",
      "Converting Layer model.layers.26.mlp with CoreInfer\n",
      "Converting Layer model.layers.27.mlp with CoreInfer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convert Llama Models: 405it [00:01, 197.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting Layer model.layers.28.mlp with CoreInfer\n",
      "Converting Layer model.layers.29.mlp with CoreInfer\n",
      "Converting Layer model.layers.30.mlp with CoreInfer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convert Llama Models: 445it [00:02, 140.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting Layer model.layers.31.mlp with CoreInfer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convert Llama Models: 462it [00:02, 173.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Model Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The woman who died after falling from a bridge over the A21 has been identified as a Sevenoaks mum.\n",
      "Marta Kendle, 37, fell from the Gracious Lane bridge on the morning of February 19.\n",
      "Police were called to the carriageway around 6.10am and the road was promptly closed in both directions.\n",
      "Despite paramedics best efforts, Marta, who was originally from Poland, was pronounced dead at the scene.\n",
      "Kent and Medway Coroners office have confirmed an inquest into her death will open on Wednesday (February 27).\n",
      "Tributes to the mum were left at the scene and on social media.\n",
      "Friend, Jodi Cahill posted on Facebook: \"I will certainly remember you. I am sorry we did not see how lost and alone you felt.\n",
      "\"Be at peace dear Marta.\"\n",
      "A floral tribute left at the scene said goodbye to the \"beautiful and kind soul\".\n",
      "It read: \"To a beautiful and kind soul, we are sorry we didn&apos;t see your pain. We will miss you always.\n",
      "\"Rest in peace Marta, Love from Lorraine, Colin and family.\"\n",
      "The inquest will take place at the Archbishop&apos;s Palace in Maidstone and will examine the circumstances surrounding Mrs Kendle&apos;s death.\n",
      "Are you struggling or know someone who is? Watch our video below to find out how the Samaritans could help.\n",
      "Mrs Kendle&apos;s death prompted the setting up of a petition calling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The woman who died after falling from a bridge over the A21 has been identified as a Sevenoaks mum.\n",
      "Marta Kendle, 37, fell from the Gracious Lane bridge on the morning of February 19.\n",
      "Police were called to the carriageway around 6.10am and the road was promptly closed in both directions.\n",
      "Despite paramedics best efforts, Marta, who was originally from Poland, was pronounced dead at the scene.\n",
      "Kent and Medway Coroners office have confirmed an inquest into her death will open on Wednesday (February 27).\n",
      "Tributes to the mum were left at the scene and on social media.\n",
      "Friend, Jodi Cahill posted on Facebook: \"I will certainly remember you. I am sorry we did not see how lost and alone you felt.\n",
      "\"Be at peace dear Marta.\"\n",
      "A floral tribute left at the scene said goodbye to the \"beautiful and kind soul\".\n",
      "It read: \"To a beautiful and kind soul, we are sorry we didn&apos;t see your pain. We will miss you always.\n",
      "\"Rest in peace Marta, Love from Lorraine, Colin and family.\"\n",
      "The inquest will take place at the Archbishop&apos;s Palace in Maidstone and will examine the circumstances surrounding Mrs Kendle&apos;s death.\n",
      "Are you struggling or know someone who is? Watch our video below to find out how the Samaritans could help.\n",
      "Mrs Kendle&apos;s death prompted the setting up of a petition calling for the\n",
      "in decode, layer 22\n",
      "Predicted neuron num: 4790.0\n",
      "Overlap count: 1024.0, Overlap ratio: 0.7146\n",
      "in decode, layer 23\n",
      "Predicted neuron num: 4743.0\n",
      "Overlap count: 1018.0, Overlap ratio: 0.7104\n",
      "in decode, layer 24\n",
      "Predicted neuron num: 4764.0\n",
      "Overlap count: 972.0, Overlap ratio: 0.6783\n",
      "in decode, layer 25\n",
      "Predicted neuron num: 4730.0\n",
      "Overlap count: 969.0, Overlap ratio: 0.6762\n",
      "in decode, layer 26\n",
      "Predicted neuron num: 4714.0\n",
      "Overlap count: 970.0, Overlap ratio: 0.6769\n",
      "in decode, layer 27\n",
      "Predicted neuron num: 4668.0\n",
      "Overlap count: 1010.0, Overlap ratio: 0.7048\n",
      "in decode, layer 28\n",
      "Predicted neuron num: 4575.0\n",
      "Overlap count: 1112.0, Overlap ratio: 0.7760\n",
      "in decode, layer 29\n",
      "Predicted neuron num: 4497.0\n",
      "Overlap count: 1120.0, Overlap ratio: 0.7816\n",
      "in decode, layer 30\n",
      "Predicted neuron num: 4361.0\n",
      "Overlap count: 1266.0, Overlap ratio: 0.8835\n",
      "in decode, layer 31\n",
      "Predicted neuron num: 4319.0\n",
      "Overlap count: 1387.0, Overlap ratio: 0.9679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import convert_llama\n",
    "# for gamma in [0,0.2]:\n",
    "#     for beta in [0,0.2]:\n",
    "SAMPLE_NUM = 1\n",
    "MAX_LENGTH = 300\n",
    "OUTPUT_LENTGH = 2\n",
    "alpha = 0.2\n",
    "# for gamma,beta in [(0,0.3),(0.3,0),(0.2,0.2)]:\n",
    "for gamma, beta in [(0.3,0.1)]:\n",
    "# for gamma, beta in [(0,0.1)]:\n",
    "# for gamma, beta in [(0.3221,0)]:\n",
    "# for gamma,beta in [(0.1,0.2),(0.2,0.1),(0.2,0.2)]:\n",
    "    print(f\"alpha: {alpha}, gamma: {gamma}, beta: {beta}\")\n",
    "    convert_llama.convert_llama_model(model, sparsity=0.1, start_num=20, end_num=32, alpha=alpha, beta=beta, gamma=gamma, use_core=True)\n",
    "    for layerid in range(22,32):\n",
    "        model.model.layers[layerid].mlp.clear_list()\n",
    "\n",
    "    # for c4_demo in c4['validation']['text'][:SAMPLE_NUM]:\n",
    "    for c4_demo in tqdm(c4['validation']['text'][:SAMPLE_NUM]):\n",
    "        input_demo = tokenizer(c4_demo, padding=False, truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "        if input_demo.input_ids.shape[1] < MAX_LENGTH:\n",
    "            continue\n",
    "        print(tokenizer.batch_decode(input_demo.input_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])\n",
    "        generate_ids = model.generate(input_demo.input_ids.to('cuda:0'), max_length=MAX_LENGTH+OUTPUT_LENTGH, generation_config=GenerationConfig(do_sample=False), pad_token_id=tokenizer.eos_token_id)\n",
    "        # tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "        print(tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])\n",
    "    for layerid in range(22,32):\n",
    "        model.model.layers[layerid].mlp.coreinfer_recall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convert Llama Models: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convert Llama Models: 462it [00:02, 208.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Model Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [03:20<00:00,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 2.3604316777980006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import convert_llama\n",
    "import numpy as np\n",
    "import math\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 1. 定义超参数\n",
    "SAMPLE_NUM = 200\n",
    "MAX_LENGTH = 300\n",
    "OUTPUT_LENTGH = 30\n",
    "alpha = 0.2\n",
    "beta = 0\n",
    "gamma = 1.0\n",
    "\n",
    "eos_token_id = tokenizer.convert_tokens_to_ids('.')\n",
    "\n",
    "convert_llama.convert_llama_model(model, sparsity=0.1, start_num=20, end_num=32, alpha=alpha, beta=beta, gamma=gamma)\n",
    "ppl_list = []\n",
    "# 2. 准备输入文本\n",
    "for c4_demo in tqdm(c4['validation']['text'][:SAMPLE_NUM]):\n",
    "    input_demo = tokenizer(c4_demo, padding=False, truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "    if input_demo.input_ids.shape[1] < MAX_LENGTH:\n",
    "        continue\n",
    "    # input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_demo.input_ids.cuda(), use_cache=True)\n",
    "        past_key_values = outputs.past_key_values\n",
    "    \n",
    "    generated = input_demo.input_ids\n",
    "    log_prob = []\n",
    "    # 3. 生成文本[不应该用model.generate, 手写一个方便测试困惑度]\n",
    "    for _ in range(OUTPUT_LENTGH):\n",
    "        with torch.no_grad():\n",
    "            input_ids = generated[:, -1:].cuda()\n",
    "            outputs = model(input_ids=input_ids, past_key_values=past_key_values, use_cache=True)\n",
    "            logits = outputs.logits\n",
    "            past_key_values = outputs.past_key_values\n",
    "\n",
    "            # print(logits.shape) # [1, 1, 128256]\n",
    "            # print(input_ids.shape) # [1, 1]\n",
    "            # labels = input_ids[:, :]\n",
    "            probs = torch.softmax(logits[:,:], dim=-1)\n",
    "            probs = torch.max(probs.squeeze(0))\n",
    "            # print(probs, labels)\n",
    "            # print(probs)\n",
    "            log_prob.append(probs.log2().cpu().numpy())\n",
    "    \n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        next_token_id = torch.argmax(next_token_logits, dim=-1)\n",
    "    \n",
    "        generated = torch.cat((generated.cuda(), next_token_id.unsqueeze(-1)), dim=1)\n",
    "    \n",
    "        next_token_text = tokenizer.decode(next_token_id)\n",
    "        # print(next_token_text, end='', flush=True)\n",
    "    \n",
    "        # if next_token_id.item() == eos_token_id:\n",
    "        #     break\n",
    "        # if '.' in next_token_text:\n",
    "        #     break\n",
    "    ppl_item = - np.sum(log_prob) / len(log_prob)\n",
    "    ppl_list.append(ppl_item)\n",
    "\n",
    "\n",
    "ce = np.sum(ppl_list) / len(ppl_list)\n",
    "ppl = 2 ** ce\n",
    "print(f\"Perplexity: {ppl.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[prefill] in gate layer: 15\n",
      "[prefill] in up layer: 15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The woman who died after falling from a bridge over the A21 has been identified as a Sevenoaks mum.\\nMarta Kendle, 37, fell from the Gracious Lane bridge on the morning of February 19.\\nPolice were called to the carriageway around 6.10am and the road was promptly closed in both directions.\\nDespite paramedics best efforts, Marta, who was originally from Poland, was pronounced dead at the scene.\\nKent and Medway Coroners office have confirmed an inquest into her death will open on Wednesday (February 27).\\nTributes to the mum were left at the scene and on social media.\\nFriend, Jodi Cahill posted on Facebook: \"I will certainly remember you. I am sorry we did not see how lost and alone you felt.\\n\"Be at peace dear Marta.\"\\nA floral tribute left at the scene said goodbye to the \"beautiful and kind soul\".\\nIt read: \"To a beautiful and kind soul. You will be missed. Rest in peace.\"\\nA spokesman for Kent Police said: \"Officers were called to the A21 at Gracious Lane,'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 使用greedy decode\n",
    "generate_ids = model.generate(input_demo.input_ids.to('cuda:0'), max_length=230, generation_config=GenerationConfig(do_sample=False), pad_token_id=tokenizer.eos_token_id)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in decode, gate layer 15\n",
      "Overlap count: 1243.8621, Overlap ratio: 0.8680\n",
      "in decode, up layer 15\n",
      "Overlap count: 1080.8966, Overlap ratio: 0.7543\n"
     ]
    }
   ],
   "source": [
    "model.model.layers[15].mlp.gate_proj.coreinfer_recall()\n",
    "model.model.layers[15].mlp.up_proj.coreinfer_recall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载保存的激活值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.init as init\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "    \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.cuda.amp import GradScaler, autocast  \n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import json\n",
    "\n",
    "with open('path.json', 'r') as file:\n",
    "    paths = json.load(file)\n",
    "    save_path = paths.get('gatemulup_path','')\n",
    "\n",
    "def load_datasets(layerid = 1, expertid = 0, startid=1, endid=4, use_x1 = False):   \n",
    "    datasets_x = []\n",
    "    datasets_y = []\n",
    "    datasets_x1 = []\n",
    "    for fileid in range(startid, endid):\n",
    "        # print(fileid)\n",
    "        # 加一个map_location\n",
    "        d = torch.load(f'{save_path}/{fileid}-{layerid}-more.pth', map_location=lambda storage, loc: storage.cuda(0))\n",
    "        datasets_x.append(d[0])\n",
    "        if use_x1:\n",
    "            datasets_x1.append(d[1])\n",
    "        datasets_y.append(d[-1])\n",
    "    x,y = torch.cat(datasets_x,dim=1), torch.cat(datasets_y,dim=1)\n",
    "    datasets_x.clear()\n",
    "    datasets_y.clear()\n",
    "    x = x.reshape(-1, 4096)\n",
    "    y = y.reshape(-1, 14336)\n",
    "    # print(x[0].shape)\n",
    "    if use_x1:\n",
    "        x1 = torch.cat(datasets_x1,dim=1)\n",
    "        datasets_x1.clear()\n",
    "        x1 = x1.reshape(-1, 14336)\n",
    "        return x,x1,y\n",
    "    return x,y\n",
    "    \n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, layerid = 1, expertid = 0, startid=1, endid=4, use_x1 =False):\n",
    "        # 加载数据self.data_x1,\n",
    "        self.use_x1 = use_x1\n",
    "        if use_x1:\n",
    "            self.data_x, self.data_x1, self.data_y = load_datasets(layerid,startid=startid,endid=endid,use_x1=use_x1)\n",
    "            print(len(self.data_x1),len(self.data_x),len(self.data_y))\n",
    "        else:\n",
    "            self.data_x, self.data_y = load_datasets(layerid,startid=startid,endid=endid,use_x1=use_x1)\n",
    "            print(len(self.data_x),len(self.data_y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.use_x1:\n",
    "            return self.data_x[idx],self.data_x1[idx],self.data_y[idx]\n",
    "        else:\n",
    "            return self.data_x[idx],self.data_y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 稀疏预测器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLinearModel(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim,hidden_dim=32):\n",
    "        super(SimpleLinearModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim,bias=False)\n",
    "        # self.activation = nn.SiLU() # 添加激活函数\n",
    "        self.linear2 = nn.Linear(hidden_dim,output_dim,bias=False)  \n",
    "        init.kaiming_normal_(self.linear1.weight, mode='fan_out', nonlinearity='relu')\n",
    "        init.kaiming_normal_(self.linear2.weight, mode='fan_out', nonlinearity='relu')\n",
    "        # self.linear1.bias.data.fill_(0)\n",
    "        # self.linear2.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x= self.activation(x)\n",
    "        return self.linear2(self.linear1(x))\n",
    "    \n",
    "model=SimpleLinearModel(4096,14336,hidden_dim=1024)\n",
    "model.to(\"cuda\")  # 假设使用 GPU\n",
    "# criterion = nn.MSELoss().to(\"cuda\")\n",
    "criterion = nn.CrossEntropyLoss().to(\"cuda\")\n",
    "# criterion = nn.KLDivLoss(reduction='batchmean').to(\"cuda\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4) #lr=5e-5\n",
    "writer = SummaryWriter('runs/predictor_sparsity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "def sparse_row(row, keep_ratio=0.1, use_abs = False):\n",
    "    # 计算需要保留的参数数量\n",
    "    num_to_keep = int(keep_ratio * row.numel())\n",
    "    \n",
    "    # 找到绝对值最大的 num_to_keep 个参数的索引\n",
    "    if use_abs:\n",
    "        row = torch.abs(row)\n",
    "    topk_indices = torch.topk(row, num_to_keep).indices\n",
    "    # topk_indices = torch.topk(row, num_to_keep).indices\n",
    "    \n",
    "    # 创建一个与 row 相同大小的零张量\n",
    "    sparse_row = torch.zeros_like(row)\n",
    "    \n",
    "    # 将 topk_indices 对应的值置为 1\n",
    "    sparse_row[topk_indices] = 1\n",
    "    \n",
    "    return sparse_row\n",
    "\n",
    "def generate_label(y, sparsity, use_abs=False):\n",
    "    # 对每一行进行稀疏化\n",
    "    sparse_tensor = torch.stack([sparse_row(row, sparsity, use_abs) for row in y])\n",
    "    return sparse_tensor\n",
    "\n",
    "def test_model(model, val_loader, sparsity=0.1):\n",
    "    model.eval()\n",
    "    # 初始化总的统计变量\n",
    "    total_correct_preds = 0\n",
    "    total_preds = 0\n",
    "    total_labels = 0\n",
    "    total_masks = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in tqdm(enumerate(val_loader)):\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "\n",
    "            preds = generate_label(outputs, sparsity)\n",
    "            truth = generate_label(targets, 0.1, use_abs = True)\n",
    "            # truth = targets\n",
    "            \n",
    "            # 计算当前batch的精度\n",
    "            dif = truth - preds\n",
    "            miss = dif > 0.0 # classifier didn't activated target neuron\n",
    "\n",
    "            total_correct_preds += (truth.sum(dim=1).float() - miss.sum(dim=1).float()).mean().item()\n",
    "            total_preds += (preds.sum(dim=1).float()).mean().item()\n",
    "            total_labels += (truth.sum(dim=1).float()).mean().item()\n",
    "\n",
    "    # print('预测占比:{:.4f}'.format((total_preds/total_masks).item()))\n",
    "    # print('标签占比:{:.4f}'.format((total_labels/total_masks).item()))\n",
    "    print('预测与标签选取的数量比:',(total_preds / total_labels))\n",
    "    print('覆盖率(Recall):',(total_correct_preds / total_labels))\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, writer, epochs=25, layerid=1):\n",
    "    scaler = GradScaler()  # 创建 GradScaler 对象\n",
    "    for epoch in range(epochs):\n",
    "        if epoch % 2 == 0:\n",
    "            print(f'---------after training {epoch} epochs---------')\n",
    "            test_model(model, val_loader, sparsity=0.2)\n",
    "        model.train()\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            targets = generate_label(targets, 0.2, use_abs =True)\n",
    "\n",
    "            # 使用 autocast 来进行自动混合精度处理\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                probs = outputs.sigmoid()\n",
    "                # cross_entropy\n",
    "                loss = criterion(probs, targets)\n",
    "\n",
    "            # 使用 GradScaler 来缩放损失，然后进行反向传播\n",
    "            # 注意：反向传播不包含在 autocast() 块中\n",
    "            scaler.scale(loss).backward()\n",
    "            writer.add_scalar('Loss/Train', loss.item(), epoch * len(train_loader) + batch_idx)\n",
    "            # 调用 scaler.step() 来更新模型权重，并调用 scaler.update() 准备下一步\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "    print(f'---------after training {epochs} epochs---------')\n",
    "    test_model(model, val_loader, sparsity=0.2)\n",
    "    global cnt\n",
    "    torch.save(model.state_dict(), f'./output/sparsity/{layerid}-{cnt}.pt')\n",
    "    cnt += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333186 333186\n",
      "333186 torch.Size([4096]) torch.Size([14336])\n",
      "---------after training 0 epochs---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [00:08,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0013956734124214\n",
      "覆盖率(Recall): 0.20009789296144784\n",
      "---------after training 2 epochs---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [00:07,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0013956734124214\n",
      "覆盖率(Recall): 0.5413863739824867\n",
      "---------after training 4 epochs---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [00:08,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0013956734124214\n",
      "覆盖率(Recall): 0.5608398667241754\n",
      "327227 327227\n",
      "327227 torch.Size([4096]) torch.Size([14336])\n",
      "---------after training 0 epochs---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:07,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0013956734124214\n",
      "覆盖率(Recall): 0.5442518565676384\n",
      "---------after training 2 epochs---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:07,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0013956734124214\n",
      "覆盖率(Recall): 0.5660076314389747\n",
      "---------after training 4 epochs---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:07,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0013956734124214\n",
      "覆盖率(Recall): 0.5710160334627693\n",
      "341549 341549\n",
      "341549 torch.Size([4096]) torch.Size([14336])\n",
      "---------after training 0 epochs---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34it [00:08,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0013956734124214\n",
      "覆盖率(Recall): 0.5556886096016526\n",
      "---------after training 2 epochs---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34it [00:08,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0013956734124214\n",
      "覆盖率(Recall): 0.5726461708435582\n",
      "---------after training 4 epochs---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34it [00:08,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0013956734124214\n",
      "覆盖率(Recall): 0.5752094106414646\n"
     ]
    }
   ],
   "source": [
    "for startid, endid in [(1,4),(4,7),(7,10)]:\n",
    "    layerid = 15\n",
    "    dataset = CustomDataset(layerid, startid=startid, endid=endid)\n",
    "    print(len(dataset), dataset[0][0].shape, dataset[0][1].shape) # torch.Size([512, 4096])\n",
    "    # 划分训练集和验证集\n",
    "    train_size = int(0.9 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "    train_model(model, train_loader, val_loader, criterion, optimizer, writer=writer, epochs=4, layerid=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载训练好的进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333186 333186\n",
      "333186 torch.Size([4096]) torch.Size([14336])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [00:09,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 3.0006978367062107\n",
      "覆盖率(Recall): 0.6927953512494317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "layerid = 15\n",
    "dataset = CustomDataset(layerid, startid=1, endid=4)\n",
    "print(len(dataset), dataset[0][0].shape, dataset[0][1].shape) # torch.Size([512, 4096])\n",
    "# 划分训练集和验证集\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False)\n",
    "# model = SimpleLinearModel(4096,14336,hidden_dim=1024).cuda()\n",
    "model.load_state_dict(torch.load(f'./output/sparsity/15-2.pt'))\n",
    "test_model(model, val_loader, sparsity=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下游任务测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHF_ENDPOINT\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://hf-mirror.com\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlm_eval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuggingface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HFLM\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlm_eval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluator\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_model\u001b[39m(model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlama3-8b\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/workspace/llama2-7b/lm-evaluation-harness/lm_eval/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate, simple_evaluate\n",
      "File \u001b[0;32m~/workspace/llama2-7b/lm-evaluation-harness/lm_eval/evaluator.py:14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlm_eval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlm_eval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlm_eval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlm_eval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcaching\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcache\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m delete_cache\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlm_eval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluator_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     consolidate_results,\n\u001b[1;32m     18\u001b[0m     get_sample_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     run_task_tests,\n\u001b[1;32m     23\u001b[0m )\n",
      "File \u001b[0;32m~/workspace/llama2-7b/lm-evaluation-harness/lm_eval/models/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     anthropic_llms,\n\u001b[1;32m      3\u001b[0m     dummy,\n\u001b[1;32m      4\u001b[0m     gguf,\n\u001b[1;32m      5\u001b[0m     huggingface,\n\u001b[1;32m      6\u001b[0m     mamba_lm,\n\u001b[1;32m      7\u001b[0m     nemo_lm,\n\u001b[1;32m      8\u001b[0m     neuralmagic,\n\u001b[1;32m      9\u001b[0m     neuron_optimum,\n\u001b[1;32m     10\u001b[0m     openai_completions,\n\u001b[1;32m     11\u001b[0m     optimum_lm,\n\u001b[1;32m     12\u001b[0m     textsynth,\n\u001b[1;32m     13\u001b[0m     vllm_causallms,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# TODO: implement __all__\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# enable hf hub transfer if available\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/llama2-7b/lm-evaluation-harness/lm_eval/models/huggingface.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HfApi\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftModel\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m PEFT_VERSION\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "File \u001b[0;32m~/anaconda3/envs/moe/lib/python3.10/site-packages/peft/__init__.py:22\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# flake8: noqa\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# There's no way to ignore \"F401 '...' imported but unused\" warnings in this\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# module, but to preserve other warnings. So, don't check this module at all.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.11.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     AutoPeftModel,\n\u001b[1;32m     24\u001b[0m     AutoPeftModelForCausalLM,\n\u001b[1;32m     25\u001b[0m     AutoPeftModelForSequenceClassification,\n\u001b[1;32m     26\u001b[0m     AutoPeftModelForSeq2SeqLM,\n\u001b[1;32m     27\u001b[0m     AutoPeftModelForTokenClassification,\n\u001b[1;32m     28\u001b[0m     AutoPeftModelForQuestionAnswering,\n\u001b[1;32m     29\u001b[0m     AutoPeftModelForFeatureExtraction,\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmapping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     32\u001b[0m     MODEL_TYPE_TO_PEFT_MODEL_MAPPING,\n\u001b[1;32m     33\u001b[0m     PEFT_TYPE_TO_CONFIG_MAPPING,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     inject_adapter_in_model,\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmixed_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftMixedModel\n",
      "File \u001b[0;32m~/anaconda3/envs/moe/lib/python3.10/site-packages/peft/auto.py:32\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     22\u001b[0m     AutoModel,\n\u001b[1;32m     23\u001b[0m     AutoModelForCausalLM,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     AutoTokenizer,\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftConfig\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmapping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MODEL_TYPE_TO_PEFT_MODEL_MAPPING\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpeft_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     34\u001b[0m     PeftModel,\n\u001b[1;32m     35\u001b[0m     PeftModelForCausalLM,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     PeftModelForTokenClassification,\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TOKENIZER_CONFIG_NAME\n",
      "File \u001b[0;32m~/anaconda3/envs/moe/lib/python3.10/site-packages/peft/mapping.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftConfig\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmixed_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftMixedModel\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpeft_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     24\u001b[0m     PeftModel,\n\u001b[1;32m     25\u001b[0m     PeftModelForCausalLM,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     PeftModelForTokenClassification,\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuners\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     33\u001b[0m     AdaLoraConfig,\n\u001b[1;32m     34\u001b[0m     AdaLoraModel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m     VeraModel,\n\u001b[1;32m     58\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/moe/lib/python3.10/site-packages/peft/mixed_model.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PushToHubMixin\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuners\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmixed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m COMPATIBLE_TUNER_TYPES\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftConfig\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpeft_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftModel\n",
      "File \u001b[0;32m~/anaconda3/envs/moe/lib/python3.10/site-packages/peft/tuners/__init__.py:26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mia3\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IA3Config, IA3Model\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madalora\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AdaLoraConfig, AdaLoraModel\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mboft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BOFTConfig, BOFTModel\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mp_tuning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PromptEncoder, PromptEncoderConfig, PromptEncoderReparameterizationType\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprefix_tuning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PrefixEncoder, PrefixTuningConfig\n",
      "File \u001b[0;32m~/anaconda3/envs/moe/lib/python3.10/site-packages/peft/tuners/boft/__init__.py:16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2023-present the HuggingFace Inc. team.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BOFTConfig\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BOFTLayer\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BOFTModel\n\u001b[1;32m     20\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBOFTConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBOFTLayer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBOFTModel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/moe/lib/python3.10/site-packages/peft/tuners/boft/layer.py:30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Function\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcpp_extension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuners\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuners_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseTunerLayer, check_adapters_to_merge\n\u001b[1;32m     35\u001b[0m _FBD_CUDA \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/moe/lib/python3.10/site-packages/torch/utils/cpp_extension.py:210\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mhip \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     ROCM_VERSION \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mint\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mhip\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[:\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m--> 210\u001b[0m CUDA_HOME \u001b[38;5;241m=\u001b[39m \u001b[43m_find_cuda_home\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_is_compiled() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    211\u001b[0m CUDNN_HOME \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDNN_HOME\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDNN_PATH\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# PyTorch releases have the version pattern major.minor.patch, whereas when\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# PyTorch is built from source, we append the git commit hash, which gives\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# it the below pattern.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/moe/lib/python3.10/site-packages/torch/utils/cpp_extension.py:117\u001b[0m, in \u001b[0;36m_find_cuda_home\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(cuda_home):\n\u001b[1;32m    116\u001b[0m             cuda_home \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cuda_home \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo CUDA runtime is found, using CUDA_HOME=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcuda_home\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    119\u001b[0m           file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cuda_home\n",
      "File \u001b[0;32m~/anaconda3/envs/moe/lib/python3.10/site-packages/torch/cuda/__init__.py:118\u001b[0m, in \u001b[0;36mis_available\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m device_count() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# The default availability inspection never throws and returns 0 if the driver is missing or can't\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# be initialized. This uses the CUDA Runtime API `cudaGetDeviceCount` which in turn initializes the CUDA Driver\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# API via `cuInit`\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_getDeviceCount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test Model\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "from convert_llama import convert_llama_model\n",
    "os.environ[\"HF_ENDPOINT\"]=\"https://hf-mirror.com\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "from lm_eval.models.huggingface import HFLM\n",
    "from lm_eval import evaluator\n",
    "\n",
    "\n",
    "def _load_model(model_name = \"Llama3-8b\"):\n",
    "    print(f\"Loading model {model_name}\")\n",
    "    ### from path.json read paths of model and dataset\n",
    "    with open('path.json', 'r') as file:\n",
    "        paths = json.load(file)\n",
    "        model_path = paths.get(model_name, '')\n",
    "\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        device_map='auto',\n",
    "        use_cache=True,\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    return model, tokenizer\n",
    "\n",
    "def evaluate(task_name, model, tokenizer, num_fewshot, device):\n",
    "    hflm = HFLM(pretrained=model, tokenizer=tokenizer)\n",
    "    results = evaluator.simple_evaluate(\n",
    "    model=hflm,\n",
    "    tasks=[task_name],\n",
    "    num_fewshot=num_fewshot)\n",
    "    print(results['results'])\n",
    "\n",
    "\n",
    "def main(task_name, model_name, sparsity, start_num, end_num, token_sparsity, memory_limit, device, num_fewshot,):\n",
    "    model, tokenizer = _load_model(model_name)\n",
    "    \n",
    "    model = convert_llama_model(model, sparsity, start_num, end_num, token_sparsity,)\n",
    "\n",
    "    evaluate(task_name, model, tokenizer, num_fewshot, device)\n",
    "\n",
    "main(task_name='boolq', model_name=\"Llama3-8b\", sparsity=0.1, start_num=21, end_num=32, token_sparsity=0.1, memory_limit=0.1, device='cuda', num_fewshot=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_factory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
