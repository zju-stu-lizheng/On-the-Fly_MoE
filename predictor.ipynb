{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lz/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model path: /home/lz/Mixtral-8x7B-v0.1 \n",
      "dataset path: /home/lz/c4 \n",
      "save path: /home/lz/On-the-Fly_MoE_Inference/saving/threshold/c4_mixtral_up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:34<00:00,  1.81s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "from transformers import MixtralForCausalLM\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\"\n",
    "### from path.json read paths of model and dataset\n",
    "model_name = \"mixtral\"\n",
    "dataset_name = \"c4\"\n",
    "with open('./path.json', 'r') as file:\n",
    "    paths = json.load(file)\n",
    "    model_path = paths.get(model_name, '')\n",
    "    dataset_path = paths.get(dataset_name, '')\n",
    "    save_path = paths.get('chess_up_threshold','')\n",
    "    print('model path:', model_path, '\\ndataset path:', dataset_path, '\\nsave path:', save_path)\n",
    "\n",
    "def get_model(model_path):\n",
    "    model = MixtralForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        device_map='auto',\n",
    "        use_cache=False,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        # attn_implementation=\"flash_attention_2\"\n",
    "    )\n",
    "    return model\n",
    "model = get_model(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lz/On-the-Fly_MoE_Inference/convert_model.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  thresholds = torch.load(threshold_path)[\"up_proj_states_thresholds_2\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Model Done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MixtralForCausalLM(\n",
       "  (model): MixtralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MixtralDecoderLayer(\n",
       "        (self_attn): MixtralSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MixtralRotaryEmbedding()\n",
       "        )\n",
       "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
       "          (gate): Linear(in_features=4096, out_features=8, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-7): 8 x MixtralLayer(\n",
       "              (act_fn): SiLU()\n",
       "              (gate_proj): Linearlayer()\n",
       "              (up_proj): Linearlayer()\n",
       "              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): MixtralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MixtralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MixtralRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from convert_model import convert_mixtral_model\n",
    "\n",
    "convert_mixtral_model(model, start_num=-1, end_num=32, gamma=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0 expert 0 ratio: 0.2007\n",
      "layer 0 expert 1 ratio: 0.1930\n",
      "layer 0 expert 2 ratio: 0.2119\n",
      "layer 0 expert 3 ratio: 0.1896\n",
      "layer 0 expert 4 ratio: 0.2090\n",
      "layer 0 expert 5 ratio: 0.2066\n",
      "layer 0 expert 6 ratio: 0.2212\n",
      "layer 0 expert 7 ratio: 0.2100\n",
      "layer 1 expert 0 ratio: 0.2008\n",
      "layer 1 expert 1 ratio: 0.2011\n",
      "layer 1 expert 2 ratio: 0.2180\n",
      "layer 1 expert 3 ratio: 0.2191\n",
      "layer 1 expert 4 ratio: 0.2060\n",
      "layer 1 expert 5 ratio: 0.1866\n",
      "layer 1 expert 6 ratio: 0.2105\n",
      "layer 1 expert 7 ratio: 0.2059\n",
      "layer 2 expert 0 ratio: 0.2078\n",
      "layer 2 expert 1 ratio: 0.2081\n",
      "layer 2 expert 2 ratio: 0.2006\n",
      "layer 2 expert 3 ratio: 0.2046\n",
      "layer 2 expert 4 ratio: 0.1965\n",
      "layer 2 expert 5 ratio: 0.2123\n",
      "layer 2 expert 6 ratio: 0.2074\n",
      "layer 2 expert 7 ratio: 0.2051\n",
      "layer 3 expert 0 ratio: 0.2083\n",
      "layer 3 expert 1 ratio: 0.1943\n",
      "layer 3 expert 2 ratio: 0.1948\n",
      "layer 3 expert 3 ratio: 0.2083\n",
      "layer 3 expert 4 ratio: 0.2040\n",
      "layer 3 expert 5 ratio: 0.2016\n",
      "layer 3 expert 6 ratio: 0.2044\n",
      "layer 3 expert 7 ratio: 0.2132\n",
      "layer 4 expert 0 ratio: 0.1996\n",
      "layer 4 expert 1 ratio: 0.2023\n",
      "layer 4 expert 2 ratio: 0.1997\n",
      "layer 4 expert 3 ratio: 0.2057\n",
      "layer 4 expert 4 ratio: 0.2085\n",
      "layer 4 expert 5 ratio: 0.2085\n",
      "layer 4 expert 6 ratio: 0.2175\n",
      "layer 4 expert 7 ratio: 0.2094\n",
      "layer 5 expert 0 ratio: 0.1974\n",
      "layer 5 expert 1 ratio: 0.2003\n",
      "layer 5 expert 2 ratio: 0.2006\n",
      "layer 5 expert 3 ratio: 0.1968\n",
      "layer 5 expert 4 ratio: 0.2042\n",
      "layer 5 expert 5 ratio: 0.2112\n",
      "layer 5 expert 6 ratio: 0.2129\n",
      "layer 5 expert 7 ratio: 0.2090\n",
      "layer 6 expert 0 ratio: 0.2116\n",
      "layer 6 expert 1 ratio: 0.2053\n",
      "layer 6 expert 2 ratio: 0.2109\n",
      "layer 6 expert 3 ratio: 0.1997\n",
      "layer 6 expert 4 ratio: 0.1999\n",
      "layer 6 expert 5 ratio: 0.2100\n",
      "layer 6 expert 6 ratio: 0.1979\n",
      "layer 6 expert 7 ratio: 0.2172\n",
      "layer 7 expert 0 ratio: 0.2144\n",
      "layer 7 expert 1 ratio: 0.2140\n",
      "layer 7 expert 2 ratio: 0.1960\n",
      "layer 7 expert 3 ratio: 0.2080\n",
      "layer 7 expert 4 ratio: 0.2076\n",
      "layer 7 expert 5 ratio: 0.2187\n",
      "layer 7 expert 6 ratio: 0.2075\n",
      "layer 7 expert 7 ratio: 0.2079\n",
      "layer 8 expert 0 ratio: 0.2239\n",
      "layer 8 expert 1 ratio: 0.2086\n",
      "layer 8 expert 2 ratio: 0.2220\n",
      "layer 8 expert 3 ratio: 0.2102\n",
      "layer 8 expert 4 ratio: 0.2432\n",
      "layer 8 expert 5 ratio: 0.2204\n",
      "layer 8 expert 6 ratio: 0.2278\n",
      "layer 8 expert 7 ratio: 0.2334\n",
      "layer 9 expert 0 ratio: 0.2365\n",
      "layer 9 expert 1 ratio: 0.2320\n",
      "layer 9 expert 2 ratio: 0.2156\n",
      "layer 9 expert 3 ratio: 0.2327\n",
      "layer 9 expert 4 ratio: 0.2521\n",
      "layer 9 expert 5 ratio: 0.2430\n",
      "layer 9 expert 6 ratio: 0.2365\n",
      "layer 9 expert 7 ratio: 0.2291\n",
      "layer 10 expert 0 ratio: 0.2444\n",
      "layer 10 expert 1 ratio: 0.2359\n",
      "layer 10 expert 2 ratio: 0.2469\n",
      "layer 10 expert 3 ratio: 0.2403\n",
      "layer 10 expert 4 ratio: 0.2378\n",
      "layer 10 expert 5 ratio: 0.2344\n",
      "layer 10 expert 6 ratio: 0.2361\n",
      "layer 10 expert 7 ratio: 0.2366\n",
      "layer 11 expert 0 ratio: 0.2464\n",
      "layer 11 expert 1 ratio: 0.2500\n",
      "layer 11 expert 2 ratio: 0.2420\n",
      "layer 11 expert 3 ratio: 0.2167\n",
      "layer 11 expert 4 ratio: 0.2492\n",
      "layer 11 expert 5 ratio: 0.2275\n",
      "layer 11 expert 6 ratio: 0.2402\n",
      "layer 11 expert 7 ratio: 0.2309\n",
      "layer 12 expert 0 ratio: 0.2366\n",
      "layer 12 expert 1 ratio: 0.2515\n",
      "layer 12 expert 2 ratio: 0.2360\n",
      "layer 12 expert 3 ratio: 0.2376\n",
      "layer 12 expert 4 ratio: 0.2313\n",
      "layer 12 expert 5 ratio: 0.2337\n",
      "layer 12 expert 6 ratio: 0.2383\n",
      "layer 12 expert 7 ratio: 0.2473\n",
      "layer 13 expert 0 ratio: 0.2318\n",
      "layer 13 expert 1 ratio: 0.2223\n",
      "layer 13 expert 2 ratio: 0.2434\n",
      "layer 13 expert 3 ratio: 0.2270\n",
      "layer 13 expert 4 ratio: 0.2311\n",
      "layer 13 expert 5 ratio: 0.2298\n",
      "layer 13 expert 6 ratio: 0.2254\n",
      "layer 13 expert 7 ratio: 0.2318\n",
      "layer 14 expert 0 ratio: 0.2316\n",
      "layer 14 expert 1 ratio: 0.2346\n",
      "layer 14 expert 2 ratio: 0.2064\n",
      "layer 14 expert 3 ratio: 0.2283\n",
      "layer 14 expert 4 ratio: 0.2463\n",
      "layer 14 expert 5 ratio: 0.2435\n",
      "layer 14 expert 6 ratio: 0.2251\n",
      "layer 14 expert 7 ratio: 0.2374\n",
      "layer 15 expert 0 ratio: 0.2371\n",
      "layer 15 expert 1 ratio: 0.2240\n",
      "layer 15 expert 2 ratio: 0.2351\n",
      "layer 15 expert 3 ratio: 0.2179\n",
      "layer 15 expert 4 ratio: 0.2297\n",
      "layer 15 expert 5 ratio: 0.2355\n",
      "layer 15 expert 6 ratio: 0.2292\n",
      "layer 15 expert 7 ratio: 0.2278\n",
      "layer 16 expert 0 ratio: 0.2213\n",
      "layer 16 expert 1 ratio: 0.2280\n",
      "layer 16 expert 2 ratio: 0.2347\n",
      "layer 16 expert 3 ratio: 0.2312\n",
      "layer 16 expert 4 ratio: 0.2315\n",
      "layer 16 expert 5 ratio: 0.2421\n",
      "layer 16 expert 6 ratio: 0.2460\n",
      "layer 16 expert 7 ratio: 0.2298\n",
      "layer 17 expert 0 ratio: 0.2241\n",
      "layer 17 expert 1 ratio: 0.2309\n",
      "layer 17 expert 2 ratio: 0.2399\n",
      "layer 17 expert 3 ratio: 0.2198\n",
      "layer 17 expert 4 ratio: 0.2417\n",
      "layer 17 expert 5 ratio: 0.2421\n",
      "layer 17 expert 6 ratio: 0.2217\n",
      "layer 17 expert 7 ratio: 0.2328\n",
      "layer 18 expert 0 ratio: 0.2369\n",
      "layer 18 expert 1 ratio: 0.2439\n",
      "layer 18 expert 2 ratio: 0.2355\n",
      "layer 18 expert 3 ratio: 0.2415\n",
      "layer 18 expert 4 ratio: 0.2343\n",
      "layer 18 expert 5 ratio: 0.2362\n",
      "layer 18 expert 6 ratio: 0.2367\n",
      "layer 18 expert 7 ratio: 0.2349\n",
      "layer 19 expert 0 ratio: 0.2488\n",
      "layer 19 expert 1 ratio: 0.2255\n",
      "layer 19 expert 2 ratio: 0.2343\n",
      "layer 19 expert 3 ratio: 0.2299\n",
      "layer 19 expert 4 ratio: 0.2335\n",
      "layer 19 expert 5 ratio: 0.2382\n",
      "layer 19 expert 6 ratio: 0.2411\n",
      "layer 19 expert 7 ratio: 0.2451\n",
      "layer 20 expert 0 ratio: 0.2453\n",
      "layer 20 expert 1 ratio: 0.2415\n",
      "layer 20 expert 2 ratio: 0.2394\n",
      "layer 20 expert 3 ratio: 0.2444\n",
      "layer 20 expert 4 ratio: 0.2628\n",
      "layer 20 expert 5 ratio: 0.2551\n",
      "layer 20 expert 6 ratio: 0.2444\n",
      "layer 20 expert 7 ratio: 0.2474\n",
      "layer 21 expert 0 ratio: 0.2391\n",
      "layer 21 expert 1 ratio: 0.2451\n",
      "layer 21 expert 2 ratio: 0.2383\n",
      "layer 21 expert 3 ratio: 0.2578\n",
      "layer 21 expert 4 ratio: 0.2350\n",
      "layer 21 expert 5 ratio: 0.2282\n",
      "layer 21 expert 6 ratio: 0.2365\n",
      "layer 21 expert 7 ratio: 0.2502\n",
      "layer 22 expert 0 ratio: 0.2411\n",
      "layer 22 expert 1 ratio: 0.2376\n",
      "layer 22 expert 2 ratio: 0.2213\n",
      "layer 22 expert 3 ratio: 0.2399\n",
      "layer 22 expert 4 ratio: 0.2539\n",
      "layer 22 expert 5 ratio: 0.2509\n",
      "layer 22 expert 6 ratio: 0.2130\n",
      "layer 22 expert 7 ratio: 0.2448\n",
      "layer 23 expert 0 ratio: 0.2521\n",
      "layer 23 expert 1 ratio: 0.2415\n",
      "layer 23 expert 2 ratio: 0.2395\n",
      "layer 23 expert 3 ratio: 0.2372\n",
      "layer 23 expert 4 ratio: 0.2391\n",
      "layer 23 expert 5 ratio: 0.2298\n",
      "layer 23 expert 6 ratio: 0.2323\n",
      "layer 23 expert 7 ratio: 0.2324\n",
      "layer 24 expert 0 ratio: 0.2317\n",
      "layer 24 expert 1 ratio: 0.2436\n",
      "layer 24 expert 2 ratio: 0.2384\n",
      "layer 24 expert 3 ratio: 0.2236\n",
      "layer 24 expert 4 ratio: 0.2399\n",
      "layer 24 expert 5 ratio: 0.2370\n",
      "layer 24 expert 6 ratio: 0.2344\n",
      "layer 24 expert 7 ratio: 0.2408\n",
      "layer 25 expert 0 ratio: 0.2373\n",
      "layer 25 expert 1 ratio: 0.2471\n",
      "layer 25 expert 2 ratio: 0.2247\n",
      "layer 25 expert 3 ratio: 0.2323\n",
      "layer 25 expert 4 ratio: 0.2389\n",
      "layer 25 expert 5 ratio: 0.2332\n",
      "layer 25 expert 6 ratio: 0.2373\n",
      "layer 25 expert 7 ratio: 0.2169\n",
      "layer 26 expert 0 ratio: 0.2231\n",
      "layer 26 expert 1 ratio: 0.2257\n",
      "layer 26 expert 2 ratio: 0.2460\n",
      "layer 26 expert 3 ratio: 0.2491\n",
      "layer 26 expert 4 ratio: 0.2290\n",
      "layer 26 expert 5 ratio: 0.2301\n",
      "layer 26 expert 6 ratio: 0.2164\n",
      "layer 26 expert 7 ratio: 0.2225\n",
      "layer 27 expert 0 ratio: 0.2326\n",
      "layer 27 expert 1 ratio: 0.2211\n",
      "layer 27 expert 2 ratio: 0.2335\n",
      "layer 27 expert 3 ratio: 0.2356\n",
      "layer 27 expert 4 ratio: 0.2201\n",
      "layer 27 expert 5 ratio: 0.2389\n",
      "layer 27 expert 6 ratio: 0.2270\n",
      "layer 27 expert 7 ratio: 0.2309\n",
      "layer 28 expert 0 ratio: 0.2216\n",
      "layer 28 expert 1 ratio: 0.2189\n",
      "layer 28 expert 2 ratio: 0.2252\n",
      "layer 28 expert 3 ratio: 0.2363\n",
      "layer 28 expert 4 ratio: 0.2262\n",
      "layer 28 expert 5 ratio: 0.2296\n",
      "layer 28 expert 6 ratio: 0.2188\n",
      "layer 28 expert 7 ratio: 0.2340\n",
      "layer 29 expert 0 ratio: 0.2273\n",
      "layer 29 expert 1 ratio: 0.2189\n",
      "layer 29 expert 2 ratio: 0.2263\n",
      "layer 29 expert 3 ratio: 0.2371\n",
      "layer 29 expert 4 ratio: 0.2291\n",
      "layer 29 expert 5 ratio: 0.2263\n",
      "layer 29 expert 6 ratio: 0.2217\n",
      "layer 29 expert 7 ratio: 0.2116\n",
      "layer 30 expert 0 ratio: 0.2162\n",
      "layer 30 expert 1 ratio: 0.2367\n",
      "layer 30 expert 2 ratio: 0.2218\n",
      "layer 30 expert 3 ratio: 0.2313\n",
      "layer 30 expert 4 ratio: 0.2343\n",
      "layer 30 expert 5 ratio: 0.2106\n",
      "layer 30 expert 6 ratio: 0.2159\n",
      "layer 30 expert 7 ratio: 0.2278\n",
      "layer 31 expert 0 ratio: 0.2068\n",
      "layer 31 expert 1 ratio: 0.2363\n",
      "layer 31 expert 2 ratio: 0.2043\n",
      "layer 31 expert 3 ratio: 0.2371\n",
      "layer 31 expert 4 ratio: 0.1973\n",
      "layer 31 expert 5 ratio: 0.2620\n",
      "layer 31 expert 6 ratio: 0.2184\n",
      "layer 31 expert 7 ratio: 0.2343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, GenerationConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "SAMPLE_NUM = 10\n",
    "MAX_LENGTH = 512\n",
    "OUTPUT_LENTGH = 1\n",
    "c4 = load_dataset(dataset_path)\n",
    "# for c4_demo in c4['validation']['text'][:SAMPLE_NUM]:\n",
    "for c4_demo in tqdm(c4['validation']['text'][:SAMPLE_NUM]):\n",
    "    input_demo = tokenizer(c4_demo, padding=False, truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "    if input_demo.input_ids.shape[1] < MAX_LENGTH:\n",
    "        continue\n",
    "    # print(tokenizer.batch_decode(input_demo.input_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])\n",
    "    generate_ids = model.generate(input_demo.input_ids.to('cuda:0'), max_length=MAX_LENGTH+OUTPUT_LENTGH, generation_config=GenerationConfig(do_sample=False), pad_token_id=tokenizer.eos_token_id)\n",
    "    # tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    # print(tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])\n",
    "for layerid in range(0,32):\n",
    "    for expertid in range(0, 8):\n",
    "        model.model.layers[layerid].block_sparse_moe.experts[expertid].print_ratio()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PD结合的思路"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分别替换gate/up层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先输入一句话，按照70%稀疏去记录prefill阶段激活的神经元，最后统计这个输入prompt对应的最高激活次数的70%的神经元。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "for layerid in range(32):\n",
    "    average_score = torch.load(f'/mnt/newdata/lz/sparsity/c4_llama/new_channelgate/{layerid}-average.pth', map_location='cpu')\n",
    "    print(\"loading done\")\n",
    "    print(average_score.max(), average_score.min(), average_score.mean(), average_score.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "712870fb8d804005a26484cdb704544b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "from transformers import GenerationConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "### from path.json read paths of model and dataset\n",
    "model_name = \"Llama3-8b\"\n",
    "dataset_name = \"c4\"\n",
    "with open('path.json', 'r') as file:\n",
    "    paths = json.load(file)\n",
    "    model_path = paths.get(model_name, '')\n",
    "    dataset_path = paths.get(dataset_name, '')\n",
    "\n",
    "c4 = load_dataset(dataset_path)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map='auto',\n",
    "    use_cache=True,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.2, gamma: 0.3, beta: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convert Llama Models: 390it [00:00, 6771.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Model Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicts max: tensor(0.2568, device='cuda:0') the max of average_gate: tensor(0.2190, device='cuda:0')\n",
      "threshold: 0.003125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicts max: tensor(0.2568, device='cuda:0') the max of average_gate: tensor(0.2190, device='cuda:0')\n",
      "threshold: 0.003125\n",
      "layer 0 ratio: 0.2676\n",
      "layer 1 ratio: 0.3256\n",
      "layer 2 ratio: 0.2508\n",
      "layer 3 ratio: 0.2575\n",
      "layer 4 ratio: 0.2455\n",
      "layer 5 ratio: 0.2389\n",
      "layer 6 ratio: 0.2434\n",
      "layer 7 ratio: 0.2525\n",
      "layer 8 ratio: 0.2448\n",
      "layer 9 ratio: 0.2560\n",
      "layer 10 ratio: 0.2593\n",
      "layer 11 ratio: 0.2831\n",
      "layer 12 ratio: 0.2926\n",
      "layer 13 ratio: 0.2915\n",
      "layer 14 ratio: 0.2839\n",
      "layer 15 ratio: 0.2847\n",
      "layer 16 ratio: 0.2708\n",
      "layer 17 ratio: 0.2871\n",
      "layer 18 ratio: 0.2681\n",
      "layer 19 ratio: 0.2740\n",
      "layer 20 ratio: 0.2742\n",
      "layer 21 ratio: 0.2785\n",
      "layer 22 ratio: 0.2651\n",
      "layer 23 ratio: 0.2485\n",
      "layer 24 ratio: 0.2464\n",
      "layer 25 ratio: 0.2418\n",
      "layer 26 ratio: 0.2356\n",
      "layer 27 ratio: 0.2379\n",
      "layer 28 ratio: 0.2515\n",
      "layer 29 ratio: 0.2783\n",
      "layer 30 ratio: 0.3277\n",
      "layer 31 ratio: 0.4161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import convert_llama\n",
    "# for gamma in [0,0.2]:\n",
    "#     for beta in [0,0.2]:\n",
    "SAMPLE_NUM = 10\n",
    "MAX_LENGTH = 512\n",
    "OUTPUT_LENTGH = 1\n",
    "alpha = 0.2\n",
    "# for gamma,beta in [(0,0.3),(0.3,0),(0.2,0.2)]:\n",
    "for gamma, beta in [(0.3,0.1)]:\n",
    "# for gamma, beta in [(0,0.1)]:\n",
    "# for gamma, beta in [(0.3221,0)]:\n",
    "# for gamma,beta in [(0.1,0.2),(0.2,0.1),(0.2,0.2)]:\n",
    "    print(f\"alpha: {alpha}, gamma: {gamma}, beta: {beta}\")\n",
    "    convert_llama.convert_llama_model(model, sparsity=0.1, start_num=-1, end_num=32, alpha=alpha, beta=beta, gamma=gamma, use_core=False)\n",
    "    # for layerid in range(22,32):\n",
    "    #     model.model.layers[layerid].mlp.clear_list()\n",
    "\n",
    "    # for c4_demo in c4['validation']['text'][:SAMPLE_NUM]:\n",
    "    for c4_demo in tqdm(c4['validation']['text'][:SAMPLE_NUM]):\n",
    "        input_demo = tokenizer(c4_demo, padding=False, truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "        if input_demo.input_ids.shape[1] < MAX_LENGTH:\n",
    "            continue\n",
    "        # print(tokenizer.batch_decode(input_demo.input_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])\n",
    "        generate_ids = model.generate(input_demo.input_ids.to('cuda:0'), max_length=MAX_LENGTH+OUTPUT_LENTGH, generation_config=GenerationConfig(do_sample=False), pad_token_id=tokenizer.eos_token_id)\n",
    "        # tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "        # print(tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])\n",
    "    for layerid in range(0,32):\n",
    "        model.model.layers[layerid].mlp.print_ratio()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convert Llama Models: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convert Llama Models: 462it [00:02, 208.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Model Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [03:20<00:00,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 2.3604316777980006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import convert_llama\n",
    "import numpy as np\n",
    "import math\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 1. 定义超参数\n",
    "SAMPLE_NUM = 200\n",
    "MAX_LENGTH = 300\n",
    "OUTPUT_LENTGH = 30\n",
    "alpha = 0.2\n",
    "beta = 0\n",
    "gamma = 1.0\n",
    "\n",
    "eos_token_id = tokenizer.convert_tokens_to_ids('.')\n",
    "\n",
    "convert_llama.convert_llama_model(model, sparsity=0.1, start_num=20, end_num=32, alpha=alpha, beta=beta, gamma=gamma)\n",
    "ppl_list = []\n",
    "# 2. 准备输入文本\n",
    "for c4_demo in tqdm(c4['validation']['text'][:SAMPLE_NUM]):\n",
    "    input_demo = tokenizer(c4_demo, padding=False, truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "    if input_demo.input_ids.shape[1] < MAX_LENGTH:\n",
    "        continue\n",
    "    # input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_demo.input_ids.cuda(), use_cache=True)\n",
    "        past_key_values = outputs.past_key_values\n",
    "    \n",
    "    generated = input_demo.input_ids\n",
    "    log_prob = []\n",
    "    # 3. 生成文本[不应该用model.generate, 手写一个方便测试困惑度]\n",
    "    for _ in range(OUTPUT_LENTGH):\n",
    "        with torch.no_grad():\n",
    "            input_ids = generated[:, -1:].cuda()\n",
    "            outputs = model(input_ids=input_ids, past_key_values=past_key_values, use_cache=True)\n",
    "            logits = outputs.logits\n",
    "            past_key_values = outputs.past_key_values\n",
    "\n",
    "            # print(logits.shape) # [1, 1, 128256]\n",
    "            # print(input_ids.shape) # [1, 1]\n",
    "            # labels = input_ids[:, :]\n",
    "            probs = torch.softmax(logits[:,:], dim=-1)\n",
    "            probs = torch.max(probs.squeeze(0))\n",
    "            # print(probs, labels)\n",
    "            # print(probs)\n",
    "            log_prob.append(probs.log2().cpu().numpy())\n",
    "    \n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        next_token_id = torch.argmax(next_token_logits, dim=-1)\n",
    "    \n",
    "        generated = torch.cat((generated.cuda(), next_token_id.unsqueeze(-1)), dim=1)\n",
    "    \n",
    "        next_token_text = tokenizer.decode(next_token_id)\n",
    "        # print(next_token_text, end='', flush=True)\n",
    "    \n",
    "        # if next_token_id.item() == eos_token_id:\n",
    "        #     break\n",
    "        # if '.' in next_token_text:\n",
    "        #     break\n",
    "    ppl_item = - np.sum(log_prob) / len(log_prob)\n",
    "    ppl_list.append(ppl_item)\n",
    "\n",
    "\n",
    "ce = np.sum(ppl_list) / len(ppl_list)\n",
    "ppl = 2 ** ce\n",
    "print(f\"Perplexity: {ppl.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[prefill] in gate layer: 15\n",
      "[prefill] in up layer: 15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The woman who died after falling from a bridge over the A21 has been identified as a Sevenoaks mum.\\nMarta Kendle, 37, fell from the Gracious Lane bridge on the morning of February 19.\\nPolice were called to the carriageway around 6.10am and the road was promptly closed in both directions.\\nDespite paramedics best efforts, Marta, who was originally from Poland, was pronounced dead at the scene.\\nKent and Medway Coroners office have confirmed an inquest into her death will open on Wednesday (February 27).\\nTributes to the mum were left at the scene and on social media.\\nFriend, Jodi Cahill posted on Facebook: \"I will certainly remember you. I am sorry we did not see how lost and alone you felt.\\n\"Be at peace dear Marta.\"\\nA floral tribute left at the scene said goodbye to the \"beautiful and kind soul\".\\nIt read: \"To a beautiful and kind soul. You will be missed. Rest in peace.\"\\nA spokesman for Kent Police said: \"Officers were called to the A21 at Gracious Lane,'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 使用greedy decode\n",
    "generate_ids = model.generate(input_demo.input_ids.to('cuda:0'), max_length=230, generation_config=GenerationConfig(do_sample=False), pad_token_id=tokenizer.eos_token_id)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in decode, gate layer 15\n",
      "Overlap count: 1243.8621, Overlap ratio: 0.8680\n",
      "in decode, up layer 15\n",
      "Overlap count: 1080.8966, Overlap ratio: 0.7543\n"
     ]
    }
   ],
   "source": [
    "model.model.layers[15].mlp.gate_proj.coreinfer_recall()\n",
    "model.model.layers[15].mlp.up_proj.coreinfer_recall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wanda剪枝的思路"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建wanda数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datasets import load_dataset\n",
    "\n",
    "def get_c4(nsamples, seed, seqlen, tokenizer):\n",
    "    # Load train and validation datasets\n",
    "    traindata = load_dataset('/home/lz/workspace/llama2-7b/HQQ/notebooks/draft', split='validation')\n",
    "\n",
    "    # Generate samples from training set\n",
    "    random.seed(seed)\n",
    "    trainloader = []\n",
    "    for _ in range(nsamples):\n",
    "        while True:\n",
    "            i = random.randint(0, len(traindata) - 1)\n",
    "            # print(traindata[i])\n",
    "            trainenc = tokenizer(traindata[i]['text'], return_tensors='pt')\n",
    "            if trainenc.input_ids.shape[1] > seqlen:\n",
    "                break\n",
    "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
    "        j = i + seqlen\n",
    "        inp = trainenc.input_ids[:, i:j]\n",
    "        tar = inp.clone()\n",
    "        tar[:, :-1] = -100\n",
    "        trainloader.append((inp, tar))\n",
    "\n",
    "    return trainloader\n",
    "from transformers import AutoTokenizer\n",
    "nsamples = 128\n",
    "seed = 0\n",
    "seqlen = 2048\n",
    "model_name = \"/mnt/storage/zyx/Meta-Llama-3-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "train_loader = get_c4(nsamples, seed, seqlen, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def prepare_calibration_input(model, dataloader, device):\n",
    "    use_cache = model.config.use_cache\n",
    "    model.config.use_cache = False\n",
    "    layers = model.model.layers\n",
    "\n",
    "    # dev = model.hf_device_map[\"model.embed_tokens\"]\n",
    "    if \"model.embed_tokens\" in model.hf_device_map:\n",
    "        device = model.hf_device_map[\"model.embed_tokens\"]\n",
    "\n",
    "    dtype = next(iter(model.parameters())).dtype\n",
    "    inps = torch.zeros((128, model.seqlen, model.config.hidden_size), dtype=dtype, device=device)\n",
    "    inps.requires_grad = False\n",
    "    cache = {'i': 0, 'attention_mask': None, \"position_ids\": None}\n",
    "\n",
    "    class Catcher(nn.Module):\n",
    "        def __init__(self, module):\n",
    "            super().__init__()\n",
    "            self.module = module\n",
    "        def forward(self, inp, **kwargs):\n",
    "            inps[cache['i']] = inp\n",
    "            cache['i'] += 1\n",
    "            cache['attention_mask'] = kwargs['attention_mask']\n",
    "            cache['position_ids'] = kwargs['position_ids']\n",
    "            raise ValueError\n",
    "    layers[0] = Catcher(layers[0])\n",
    "    for batch in dataloader:\n",
    "        try:\n",
    "            model(batch[0].to(device))\n",
    "        except ValueError:\n",
    "            pass \n",
    "    layers[0] = layers[0].module\n",
    "\n",
    "    outs = torch.zeros_like(inps)\n",
    "    attention_mask = cache['attention_mask']\n",
    "    position_ids = cache['position_ids']\n",
    "    model.config.use_cache = use_cache\n",
    "\n",
    "    return inps, outs, attention_mask, position_ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM\n",
    "device = 'cuda:1'\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device,\n",
    "    use_cache=False,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.seqlen = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedGPT:\n",
    "    \"\"\"\n",
    "    This class wraps a GPT layer for specific operations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer, layer_id=0, layer_name=\"none\"):\n",
    "        self.layer = layer\n",
    "        self.dev = self.layer.weight.device\n",
    "        self.rows = layer.weight.data.shape[0]\n",
    "        self.columns = layer.weight.data.shape[1]\n",
    "\n",
    "        self.scaler_row = torch.zeros((self.columns), device=self.dev)\n",
    "        self.nsamples = 0\n",
    "\n",
    "        self.layer_id = layer_id \n",
    "        self.layer_name = layer_name\n",
    "\n",
    "    def add_batch(self, inp, out):\n",
    "        if len(inp.shape) == 2:\n",
    "            inp = inp.unsqueeze(0)\n",
    "        tmp = inp.shape[0]\n",
    "        if isinstance(self.layer, nn.Linear):\n",
    "            if len(inp.shape) == 3:\n",
    "                inp = inp.reshape((-1, inp.shape[-1]))\n",
    "            inp = inp.t()\n",
    "\n",
    "        self.scaler_row *= self.nsamples / (self.nsamples+tmp)\n",
    "        self.nsamples += tmp\n",
    "\n",
    "        inp = inp.type(torch.float32)\n",
    "        self.scaler_row += torch.norm(inp, p=2, dim=1) ** 2  / self.nsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    inps, outs, attention_mask, position_ids = prepare_calibration_input(model, train_loader, device)\n",
    "layers = model.model.layers\n",
    "\n",
    "gate_scaler_row = []\n",
    "up_scaler_row = []\n",
    "\n",
    "for layer in layers:\n",
    "    ### 改成字典类型的集合 存 layer.mlp.gate_proj 和 layer.mlp.up_proj\n",
    "    subset = {\n",
    "        'gate_proj': layer.mlp.gate_proj,\n",
    "        'up_proj': layer.mlp.up_proj\n",
    "    }\n",
    "    wrapped_layers = {}\n",
    "    for name in subset:\n",
    "        wrapped_layers[name] = WrappedGPT(subset[name])\n",
    "    ### 定义hook函数\n",
    "    def add_batch(name):\n",
    "        def tmp(_, inp, out):\n",
    "            wrapped_layers[name].add_batch(inp[0].data, out.data)\n",
    "        return tmp\n",
    "\n",
    "    handles = []\n",
    "    for name in wrapped_layers:\n",
    "        handles.append(subset[name].register_forward_hook(add_batch(name)))\n",
    "    # handles.append(subset.register_forward_hook(add_batch('gate_proj')))\n",
    "    for j in range(nsamples):\n",
    "        with torch.no_grad():\n",
    "            outs[j] = layer(inps[j].unsqueeze(0),attention_mask=attention_mask, position_ids=position_ids)[0]\n",
    "    for h in handles:\n",
    "        h.remove()\n",
    "    inps, outs = outs, inps\n",
    "    print(wrapped_layers['gate_proj'].scaler_row)\n",
    "    gate_scaler_row.append(wrapped_layers['gate_proj'].scaler_row)\n",
    "    up_scaler_row.append(wrapped_layers['up_proj'].scaler_row)\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_layer(specific_scaler_row, weight, layerid, sparsity_ratio=0.1):\n",
    "    W_metric = weight * torch.sqrt(specific_scaler_row[layerid].reshape((1,-1)))\n",
    "    W_mask = (torch.zeros_like(W_metric) == 1)  ## initialize a mask to be all False\n",
    "\n",
    "    sort_res = torch.sort(W_metric, dim=-1, stable=True)\n",
    "    # unstructured pruning\n",
    "    indices = sort_res[1][:,:int(W_metric.shape[1]*sparsity_ratio)]\n",
    "    W_mask.scatter_(1, indices, True)\n",
    "\n",
    "    return W_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layerid = 15\n",
    "with open('path.json', 'r') as file:\n",
    "    paths = json.load(file)\n",
    "    wanda_path = paths.get(\"wanda_path\", '')\n",
    "\n",
    "for sparsity_ratio in [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    #### gate\n",
    "    this_data = model.model.layers[layerid].mlp.gate_proj.weight.data.clone()\n",
    "    this_mask = prune_layer(gate_scaler_row[layerid], this_data, layerid, sparsity_ratio)\n",
    "    this_data[this_mask] = 0\n",
    "    torch.save(this_data, f'{wanda_path}/gate_proj_{sparsity_ratio}.pt')\n",
    "    #### up\n",
    "    this_data_up = model.model.layers[layerid].mlp.up_proj.weight.data.clone()\n",
    "    this_mask_up = prune_layer(up_scaler_row[layerid], this_data_up, layerid, sparsity_ratio)\n",
    "    this_data_up[this_mask_up] = 0\n",
    "    torch.save(this_data_up, f'{wanda_path}/up_proj_{sparsity_ratio}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试剪枝后的准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, gate_proj_shape, up_proj_shape):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.gate_proj = nn.Parameter(torch.zeros(gate_proj_shape, dtype=torch.float32))\n",
    "        self.up_proj = nn.Parameter(torch.zeros(up_proj_shape, dtype=torch.float32))\n",
    "        self.activation = nn.SiLU()\n",
    "        self.gate_mask = None\n",
    "        self.up_mask = None\n",
    "\n",
    "    def load(self, gate_proj_path, up_proj_path):\n",
    "        self.gate_proj.data = torch.load(gate_proj_path).to(torch.float32)\n",
    "        self.up_proj.data = torch.load(up_proj_path).to(torch.float32)\n",
    "        self.gate_mask = self.gate_proj != 0\n",
    "        self.up_mask = self.up_proj != 0\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        gate_outputs = self.activation(inputs @ (self.gate_proj * self.gate_mask).T)\n",
    "        up_outputs = inputs @ (self.up_proj * self.up_mask).T\n",
    "        outputs = gate_outputs * up_outputs\n",
    "        return outputs\n",
    "\n",
    "# 初始化自定义模型实例\n",
    "model = CustomModel(gate_proj_shape=(14336, 4096), up_proj_shape=(14336, 4096))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333186 333186\n",
      "333186 torch.Size([4096]) torch.Size([14336])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [00:10,  3.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0006978367062107\n",
      "覆盖率(Recall): 0.6892014713033654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def sparse_row(row, keep_ratio=0.1, use_abs = False):\n",
    "    # 计算需要保留的参数数量\n",
    "    num_to_keep = int(keep_ratio * row.numel())\n",
    "    \n",
    "    # 找到绝对值最大的 num_to_keep 个参数的索引\n",
    "    if use_abs:\n",
    "        row = torch.abs(row)\n",
    "    topk_indices = torch.topk(row, num_to_keep).indices\n",
    "    # topk_indices = torch.topk(row, num_to_keep).indices\n",
    "    \n",
    "    # 创建一个与 row 相同大小的零张量\n",
    "    sparse_row = torch.zeros_like(row)\n",
    "    \n",
    "    # 将 topk_indices 对应的值置为 1\n",
    "    sparse_row[topk_indices] = 1\n",
    "    \n",
    "    return sparse_row\n",
    "\n",
    "def generate_label(y, sparsity, use_abs=False):\n",
    "    # 对每一行进行稀疏化\n",
    "    sparse_tensor = torch.stack([sparse_row(row, sparsity, use_abs) for row in y])\n",
    "    return sparse_tensor\n",
    "\n",
    "def test_model(model, val_loader, sparsity=0.1):\n",
    "    # 初始化总的统计变量\n",
    "    total_correct_preds = 0\n",
    "    total_preds = 0\n",
    "    total_labels = 0\n",
    "    total_masks = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in tqdm(enumerate(val_loader)):\n",
    "            #### model.forward\n",
    "            outputs = model(inputs.to(torch.float32))\n",
    "\n",
    "            preds = generate_label(outputs, sparsity, use_abs = True)\n",
    "            truth = generate_label(targets, 0.1, use_abs = True)\n",
    "            # truth = targets\n",
    "            \n",
    "            # 计算当前batch的精度\n",
    "            dif = truth - preds\n",
    "            miss = dif > 0.0 # classifier didn't activated target neuron\n",
    "\n",
    "            total_correct_preds += (truth.sum(dim=1).float() - miss.sum(dim=1).float()).mean().item()\n",
    "            total_preds += (preds.sum(dim=1).float()).mean().item()\n",
    "            total_labels += (truth.sum(dim=1).float()).mean().item()\n",
    "\n",
    "    print('预测与标签选取的数量比:',(total_preds / total_labels))\n",
    "    print('覆盖率(Recall):',(total_correct_preds / total_labels))\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, writer=None, epochs=25, layerid=1):\n",
    "    scaler = GradScaler()  # 创建 GradScaler 对象\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0  # 初始化每个epoch的总loss\n",
    "        if epoch % 1 == 0:\n",
    "            print(f'---------after training {epoch} epochs---------')\n",
    "            test_model(model, val_loader, sparsity=0.2)\n",
    "        model.train()\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            targets = generate_label(targets, 0.2, use_abs =True)\n",
    "\n",
    "            # 使用 autocast 来进行自动混合精度处理\n",
    "            with autocast():\n",
    "                outputs = model(inputs.to(torch.float32))\n",
    "                probs = outputs.sigmoid()\n",
    "                # cross_entropy\n",
    "                loss = criterion(probs, targets)\n",
    "\n",
    "            # 使用 GradScaler 来缩放损失，然后进行反向传播\n",
    "            # 注意：反向传播不包含在 autocast() 块中\n",
    "            scaler.scale(loss).backward()\n",
    "            # loss.backward()\n",
    "            optimizer.step()\n",
    "            # writer.add_scalar('Loss/Train', loss.item(), epoch * len(train_loader) + batch_idx)\n",
    "            # 调用 scaler.step() 来更新模型权重，并调用 scaler.update() 准备下一步\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            epoch_loss += loss.item()  # 累加每个batch的loss\n",
    "\n",
    "        print(f'Epoch {epoch} Loss: {epoch_loss / len(train_loader)}')  # 打印每个epoch的平均loss\n",
    "\n",
    "    print(f'---------after training {epochs} epochs---------')\n",
    "    test_model(model, val_loader, sparsity=0.2)\n",
    "    #torch.save(model.state_dict(), './predictor_wight/predictor01_twodata.pt')\n",
    "    torch.save(model.state_dict(), f'/home/lz/workspace/llama2-7b/moe-offloading/notebooks/output/sparsity/{layerid}.pt')\n",
    "\n",
    "layerid = 15\n",
    "dataset = CustomDataset(layerid, startid=1, endid=4)\n",
    "print(len(dataset), dataset[0][0].shape, dataset[0][1].shape) # torch.Size([512, 4096])\n",
    "# 划分训练集和验证集\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "# train_model(model, train_loader, val_loader, criterion, optimizer, writer=writer, epochs=4, layerid=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gate_sparsity: 0, up_sparsity: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [00:09,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0006978367062107\n",
      "覆盖率(Recall): 1.0\n",
      "gate_sparsity: 0, up_sparsity: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [00:09,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0006978367062107\n",
      "覆盖率(Recall): 0.8319152071818366\n",
      "gate_sparsity: 0, up_sparsity: 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [00:09,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0006978367062107\n",
      "覆盖率(Recall): 0.7632567832775923\n",
      "gate_sparsity: 0, up_sparsity: 0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [00:09,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0006978367062107\n",
      "覆盖率(Recall): 0.7390745018124921\n",
      "gate_sparsity: 0.1, up_sparsity: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [00:09,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0006978367062107\n",
      "覆盖率(Recall): 0.9057734071844139\n",
      "gate_sparsity: 0.1, up_sparsity: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [00:09,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0006978367062107\n",
      "覆盖率(Recall): 0.741349908028955\n",
      "gate_sparsity: 0.1, up_sparsity: 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [00:09,  3.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0006978367062107\n",
      "覆盖率(Recall): 0.6827520058984878\n",
      "gate_sparsity: 0.1, up_sparsity: 0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [00:09,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0006978367062107\n",
      "覆盖率(Recall): 0.6621179141654243\n",
      "gate_sparsity: 0.2, up_sparsity: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [00:09,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0006978367062107\n",
      "覆盖率(Recall): 0.8692518354972218\n",
      "gate_sparsity: 0.2, up_sparsity: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [00:09,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0006978367062107\n",
      "覆盖率(Recall): 0.7150472766711901\n",
      "gate_sparsity: 0.2, up_sparsity: 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [00:09,  3.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0006978367062107\n",
      "覆盖率(Recall): 0.6608203301565573\n",
      "gate_sparsity: 0.2, up_sparsity: 0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [00:09,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0006978367062107\n",
      "覆盖率(Recall): 0.6418193925314125\n",
      "gate_sparsity: 0.3, up_sparsity: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [00:09,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0006978367062107\n",
      "覆盖率(Recall): 0.853785499282834\n",
      "gate_sparsity: 0.3, up_sparsity: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [00:09,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0006978367062107\n",
      "覆盖率(Recall): 0.704894166389118\n",
      "gate_sparsity: 0.3, up_sparsity: 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [00:09,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0006978367062107\n",
      "覆盖率(Recall): 0.6532204083947245\n",
      "gate_sparsity: 0.3, up_sparsity: 0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [00:09,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0006978367062107\n",
      "覆盖率(Recall): 0.635375651310129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 加载gate_proj和up_proj\n",
    "for gate_sparsity in [0, 0.1, 0.2, 0.3]:\n",
    "    for up_sparsity in [0, 0.1, 0.2, 0.3]:\n",
    "        print(f'gate_sparsity: {gate_sparsity}, up_sparsity: {up_sparsity}')\n",
    "        \n",
    "        model = CustomModel(gate_proj_shape=(14336, 4096), up_proj_shape=(14336, 4096))\n",
    "        model.load(\n",
    "            gate_proj_path=f'/home/lz/workspace/llama2-7b/moe-offloading/notebooks/output/sparsity/wanda/gate_proj_{gate_sparsity}.pt',\n",
    "            up_proj_path=f'/home/lz/workspace/llama2-7b/moe-offloading/notebooks/output/sparsity/wanda/up_proj_{up_sparsity}.pt'\n",
    "        )\n",
    "        model.to(\"cuda\")  # 假设使用 GPU\n",
    "        test_model(model, val_loader, sparsity=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载保存的激活值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.init as init\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.cuda.amp import GradScaler, autocast  \n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import json\n",
    "\n",
    "with open('path.json', 'r') as file:\n",
    "    paths = json.load(file)\n",
    "    save_path = paths.get('channel_gate_path','')\n",
    "\n",
    "def load_datasets(layerid = 1, expertid = 0, startid=1, endid=4, use_x1 = False):   \n",
    "    datasets_x = []\n",
    "    datasets_y = []\n",
    "    datasets_x1 = []\n",
    "    for fileid in range(startid, endid):\n",
    "        # print(fileid)\n",
    "        # 加一个map_location\n",
    "        d = torch.load(f'{save_path}/{fileid}-{layerid}-gate.pth', map_location=lambda storage, loc: storage.cuda(0))\n",
    "        datasets_x.append(d[0])\n",
    "        if use_x1:\n",
    "            datasets_x1.append(d[1])\n",
    "        datasets_y.append(d[-1])\n",
    "    x,y = torch.cat(datasets_x,dim=1), torch.cat(datasets_y,dim=1)\n",
    "    datasets_x.clear()\n",
    "    datasets_y.clear()\n",
    "    x = x.reshape(-1, 4096)\n",
    "    y = y.reshape(-1, 14336)\n",
    "    # print(x[0].shape)\n",
    "    if use_x1:\n",
    "        x1 = torch.cat(datasets_x1,dim=1)\n",
    "        datasets_x1.clear()\n",
    "        x1 = x1.reshape(-1, 14336)\n",
    "        return x,x1,y\n",
    "    return x,y\n",
    "    \n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, layerid = 1, expertid = 0, startid=1, endid=4, use_x1 =False):\n",
    "        # 加载数据self.data_x1,\n",
    "        self.use_x1 = use_x1\n",
    "        if use_x1:\n",
    "            self.data_x, self.data_x1, self.data_y = load_datasets(layerid,startid=startid,endid=endid,use_x1=use_x1)\n",
    "            print(len(self.data_x1),len(self.data_x),len(self.data_y))\n",
    "        else:\n",
    "            self.data_x, self.data_y = load_datasets(layerid,startid=startid,endid=endid,use_x1=use_x1)\n",
    "            print(len(self.data_x),len(self.data_y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.use_x1:\n",
    "            return self.data_x[idx],self.data_x1[idx],self.data_y[idx]\n",
    "        else:\n",
    "            return self.data_x[idx],self.data_y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 查看分channel的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110137 110137\n",
      "31 110137 torch.Size([4096]) torch.Size([14336])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "### 统计的个数\n",
    "counts = 20000\n",
    "\n",
    "for layerid in range(22,32):\n",
    "    dataset = CustomDataset(layerid, startid=1, endid=2)\n",
    "    print(layerid, len(dataset), dataset[0][0].shape, dataset[0][1].shape) # torch.Size([512, 4096])\n",
    "    \n",
    "    updata_sum = torch.zeros_like(dataset[0][1])\n",
    "\n",
    "    for i in range(counts):\n",
    "        updata_sum += torch.abs(dataset[i][1])\n",
    "        # break\n",
    "\n",
    "    updata_sum /= counts\n",
    "    torch.save(updata_sum, f'{save_path}/{layerid}-average.pth')\n",
    "\n",
    "    # ### 遍历所有的数据集,计算每个位置的中位数\n",
    "    # dim = dataset[0][1].shape[0]\n",
    "\n",
    "    # # 创建一个列表,每个位置存储该位置所有非0值\n",
    "    # nonzero_values = [[] for _ in range(dim)]\n",
    "\n",
    "    # # 收集所有非0值\n",
    "    # for i in range(counts):\n",
    "    #     values = torch.abs(dataset[i][1])\n",
    "    #     for j in range(dim):\n",
    "    #         if values[j] != 0:\n",
    "    #             nonzero_values[j].append(values[j].item())\n",
    "\n",
    "    # # 计算每个位置非0值的中位数\n",
    "    # median_values = torch.zeros(dim, device=dataset[0][1].device)\n",
    "    # for i in range(dim):\n",
    "    #     if len(nonzero_values[i]) > 0:\n",
    "    #         median_values[i] = torch.tensor(sorted(nonzero_values[i])[len(nonzero_values[i])//2])\n",
    "    #     else:\n",
    "    #         median_values[i] = 0\n",
    "    # # 保存中位数\n",
    "    # torch.save(median_values, f'{save_path}/{layerid}-median.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "layerid = 22\n",
    "aa = torch.load(f'{save_path}/{layerid}-median.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14336])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110137 110137\n"
     ]
    }
   ],
   "source": [
    "layerid = 22\n",
    "dataset = CustomDataset(layerid, startid=1, endid=2)\n",
    "import torch\n",
    "### 遍历所有的数据集,计算每个位置的中位数\n",
    "counts = 300\n",
    "dim = dataset[0][1].shape[0]\n",
    "\n",
    "# 创建一个列表,每个位置存储该位置所有非0值\n",
    "nonzero_values = [[] for _ in range(dim)]\n",
    "\n",
    "# 收集所有非0值\n",
    "for i in range(counts):\n",
    "    values = torch.abs(dataset[i][1])\n",
    "    for j in range(dim):\n",
    "        if values[j] != 0:\n",
    "            nonzero_values[j].append(values[j].item())\n",
    "\n",
    "# 计算每个位置非0值的中位数\n",
    "median_values = torch.zeros(dim, device=dataset[0][1].device)\n",
    "for i in range(dim):\n",
    "    if len(nonzero_values[i]) > 0:\n",
    "        median_values[i] = torch.tensor(sorted(nonzero_values[i])[len(nonzero_values[i])//2])\n",
    "    else:\n",
    "        median_values[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存中位数\n",
    "torch.save(median_values, f'{save_path}/{layerid}-median.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(11277, device='cuda:0'),\n",
       " tensor(3049, device='cuda:0'),\n",
       " tensor(14070, device='cuda:0'))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 按中位数从大到小排序\n",
    "sorted_indices = torch.argsort(median_values, descending=True)\n",
    "sorted_medians = median_values[sorted_indices]\n",
    "sorted_indices[0], sorted_indices[200], sorted_indices[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([150., 213., 150., 120.,  79.,  65.,  60.,  45.,  43.,  19.,  20.,\n",
       "         12.,   7.,   9.,   4.,   0.,   1.,   0.,   2.,   4.]),\n",
       " array([0.0322876 , 0.06641541, 0.10054321, 0.13467102, 0.16879883,\n",
       "        0.20292664, 0.23705444, 0.27118225, 0.30531006, 0.33943787,\n",
       "        0.37356567, 0.40769348, 0.44182129, 0.4759491 , 0.5100769 ,\n",
       "        0.54420471, 0.57833252, 0.61246033, 0.64658813, 0.68071594,\n",
       "        0.71484375]),\n",
       " <BarContainer object of 20 artists>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "### 遍历所有的数据集\n",
    "index = 14070\n",
    "counts = 2000\n",
    "updata_list = []\n",
    "\n",
    "for i in range(counts):\n",
    "    up_data = dataset[i][1]\n",
    "    if up_data[index] != 0:\n",
    "        updata_list.append(math.fabs(up_data[index]))\n",
    "    # break\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "### 画一个频次图\n",
    "plt.hist(updata_list, bins=20, alpha=0.75, log=False)\n",
    "# plt.plot(updata_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  43.,    0.,    0.,    0.,    6.,    7.,   30.,   40.,   63.,\n",
       "          76.,  111.,  142.,  168.,  213.,  307.,  386.,  499.,  638.,\n",
       "         711.,  853.,  912., 1005.,  989.,  953.,  897.,  788.,  688.,\n",
       "         524.,  372.,  309.,  195.,  158.,   83.,   41.,   14.,   15.,\n",
       "          18.,    5.,    0.,    3.]),\n",
       " array([0.00754166, 0.08264608, 0.15775051, 0.23285494, 0.30795937,\n",
       "        0.38306379, 0.45816822, 0.53327265, 0.60837708, 0.6834815 ,\n",
       "        0.75858593, 0.83369036, 0.90879478, 0.98389921, 1.05900364,\n",
       "        1.13410807, 1.20921249, 1.28431692, 1.35942135, 1.43452578,\n",
       "        1.5096302 , 1.58473463, 1.65983906, 1.73494349, 1.81004791,\n",
       "        1.88515234, 1.96025677, 2.03536119, 2.11046562, 2.18557005,\n",
       "        2.26067448, 2.3357789 , 2.41088333, 2.48598776, 2.56109219,\n",
       "        2.63619661, 2.71130104, 2.78640547, 2.8615099 , 2.93661432,\n",
       "        3.01171875]),\n",
       " <BarContainer object of 40 artists>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "### 遍历所有的数据集\n",
    "index = 7048\n",
    "counts = 12263\n",
    "updata_list = []\n",
    "\n",
    "for i in range(1,counts):\n",
    "    up_data = dataset[i][1]\n",
    "    updata_list.append(math.fabs(up_data[index]))\n",
    "    # break\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "### 画一个频次图\n",
    "plt.hist(updata_list, bins=40, alpha=0.75)\n",
    "# plt.plot(updata_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 稀疏预测器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLinearModel(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim,hidden_dim=32):\n",
    "        super(SimpleLinearModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim,bias=False)\n",
    "        # self.activation = nn.SiLU() # 添加激活函数\n",
    "        self.linear2 = nn.Linear(hidden_dim,output_dim,bias=False)  \n",
    "        init.kaiming_normal_(self.linear1.weight, mode='fan_out', nonlinearity='relu')\n",
    "        init.kaiming_normal_(self.linear2.weight, mode='fan_out', nonlinearity='relu')\n",
    "        # self.linear1.bias.data.fill_(0)\n",
    "        # self.linear2.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x= self.activation(x)\n",
    "        return self.linear2(self.linear1(x))\n",
    "    \n",
    "model=SimpleLinearModel(4096,14336,hidden_dim=1024)\n",
    "model.to(\"cuda\")  # 假设使用 GPU\n",
    "# criterion = nn.MSELoss().to(\"cuda\")\n",
    "criterion = nn.CrossEntropyLoss().to(\"cuda\")\n",
    "# criterion = nn.KLDivLoss(reduction='batchmean').to(\"cuda\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4) #lr=5e-5\n",
    "writer = SummaryWriter('runs/predictor_sparsity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "def sparse_row(row, keep_ratio=0.1, use_abs = False):\n",
    "    # 计算需要保留的参数数量\n",
    "    num_to_keep = int(keep_ratio * row.numel())\n",
    "    \n",
    "    # 找到绝对值最大的 num_to_keep 个参数的索引\n",
    "    if use_abs:\n",
    "        row = torch.abs(row)\n",
    "    topk_indices = torch.topk(row, num_to_keep).indices\n",
    "    # topk_indices = torch.topk(row, num_to_keep).indices\n",
    "    \n",
    "    # 创建一个与 row 相同大小的零张量\n",
    "    sparse_row = torch.zeros_like(row)\n",
    "    \n",
    "    # 将 topk_indices 对应的值置为 1\n",
    "    sparse_row[topk_indices] = 1\n",
    "    \n",
    "    return sparse_row\n",
    "\n",
    "def generate_label(y, sparsity, use_abs=False):\n",
    "    # 对每一行进行稀疏化\n",
    "    sparse_tensor = torch.stack([sparse_row(row, sparsity, use_abs) for row in y])\n",
    "    return sparse_tensor\n",
    "\n",
    "def test_model(model, val_loader, sparsity=0.1):\n",
    "    model.eval()\n",
    "    # 初始化总的统计变量\n",
    "    total_correct_preds = 0\n",
    "    total_preds = 0\n",
    "    total_labels = 0\n",
    "    total_masks = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in tqdm(enumerate(val_loader)):\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "\n",
    "            preds = generate_label(outputs, sparsity)\n",
    "            truth = generate_label(targets, 0.1, use_abs = True)\n",
    "            # truth = targets\n",
    "            \n",
    "            # 计算当前batch的精度\n",
    "            dif = truth - preds\n",
    "            miss = dif > 0.0 # classifier didn't activated target neuron\n",
    "\n",
    "            total_correct_preds += (truth.sum(dim=1).float() - miss.sum(dim=1).float()).mean().item()\n",
    "            total_preds += (preds.sum(dim=1).float()).mean().item()\n",
    "            total_labels += (truth.sum(dim=1).float()).mean().item()\n",
    "\n",
    "    # print('预测占比:{:.4f}'.format((total_preds/total_masks).item()))\n",
    "    # print('标签占比:{:.4f}'.format((total_labels/total_masks).item()))\n",
    "    print('预测与标签选取的数量比:',(total_preds / total_labels))\n",
    "    print('覆盖率(Recall):',(total_correct_preds / total_labels))\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, writer, epochs=25, layerid=1):\n",
    "    scaler = GradScaler()  # 创建 GradScaler 对象\n",
    "    for epoch in range(epochs):\n",
    "        if epoch % 2 == 0:\n",
    "            print(f'---------after training {epoch} epochs---------')\n",
    "            test_model(model, val_loader, sparsity=0.2)\n",
    "        model.train()\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            targets = generate_label(targets, 0.2, use_abs =True)\n",
    "\n",
    "            # 使用 autocast 来进行自动混合精度处理\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                probs = outputs.sigmoid()\n",
    "                # cross_entropy\n",
    "                loss = criterion(probs, targets)\n",
    "\n",
    "            # 使用 GradScaler 来缩放损失，然后进行反向传播\n",
    "            # 注意：反向传播不包含在 autocast() 块中\n",
    "            scaler.scale(loss).backward()\n",
    "            writer.add_scalar('Loss/Train', loss.item(), epoch * len(train_loader) + batch_idx)\n",
    "            # 调用 scaler.step() 来更新模型权重，并调用 scaler.update() 准备下一步\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "    print(f'---------after training {epochs} epochs---------')\n",
    "    test_model(model, val_loader, sparsity=0.2)\n",
    "    global cnt\n",
    "    torch.save(model.state_dict(), f'./output/sparsity/{layerid}-{cnt}.pt')\n",
    "    cnt += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333186 333186\n",
      "333186 torch.Size([4096]) torch.Size([14336])\n",
      "---------after training 0 epochs---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [00:08,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0013956734124214\n",
      "覆盖率(Recall): 0.20009789296144784\n",
      "---------after training 2 epochs---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [00:07,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0013956734124214\n",
      "覆盖率(Recall): 0.5413863739824867\n",
      "---------after training 4 epochs---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [00:08,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0013956734124214\n",
      "覆盖率(Recall): 0.5608398667241754\n",
      "327227 327227\n",
      "327227 torch.Size([4096]) torch.Size([14336])\n",
      "---------after training 0 epochs---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:07,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0013956734124214\n",
      "覆盖率(Recall): 0.5442518565676384\n",
      "---------after training 2 epochs---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:07,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0013956734124214\n",
      "覆盖率(Recall): 0.5660076314389747\n",
      "---------after training 4 epochs---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:07,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0013956734124214\n",
      "覆盖率(Recall): 0.5710160334627693\n",
      "341549 341549\n",
      "341549 torch.Size([4096]) torch.Size([14336])\n",
      "---------after training 0 epochs---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34it [00:08,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0013956734124214\n",
      "覆盖率(Recall): 0.5556886096016526\n",
      "---------after training 2 epochs---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34it [00:08,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0013956734124214\n",
      "覆盖率(Recall): 0.5726461708435582\n",
      "---------after training 4 epochs---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34it [00:08,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0013956734124214\n",
      "覆盖率(Recall): 0.5752094106414646\n"
     ]
    }
   ],
   "source": [
    "for startid, endid in [(1,4),(4,7),(7,10)]:\n",
    "    layerid = 15\n",
    "    dataset = CustomDataset(layerid, startid=startid, endid=endid)\n",
    "    print(len(dataset), dataset[0][0].shape, dataset[0][1].shape) # torch.Size([512, 4096])\n",
    "    # 划分训练集和验证集\n",
    "    train_size = int(0.9 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "    train_model(model, train_loader, val_loader, criterion, optimizer, writer=writer, epochs=4, layerid=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载训练好的进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333186 333186\n",
      "333186 torch.Size([4096]) torch.Size([14336])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [00:09,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 3.0006978367062107\n",
      "覆盖率(Recall): 0.6927953512494317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "layerid = 15\n",
    "dataset = CustomDataset(layerid, startid=1, endid=4)\n",
    "print(len(dataset), dataset[0][0].shape, dataset[0][1].shape) # torch.Size([512, 4096])\n",
    "# 划分训练集和验证集\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False)\n",
    "# model = SimpleLinearModel(4096,14336,hidden_dim=1024).cuda()\n",
    "model.load_state_dict(torch.load(f'./output/sparsity/15-2.pt'))\n",
    "test_model(model, val_loader, sparsity=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下游任务测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHF_ENDPOINT\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://hf-mirror.com\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlm_eval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuggingface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HFLM\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlm_eval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluator\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_model\u001b[39m(model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlama3-8b\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/workspace/llama2-7b/lm-evaluation-harness/lm_eval/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate, simple_evaluate\n",
      "File \u001b[0;32m~/workspace/llama2-7b/lm-evaluation-harness/lm_eval/evaluator.py:14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlm_eval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlm_eval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlm_eval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlm_eval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcaching\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcache\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m delete_cache\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlm_eval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluator_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     consolidate_results,\n\u001b[1;32m     18\u001b[0m     get_sample_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     run_task_tests,\n\u001b[1;32m     23\u001b[0m )\n",
      "File \u001b[0;32m~/workspace/llama2-7b/lm-evaluation-harness/lm_eval/models/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     anthropic_llms,\n\u001b[1;32m      3\u001b[0m     dummy,\n\u001b[1;32m      4\u001b[0m     gguf,\n\u001b[1;32m      5\u001b[0m     huggingface,\n\u001b[1;32m      6\u001b[0m     mamba_lm,\n\u001b[1;32m      7\u001b[0m     nemo_lm,\n\u001b[1;32m      8\u001b[0m     neuralmagic,\n\u001b[1;32m      9\u001b[0m     neuron_optimum,\n\u001b[1;32m     10\u001b[0m     openai_completions,\n\u001b[1;32m     11\u001b[0m     optimum_lm,\n\u001b[1;32m     12\u001b[0m     textsynth,\n\u001b[1;32m     13\u001b[0m     vllm_causallms,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# TODO: implement __all__\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# enable hf hub transfer if available\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/llama2-7b/lm-evaluation-harness/lm_eval/models/huggingface.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HfApi\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftModel\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m PEFT_VERSION\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "File \u001b[0;32m~/anaconda3/envs/moe/lib/python3.10/site-packages/peft/__init__.py:22\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# flake8: noqa\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# There's no way to ignore \"F401 '...' imported but unused\" warnings in this\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# module, but to preserve other warnings. So, don't check this module at all.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.11.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     AutoPeftModel,\n\u001b[1;32m     24\u001b[0m     AutoPeftModelForCausalLM,\n\u001b[1;32m     25\u001b[0m     AutoPeftModelForSequenceClassification,\n\u001b[1;32m     26\u001b[0m     AutoPeftModelForSeq2SeqLM,\n\u001b[1;32m     27\u001b[0m     AutoPeftModelForTokenClassification,\n\u001b[1;32m     28\u001b[0m     AutoPeftModelForQuestionAnswering,\n\u001b[1;32m     29\u001b[0m     AutoPeftModelForFeatureExtraction,\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmapping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     32\u001b[0m     MODEL_TYPE_TO_PEFT_MODEL_MAPPING,\n\u001b[1;32m     33\u001b[0m     PEFT_TYPE_TO_CONFIG_MAPPING,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     inject_adapter_in_model,\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmixed_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftMixedModel\n",
      "File \u001b[0;32m~/anaconda3/envs/moe/lib/python3.10/site-packages/peft/auto.py:32\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     22\u001b[0m     AutoModel,\n\u001b[1;32m     23\u001b[0m     AutoModelForCausalLM,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     AutoTokenizer,\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftConfig\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmapping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MODEL_TYPE_TO_PEFT_MODEL_MAPPING\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpeft_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     34\u001b[0m     PeftModel,\n\u001b[1;32m     35\u001b[0m     PeftModelForCausalLM,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     PeftModelForTokenClassification,\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TOKENIZER_CONFIG_NAME\n",
      "File \u001b[0;32m~/anaconda3/envs/moe/lib/python3.10/site-packages/peft/mapping.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftConfig\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmixed_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftMixedModel\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpeft_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     24\u001b[0m     PeftModel,\n\u001b[1;32m     25\u001b[0m     PeftModelForCausalLM,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     PeftModelForTokenClassification,\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuners\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     33\u001b[0m     AdaLoraConfig,\n\u001b[1;32m     34\u001b[0m     AdaLoraModel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m     VeraModel,\n\u001b[1;32m     58\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/moe/lib/python3.10/site-packages/peft/mixed_model.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PushToHubMixin\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuners\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmixed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m COMPATIBLE_TUNER_TYPES\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftConfig\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpeft_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftModel\n",
      "File \u001b[0;32m~/anaconda3/envs/moe/lib/python3.10/site-packages/peft/tuners/__init__.py:26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mia3\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IA3Config, IA3Model\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madalora\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AdaLoraConfig, AdaLoraModel\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mboft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BOFTConfig, BOFTModel\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mp_tuning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PromptEncoder, PromptEncoderConfig, PromptEncoderReparameterizationType\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprefix_tuning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PrefixEncoder, PrefixTuningConfig\n",
      "File \u001b[0;32m~/anaconda3/envs/moe/lib/python3.10/site-packages/peft/tuners/boft/__init__.py:16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2023-present the HuggingFace Inc. team.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BOFTConfig\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BOFTLayer\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BOFTModel\n\u001b[1;32m     20\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBOFTConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBOFTLayer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBOFTModel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/moe/lib/python3.10/site-packages/peft/tuners/boft/layer.py:30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Function\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcpp_extension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuners\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuners_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseTunerLayer, check_adapters_to_merge\n\u001b[1;32m     35\u001b[0m _FBD_CUDA \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/moe/lib/python3.10/site-packages/torch/utils/cpp_extension.py:210\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mhip \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     ROCM_VERSION \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mint\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mhip\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[:\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m--> 210\u001b[0m CUDA_HOME \u001b[38;5;241m=\u001b[39m \u001b[43m_find_cuda_home\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_is_compiled() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    211\u001b[0m CUDNN_HOME \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDNN_HOME\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDNN_PATH\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# PyTorch releases have the version pattern major.minor.patch, whereas when\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# PyTorch is built from source, we append the git commit hash, which gives\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# it the below pattern.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/moe/lib/python3.10/site-packages/torch/utils/cpp_extension.py:117\u001b[0m, in \u001b[0;36m_find_cuda_home\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(cuda_home):\n\u001b[1;32m    116\u001b[0m             cuda_home \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cuda_home \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo CUDA runtime is found, using CUDA_HOME=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcuda_home\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    119\u001b[0m           file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cuda_home\n",
      "File \u001b[0;32m~/anaconda3/envs/moe/lib/python3.10/site-packages/torch/cuda/__init__.py:118\u001b[0m, in \u001b[0;36mis_available\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m device_count() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# The default availability inspection never throws and returns 0 if the driver is missing or can't\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# be initialized. This uses the CUDA Runtime API `cudaGetDeviceCount` which in turn initializes the CUDA Driver\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# API via `cuInit`\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_getDeviceCount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test Model\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "from convert_llama import convert_llama_model\n",
    "os.environ[\"HF_ENDPOINT\"]=\"https://hf-mirror.com\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "from lm_eval.models.huggingface import HFLM\n",
    "from lm_eval import evaluator\n",
    "\n",
    "\n",
    "def _load_model(model_name = \"Llama3-8b\"):\n",
    "    print(f\"Loading model {model_name}\")\n",
    "    ### from path.json read paths of model and dataset\n",
    "    with open('path.json', 'r') as file:\n",
    "        paths = json.load(file)\n",
    "        model_path = paths.get(model_name, '')\n",
    "\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        device_map='auto',\n",
    "        use_cache=True,\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    return model, tokenizer\n",
    "\n",
    "def evaluate(task_name, model, tokenizer, num_fewshot, device):\n",
    "    hflm = HFLM(pretrained=model, tokenizer=tokenizer)\n",
    "    results = evaluator.simple_evaluate(\n",
    "    model=hflm,\n",
    "    tasks=[task_name],\n",
    "    num_fewshot=num_fewshot)\n",
    "    print(results['results'])\n",
    "\n",
    "\n",
    "def main(task_name, model_name, sparsity, start_num, end_num, token_sparsity, memory_limit, device, num_fewshot,):\n",
    "    model, tokenizer = _load_model(model_name)\n",
    "    \n",
    "    model = convert_llama_model(model, sparsity, start_num, end_num, token_sparsity,)\n",
    "\n",
    "    evaluate(task_name, model, tokenizer, num_fewshot, device)\n",
    "\n",
    "main(task_name='boolq', model_name=\"Llama3-8b\", sparsity=0.1, start_num=21, end_num=32, token_sparsity=0.1, memory_limit=0.1, device='cuda', num_fewshot=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_factory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
