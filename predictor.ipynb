{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PD结合的思路"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分别替换gate/up层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先输入一句话，按照70%稀疏去记录prefill阶段激活的神经元，最后统计这个输入prompt对应的最高激活次数的70%的神经元。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d68b4d44787044c698c529c40ba018b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "import convert_llama\n",
    "from transformers import GenerationConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "### from path.json read paths of model and dataset\n",
    "model_name = \"Llama3-8b\"\n",
    "dataset_name = \"c4\"\n",
    "with open('path.json', 'r') as file:\n",
    "    paths = json.load(file)\n",
    "    model_path = paths.get(model_name, '')\n",
    "    dataset_path = paths.get(dataset_name, '')\n",
    "\n",
    "c4 = load_dataset(dataset_path)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map='auto',\n",
    "    use_cache=True,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma: 0, beta: 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convert Llama Models: 457it [00:00, 1358.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Model Done\n",
      "in decode, layer 15\n",
      "Predicted neuron num: 2867.0000\n",
      "Overlap count: 674.2069, Overlap ratio: 0.4705\n",
      "gamma: 0, beta: 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convert Llama Models: 457it [00:00, 1918.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Model Done\n",
      "in decode, layer 15\n",
      "Predicted neuron num: 3584.0000\n",
      "Overlap count: 772.4138, Overlap ratio: 0.5390\n",
      "gamma: 0.05, beta: 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convert Llama Models: 457it [00:00, 2056.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Model Done\n",
      "in decode, layer 15\n",
      "Predicted neuron num: 3203.6897\n",
      "Overlap count: 723.7241, Overlap ratio: 0.5050\n",
      "gamma: 0.05, beta: 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convert Llama Models: 457it [00:00, 2072.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Model Done\n",
      "in decode, layer 15\n",
      "Predicted neuron num: 3885.6552\n",
      "Overlap count: 813.5172, Overlap ratio: 0.5677\n",
      "gamma: 0.1, beta: 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convert Llama Models: 457it [00:00, 2056.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Model Done\n",
      "in decode, layer 15\n",
      "Predicted neuron num: 3610.0690\n",
      "Overlap count: 780.1034, Overlap ratio: 0.5444\n",
      "gamma: 0.1, beta: 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convert Llama Models: 457it [00:00, 1973.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Model Done\n",
      "in decode, layer 15\n",
      "Predicted neuron num: 4250.3793\n",
      "Overlap count: 860.0345, Overlap ratio: 0.6002\n"
     ]
    }
   ],
   "source": [
    "for gamma in [0,0.05,0.1]:\n",
    "    for beta in [0.2,0.25]:\n",
    "        print(f\"gamma: {gamma}, beta: {beta}\")\n",
    "        convert_llama.convert_llama_model(model, sparsity=0.1, start_num=13, end_num=16, alpha=0.1, beta=beta, gamma=gamma)\n",
    "\n",
    "        for c4_demo in c4['validation']['text'][:1]:\n",
    "            input_demo = tokenizer(c4_demo, padding=\"max_length\", truncation=True, max_length=200, return_tensors=\"pt\")\n",
    "            \n",
    "            generate_ids = model.generate(input_demo.input_ids.to('cuda:0'), max_length=230, generation_config=GenerationConfig(do_sample=False), pad_token_id=tokenizer.eos_token_id)\n",
    "            tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "            # model.model.layers[15].mlp.gate_proj.coreinfer_recall()\n",
    "            # model.model.layers[15].mlp.up_proj.coreinfer_recall()\n",
    "            model.model.layers[15].mlp.coreinfer_recall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in decode, gate layer 15\n",
      "Overlap count: 1212.6552, Overlap ratio: 0.8462\n"
     ]
    }
   ],
   "source": [
    "model.model.layers[15].mlp.gate_proj.coreinfer_recall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[prefill] in gate layer: 15\n",
      "[prefill] in up layer: 15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The woman who died after falling from a bridge over the A21 has been identified as a Sevenoaks mum.\\nMarta Kendle, 37, fell from the Gracious Lane bridge on the morning of February 19.\\nPolice were called to the carriageway around 6.10am and the road was promptly closed in both directions.\\nDespite paramedics best efforts, Marta, who was originally from Poland, was pronounced dead at the scene.\\nKent and Medway Coroners office have confirmed an inquest into her death will open on Wednesday (February 27).\\nTributes to the mum were left at the scene and on social media.\\nFriend, Jodi Cahill posted on Facebook: \"I will certainly remember you. I am sorry we did not see how lost and alone you felt.\\n\"Be at peace dear Marta.\"\\nA floral tribute left at the scene said goodbye to the \"beautiful and kind soul\".\\nIt read: \"To a beautiful and kind soul. You will be missed. Rest in peace.\"\\nA spokesman for Kent Police said: \"Officers were called to the A21 at Gracious Lane,'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 使用greedy decode\n",
    "generate_ids = model.generate(input_demo.input_ids.to('cuda:0'), max_length=230, generation_config=GenerationConfig(do_sample=False), pad_token_id=tokenizer.eos_token_id)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in decode, gate layer 15\n",
      "Overlap count: 1243.8621, Overlap ratio: 0.8680\n",
      "in decode, up layer 15\n",
      "Overlap count: 1080.8966, Overlap ratio: 0.7543\n"
     ]
    }
   ],
   "source": [
    "model.model.layers[15].mlp.gate_proj.coreinfer_recall()\n",
    "model.model.layers[15].mlp.up_proj.coreinfer_recall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载保存的激活值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.init as init\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "    \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.cuda.amp import GradScaler, autocast  \n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import json\n",
    "\n",
    "with open('path.json', 'r') as file:\n",
    "    paths = json.load(file)\n",
    "    save_path = paths.get('gatemulup_path','')\n",
    "\n",
    "def load_datasets(layerid = 1, expertid = 0, use_x1 = False):   \n",
    "    datasets_x = []\n",
    "    datasets_y = []\n",
    "    datasets_x1 = []\n",
    "    for fileid in range(1, 3):\n",
    "        # print(fileid)\n",
    "        # 加一个map_location\n",
    "        d = torch.load(f'{save_path}/{fileid}-{layerid}-more.pth', map_location=lambda storage, loc: storage.cuda(0))\n",
    "        datasets_x.append(d[0])\n",
    "        if use_x1:\n",
    "            datasets_x1.append(d[1])\n",
    "        datasets_y.append(d[-1])\n",
    "    # \n",
    "    \n",
    "    x,y = torch.cat(datasets_x,dim=1), torch.cat(datasets_y,dim=1)\n",
    "    datasets_x.clear()\n",
    "    datasets_y.clear()\n",
    "    x = x.reshape(-1, 4096)\n",
    "    y = y.reshape(-1, 14336)\n",
    "    # print(x[0].shape)\n",
    "    if use_x1:\n",
    "        x1 = torch.cat(datasets_x1,dim=1)\n",
    "        datasets_x1.clear()\n",
    "        x1 = x1.reshape(-1, 14336)\n",
    "        return x,x1,y\n",
    "    return x,y\n",
    "    \n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, layerid = 1, expertid = 0, use_x1 =False):\n",
    "        # 加载数据self.data_x1,\n",
    "        self.use_x1 = use_x1\n",
    "        if use_x1:\n",
    "            self.data_x, self.data_x1, self.data_y = load_datasets(layerid,use_x1=use_x1)\n",
    "            print(len(self.data_x1),len(self.data_x),len(self.data_y))\n",
    "        else:\n",
    "            self.data_x, self.data_y = load_datasets(layerid,use_x1=use_x1)\n",
    "            print(len(self.data_x),len(self.data_y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.use_x1:\n",
    "            return self.data_x[idx],self.data_x1[idx],self.data_y[idx]\n",
    "        else:\n",
    "            return self.data_x[idx],self.data_y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222908 222908\n",
      "222908 torch.Size([4096]) torch.Size([14336])\n"
     ]
    }
   ],
   "source": [
    "# expertid = 2\n",
    "layerid = 15\n",
    "dataset = CustomDataset(layerid)\n",
    "print(len(dataset), dataset[0][0].shape, dataset[0][1].shape) # torch.Size([512, 4096])\n",
    "# 划分训练集和验证集\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 稀疏预测器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLinearModel(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim,hidden_dim=32):\n",
    "        super(SimpleLinearModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim,bias=False)\n",
    "        # self.activation = nn.SiLU() # 添加激活函数\n",
    "        self.linear2 = nn.Linear(hidden_dim,output_dim,bias=False)  \n",
    "        init.kaiming_normal_(self.linear1.weight, mode='fan_out', nonlinearity='relu')\n",
    "        init.kaiming_normal_(self.linear2.weight, mode='fan_out', nonlinearity='relu')\n",
    "        # self.linear1.bias.data.fill_(0)\n",
    "        # self.linear2.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x= self.activation(x)\n",
    "        return self.linear2(self.linear1(x))\n",
    "    \n",
    "model=SimpleLinearModel(4096,14336,hidden_dim=1024)\n",
    "model.to(\"cuda\")  # 假设使用 GPU\n",
    "# criterion = nn.MSELoss().to(\"cuda\")\n",
    "criterion = nn.CrossEntropyLoss().to(\"cuda\")\n",
    "# criterion = nn.KLDivLoss(reduction='batchmean').to(\"cuda\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4) #lr=5e-5\n",
    "writer = SummaryWriter('runs/predictor_sparsity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def sparse_row(row, keep_ratio=0.1, use_abs = False):\n",
    "    # 计算需要保留的参数数量\n",
    "    num_to_keep = int(keep_ratio * row.numel())\n",
    "    \n",
    "    # 找到绝对值最大的 num_to_keep 个参数的索引\n",
    "    if use_abs:\n",
    "        row = torch.abs(row)\n",
    "    topk_indices = torch.topk(row, num_to_keep).indices\n",
    "    # topk_indices = torch.topk(row, num_to_keep).indices\n",
    "    \n",
    "    # 创建一个与 row 相同大小的零张量\n",
    "    sparse_row = torch.zeros_like(row)\n",
    "    \n",
    "    # 将 topk_indices 对应的值置为 1\n",
    "    sparse_row[topk_indices] = 1\n",
    "    \n",
    "    return sparse_row\n",
    "\n",
    "def generate_label(y, sparsity, use_abs=False):\n",
    "    # 对每一行进行稀疏化\n",
    "    sparse_tensor = torch.stack([sparse_row(row, sparsity, use_abs) for row in y])\n",
    "    return sparse_tensor\n",
    "\n",
    "def test_model(model, val_loader, sparsity=0.1):\n",
    "    model.eval()\n",
    "    # 初始化总的统计变量\n",
    "    total_correct_preds = 0\n",
    "    total_preds = 0\n",
    "    total_labels = 0\n",
    "    total_masks = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in tqdm(enumerate(val_loader)):\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "\n",
    "            preds = generate_label(outputs, sparsity)\n",
    "            truth = generate_label(targets, 0.1, use_abs = True)\n",
    "            # truth = targets\n",
    "            \n",
    "            # 计算当前batch的精度\n",
    "            dif = truth - preds\n",
    "            miss = dif > 0.0 # classifier didn't activated target neuron\n",
    "\n",
    "            total_correct_preds += (truth.sum(dim=1).float() - miss.sum(dim=1).float()).mean().item()\n",
    "            total_preds += (preds.sum(dim=1).float()).mean().item()\n",
    "            total_labels += (truth.sum(dim=1).float()).mean().item()\n",
    "\n",
    "    # print('预测占比:{:.4f}'.format((total_preds/total_masks).item()))\n",
    "    # print('标签占比:{:.4f}'.format((total_labels/total_masks).item()))\n",
    "    print('预测与标签选取的数量比:',(total_preds / total_labels))\n",
    "    print('覆盖率(Recall):',(total_correct_preds / total_labels))\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, writer, epochs=25, layerid=1):\n",
    "    scaler = GradScaler()  # 创建 GradScaler 对象\n",
    "    for epoch in range(epochs):\n",
    "        if epoch % 2 == 0:\n",
    "            print(f'---------after training {epoch} epochs---------')\n",
    "            test_model(model, val_loader, sparsity=0.2)\n",
    "        model.train()\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            targets = generate_label(targets, 0.2, use_abs =True)\n",
    "\n",
    "            # 使用 autocast 来进行自动混合精度处理\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                probs = outputs.sigmoid()\n",
    "                # cross_entropy\n",
    "                loss = criterion(probs, targets)\n",
    "\n",
    "            # 使用 GradScaler 来缩放损失，然后进行反向传播\n",
    "            # 注意：反向传播不包含在 autocast() 块中\n",
    "            scaler.scale(loss).backward()\n",
    "            writer.add_scalar('Loss/Train', loss.item(), epoch * len(train_loader) + batch_idx)\n",
    "            # 调用 scaler.step() 来更新模型权重，并调用 scaler.update() 准备下一步\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "    print(f'---------after training {epochs} epochs---------')\n",
    "    test_model(model, val_loader, sparsity=0.2)\n",
    "    torch.save(model.state_dict(), f'./output/sparsity/{layerid}.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------after training 0 epochs---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [00:05,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0013956734178255\n",
      "覆盖率(Recall): 0.19975424976201692\n",
      "---------after training 2 epochs---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [00:05,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0013956734178255\n",
      "覆盖率(Recall): 0.5297148883007268\n",
      "---------after training 4 epochs---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [00:05,  4.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0013956734178255\n",
      "覆盖率(Recall): 0.5535088475673045\n",
      "---------after training 6 epochs---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [00:05,  4.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0013956734178255\n",
      "覆盖率(Recall): 0.5628978439101552\n",
      "---------after training 8 epochs---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [00:05,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测与标签选取的数量比: 2.0013956734178255\n",
      "覆盖率(Recall): 0.567358347200023\n"
     ]
    }
   ],
   "source": [
    "### c4_llama3\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, writer=writer, epochs=8, layerid=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_factory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
