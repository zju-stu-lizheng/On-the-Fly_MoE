{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='longrope': {'long_mscale', 'short_mscale'}\n",
      "This model has set a `original_max_position_embeddings` field, to be used together with `max_position_embeddings` to determine a scaling factor. Please set the `factor` field of `rope_scaling`with this ratio instead -- we recommend the use of this field over `original_max_position_embeddings`, as it is compatible with most model architectures.\n",
      "Loading checkpoint shards: 100%|██████████| 17/17 [00:22<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with sparsity of 0.5\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from hqq.core.quantize import *\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "sys.path.append('../saving')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2\"\n",
    "from transformers import AutoTokenizer, MixtralConfig \n",
    "from modeling_phimoe import PhimoeForCausalLM\n",
    "# from modeling_mixtral_sparsity import MixtralForCausalLM, load_thresholds\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # 如果使用 GPU\n",
    "    torch.backends.cudnn.deterministic = True  # 确保 CUDA 操作是确定性的\n",
    "    torch.backends.cudnn.benchmark = False  # 禁用 CUDA 的自动优化\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "# from configuration_phimoe import PhiMoEConfig\n",
    "# from modeling_phimoe import PhiMoEForCausalLM, load_thresholds, set_profile_mode\n",
    "set_seed(42)\n",
    "\n",
    "import transformers\n",
    "from hqq.models.base import BaseHQQModel\n",
    "from accelerate import init_empty_weights\n",
    "from hqq.models.hf.mixtral import MixtralPatch\n",
    "\n",
    "with open(\"/home/bcds/On-the-Fly_MoE_Inference/quantize/device_map.json\", \"r\") as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "with open('../path.json', 'r') as file:\n",
    "    paths = json.load(file)\n",
    "    # threshold_path = paths.get('mixtral_threshold','')\n",
    "    # model_name = paths.get(\"mixtral\",\"\")\n",
    "    threshold_path = paths.get('phi_threshold','')\n",
    "    model_name = paths.get(\"phi\",\"\")\n",
    "\n",
    "# filepath = str(0.5).replace('.', '_')\n",
    "# load_thresholds(f'{threshold_path}/thresholds_{filepath}.pt', use_type=\"gate\", zero=False)\n",
    "# set_profile_mode(False)\n",
    "# llm = get_model(model_name, device_map)\n",
    "# configuration = PhiMoEConfig(sparsity_selection=\"gate\")\n",
    "llm = PhimoeForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device_map,\n",
    "    # config=configuration,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=False,  \n",
    "    # attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "    \n",
    "print(f'with sparsity of {0.5}')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "from datasets import load_dataset\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "valenc = []\n",
    "# seqlen=512\n",
    "seqlen=512\n",
    "random.seed(0)\n",
    "llm.eval()\n",
    "llm.config.use_cache=False\n",
    "valdata = load_dataset('/home/bcds/venv/dilab/floe/dataset/wikitext/wikitext-2-raw-v1', split='test')\n",
    "\n",
    "valdata_tok = tokenizer(\"\\n\\n\".join(valdata['text'][:1000]), return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsamples 164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164/164 [00:27<00:00,  5.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPL: 5.1181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class TokenizerWrapper:\n",
    "    def __init__(self, input_ids):\n",
    "        self.input_ids = input_ids\n",
    "# valenc = TokenizerWrapper(valenc)\n",
    "# print(valdata)\n",
    "testenc = valdata_tok.input_ids\n",
    "\n",
    "\n",
    "# Calculate number of samples\n",
    "nsamples = testenc.numel() // seqlen\n",
    "\n",
    "# List to store negative log likelihoods\n",
    "nlls = []\n",
    "print(f\"nsamples {nsamples}\")\n",
    "\n",
    "from tqdm import trange\n",
    "with torch.no_grad():\n",
    "    # Loop through each batch\n",
    "    for i in trange(0, nsamples, 1):\n",
    "        # Calculate end index\n",
    "        j = min(i+1, nsamples)\n",
    "\n",
    "        # Prepare inputs and move to device\n",
    "        inputs = testenc[:,(i * seqlen):(j * seqlen)].to(llm.device)\n",
    "        inputs = inputs.reshape(j-i, seqlen)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        lm_logits = llm(inputs,use_cache=False).logits\n",
    "\n",
    "        # Shift logits and labels for next token prediction\n",
    "        shift_logits = lm_logits[:, :-1, :].contiguous()\n",
    "        shift_labels = inputs[:, 1:]\n",
    "\n",
    "        # Compute loss\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(shift_logits.reshape(-1, shift_logits.size(-1)), shift_labels.reshape(-1))\n",
    "        # Calculate negative log likelihood\n",
    "        neg_log_likelihood = loss.float() * seqlen * (j-i)\n",
    "        # print(neg_log_likelihood.item())\n",
    "        # Append to list of negative log likelihoods\n",
    "        nlls.append(torch.tensor(neg_log_likelihood.item()))\n",
    "        # print(nlls)\n",
    "        # del inputs, lm_logits, shift_logits, shift_labels, loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Compute perplexity\n",
    "ppl = torch.exp(torch.stack(nlls).sum() / (nsamples * seqlen))\n",
    "\n",
    "# Empty CUDA cache to save memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print('PPL: {:.4f}'.format(ppl.item()))\n",
    "\n",
    "\n",
    "# avg_list = []\n",
    "# for layerid in range(32):\n",
    "#     for expertid in range(16):\n",
    "#         avg_list.append(llm.model.layers[layerid].block_sparse_moe.experts[expertid].get_ratio())\n",
    "\n",
    "# print('Average Sparsity: ', f'{sum(avg_list)/len(avg_list):.4f}')\n",
    "# print('Max Sparsity: {:.4f}'.format(max(avg_list)))\n",
    "# print('Min Sparsity: {:.4f}'.format(min(avg_list)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
