{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先都加载到cpu上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:23<00:00,  5.79s/it]\n",
      "  0%|          | 0/225 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "_convert_weight_to_int4pack_cuda is not available for build.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#Patching\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhqq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpatching\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prepare_for_inference\n\u001b[0;32m---> 33\u001b[0m \u001b[43mprepare_for_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m########################################################################\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#Inference\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhqq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneration_hf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m patch_model_for_compiled_runtime\n",
      "File \u001b[0;32m~/venv/dilab/floe/hqq/hqq/utils/patching.py:126\u001b[0m, in \u001b[0;36mprepare_for_inference\u001b[0;34m(model, allow_merge, backend, verbose)\u001b[0m\n\u001b[1;32m    124\u001b[0m         patch_linearlayers(model, patch_hqq_to_bitblas, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchao_int4\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 126\u001b[0m     \u001b[43mpatch_linearlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_hqq_to_aoint4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     recommended_inductor_config_setter()\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_merge:  \u001b[38;5;66;03m# only compatible with symmetric quant kernels\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/dilab/floe/hqq/hqq/utils/patching.py:28\u001b[0m, in \u001b[0;36mpatch_linearlayers\u001b[0;34m(model, fct, patch_param, verbose)\u001b[0m\n\u001b[1;32m     26\u001b[0m base_class \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mbase_class \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_class\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01melse\u001b[39;00m AutoHQQHFModel\n\u001b[1;32m     27\u001b[0m base_class\u001b[38;5;241m.\u001b[39msetup_model(model)\n\u001b[0;32m---> 28\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_linearlayers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_param\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_tags\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/dilab/floe/hqq/hqq/models/base.py:154\u001b[0m, in \u001b[0;36mBasePatch.patch_linearlayers\u001b[0;34m(cls, model, patch_fct, patch_params, verbose)\u001b[0m\n\u001b[1;32m    147\u001b[0m     linear_tag \u001b[38;5;241m=\u001b[39m name_to_linear_tag(name)\n\u001b[1;32m    148\u001b[0m     patch_param \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    149\u001b[0m         patch_params[linear_tag] \u001b[38;5;28;01mif\u001b[39;00m (linear_tag \u001b[38;5;129;01min\u001b[39;00m patch_params) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     )\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(\n\u001b[1;32m    152\u001b[0m         find_parent(model, name),\n\u001b[1;32m    153\u001b[0m         name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m--> 154\u001b[0m         \u001b[43mpatch_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_param\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    155\u001b[0m     )\n\u001b[1;32m    157\u001b[0m cleanup()\n",
      "File \u001b[0;32m~/venv/dilab/floe/hqq/hqq/backends/torchao.py:324\u001b[0m, in \u001b[0;36mpatch_hqq_to_aoint4\u001b[0;34m(layer, patch_params)\u001b[0m\n\u001b[1;32m    313\u001b[0m quant_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(hqq_layer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquant_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    315\u001b[0m hqq_aoint4_layer \u001b[38;5;241m=\u001b[39m HQQLinearTorchWeightOnlynt4(\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    317\u001b[0m     quant_config\u001b[38;5;241m=\u001b[39mquant_config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    322\u001b[0m     padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    323\u001b[0m )\n\u001b[0;32m--> 324\u001b[0m \u001b[43mhqq_aoint4_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_with_hqq_quants\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhqq_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhqq_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhqq_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m hqq_layer\u001b[38;5;241m.\u001b[39mW_q\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m hqq_layer\u001b[38;5;241m.\u001b[39mmeta\n",
      "File \u001b[0;32m~/venv/dilab/floe/hqq/hqq/backends/torchao.py:91\u001b[0m, in \u001b[0;36mHQQLinearTorchWeightOnlynt4.initialize_with_hqq_quants\u001b[0;34m(self, W_q, meta, bias)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# Force padding off, a bit tricky to post-pad with grouping\u001b[39;00m\n\u001b[1;32m     88\u001b[0m )\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_shape(meta[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 91\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_hqq_quants\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m bias\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/dilab/floe/hqq/hqq/backends/torchao.py:193\u001b[0m, in \u001b[0;36mHQQLinearTorchWeightOnlynt4.process_hqq_quants\u001b[0;34m(self, W_q, meta)\u001b[0m\n\u001b[1;32m    188\u001b[0m     zeros \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreshape_meta_axis1(zeros, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroupsize, shape)\n\u001b[1;32m    190\u001b[0m W_q_torch, scales_torch, zeros_torch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhqq_quants_to_torch_quants(\n\u001b[1;32m    191\u001b[0m     W_q\u001b[38;5;241m=\u001b[39mW_q, scales\u001b[38;5;241m=\u001b[39mscales, zeros\u001b[38;5;241m=\u001b[39mzeros, shape\u001b[38;5;241m=\u001b[39mshape, nbits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnbits\n\u001b[1;32m    192\u001b[0m )\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_int4pack \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maten\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_weight_to_int4pack\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mW_q_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner_k_tiles\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscales_and_zeros \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpack_scales_and_zeros(scales_torch, zeros_torch)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m W_q_torch, scales_torch, zeros_torch\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/_ops.py:1116\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[0;32m-> 1116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: _convert_weight_to_int4pack_cuda is not available for build."
     ]
    }
   ],
   "source": [
    "# pip install git+https://github.com/mobiusml/hqq.git;\n",
    "# pip install bitblas #to use the bitblas backend\n",
    "# OMP_NUM_THREADS=16 CUDA_VISIBLE_DEVICES=0 ipython3 \n",
    "########################################################################\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "import torch\n",
    "device        = 'cuda:0'\n",
    "backend       = 'torchao_int4' #\"torchao_int4\" (4-bit only) or \"bitblas\" (4-bit + 2-bit)\n",
    "compute_dtype = torch.bfloat16 if backend==\"torchao_int4\" else torch.float16\n",
    "cache_dir     = '.' \n",
    "model_id      = '/home/bcds/venv/dilab/Meta-Llama-3.1-8B'\n",
    "\n",
    "########################################################################\n",
    "#Load model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, HqqConfig \n",
    "\n",
    "quant_config = HqqConfig(nbits=4, group_size=64, axis=1)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=compute_dtype, \n",
    "    cache_dir=cache_dir,\n",
    "    device_map=device, \n",
    "    low_cpu_mem_usage=True,\n",
    "    quantization_config=quant_config\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir)\n",
    "\n",
    "#Patching\n",
    "from hqq.utils.patching import prepare_for_inference\n",
    "prepare_for_inference(model, backend=backend, verbose=True) \n",
    "\n",
    "########################################################################\n",
    "#Inference\n",
    "from hqq.utils.generation_hf import patch_model_for_compiled_runtime\n",
    "patch_model_for_compiled_runtime(model, tokenizer, warmup=True) \n",
    "\n",
    "prompt = \"Write an essay about large language models.\"\n",
    "\n",
    "inputs = tokenizer([tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt},], tokenize=False)], return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=1000, cache_implementation=\"static\", pad_token_id=tokenizer.pad_token_id) \n",
    "#print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 10.35it/s]\n",
      "100%|██████████| 131/131 [00:00<00:00, 267.59it/s]\n",
      "100%|██████████| 225/225 [00:22<00:00, 10.06it/s]\n",
      "  0%|          | 0/225 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "_convert_weight_to_int4pack_cuda is not available for build.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#Optimize\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhqq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpatching\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prepare_for_inference\n\u001b[0;32m---> 28\u001b[0m \u001b[43mprepare_for_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#Load GemLite cache\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgemlite\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m~/venv/dilab/floe/hqq/hqq/utils/patching.py:126\u001b[0m, in \u001b[0;36mprepare_for_inference\u001b[0;34m(model, allow_merge, backend, verbose)\u001b[0m\n\u001b[1;32m    124\u001b[0m         patch_linearlayers(model, patch_hqq_to_bitblas, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchao_int4\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 126\u001b[0m     \u001b[43mpatch_linearlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_hqq_to_aoint4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     recommended_inductor_config_setter()\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_merge:  \u001b[38;5;66;03m# only compatible with symmetric quant kernels\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/dilab/floe/hqq/hqq/utils/patching.py:28\u001b[0m, in \u001b[0;36mpatch_linearlayers\u001b[0;34m(model, fct, patch_param, verbose)\u001b[0m\n\u001b[1;32m     26\u001b[0m base_class \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mbase_class \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_class\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01melse\u001b[39;00m AutoHQQHFModel\n\u001b[1;32m     27\u001b[0m base_class\u001b[38;5;241m.\u001b[39msetup_model(model)\n\u001b[0;32m---> 28\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_linearlayers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_param\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_tags\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/dilab/floe/hqq/hqq/models/base.py:154\u001b[0m, in \u001b[0;36mBasePatch.patch_linearlayers\u001b[0;34m(cls, model, patch_fct, patch_params, verbose)\u001b[0m\n\u001b[1;32m    147\u001b[0m     linear_tag \u001b[38;5;241m=\u001b[39m name_to_linear_tag(name)\n\u001b[1;32m    148\u001b[0m     patch_param \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    149\u001b[0m         patch_params[linear_tag] \u001b[38;5;28;01mif\u001b[39;00m (linear_tag \u001b[38;5;129;01min\u001b[39;00m patch_params) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     )\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(\n\u001b[1;32m    152\u001b[0m         find_parent(model, name),\n\u001b[1;32m    153\u001b[0m         name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m--> 154\u001b[0m         \u001b[43mpatch_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_param\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    155\u001b[0m     )\n\u001b[1;32m    157\u001b[0m cleanup()\n",
      "File \u001b[0;32m~/venv/dilab/floe/hqq/hqq/backends/torchao.py:324\u001b[0m, in \u001b[0;36mpatch_hqq_to_aoint4\u001b[0;34m(layer, patch_params)\u001b[0m\n\u001b[1;32m    313\u001b[0m quant_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(hqq_layer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquant_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    315\u001b[0m hqq_aoint4_layer \u001b[38;5;241m=\u001b[39m HQQLinearTorchWeightOnlynt4(\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    317\u001b[0m     quant_config\u001b[38;5;241m=\u001b[39mquant_config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    322\u001b[0m     padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    323\u001b[0m )\n\u001b[0;32m--> 324\u001b[0m \u001b[43mhqq_aoint4_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_with_hqq_quants\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhqq_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhqq_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhqq_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m hqq_layer\u001b[38;5;241m.\u001b[39mW_q\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m hqq_layer\u001b[38;5;241m.\u001b[39mmeta\n",
      "File \u001b[0;32m~/venv/dilab/floe/hqq/hqq/backends/torchao.py:91\u001b[0m, in \u001b[0;36mHQQLinearTorchWeightOnlynt4.initialize_with_hqq_quants\u001b[0;34m(self, W_q, meta, bias)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# Force padding off, a bit tricky to post-pad with grouping\u001b[39;00m\n\u001b[1;32m     88\u001b[0m )\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_shape(meta[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 91\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_hqq_quants\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m bias\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/dilab/floe/hqq/hqq/backends/torchao.py:193\u001b[0m, in \u001b[0;36mHQQLinearTorchWeightOnlynt4.process_hqq_quants\u001b[0;34m(self, W_q, meta)\u001b[0m\n\u001b[1;32m    188\u001b[0m     zeros \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreshape_meta_axis1(zeros, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroupsize, shape)\n\u001b[1;32m    190\u001b[0m W_q_torch, scales_torch, zeros_torch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhqq_quants_to_torch_quants(\n\u001b[1;32m    191\u001b[0m     W_q\u001b[38;5;241m=\u001b[39mW_q, scales\u001b[38;5;241m=\u001b[39mscales, zeros\u001b[38;5;241m=\u001b[39mzeros, shape\u001b[38;5;241m=\u001b[39mshape, nbits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnbits\n\u001b[1;32m    192\u001b[0m )\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_int4pack \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maten\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_weight_to_int4pack\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mW_q_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner_k_tiles\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscales_and_zeros \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpack_scales_and_zeros(scales_torch, zeros_torch)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m W_q_torch, scales_torch, zeros_torch\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/_ops.py:1116\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[0;32m-> 1116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: _convert_weight_to_int4pack_cuda is not available for build."
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"\n",
    "import torch\n",
    "device        = 'cuda:0'\n",
    "backend       = \"torchao_int4\" #'torchao_int4' #\"torchao_int4\" (4-bit only) or \"bitblas\" (4-bit + 2-bit) or \"gemlite\" (8-bit, 4-bit, 2-bit, 1-bit)\n",
    "compute_dtype = torch.bfloat16 if backend==\"torchao_int4\" else torch.float16\n",
    "cache_dir     = '.' \n",
    "model_id      = '/home/bcds/venv/dilab/Meta-Llama-3.1-8B'\n",
    "\n",
    "########################################################################\n",
    "#Load model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM\n",
    "from hqq.models.hf.base import AutoHQQHFModel\n",
    "from hqq.core.quantize import *\n",
    "\n",
    "#Load\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir)\n",
    "model     = LlamaForCausalLM.from_pretrained(model_id, cache_dir=cache_dir, torch_dtype=compute_dtype, attn_implementation=\"sdpa\")\n",
    "\n",
    "#Quantize\n",
    "quant_config = BaseQuantizeConfig(nbits=4, group_size=64, axis=1)\n",
    "AutoHQQHFModel.quantize_model(model, quant_config=quant_config, compute_dtype=compute_dtype, device=device)\n",
    "HQQLinear.set_backend(HQQBackend.PYTORCH)\n",
    "\n",
    "#Optimize\n",
    "from hqq.utils.patching import prepare_for_inference\n",
    "prepare_for_inference(model, backend=backend, verbose=True)\n",
    "\n",
    "#Load GemLite cache\n",
    "if(backend == 'gemlite'):\n",
    "\timport gemlite\n",
    "\tgemlite.core.GEMLITE_TRITON_RESTRICT_M = True\n",
    "\tgemlite.core.GemLiteLinear.load_config('/tmp/gemlite_config.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a friendly chatbot who always responds in the style of a pirateUser: How many helicopters can a human eat in one sitting?\n"
     ]
    }
   ],
   "source": [
    "chat_template = \"{% for message in messages %}{% if message['role'] == 'user' %}{{ 'User: ' + message['content'] }}{% elif message['role'] == 'assistant' %}{{ 'Assistant: ' + message['content'] }}{% else %}{{ message['content'] }}{% endif %}{% if not loop.last %}\\n{% endif %}{% endfor %}\"\n",
    "tokenizer.chat_template = chat_template\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    " ]\n",
    "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "print(tokenizer.decode(tokenized_chat[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]The 'batch_size' attribute of StaticCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n",
      "Using `torch.compile`.\n",
      "  0%|          | 0/5 [02:20<?, ?it/s]\n"
     ]
    },
    {
     "ename": "Unsupported",
     "evalue": "'skip function sdpa_kernel in file /home/bcds/.conda/envs/llm/lib/python3.9/contextlib.py'\n\nfrom user code:\n   File \"/home/bcds/venv/dilab/floe/hqq/hqq/utils/generation_hf.py\", line 80, in custom_forward\n    with sdpa_kernel([SDPBackend.MATH]):\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupported\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Inference\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m########################################################################\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# #Using a custom generator\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m########################################################################\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#Using HF model.generate()\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhqq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneration_hf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m patch_model_for_compiled_runtime\n\u001b[0;32m---> 12\u001b[0m \u001b[43mpatch_model_for_compiled_runtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/dilab/floe/hqq/hqq/utils/generation_hf.py:91\u001b[0m, in \u001b[0;36mpatch_model_for_compiled_runtime\u001b[0;34m(model, tokenizer, warmup, max_new_tokens, patch_accelerate)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m tqdm(WARMUP_PROMPTS):\n\u001b[0;32m---> 91\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_implementation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstatic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/transformers/generation/utils.py:2252\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2244\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2245\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2246\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2247\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2248\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2249\u001b[0m     )\n\u001b[1;32m   2251\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2252\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2253\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2262\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2265\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2266\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2272\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/transformers/generation/utils.py:3254\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3252\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3254\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3256\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3257\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3258\u001b[0m     outputs,\n\u001b[1;32m   3259\u001b[0m     model_kwargs,\n\u001b[1;32m   3260\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3261\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:465\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m saved_dynamic_layer_stack_depth \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    461\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mget_dynamic_layer_stack_depth()\n\u001b[1;32m    462\u001b[0m )\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[1;32m    468\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mpop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[1;32m    469\u001b[0m         saved_dynamic_layer_stack_depth\n\u001b[1;32m    470\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:1269\u001b[0m, in \u001b[0;36mCatchErrorsWrapper.__call__\u001b[0;34m(self, frame, cache_entry, frame_state)\u001b[0m\n\u001b[1;32m   1263\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m hijacked_callback(\n\u001b[1;32m   1264\u001b[0m                 frame, cache_entry, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks, frame_state\n\u001b[1;32m   1265\u001b[0m             )\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;66;03m# skip=1: skip this frame\u001b[39;00m\n\u001b[0;32m-> 1269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:526\u001b[0m, in \u001b[0;36mConvertFrameAssert.__call__\u001b[0;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m    510\u001b[0m compile_id \u001b[38;5;241m=\u001b[39m CompileId(frame_id, frame_compile_id)\n\u001b[1;32m    512\u001b[0m signpost_event(\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_convert_frame_assert._compile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m     },\n\u001b[1;32m    524\u001b[0m )\n\u001b[0;32m--> 526\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_globals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_locals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_builtins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_one_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompile_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompile_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:924\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip)\u001b[0m\n\u001b[1;32m    922\u001b[0m guarded_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 924\u001b[0m     guarded_code \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m guarded_code\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:666\u001b[0m, in \u001b[0;36m_compile.<locals>.compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_compile.compile_inner\u001b[39m\u001b[38;5;124m\"\u001b[39m, phase_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentire_frame_compile\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m CompileTimeInstructionCounter\u001b[38;5;241m.\u001b[39mrecord():\n\u001b[0;32m--> 666\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/_utils_internal.py:87\u001b[0m, in \u001b[0;36mcompile_time_strobelight_meta.<locals>.compile_time_strobelight_meta_inner.<locals>.wrapper_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m StrobelightCompileTimeProfiler\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StrobelightCompileTimeProfiler\u001b[38;5;241m.\u001b[39mprofile_compile_time(\n\u001b[1;32m     90\u001b[0m     function, phase_name, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     91\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:699\u001b[0m, in \u001b[0;36m_compile.<locals>._compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    697\u001b[0m CompileContext\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39mattempt \u001b[38;5;241m=\u001b[39m attempt\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 699\u001b[0m     out_code \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_code_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mRestartAnalysis \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/_dynamo/bytecode_transformation.py:1322\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m   1319\u001b[0m instructions \u001b[38;5;241m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m   1320\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m-> 1322\u001b[0m \u001b[43mtransformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:219\u001b[0m, in \u001b[0;36mpreserve_global_state.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m exit_stack\u001b[38;5;241m.\u001b[39menter_context(\n\u001b[1;32m    216\u001b[0m     torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39m_symbolic_trace\u001b[38;5;241m.\u001b[39m_maybe_revert_all_patches()\n\u001b[1;32m    217\u001b[0m )\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     cleanup\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:634\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracing(tracer\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mtracing_context), tracer\u001b[38;5;241m.\u001b[39mset_current_tx():\n\u001b[0;32m--> 634\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mUnspecializeRestartAnalysis:\n\u001b[1;32m    636\u001b[0m     speculation_log\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:2796\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 2796\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:983\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 983\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    984\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:895\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 895\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:582\u001b[0m, in \u001b[0;36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_graph_break(\u001b[38;5;28mself\u001b[39m, inst, speculation\u001b[38;5;241m.\u001b[39mreason)\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m excp:\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneric_context_manager_depth \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    585\u001b[0m         \u001b[38;5;66;03m# We don't support graph break under GenericContextWrappingVariable,\u001b[39;00m\n\u001b[1;32m    586\u001b[0m         \u001b[38;5;66;03m# If there is, we roll back to the checkpoint and fall back.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:1602\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.CALL_FUNCTION\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1600\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpopn(inst\u001b[38;5;241m.\u001b[39margval)\n\u001b[1;32m   1601\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m-> 1602\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:830\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.call_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inner_fn \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(inner_fn) \u001b[38;5;129;01mand\u001b[39;00m is_forbidden(inner_fn):\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt to trace forbidden callable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minner_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 830\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush(\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/_dynamo/variables/functions.py:727\u001b[0m, in \u001b[0;36mSkipFunctionVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mwarn_once(msg)\n\u001b[1;32m    726\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreason \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 727\u001b[0m \u001b[43munimplemented\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/_dynamo/exc.py:297\u001b[0m, in \u001b[0;36munimplemented\u001b[0;34m(msg, from_exc, case_name)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _NOTHING:\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Unsupported(msg, case_name\u001b[38;5;241m=\u001b[39mcase_name) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfrom_exc\u001b[39;00m\n\u001b[0;32m--> 297\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Unsupported(msg, case_name\u001b[38;5;241m=\u001b[39mcase_name)\n",
      "\u001b[0;31mUnsupported\u001b[0m: 'skip function sdpa_kernel in file /home/bcds/.conda/envs/llm/lib/python3.9/contextlib.py'\n\nfrom user code:\n   File \"/home/bcds/venv/dilab/floe/hqq/hqq/utils/generation_hf.py\", line 80, in custom_forward\n    with sdpa_kernel([SDPBackend.MATH]):\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "#Inference\n",
    "########################################################################\n",
    "# #Using a custom generator\n",
    "# from hqq.utils.generation_hf import HFGenerator\n",
    "# gen = HFGenerator(model, tokenizer, max_new_tokens=1000, do_sample=True, compile=\"partial\").warmup() \n",
    "\n",
    "# out = gen.generate(\"Write an essay about large language models.\", print_tokens=True)\n",
    "########################################################################\n",
    "#Using HF model.generate()\n",
    "from hqq.utils.generation_hf import patch_model_for_compiled_runtime\n",
    "\n",
    "patch_model_for_compiled_runtime(model, tokenizer, warmup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [03:55<?, ?it/s]\n"
     ]
    },
    {
     "ename": "Unsupported",
     "evalue": "'skip function sdpa_kernel in file /home/bcds/.conda/envs/llm/lib/python3.9/contextlib.py'\n\nfrom user code:\n   File \"/home/bcds/venv/dilab/floe/hqq/hqq/utils/generation_hf.py\", line 80, in custom_forward\n    with sdpa_kernel([SDPBackend.MATH]):\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupported\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Inference\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m########################################################################\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# #Using a custom generator\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m########################################################################\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#Using HF model.generate()\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhqq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneration_hf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m patch_model_for_compiled_runtime\n\u001b[0;32m---> 12\u001b[0m \u001b[43mpatch_model_for_compiled_runtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Prompt\u001b[39;00m\n\u001b[1;32m     15\u001b[0m system_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;66;03m#\"You are a helpful assistant.\"\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/dilab/floe/hqq/hqq/utils/generation_hf.py:91\u001b[0m, in \u001b[0;36mpatch_model_for_compiled_runtime\u001b[0;34m(model, tokenizer, warmup, max_new_tokens, patch_accelerate)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m tqdm(WARMUP_PROMPTS):\n\u001b[0;32m---> 91\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_implementation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstatic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/transformers/generation/utils.py:2252\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2244\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2245\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2246\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2247\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2248\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2249\u001b[0m     )\n\u001b[1;32m   2251\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2252\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2253\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2262\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2265\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2266\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2272\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/transformers/generation/utils.py:3254\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3252\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3254\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3256\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3257\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3258\u001b[0m     outputs,\n\u001b[1;32m   3259\u001b[0m     model_kwargs,\n\u001b[1;32m   3260\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3261\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:465\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m saved_dynamic_layer_stack_depth \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    461\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mget_dynamic_layer_stack_depth()\n\u001b[1;32m    462\u001b[0m )\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[1;32m    468\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mpop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[1;32m    469\u001b[0m         saved_dynamic_layer_stack_depth\n\u001b[1;32m    470\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:1269\u001b[0m, in \u001b[0;36mCatchErrorsWrapper.__call__\u001b[0;34m(self, frame, cache_entry, frame_state)\u001b[0m\n\u001b[1;32m   1263\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m hijacked_callback(\n\u001b[1;32m   1264\u001b[0m                 frame, cache_entry, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks, frame_state\n\u001b[1;32m   1265\u001b[0m             )\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;66;03m# skip=1: skip this frame\u001b[39;00m\n\u001b[0;32m-> 1269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:526\u001b[0m, in \u001b[0;36mConvertFrameAssert.__call__\u001b[0;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m    510\u001b[0m compile_id \u001b[38;5;241m=\u001b[39m CompileId(frame_id, frame_compile_id)\n\u001b[1;32m    512\u001b[0m signpost_event(\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_convert_frame_assert._compile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m     },\n\u001b[1;32m    524\u001b[0m )\n\u001b[0;32m--> 526\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_globals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_locals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_builtins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_one_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompile_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompile_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:924\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip)\u001b[0m\n\u001b[1;32m    922\u001b[0m guarded_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 924\u001b[0m     guarded_code \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m guarded_code\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:666\u001b[0m, in \u001b[0;36m_compile.<locals>.compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_compile.compile_inner\u001b[39m\u001b[38;5;124m\"\u001b[39m, phase_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentire_frame_compile\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m CompileTimeInstructionCounter\u001b[38;5;241m.\u001b[39mrecord():\n\u001b[0;32m--> 666\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/_utils_internal.py:87\u001b[0m, in \u001b[0;36mcompile_time_strobelight_meta.<locals>.compile_time_strobelight_meta_inner.<locals>.wrapper_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m StrobelightCompileTimeProfiler\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StrobelightCompileTimeProfiler\u001b[38;5;241m.\u001b[39mprofile_compile_time(\n\u001b[1;32m     90\u001b[0m     function, phase_name, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     91\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:699\u001b[0m, in \u001b[0;36m_compile.<locals>._compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    697\u001b[0m CompileContext\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39mattempt \u001b[38;5;241m=\u001b[39m attempt\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 699\u001b[0m     out_code \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_code_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mRestartAnalysis \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/_dynamo/bytecode_transformation.py:1322\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m   1319\u001b[0m instructions \u001b[38;5;241m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m   1320\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m-> 1322\u001b[0m \u001b[43mtransformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:219\u001b[0m, in \u001b[0;36mpreserve_global_state.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m exit_stack\u001b[38;5;241m.\u001b[39menter_context(\n\u001b[1;32m    216\u001b[0m     torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39m_symbolic_trace\u001b[38;5;241m.\u001b[39m_maybe_revert_all_patches()\n\u001b[1;32m    217\u001b[0m )\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     cleanup\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:634\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracing(tracer\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mtracing_context), tracer\u001b[38;5;241m.\u001b[39mset_current_tx():\n\u001b[0;32m--> 634\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mUnspecializeRestartAnalysis:\n\u001b[1;32m    636\u001b[0m     speculation_log\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:2796\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 2796\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:983\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 983\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    984\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:895\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 895\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:582\u001b[0m, in \u001b[0;36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_graph_break(\u001b[38;5;28mself\u001b[39m, inst, speculation\u001b[38;5;241m.\u001b[39mreason)\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m excp:\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneric_context_manager_depth \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    585\u001b[0m         \u001b[38;5;66;03m# We don't support graph break under GenericContextWrappingVariable,\u001b[39;00m\n\u001b[1;32m    586\u001b[0m         \u001b[38;5;66;03m# If there is, we roll back to the checkpoint and fall back.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:1602\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.CALL_FUNCTION\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1600\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpopn(inst\u001b[38;5;241m.\u001b[39margval)\n\u001b[1;32m   1601\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m-> 1602\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:830\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.call_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inner_fn \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(inner_fn) \u001b[38;5;129;01mand\u001b[39;00m is_forbidden(inner_fn):\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt to trace forbidden callable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minner_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 830\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush(\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/_dynamo/variables/functions.py:727\u001b[0m, in \u001b[0;36mSkipFunctionVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mwarn_once(msg)\n\u001b[1;32m    726\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreason \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 727\u001b[0m \u001b[43munimplemented\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/_dynamo/exc.py:297\u001b[0m, in \u001b[0;36munimplemented\u001b[0;34m(msg, from_exc, case_name)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _NOTHING:\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Unsupported(msg, case_name\u001b[38;5;241m=\u001b[39mcase_name) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfrom_exc\u001b[39;00m\n\u001b[0;32m--> 297\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Unsupported(msg, case_name\u001b[38;5;241m=\u001b[39mcase_name)\n",
      "\u001b[0;31mUnsupported\u001b[0m: 'skip function sdpa_kernel in file /home/bcds/.conda/envs/llm/lib/python3.9/contextlib.py'\n\nfrom user code:\n   File \"/home/bcds/venv/dilab/floe/hqq/hqq/utils/generation_hf.py\", line 80, in custom_forward\n    with sdpa_kernel([SDPBackend.MATH]):\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "system_prompt = None #\"You are a helpful assistant.\"\n",
    "prompt        = \"Write an essay about large language models.\"\n",
    "\n",
    "messages  = [] if(system_prompt is None) else [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "messages += [{\"role\": \"user\", \"content\": prompt},]\n",
    "\n",
    "inputs = tokenizer([tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)],return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate\n",
    "outputs = model.generate(**inputs, max_new_tokens=32, cache_implementation=\"static\", pad_token_id=tokenizer.pad_token_id) \n",
    "print(tokenizer.decode(outputs[0]))\n",
    "\n",
    "########################################################################\n",
    "#Save gemlite cache\n",
    "if(backend == 'gemlite'):\n",
    "\tgemlite.core.GemLiteLinear.cache_config('/tmp/gemlite_config.json') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/torch/cuda/graphs.py:84: UserWarning: The CUDA Graph is empty. This usually means that the graph was attempted to be captured on wrong device or stream. (Triggered internally at ../aten/src/ATen/cuda/CUDAGraph.cpp:208.)\n",
      "  super().capture_end()\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 1\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 32\n",
    "device_id = 0\n",
    "test_samples = 1\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "generated_all, decode_time, prefill_time = 0, 0, 0\n",
    "# print(\"max output length is {}\".format(output_length))\n",
    "text = \"The future of AI is here, and \"\n",
    "\n",
    "inputs = preprocess_data(text, tokenizer)\n",
    "\n",
    "# 前向传播\n",
    "for i in range(2):\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(device_id),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(device_id),\n",
    "            max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.pad_token_id, \n",
    "            # cache_implementation=\"static\" ## moe not support\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试一个正常输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The 2019-20 season will']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 4\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 10\n",
    "device_id = 0\n",
    "test_samples = 1\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "generated_all, decode_time, prefill_time = 0, 0, 0\n",
    "# print(\"max output length is {}\".format(output_length))\n",
    "text = \"The future of AI is here, and \"\n",
    "\n",
    "inputs = preprocess_data(text, tokenizer)\n",
    "# # 测试时间\n",
    "# start_event = torch.cuda.Event(enable_timing=True)\n",
    "# end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "# 开始计时\n",
    "# torch.cuda.synchronize()\n",
    "# start_event.record()\n",
    "\n",
    "# 前向传播\n",
    "with torch.no_grad():\n",
    "    output = llm.generate(\n",
    "        input_ids=inputs[\"input_ids\"].cuda(device_id),\n",
    "        attention_mask=inputs[\"attention_mask\"].cuda(device_id),\n",
    "        max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "        generation_config=GenerationConfig(do_sample=False),\n",
    "        pad_token_id=tokenizer.pad_token_id, \n",
    "        # cache_implementation=\"static\" ## moe not support\n",
    "    )\n",
    "\n",
    "# 结束计时\n",
    "# end_event.record()\n",
    "# torch.cuda.synchronize()\n",
    "\n",
    "# 计算时间\n",
    "# elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "# decode_time += elapsed_time\n",
    "# print(f\"Generated length: {len(output[0]) - input_length}\", f\"Time taken: {elapsed_time:.2f} s\")\n",
    "# print(output)\n",
    "print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "\n",
    "# generated_all += (len(output[0]) - input_length -1)\n",
    "\n",
    "# timepertoken = (decode_time) / (generated_all)\n",
    "# print(\"decode phase speed:\", '{:.4f}'.format(1/timepertoken) , ' token/s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/torch/cuda/graphs.py:84: UserWarning: The CUDA Graph is empty. This usually means that the graph was attempted to be captured on wrong device or stream. (Triggered internally at ../aten/src/ATen/cuda/CUDAGraph.cpp:208.)\n",
      "  super().capture_end()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                  Torch-Compiled Region        77.45%        2.833s        78.41%        2.869s       5.603ms       3.038ms         5.41%      16.160ms      31.563us           512  \n",
      "                       Scheduler.codegen (dynamo_timed)         9.00%     329.124ms         9.00%     329.124ms     164.562ms       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                        compile_fx_inner (dynamo_timed)         2.89%     105.613ms        12.73%     465.645ms     232.822ms       0.000us         0.00%       0.000us       0.000us             2  \n",
      "    compile_fx.<locals>.fw_compiler_base (dynamo_timed)         1.76%      64.536ms        15.10%     552.321ms     276.160ms       0.000us         0.00%      91.675us      45.838us             2  \n",
      "                  _compile.compile_inner (dynamo_timed)         1.50%      55.000ms        17.91%     655.166ms     327.583ms       0.000us         0.00%      91.675us      45.838us             2  \n",
      "                                       cudaLaunchKernel         0.78%      28.426ms         0.78%      28.426ms       3.535us       0.000us         0.00%       0.000us       0.000us          8041  \n",
      "          create_aot_dispatcher_function (dynamo_timed)         0.44%      16.238ms        16.04%     587.012ms     293.506ms       0.000us         0.00%      91.675us      45.838us             2  \n",
      "                                            aten::empty         0.39%      14.257ms         0.45%      16.485ms      15.493us       0.000us         0.00%       0.000us       0.000us          1064  \n",
      "                                               aten::mm         0.36%      12.993ms         0.49%      18.000ms      13.373us      17.978ms        32.02%      17.978ms      13.357us          1346  \n",
      "                                          aten::nonzero         0.33%      11.984ms         0.95%      34.775ms      67.919us       5.817ms        10.36%       7.343ms      14.343us           512  \n",
      "                                        cudaMemcpyAsync         0.32%      11.884ms         0.32%      11.884ms       5.286us       0.000us         0.00%       0.000us       0.000us          2248  \n",
      "         GraphLowering.compile_to_module (dynamo_timed)         0.31%      11.186ms         9.40%     343.756ms     171.878ms       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                       aten::lift_fresh         0.30%      10.802ms         0.34%      12.459ms     120.956us       0.000us         0.00%      53.788us       0.522us           103  \n",
      "                                             aten::view         0.27%       9.847ms         0.32%      11.656ms       3.769us       0.000us         0.00%       0.000us       0.000us          3093  \n",
      "                           cudaStreamCreateWithPriority         0.24%       8.814ms         0.24%       8.814ms      68.859us       0.000us         0.00%       0.000us       0.000us           128  \n",
      "                                            aten::index         0.21%       7.674ms         0.34%      12.355ms      10.697us       1.790ms         3.19%       1.790ms       1.550us          1155  \n",
      "                                            aten::copy_         0.20%       7.424ms         0.55%      20.299ms       6.353us      14.214ms        25.31%      14.214ms       4.449us          3195  \n",
      "          OutputGraph.call_user_compiler (dynamo_timed)         0.20%       7.312ms        16.25%     594.639ms     297.320ms       0.000us         0.00%      91.675us      45.838us             2  \n",
      "                               TorchDynamo Cache Lookup         0.18%       6.544ms         0.18%       6.544ms      12.707us       0.000us         0.00%       0.000us       0.000us           515  \n",
      "                                              aten::mul         0.17%       6.337ms         0.24%       8.909ms       5.770us       2.646ms         4.71%       2.646ms       1.714us          1544  \n",
      "                       GraphLowering.run (dynamo_timed)         0.17%       6.324ms         0.25%       9.117ms       4.558ms       0.000us         0.00%       0.000us       0.000us             2  \n",
      "gemlite::gemv_revsplitK_A16fWnO16f_int32packing_forw...         0.14%       5.217ms         0.21%       7.543ms     471.422us      23.904us         0.04%      26.400us       1.650us            16  \n",
      "                                  cudaStreamSynchronize         0.14%       5.191ms         0.14%       5.191ms       9.489us       0.000us         0.00%       0.000us       0.000us           547  \n",
      "                                          aten::squeeze         0.14%       4.948ms         0.22%       8.091ms     505.698us       0.000us         0.00%       0.000us       0.000us            16  \n",
      "                                       aten::as_strided         0.13%       4.666ms         0.13%       4.731ms       0.598us       0.000us         0.00%       0.000us       0.000us          7910  \n",
      "                                    aten::empty_strided         0.12%       4.445ms         0.13%       4.737ms       5.777us       0.000us         0.00%       0.000us       0.000us           820  \n",
      "                                       aten::index_add_         0.08%       2.928ms         0.14%       5.024ms       9.812us       1.316ms         2.34%       1.316ms       2.570us           512  \n",
      "                                            aten::fill_         0.08%       2.879ms         0.19%       6.770ms       5.782us       2.598ms         4.63%       2.598ms       2.219us          1171  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.07%       2.408ms         0.07%       2.408ms       3.745us       0.000us         0.00%       0.000us       0.000us           643  \n",
      "                                              aten::add         0.06%       2.372ms         0.11%       3.971ms       8.708us       1.187ms         2.11%       1.187ms       2.602us           456  \n",
      "                                   aten::_foreach_copy_         0.06%       2.236ms         0.38%      14.042ms      27.534us       0.000us         0.00%      10.808ms      21.193us           510  \n",
      "                                            aten::slice         0.06%       2.182ms         0.11%       3.988ms       3.495us       0.000us         0.00%       0.000us       0.000us          1141  \n",
      "                 WrapperCodeGen.generate (dynamo_timed)         0.06%       2.091ms         0.06%       2.091ms       1.046ms       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                            aten::clone         0.05%       1.975ms         0.28%      10.148ms      13.978us       0.000us         0.00%       2.037ms       2.806us           726  \n",
      "                                             aten::silu         0.05%       1.957ms         0.07%       2.491ms       4.865us     422.659us         0.75%     422.659us       0.826us           512  \n",
      "                                                 detach         0.05%       1.942ms         0.07%       2.408ms      19.417us       0.000us         0.00%       0.000us       0.000us           124  \n",
      "                                        aten::unsqueeze         0.05%       1.833ms         0.06%       2.205ms       1.624us       0.000us         0.00%       0.000us       0.000us          1358  \n",
      "                                           aten::select         0.05%       1.758ms         0.06%       2.246ms       1.455us       0.000us         0.00%       0.000us       0.000us          1543  \n",
      "                                                aten::t         0.05%       1.737ms         0.10%       3.588ms       1.931us       0.000us         0.00%       0.000us       0.000us          1858  \n",
      "                                           aten::matmul         0.05%       1.694ms         0.62%      22.536ms      15.289us       0.000us         0.00%      18.392ms      12.477us          1474  \n",
      "                                          aten::reshape         0.05%       1.653ms         0.14%       5.170ms       1.749us       0.000us         0.00%     432.707us       0.146us          2955  \n",
      "                                        cudaGraphLaunch         0.04%       1.637ms         0.04%       1.637ms       3.211us       0.000us         0.00%       0.000us       0.000us           510  \n",
      "                                             cudaMalloc         0.04%       1.571ms         0.04%       1.571ms     392.761us       0.000us         0.00%       0.000us       0.000us             4  \n",
      "                                        aten::transpose         0.04%       1.449ms         0.06%       2.319ms       1.065us       0.000us         0.00%       0.000us       0.000us          2178  \n",
      "                                              aten::cat         0.04%       1.358ms         0.06%       2.092ms      10.674us     801.094us         1.43%     801.094us       4.087us           196  \n",
      "                      Scheduler.__init__ (dynamo_timed)         0.04%       1.355ms         0.04%       1.355ms     677.567us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                           aten::linear         0.04%       1.339ms         0.65%      23.727ms      17.628us       0.000us         0.00%      17.978ms      13.357us          1346  \n",
      "                                              aten::bmm         0.04%       1.333ms         0.05%       1.804ms      14.096us     413.192us         0.74%     413.192us       3.228us           128  \n",
      "                                    cudaLaunchKernelExC         0.03%       1.203ms         0.03%       1.203ms       4.663us       0.000us         0.00%       0.000us       0.000us           258  \n",
      "                                             aten::mean         0.03%       1.179ms         0.05%       1.666ms      12.812us     689.895us         1.23%     689.895us       5.307us           130  \n",
      "                                               aten::to         0.03%       1.145ms         0.18%       6.488ms       5.275us       0.000us         0.00%       1.344ms       1.093us          1230  \n",
      "                                            aten::zero_         0.03%       1.082ms         0.05%       1.910ms      13.643us       0.000us         0.00%     282.208us       2.016us           140  \n",
      "                                             aten::set_         0.03%       1.053ms         0.03%       1.053ms       1.854us       0.000us         0.00%       0.000us       0.000us           568  \n",
      "                                           aten::unbind         0.03%       1.015ms         0.06%       2.018ms       3.942us       0.000us         0.00%       0.000us       0.000us           512  \n",
      "                                              aten::pow         0.03%       1.001ms         0.04%       1.547ms      11.899us     311.625us         0.55%     311.625us       2.397us           130  \n",
      "                                         aten::randperm         0.03%     954.385us         0.04%       1.496ms     299.193us       0.000us         0.00%       0.000us       0.000us             5  \n",
      "                                             aten::topk         0.03%     923.914us         0.11%       3.897ms      60.886us     821.132us         1.46%     821.132us      12.830us            64  \n",
      "                                         aten::_to_copy         0.02%     910.569us         0.15%       5.343ms      12.751us       0.000us         0.00%       1.344ms       3.209us           419  \n",
      "                                  cudaFuncGetAttributes         0.02%     827.632us         0.02%     827.632us       2.570us       0.000us         0.00%       0.000us       0.000us           322  \n",
      "                                       aten::empty_like         0.02%     809.679us         0.07%       2.726ms       4.143us       0.000us         0.00%       0.000us       0.000us           658  \n",
      "                                         aten::_softmax         0.02%     721.926us         0.03%       1.185ms       9.257us     322.434us         0.57%     322.434us       2.519us           128  \n",
      "                                              aten::neg         0.02%     717.590us         0.03%       1.151ms       8.995us     457.483us         0.81%     457.483us       3.574us           128  \n",
      "                                        cudaMemsetAsync         0.02%     716.705us         0.02%     716.705us       5.513us       0.000us         0.00%       0.000us       0.000us           130  \n",
      "                                    aten::nonzero_numpy         0.02%     683.616us         1.02%      37.476ms      73.196us       0.000us         0.00%       7.343ms      14.343us           512  \n",
      "                                        aten::index_put         0.02%     660.927us         0.04%       1.438ms     479.172us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                            aten::rsqrt         0.02%     659.860us         0.03%       1.095ms       8.425us     321.533us         0.57%     321.533us       2.473us           130  \n",
      "                                              aten::sum         0.01%     519.726us         0.02%     753.354us      11.771us     277.191us         0.49%     277.191us       4.331us            64  \n",
      "                                  cudaStreamIsCapturing         0.01%     489.460us         0.01%     489.460us       0.463us       0.000us         0.00%       0.000us       0.000us          1057  \n",
      "                                          aten::resize_         0.01%     482.905us         0.01%     482.905us       0.932us       0.000us         0.00%       0.000us       0.000us           518  \n",
      "                                              aten::div         0.01%     473.122us         0.02%     722.343us      11.287us     153.260us         0.27%     153.260us       2.395us            64  \n",
      "                                         aten::scatter_         0.01%     465.536us         0.02%     741.771us      11.590us     188.967us         0.34%     188.967us       2.953us            64  \n",
      "                                           aten::detach         0.01%     408.802us         0.08%       3.046ms       6.846us       0.000us         0.00%       0.000us       0.000us           445  \n",
      "                                           aten::expand         0.01%     404.391us         0.01%     509.544us       1.320us       0.000us         0.00%       0.000us       0.000us           386  \n",
      "                                        aten::new_empty         0.01%     375.381us         0.01%     436.146us     109.036us       0.000us         0.00%       0.000us       0.000us             4  \n",
      "                                               cudaFree         0.01%     374.886us         0.01%     374.886us     374.886us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                       aten::contiguous         0.01%     373.304us         0.19%       7.131ms      13.928us       0.000us         0.00%       1.526ms       2.981us           512  \n",
      "                                          aten::detach_         0.01%     372.704us         0.01%     397.512us       7.500us       0.000us         0.00%       0.000us       0.000us            53  \n",
      "                            cudagraphify (dynamo_timed)         0.01%     369.909us         0.01%     442.475us     221.238us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                          cudaGraphInstantiateWithFlags         0.01%     352.119us         0.01%     352.119us     117.373us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                             aten::div_         0.01%     350.962us         0.02%     578.069us       9.032us     185.959us         0.33%     185.959us       2.906us            64  \n",
      "                                 cudaDeviceGetAttribute         0.01%     315.687us         0.01%     315.687us       0.409us       0.000us         0.00%       0.000us       0.000us           771  \n",
      "                                          aten::softmax         0.01%     307.454us         0.04%       1.492ms      11.659us       0.000us         0.00%     322.434us       2.519us           128  \n",
      "                                            aten::where         0.01%     304.299us         1.03%      37.781ms      73.790us       0.000us         0.00%       7.343ms      14.343us           512  \n",
      "                                    cudaPeekAtLastError         0.01%     298.892us         0.01%     298.892us       0.097us       0.000us         0.00%       0.000us       0.000us          3084  \n",
      "                                     aten::_unsafe_view         0.01%     288.119us         0.01%     288.119us       0.561us       0.000us         0.00%       0.000us       0.000us           514  \n",
      "                                   cudaFuncSetAttribute         0.01%     279.361us         0.01%     279.361us       0.539us       0.000us         0.00%       0.000us       0.000us           518  \n",
      "                      aten::_debug_has_internal_overlap         0.01%     273.946us         0.01%     273.946us      34.243us       0.000us         0.00%       0.000us       0.000us             8  \n",
      "                                  cudaDeviceSynchronize         0.01%     262.532us         0.01%     262.532us      17.502us       0.000us         0.00%       0.000us       0.000us            15  \n",
      "                                          aten::one_hot         0.01%     196.297us         0.05%       1.733ms      27.077us       0.000us         0.00%     328.167us       5.128us            64  \n",
      "                                            aten::zeros         0.01%     189.752us         0.04%       1.515ms      11.834us       0.000us         0.00%     279.712us       2.185us           128  \n",
      "                       mkldnn::_is_mkldnn_acl_supported         0.00%     182.113us         0.00%     182.113us     182.113us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                         prims::view_of         0.00%     156.576us         0.01%     225.995us      28.249us       0.000us         0.00%       0.000us       0.000us             8  \n",
      "                                        aten::index_add         0.00%     133.691us         0.02%     900.486us     900.486us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                          aten::permute         0.00%     127.257us         0.00%     146.265us       2.285us       0.000us         0.00%       0.000us       0.000us            64  \n",
      "                                      prims::as_strided         0.00%      93.462us         0.00%     125.717us      41.906us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                               aten::eq         0.00%      82.151us         0.00%     152.640us      16.960us      21.217us         0.04%      21.217us       2.357us             9  \n",
      "                                        cudaEventRecord         0.00%      74.562us         0.00%      74.562us       6.778us       0.000us         0.00%       0.000us       0.000us            11  \n",
      "                                   cudaDriverGetVersion         0.00%      57.249us         0.00%      57.249us       0.112us       0.000us         0.00%       0.000us       0.000us           513  \n",
      "                                      aten::result_type         0.00%      56.902us         0.00%      56.902us       0.438us       0.000us         0.00%       0.000us       0.000us           130  \n",
      "                                           aten::cumsum         0.00%      56.606us         0.00%     108.757us      36.252us      15.617us         0.03%      15.617us       5.206us             3  \n",
      "                                             aten::isin         0.00%      53.090us         0.01%     267.085us      89.028us       0.000us         0.00%      28.031us       9.344us             3  \n",
      "                                              aten::sub         0.00%      52.595us         0.00%      83.402us      11.915us      16.545us         0.03%      16.545us       2.364us             7  \n",
      "                                   aten::_reshape_alias         0.00%      50.877us         0.00%      50.877us       1.590us       0.000us         0.00%       0.000us       0.000us            32  \n",
      "                                            aten::alias         0.00%      46.801us         0.01%     229.934us      57.483us       0.000us         0.00%       0.000us       0.000us             4  \n",
      "                                         prims::squeeze         0.00%      44.957us         0.00%      51.023us      51.023us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                     aten::masked_fill_         0.00%      40.133us         0.00%      71.564us      17.891us       9.439us         0.02%       9.439us       2.360us             4  \n",
      "                                       aten::bitwise_or         0.00%      33.599us         0.00%      49.808us      12.452us       9.729us         0.02%       9.729us       2.432us             4  \n",
      "                                   aten::empty_permuted         0.00%      33.041us         0.00%      41.817us      41.817us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                              aten::any         0.00%      33.013us         0.00%      90.323us      18.065us       0.000us         0.00%      11.871us       2.374us             5  \n",
      "                                        aten::embedding         0.00%      32.644us         0.00%      94.576us      47.288us       0.000us         0.00%       7.936us       3.968us             2  \n",
      "                                           aten::arange         0.00%      30.730us         0.00%     114.320us      14.290us       8.704us         0.02%      17.408us       2.176us             8  \n",
      "                                     aten::index_select         0.00%      30.461us         0.00%      58.228us      29.114us       7.936us         0.01%       7.936us       3.968us             2  \n",
      "                              aten::_local_scalar_dense         0.00%      28.295us         0.00%     114.979us      19.163us      13.376us         0.02%      13.376us       2.229us             6  \n",
      "                                                detach_         0.00%      24.808us         0.00%      24.808us       0.468us       0.000us         0.00%       0.000us       0.000us            53  \n",
      "                                              aten::max         0.00%      21.988us         0.00%      40.802us      20.401us      10.017us         0.02%      10.017us       5.008us             2  \n",
      "                                               aten::ge         0.00%      21.861us         0.00%      32.674us      16.337us       4.512us         0.01%       4.512us       2.256us             2  \n",
      "                                           aten::argmax         0.00%      21.826us         0.00%      36.740us      18.370us      24.417us         0.04%      24.417us      12.208us             2  \n",
      "                                          aten::dropout         0.00%      21.357us         0.00%      21.357us       0.334us       0.000us         0.00%       0.000us       0.000us            64  \n",
      "                                               aten::gt         0.00%      21.223us         0.00%      37.277us      18.638us       5.728us         0.01%       5.728us       2.864us             2  \n",
      "                                      aten::bitwise_or_         0.00%      19.328us         0.00%      30.537us      15.268us       5.280us         0.01%       5.280us       2.640us             2  \n",
      "                                    cudaStreamWaitEvent         0.00%      17.347us         0.00%      17.347us       1.577us       0.000us         0.00%       0.000us       0.000us            11  \n",
      "                                               aten::lt         0.00%      16.494us         0.00%      27.216us      27.216us       2.272us         0.00%       2.272us       2.272us             1  \n",
      "                                             aten::mul_         0.00%      16.095us         0.00%      30.868us      15.434us       5.664us         0.01%       5.664us       2.832us             2  \n",
      "                                      aten::bitwise_not         0.00%      15.908us         0.00%      26.122us      13.061us       4.672us         0.01%       4.672us       2.336us             2  \n",
      "                                      aten::bitwise_and         0.00%      14.886us         0.00%      25.816us      12.908us       5.662us         0.01%       5.662us       2.831us             2  \n",
      "                                 cudaStreamBeginCapture         0.00%      12.850us         0.00%      12.850us       4.283us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                             aten::rsub         0.00%      11.944us         0.00%      37.540us      18.770us       0.000us         0.00%       4.832us       2.416us             2  \n",
      "                                         cuLaunchKernel         0.00%      11.775us         0.00%      11.775us       5.888us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                               aten::le         0.00%      11.495us         0.00%      17.313us       8.657us       5.440us         0.01%       5.440us       2.720us             2  \n",
      "                                             aten::full         0.00%      11.259us         0.00%      71.189us      11.865us       0.000us         0.00%      13.023us       2.170us             6  \n",
      "                                      aten::masked_fill         0.00%       9.541us         0.00%      56.020us      28.010us       0.000us         0.00%       9.185us       4.592us             2  \n",
      "                                       aten::is_nonzero         0.00%       9.263us         0.00%     132.830us      22.138us       0.000us         0.00%      13.376us       2.229us             6  \n",
      "                                             aten::item         0.00%       8.588us         0.00%     123.567us      20.594us       0.000us         0.00%      13.376us       2.229us             6  \n",
      "                                         aten::new_ones         0.00%       8.299us         0.00%      31.305us      15.653us       0.000us         0.00%       5.151us       2.576us             2  \n",
      "                                       cudaGraphDestroy         0.00%       7.625us         0.00%       7.625us       2.542us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                   cudaStreamEndCapture         0.00%       7.304us         0.00%       7.304us       2.435us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                           aten::__or__         0.00%       5.989us         0.00%      55.797us      13.949us       0.000us         0.00%       9.729us       2.432us             4  \n",
      "                                      cudaGraphGetNodes         0.00%       5.755us         0.00%       5.755us       1.918us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                          aten::__and__         0.00%       5.052us         0.00%      30.868us      15.434us       0.000us         0.00%       5.662us       2.831us             2  \n",
      "                                             aten::ones         0.00%       4.493us         0.00%      21.156us      21.156us       0.000us         0.00%       2.272us       2.272us             1  \n",
      "                                          aten::view_as         0.00%       3.998us         0.00%       5.905us       1.181us       0.000us         0.00%       0.000us       0.000us             5  \n",
      "                               cudaStreamGetCaptureInfo         0.00%       3.598us         0.00%       3.598us       0.900us       0.000us         0.00%       0.000us       0.000us             4  \n",
      "                                        aten::ones_like         0.00%       2.326us         0.00%      13.273us      13.273us       0.000us         0.00%       2.048us       2.048us             1  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      46.592us         0.08%      46.592us       1.607us            29  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      14.912us         0.03%      14.912us       2.130us             7  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.272us         0.00%       2.272us       2.272us             1  \n",
      "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      10.910ms        19.43%      10.910ms       6.414us          1701  \n",
      "                         Memcpy DtoH (Device -> Pinned)         0.00%       0.000us         0.00%       0.000us       0.000us       1.216ms         2.17%       1.216ms       2.348us           518  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       6.784us         0.01%       6.784us       2.261us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.436ms         4.34%       2.436ms       2.227us          1094  \n",
      "void at_cuda_detail::cub::DeviceScanInitKernel<at_cu...         0.00%       0.000us         0.00%       0.000us       0.000us       6.977us         0.01%       6.977us       2.326us             3  \n",
      "void at_cuda_detail::cub::DeviceScanKernel<at_cuda_d...         0.00%       0.000us         0.00%       0.000us       0.000us       8.640us         0.02%       8.640us       2.880us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      16.865us         0.03%      16.865us       2.409us             7  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       9.280us         0.02%       9.280us       2.320us             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.735us         0.01%       4.735us       2.368us             2  \n",
      "void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us       7.936us         0.01%       7.936us       3.968us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     350.274us         0.62%     350.274us       1.806us           194  \n",
      "void (anonymous namespace)::elementwise_kernel_with_...         0.00%       0.000us         0.00%       0.000us       0.000us       8.704us         0.02%       8.704us       2.176us             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.120us         0.01%       5.120us       2.560us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      15.009us         0.03%      15.009us       2.501us             6  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       5.664us         0.01%       5.664us       2.832us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       5.441us         0.01%       5.441us       2.720us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.801us         0.01%       4.801us       2.401us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.704us         0.01%       4.704us       2.352us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     472.551us         0.84%     472.551us       3.580us           132  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     311.625us         0.55%     311.625us       2.397us           130  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     689.895us         1.23%     689.895us       5.307us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     323.679us         0.58%     323.679us       2.490us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     321.533us         0.57%     321.533us       2.473us           130  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     437.705us         0.78%     437.705us       3.367us           130  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     825.264us         1.47%     825.264us       3.199us           258  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     696.035us         1.24%     696.035us       2.698us           258  \n",
      "sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x...         0.00%       0.000us         0.00%       0.000us       0.000us       2.546ms         4.53%       2.546ms      19.889us           128  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       1.067ms         1.90%       1.067ms       8.334us           128  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.786ms         3.18%       1.786ms       4.652us           384  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       1.502ms         2.67%       1.502ms       3.910us           384  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     457.483us         0.81%     457.483us       3.574us           128  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     561.482us         1.00%     561.482us       4.387us           128  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     646.152us         1.15%     646.152us       2.524us           256  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     432.707us         0.77%     432.707us       3.381us           128  \n",
      "void gemv2T_kernel_val<int, int, __half, __half, __h...         0.00%       0.000us         0.00%       0.000us       0.000us     207.843us         0.37%     207.843us       3.248us            64  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     153.260us         0.27%     153.260us       2.395us            64  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     200.678us         0.36%     200.678us       3.136us            64  \n",
      "void (anonymous namespace)::softmax_warp_forward<c10...         0.00%       0.000us         0.00%       0.000us       0.000us      80.227us         0.14%      80.227us       2.507us            32  \n",
      "void gemmk1_kernel<int, float, 256, 5, false, false,...         0.00%       0.000us         0.00%       0.000us       0.000us      93.955us         0.17%      93.955us       2.936us            32  \n",
      "void dot_kernel<float, 128, 0, cublasDotParams<cubla...         0.00%       0.000us         0.00%       0.000us       0.000us     189.856us         0.34%     189.856us       2.966us            64  \n",
      "void reduce_1Block_kernel<float, 128, 7, cublasGemvT...         0.00%       0.000us         0.00%       0.000us       0.000us     173.312us         0.31%     173.312us       2.708us            64  \n",
      "void (anonymous namespace)::softmax_warp_forward<c10...         0.00%       0.000us         0.00%       0.000us       0.000us     161.632us         0.29%     161.632us       2.525us            64  \n",
      "void at::native::sbtopk::gatherTopK<float, unsigned ...         0.00%       0.000us         0.00%       0.000us       0.000us     449.994us         0.80%     449.994us       7.031us            64  \n",
      "void at::native::bitonicSortKVInPlace<-2, -1, 16, 16...         0.00%       0.000us         0.00%       0.000us       0.000us     371.138us         0.66%     371.138us       5.799us            64  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     277.191us         0.49%     277.191us       4.331us            64  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     185.959us         0.33%     185.959us       2.906us            64  \n",
      "void at::native::_scatter_gather_elementwise_kernel<...         0.00%       0.000us         0.00%       0.000us       0.000us     188.967us         0.34%     188.967us       2.953us            64  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       1.526ms         2.72%       1.526ms       2.981us           512  \n",
      "void at_cuda_detail::cub::DeviceReduceSingleTileKern...         0.00%       0.000us         0.00%       0.000us       0.000us       1.376ms         2.45%       1.376ms       2.687us           512  \n",
      "void at_cuda_detail::cub::DeviceCompactInitKernel<at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.251ms         2.23%       1.251ms       2.443us           512  \n",
      "void at_cuda_detail::cub::DeviceSelectSweepKernel<at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.452ms         2.59%       1.452ms       2.837us           512  \n",
      "    compile_fx.<locals>.fw_compiler_base (dynamo_timed)         0.00%       0.000us         0.00%       0.000us       0.000us      51.930ms        92.48%      51.930ms      51.930ms             1  \n",
      "void at::native::indexFuncSmallIndex<c10::Half, long...         0.00%       0.000us         0.00%       0.000us       0.000us       1.316ms         2.34%       1.316ms       2.570us           512  \n",
      "void at::native::(anonymous namespace)::write_indice...         0.00%       0.000us         0.00%       0.000us       0.000us     535.439us         0.95%     535.439us       4.183us           128  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us     204.525us         0.36%     204.525us       1.573us           130  \n",
      "sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x...         0.00%       0.000us         0.00%       0.000us       0.000us       6.753ms        12.03%       6.753ms      51.947us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     422.659us         0.75%     422.659us       3.302us           128  \n",
      "          gemv_revsplitK_A16fWnO16f_int32packing_kernel         0.00%       0.000us         0.00%       0.000us       0.000us       2.859ms         5.09%       2.859ms      22.338us           128  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_...         0.00%       0.000us         0.00%       0.000us       0.000us       6.578ms        11.72%       6.578ms      51.393us           128  \n",
      "void splitKreduce_kernel<32, 16, int, float, __half,...         0.00%       0.000us         0.00%       0.000us       0.000us     466.759us         0.83%     466.759us       3.647us           128  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us      13.215us         0.02%      13.215us       3.304us             4  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      24.417us         0.04%      24.417us      12.208us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      10.751us         0.02%      10.751us       2.688us             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.832us         0.01%       4.832us       2.416us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.504us         0.01%       5.504us       2.752us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       2.400us         0.00%       2.400us       2.400us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.672us         0.01%       4.672us       2.336us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       5.662us         0.01%       5.662us       2.831us             2  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      10.017us         0.02%      10.017us       5.008us             2  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.968us         0.01%       3.968us       3.968us             1  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       6.048us         0.01%       6.048us       3.024us             2  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     226.397us         0.40%     226.397us       3.537us            64  \n",
      "void (anonymous namespace)::softmax_warp_forward<c10...         0.00%       0.000us         0.00%       0.000us       0.000us      80.575us         0.14%      80.575us       2.518us            32  \n",
      "void gemv2N_kernel<int, int, __half, __half, __half,...         0.00%       0.000us         0.00%       0.000us       0.000us     111.394us         0.20%     111.394us       3.481us            32  \n",
      "void at::native::vectorized_elementwise_kernel<2, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.464us         0.00%       2.464us       2.464us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 3.659s\n",
      "Self CUDA time total: 56.151ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 1\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 2\n",
    "test_samples = 4\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "fineweb = load_dataset(\"parquet\",data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text'][:test_samples] \n",
    "\n",
    "print(\"output length is {}\".format(output_length))\n",
    "text = fineweb_text[0]\n",
    "inputs = preprocess_data(text, tokenizer)\n",
    "\n",
    "# cached_mlp.clear_load_from_cpu_stats()\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ]\n",
    ") as p:\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(),\n",
    "            max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "print(p.key_averages().table(\n",
    "    sort_by=\"self_cpu_time_total\", row_limit=-1))\n",
    "p.export_chrome_trace(\"./compile.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
