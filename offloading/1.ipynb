{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先都加载到cpu上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lz/miniconda3/envs/hqq/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using atten... sdpa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/lz/hqq-master/hqq/models/base.py:251: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(cls.get_weight_file(save_dir), map_location=map_location)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5,1,2\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"\n",
    "from modeling_mixtral import MixtralForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import json\n",
    "### HQQ量化\n",
    "from hqq.core.quantize import *\n",
    "from hqq.models.hf.mixtral import MixtralPatch\n",
    "import transformers\n",
    "from hqq.models.base import BaseHQQModel\n",
    "from accelerate import init_empty_weights\n",
    "import gemlite\n",
    "from gemlite import GemLiteLinearTriton \n",
    "GemLiteLinearTriton.get_default_gemv = lambda *args, **kwargs: 'GEMV'\n",
    "\n",
    "class BaseHQQHFModel(BaseHQQModel):\n",
    "    # Save model architecture\n",
    "    @classmethod\n",
    "    def cache_model(cls, model, save_dir):\n",
    "        model.config.save_pretrained(save_dir)\n",
    "\n",
    "    # Create empty model from config\n",
    "    @classmethod\n",
    "    def create_model(cls, save_dir, kwargs):\n",
    "        model_kwargs = {}\n",
    "        for key in [\"attn_implementation\"]:\n",
    "            if key in kwargs:\n",
    "                model_kwargs[key] = kwargs[key]\n",
    "\n",
    "        config = transformers.AutoConfig.from_pretrained(\n",
    "            cls.get_config_file(save_dir)\n",
    "        )\n",
    "\n",
    "        with init_empty_weights():\n",
    "            model = MixtralForCausalLM._from_config(config, **model_kwargs)\n",
    "\n",
    "        return model\n",
    "\n",
    "class MixtralHQQ(MixtralPatch, BaseHQQHFModel):\n",
    "    pass\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    threshold_path = path['chess_up_threshold']\n",
    "\n",
    "save_dir = './hqqsaved'\n",
    "dtype = torch.float16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "### 从保存的权重中加载\n",
    "llm = MixtralHQQ.from_quantized(save_dir, compute_dtype=dtype, device='cpu')\n",
    "HQQLinear.set_backend(HQQBackend.PYTORCH)\n",
    "\n",
    "backend       = \"gemlite\" #'torchao_int4' #\"torchao_int4\" (4-bit only) or \"gemlite\" (4-bit + 2-bit)\n",
    "# #Optimize\n",
    "from hqq.utils.patching import prepare_for_inference\n",
    "prepare_for_inference(llm, backend=backend, verbose=True)\n",
    "\n",
    "if(backend == 'gemlite'):\n",
    "\tgemlite.core.GEMLITE_TRITON_RESTRICT_M = True\n",
    "\tgemlite.core.GemLiteLinear.load_config('/tmp/gemlite_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/lz/On-the-Fly_MoE_Inference/offloading/convert.py:160: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  up_th = torch.load(threshold_path, map_location='cuda')[\"up_proj_states_thresholds\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thresholds loaded from /data2/lz/On-the-Fly_MoE_Inference/saving/threshold/c4_mixtral_up/thresholds_0_8.pt\n",
      "active neural num  2867\n",
      "active neural num  2867\n",
      "... loading layer 6 for pipelineLLM\n",
      "... loading layer 7 for pipelineLLM\n",
      "... loading layer 8 for pipelineLLM\n",
      "... loading layer 9 for pipelineLLM\n",
      "... loading layer 10 for pipelineLLM\n",
      "... loading layer 11 for pipelineLLM\n",
      "... loading layer 12 for pipelineLLM\n",
      "... loading layer 13 for pipelineLLM\n",
      "... loading layer 14 for pipelineLLM\n",
      "... loading layer 15 for pipelineLLM\n",
      "... loading layer 16 for pipelineLLM\n",
      "... loading layer 17 for pipelineLLM\n",
      "... loading layer 18 for pipelineLLM\n",
      "... loading layer 19 for pipelineLLM\n",
      "... loading layer 20 for pipelineLLM\n",
      "... loading layer 21 for pipelineLLM\n",
      "... loading layer 22 for pipelineLLM\n",
      "... loading layer 23 for pipelineLLM\n",
      "... loading layer 24 for pipelineLLM\n",
      "... loading layer 25 for pipelineLLM\n",
      "... loading layer 26 for pipelineLLM\n",
      "... loading layer 27 for pipelineLLM\n",
      "... loading layer 28 for pipelineLLM\n",
      "... loading layer 29 for pipelineLLM\n",
      "... loading layer 30 for pipelineLLM\n",
      "... loading layer 31 for pipelineLLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/lz/On-the-Fly_MoE_Inference/offloading/pipelinellm.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(f'../expert_predictor/training/{layer_idx}-{training_epoch}.pth'))\n"
     ]
    }
   ],
   "source": [
    "device_map = {layer_idx: 'cuda:1' if layer_idx <= 16 else 'cuda:2' for layer_idx in range(1, 32)}\n",
    "from convert import convert_mixtral_to_cached_mlp\n",
    "\n",
    "prefill_layers = 6  ### 固定在device上的MLP层数\n",
    "llm, cached_mlps = convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.8, backends=backend, \n",
    "                                                 device='cuda:0', device_map=device_map, threshold_path = threshold_path, prefill_layers=prefill_layers)\n",
    "\n",
    "device_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipelinellm import PipelineLLM\n",
    "# 创建流水线模型\n",
    "PLLM = PipelineLLM(llm, cached_mlps, 1, 3, training_epoch=20,\n",
    "                   device='cuda:0', device_map=device_map, prefill_layers=prefill_layers, print_layer_info=True) ### use ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7646, -0.2427,  0.2208,  ...,  0.5122,  0.3179, -0.2690]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.7651, -0.2432,  0.2219,  ...,  0.5132,  0.3186, -0.2698]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.4468,  0.2944,  0.5732,  ...,  0.7163, -0.2991,  0.4709]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.4451,  0.2954,  0.5718,  ...,  0.7173, -0.2983,  0.4714]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.3928, -0.1058, -0.3047,  ..., -0.3560,  0.7578, -0.2568]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.3928, -0.1052, -0.3044,  ..., -0.3567,  0.7588, -0.2529]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.2124,  0.4792,  0.1733,  ..., -0.4539,  0.1676,  0.5039]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.2140,  0.4805,  0.1744,  ..., -0.4539,  0.1653,  0.5039]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.2910,  0.6528,  0.2764,  ...,  0.5068, -0.4456, -0.4441]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.2922,  0.6484,  0.2769,  ...,  0.5059, -0.4463, -0.4478]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.0589,  0.2595,  0.2241,  ..., -0.0790, -0.4595,  0.4231]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.0587,  0.2556,  0.2217,  ..., -0.0778, -0.4585,  0.4250]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.2539,  0.1608,  0.3901,  ..., -0.1309, -0.2634,  0.3135]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.2561,  0.1598,  0.3882,  ..., -0.1304, -0.2642,  0.3132]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.2351,  0.4441, -0.2708,  ..., -0.9971,  0.8662,  0.0898]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.2356,  0.4441, -0.2695,  ..., -0.9932,  0.8667,  0.0926]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.2830,  0.5923,  0.4263,  ..., -0.1256,  0.6616, -0.2051]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.2854,  0.5933,  0.4292,  ..., -0.1284,  0.6621, -0.2035]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.0225,  0.1299, -0.1700,  ..., -0.2097,  0.3264, -0.6484]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.0262,  0.1305, -0.1726,  ..., -0.2108,  0.3276, -0.6528]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.7310, -0.6309,  0.2041,  ...,  0.1174, -0.5635, -0.3655]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.7319, -0.6289,  0.2043,  ...,  0.1194, -0.5620, -0.3682]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.1598,  0.2776, -0.1125,  ..., -0.0400,  0.5449,  0.4558]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.1586,  0.2764, -0.1157,  ..., -0.0390,  0.5405,  0.4541]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.1747, -0.2184, -0.2167,  ..., -0.6367, -0.0636, -0.3303]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.1750, -0.2190, -0.2150,  ..., -0.6406, -0.0641, -0.3276]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.3999,  0.4902, -0.2118,  ..., -0.6548, -0.3945,  0.4368]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.4001,  0.4858, -0.2122,  ..., -0.6577, -0.3948,  0.4395]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.0533, -0.2354, -0.8828,  ..., -0.1810, -0.2981, -0.3826]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.0540, -0.2338, -0.8867,  ..., -0.1807, -0.3000, -0.3879]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.7563, -0.2382, -0.5269,  ...,  0.0282, -0.0594, -0.2607]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.7568, -0.2393, -0.5264,  ...,  0.0258, -0.0597, -0.2620]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.2661,  0.2159, -0.8140,  ..., -0.4585,  0.5947,  0.5049]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.2661,  0.2197, -0.8188,  ..., -0.4580,  0.5977,  0.5078]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.2817, -0.3127, -0.3425,  ...,  0.1256, -0.0483,  0.1777]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.2832, -0.3145, -0.3440,  ...,  0.1260, -0.0472,  0.1790]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.1309,  0.1639,  0.7070,  ...,  0.1642,  0.7798, -0.7983]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.1284,  0.1625,  0.7061,  ...,  0.1648,  0.7773, -0.7969]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.2981,  0.3369,  0.6187,  ...,  0.2727, -0.1092,  0.5479]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.2971,  0.3372,  0.6182,  ...,  0.2739, -0.1086,  0.5498]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.3418,  0.6504,  0.2549,  ..., -0.3333,  0.5601, -0.0567]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.3408,  0.6519,  0.2549,  ..., -0.3325,  0.5591, -0.0569]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.5532, -0.9453,  0.3701,  ...,  0.0845, -0.9971, -0.4810]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.5508, -0.9395,  0.3679,  ...,  0.0833, -0.9897, -0.4824]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.0774,  0.4802,  0.3003,  ...,  0.6245, -0.1111, -0.5151]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.0775,  0.4766,  0.3032,  ...,  0.6211, -0.1122, -0.5142]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.0301, -0.2776,  0.0901,  ..., -0.0792,  0.6753,  0.3625]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.0291, -0.2761,  0.0890,  ..., -0.0806,  0.6748,  0.3667]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.2312,  0.0658, -0.5576,  ...,  0.5801, -0.3562, -0.5776]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.2332,  0.0651, -0.5615,  ...,  0.5820, -0.3569, -0.5771]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.7563, -0.3687, -0.6865,  ..., -0.0814, -0.1854,  0.0254]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.7510, -0.3691, -0.6846,  ..., -0.0820, -0.1843,  0.0265]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.4153, -0.4358, -0.6851,  ..., -0.1573,  0.3364,  1.1396]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.4207, -0.4336, -0.6904,  ..., -0.1564,  0.3350,  1.1396]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.2310,  0.7979, -0.2651,  ...,  0.1721, -0.6826,  0.7559]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.2357,  0.7983, -0.2678,  ...,  0.1711, -0.6826,  0.7554]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.0969, -0.3926,  0.0776,  ..., -0.1469,  0.2981, -0.8608]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.0977, -0.3914,  0.0787,  ..., -0.1482,  0.2991, -0.8589]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.1829,  0.2617,  0.0607,  ..., -0.1902, -0.1744, -0.3967]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.1823,  0.2617,  0.0604,  ..., -0.1904, -0.1753, -0.3975]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.1796,  0.3569, -0.3274,  ..., -0.1503,  0.3293,  0.1479]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.1820,  0.3586, -0.3289,  ..., -0.1450,  0.3337,  0.1493]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.3884, -0.0513,  0.3242,  ...,  0.1298,  0.0070, -0.9858]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.3911, -0.0463,  0.3286,  ...,  0.1289,  0.0026, -0.9839]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.3701, -0.7085, -0.1997,  ...,  0.4048, -0.0499, -1.0850]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.3721, -0.7056, -0.1958,  ...,  0.4033, -0.0478, -1.0850]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.6270,  0.4893,  1.0615,  ..., -0.5845,  0.9590, -0.5078]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.6274,  0.4905,  1.0596,  ..., -0.5889,  0.9668, -0.5054]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.0594,  0.2003,  0.0241,  ...,  0.7505, -0.0193, -0.9541]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.0582,  0.1990,  0.0216,  ...,  0.7529, -0.0190, -0.9502]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.0552, -0.2986,  0.5576,  ..., -0.3120,  0.3193,  0.0081]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.0533, -0.2974,  0.5562,  ..., -0.3145,  0.3176,  0.0077]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.3154,  0.2690,  0.1802,  ..., -1.0205, -0.3159, -0.7417]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.3159,  0.2666,  0.1807,  ..., -1.0146, -0.3201, -0.7480]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.2061,  0.1736,  0.6411,  ..., -0.0221,  0.3057,  0.4099]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.2080,  0.1737,  0.6445,  ..., -0.0202,  0.3027,  0.4153]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.2277, -0.1381,  0.3079,  ..., -0.2307,  0.0696, -0.3557]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.2257, -0.1410,  0.3030,  ..., -0.2249,  0.0698, -0.3550]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.0219, -0.4939,  0.6548,  ..., -0.8950,  0.7754, -0.5806]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.0225, -0.4905,  0.6562,  ..., -0.8970,  0.7744, -0.5840]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[0.3992, 0.2986, 0.2529,  ..., 0.6807, 0.2306, 0.3330]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[0.4026, 0.3008, 0.2524,  ..., 0.6836, 0.2308, 0.3325]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.3303, -0.3645,  0.2744,  ..., -0.6143,  0.0104,  0.0544]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.3352, -0.3679,  0.2747,  ..., -0.6050,  0.0078,  0.0676]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.9209, -0.0074, -0.1014,  ...,  0.0723, -0.3218,  0.6548]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.9150, -0.0052, -0.1000,  ...,  0.0737, -0.3145,  0.6543]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.4956,  0.5479,  0.0438,  ..., -0.3645, -0.5000, -1.0889]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.4995,  0.5459,  0.0415,  ..., -0.3625, -0.5029, -1.0801]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.4148, -0.0565,  0.0204,  ..., -0.6455, -0.2004, -0.6255]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.4167, -0.0472,  0.0296,  ..., -0.6484, -0.1974, -0.6235]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.9858,  0.2749,  0.4336,  ..., -0.2944,  0.2773,  0.2123]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.9834,  0.2810,  0.4336,  ..., -0.2952,  0.2734,  0.2063]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.5615, -0.5312, -0.1968,  ..., -0.3311,  0.6689,  0.5562]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.5552, -0.5312, -0.1932,  ..., -0.3298,  0.6631,  0.5547]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.0685,  0.3074,  0.3342,  ...,  0.4016, -0.5479, -0.2947]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.0791,  0.3064,  0.3411,  ...,  0.4043, -0.5439, -0.2917]],\n",
      "       device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "class CUDAGraphRunner():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.cuda_graph = None\n",
    "        self.graph_input = torch.zeros((1,4096), dtype=torch.float16, device=f'cuda:{device_id}')\n",
    "        self.graph_output = None\n",
    "    \n",
    "    def capture(self, x,):\n",
    "        assert self.cuda_graph is None\n",
    "        self.graph_input = self.graph_input.copy_(x).to(x.device)\n",
    "        self.cuda_graph = torch.cuda.CUDAGraph()\n",
    "        # self.cuda_graph.enable_debug_mode()\n",
    "        with torch.cuda.graph(self.cuda_graph):\n",
    "            self.graph_output = self.model(self.graph_input,)\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "    def forward(self, x,):\n",
    "        self.graph_input.copy_(x)\n",
    "        self.cuda_graph.replay()\n",
    "        return self.graph_output\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "    \n",
    "inp = torch.randn(1, 4096).half().cuda(device_id)\n",
    "graphs = [[None for _ in range(8)] for _ in range(prefill_layers)]\n",
    "# expert=llm.model.layers[0].block_sparse_moe.experts[1]\n",
    "# print(expert(inp))\n",
    "# graph_runner = CUDAGraphRunner(expert)\n",
    "# graph_runner.capture(inp)\n",
    "# graphs[0][1]=graph_runner\n",
    "# print(graphs[0][1](inp))\n",
    "for i in range(prefill_layers):\n",
    "    for j in range(8):\n",
    "        expert=llm.model.layers[i].block_sparse_moe.experts[j]\n",
    "        print(expert(inp))\n",
    "        graph_runner = CUDAGraphRunner(expert)\n",
    "        graph_runner.capture(inp)\n",
    "        graphs[i][j]=graph_runner\n",
    "        print(graphs[i][j](inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, prefill_layers):\n",
    "    for j in range(len(llm.model.layers[0].block_sparse_moe.experts)):\n",
    "        llm.model.layers[i].block_sparse_moe.experts[j].graph = graphs[i][j]\n",
    "        # graphs[i][j]\n",
    "    # torch.compile(llm.model.layers[i].block_sparse_moe.forward, fullgraph=False, mode='max-autotune-no-cudagraphs')\n",
    "        # graphs[i][j]\n",
    "        # torch.compile(llm.model.layers[i].block_sparse_moe.experts[j].kernel_forward, fullgraph=True, mode='max-autotune-no-cudagraphs')\n",
    "        # llm.model.layers[i].block_sparse_moe.experts[j].kernel_forward\n",
    "        # \\torch.compile(llm.model.layers[i].block_sparse_moe.experts[j].kernel_forward, fullgraph=True, mode=\"reduce-overhead\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((1,4096), device='cuda:0', dtype=torch.float16)\n",
    "print(\"None:\", llm.model.layers[0].block_sparse_moe.experts[1](x))\n",
    "llm.model.layers[0].block_sparse_moe.experts[1].graph = graphs[0][1]\n",
    "print(\"cuda:graph\", llm.model.layers[0].block_sparse_moe.experts[1](x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试时间开销"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warm up ...\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# import torch._dynamo.config\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 2\n",
    "MAX_LENGTH = 2\n",
    "output_length = 10\n",
    "test_samples = 1\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "filt_type = fineweb_path.split('.')[-1]\n",
    "fineweb = load_dataset(filt_type, data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text']\n",
    "\n",
    "print(\"warm up ...\")\n",
    "# 预热（避免第一次运行时的额外开销）\n",
    "for text in fineweb_text[:5]:\n",
    "    inputs = preprocess_data(text, tokenizer)\n",
    "    with torch.no_grad():\n",
    "        output = llm(input_ids=inputs[\"input_ids\"].cuda(device_id), attention_mask=inputs[\"attention_mask\"].cuda(device_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "import torch\n",
    "\n",
    "# 定义 input_length 和 output_length 的范围\n",
    "# input_length_range = [16]\n",
    "# output_length_range = [128,256]\n",
    "input_length_range = [16,32]  # 16到32\n",
    "output_length_range = [128,256,512,1024]  # 128到1024\n",
    "\n",
    "test_samples = 5\n",
    "device_id = 0\n",
    "\n",
    "def preprocess_data(data, tokenizer, max_length):\n",
    "    # 使用 tokenizer 将文本数据转换为模型输入\n",
    "    inputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    inputs[\"labels\"] = inputs.input_ids.clone()\n",
    "    return inputs\n",
    "\n",
    "for input_length in input_length_range:\n",
    "    for output_length in output_length_range:\n",
    "        MAX_LENGTH = input_length\n",
    "        generated_all = 0\n",
    "        prefill_time, decode_time = 0, 0\n",
    "        reloaded_experts = 0\n",
    "\n",
    "        # 打开文件以写入结果\n",
    "        with open(f\"{input_length}-{output_length}.out\", \"w\") as f:\n",
    "            print(f\"output length is {output_length}\", file=f)\n",
    "            for text in fineweb_text[2:2+test_samples]:\n",
    "                inputs = preprocess_data(text, tokenizer, MAX_LENGTH)\n",
    "                ### 清空统计数据\n",
    "                PLLM.get_prefill_time()\n",
    "                PLLM.get_reload_experts()\n",
    "\n",
    "                # 测试时间\n",
    "                start_event = torch.cuda.Event(enable_timing=True)\n",
    "                end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "                # 开始计时\n",
    "                torch.cuda.synchronize()\n",
    "                start_event.record()\n",
    "\n",
    "                # 前向传播\n",
    "                with torch.no_grad():\n",
    "                    output = llm.generate(\n",
    "                        input_ids=inputs[\"input_ids\"].cuda(device_id),\n",
    "                        attention_mask=inputs[\"attention_mask\"].cuda(device_id),\n",
    "                        max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "                        generation_config=GenerationConfig(do_sample=False),\n",
    "                        pad_token_id=tokenizer.pad_token_id, \n",
    "                        # cache_implementation=\"static\" ## moe not support\n",
    "                    )\n",
    "\n",
    "                # 结束计时\n",
    "                end_event.record()\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "                # 计算时间\n",
    "                elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "                decode_time += elapsed_time\n",
    "                cur_prefill_time = PLLM.get_prefill_time()\n",
    "                prefill_time += cur_prefill_time\n",
    "                print(f\"Generated length: {len(output[0]) - input_length}\", f\"Time taken: {elapsed_time:.2f} s,\", f\"prefill time: {cur_prefill_time:.2f} s\", file=f)\n",
    "                # print(output, file=f)\n",
    "                print(tokenizer.batch_decode(output, skip_special_tokens=True), file=f)\n",
    "\n",
    "                generated_all += (len(output[0]) - input_length - 1)\n",
    "                reloaded_experts += PLLM.get_reload_experts()\n",
    "\n",
    "            print(\"Generate speed:\", '{:.4f}'.format((generated_all+test_samples) / decode_time) , 'token/s', file=f)\n",
    "            timepertoken = (decode_time - prefill_time) / (generated_all)\n",
    "            print(\"decode phase speed(not cover prefill phase):\", '{:.4f}'.format(1/timepertoken) , 'token/s', file=f)\n",
    "            expertpertoken = reloaded_experts / generated_all\n",
    "            print(\"the number of reloaded experts per token:\", '{:.3f}, ({:.2f}%)'.format(expertpertoken, 100 * expertpertoken / ((32-prefill_layers) * 2)), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 256\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "Generated length: 256 Time taken: 57.54 s, prefill time: 17.62 s\n",
      "['Helpful Hints on the Iodine Patch Test\\n\\nThe iodine patch test is a simple, inexpensive, and effective way to test for food allergies. It is also a great way to test for food allergies in children.\\n\\nThe iodine patch test is a simple, inexpensive, and effective way to test for food allergies. It is also a great way to test for food allergies in children.\\n\\nThe iodine patch test is a simple, inexpensive, and effective way to test for food allergies. It is also a great way to test for food allergies in children.\\n\\nThe iodine patch test is a simple, inexpensive, and effective way to test for food allergies. It is also a great way to test for food allergies in children.\\n\\nThe iodine patch test is a simple, inexpensive, and effective way to test for food allergies. It is also a great way to test for food allergies in children.\\n\\nThe iodine patch test is a simple, inexpensive, and effective way to test for food allergies. It is also a great way to test for food allergies in children.\\n\\nThe iodine']\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "Generated length: 256 Time taken: 53.51 s, prefill time: 17.64 s\n",
      "['In 1814, Goya painted a series of works on canvas, which he called “The Disasters of War.” The series was a response to the Peninsular War, which was fought between Napoleon’s forces and the Spanish Empire. The paintings depict the horrors of war, including scenes of torture, rape, and murder.\\n\\nThe Disasters of War is considered one of Goya’s most important works, and it has been praised for its realism and its ability to capture the brutality of war. The paintings are also notable for their use of light and shadow, which creates a sense of drama and tension.\\n\\nThe Disasters of War is a powerful reminder of the horrors of war, and it is a testament to Goya’s skill as an artist.\\n\\n## What is the meaning of the painting The Disasters of War?\\n\\nThe painting The Disasters of War is a work of art that was created by Francisco Goya in the early 19th century. The painting is a depiction of the horrors of war, and it is considered to be one of the most powerful and moving works of art in the world.\\n\\nThe painting is divided into two halves, with the left half depicting the']\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "Generated length: 256 Time taken: 55.53 s, prefill time: 17.64 s\n",
      "['Press release from the Humboldt County Sheriff’s Office:\\n\\n> On 06-20-2012 at approximately 12:30 p.m. the Humboldt County Sheriff’s Office was notified of a possible drowning at the mouth of the Eel River. The Humboldt County Sheriff’s Office, Humboldt Bay Fire Department, Rio Dell Fire Department, and Cal Fire responded to the scene.\\n>\\n> When deputies arrived they learned that a 16 year old male juvenile from Rio Dell was swimming with his friends when he was swept out to sea. The juvenile was swimming with his friends when he was swept out to sea. The juvenile was swimming with his friends when he was swept out to sea. The juvenile was swimming with his friends when he was swept out to sea. The juvenile was swimming with his friends when he was swept out to sea. The juvenile was swimming with his friends when he was swept out to sea. The juvenile was swimming with his friends when he was swept out to sea. The juvenile was swimming with his friends when he was swept out to sea. The juvenile']\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "Generated length: 256 Time taken: 53.80 s, prefill time: 17.63 s\n",
      "['For those that live in the United States, you’re probably familiar with the fact that the country is divided into 50 states. But did you know that there are actually 51 states? That’s right, there’s a 51st state that you’ve probably never heard of.\\n\\nThe 51st state is actually a territory of the United States, and it’s called Puerto Rico. Puerto Rico is an island located in the Caribbean Sea, and it’s been a part of the United States since 1898.\\n\\nDespite being a part of the United States, Puerto Rico is not a state. Instead, it’s a territory, which means that it’s not fully integrated into the country. Puerto Rico has its own government, its own laws, and its own currency.\\n\\nDespite not being a state, Puerto Rico is still a part of the United States. Puerto Ricans are U.S. citizens, and they can vote in U.S. elections. They also have the right to travel to the United States without a visa.\\n\\nPuerto Rico is a beautiful island with a rich culture and history. It’s a great place to visit, and it’s a great place to live.']\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "Generated length: 256 Time taken: 53.76 s, prefill time: 17.61 s\n",
      "['In this study, the researchers sought to determine the effects of a 12-week resistance training program on the physical and psychological health of women with fibromyalgia.\\n\\nThe study included 39 women with fibromyalgia who were randomly assigned to either a resistance training group or a control group. The resistance training group participated in a 12-week resistance training program, while the control group did not.\\n\\nThe results of the study showed that the resistance training group had significant improvements in their physical and psychological health, while the control group did not. The resistance training group had significant improvements in their muscle strength, endurance, and flexibility, as well as their mood and quality of life.\\n\\nThe study also found that the resistance training group had significant reductions in their pain and fatigue levels.\\n\\nThe study concluded that resistance training is an effective intervention for improving the physical and psychological health of women with fibromyalgia.\\n\\n## What type of exercise is best for fibromyalgia?\\n\\nThere is no one-size-fits-all answer to this question, as the best type of exercise for fibromyalgia will vary depending on the individual. However, some types of exercise that may be beneficial for people with fib']\n",
      "Generate speed: 4.6692 token/s\n",
      "decode phase speed(not cover prefill phase): 6.8549 token/s\n",
      "the number of reloaded experts per token: 6.444, (12.39%)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 10\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 256\n",
    "test_samples = 5\n",
    "device_id = 0\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "generated_all = 0\n",
    "prefill_time, decode_time = 0, 0\n",
    "reloaded_experts = 0\n",
    "print(\"output length is {}\".format(output_length))\n",
    "for text in fineweb_text[2:2+test_samples] :\n",
    "    inputs = preprocess_data(text, tokenizer)\n",
    "    ### 清空统计数据\n",
    "    PLLM.get_prefill_time()\n",
    "    PLLM.get_reload_experts()\n",
    "\n",
    "    # 测试时间\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # 开始计时\n",
    "    torch.cuda.synchronize()\n",
    "    start_event.record()\n",
    "\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(device_id),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(device_id),\n",
    "            max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.pad_token_id, \n",
    "            # cache_implementation=\"static\" ## moe not support\n",
    "        )\n",
    "\n",
    "    # 结束计时\n",
    "    end_event.record()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # 计算时间\n",
    "    elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "    decode_time += elapsed_time\n",
    "    cur_prefill_time = PLLM.get_prefill_time()\n",
    "    prefill_time += cur_prefill_time\n",
    "    print(f\"Generated length: {len(output[0]) - input_length}\", f\"Time taken: {elapsed_time:.2f} s,\", f\"prefill time: {cur_prefill_time:.2f} s\")\n",
    "    # print(output)\n",
    "    print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "\n",
    "    generated_all += (len(output[0]) - input_length - 1)\n",
    "    reloaded_experts += PLLM.get_reload_experts()\n",
    "\n",
    "print(\"Generate speed:\", '{:.4f}'.format((generated_all+test_samples) / decode_time) , 'token/s')\n",
    "timepertoken = (decode_time - prefill_time) / (generated_all)\n",
    "print(\"decode phase speed(not cover prefill phase):\", '{:.4f}'.format(1/timepertoken) , 'token/s')\n",
    "expertpertoken = reloaded_experts / generated_all\n",
    "print(\"the number of reloaded experts per token:\", '{:.3f}, ({:.2f}%)'.format(expertpertoken, 100 * expertpertoken / ((32-prefill_layers) * 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################################################################\n",
    "#Save gemlite cache\n",
    "if(backend == 'gemlite'):\n",
    "\tgemlite.core.GemLiteLinear.cache_config('/tmp/gemlite_config.json') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.profile\n",
    "\n",
    "attention使用sdpa实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 2\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                       cudaLaunchKernel        17.36%      43.880ms        17.36%      43.880ms       9.768us       0.000us         0.00%       0.000us       0.000us          4492  \n",
      "gemlite::gemv_revsplitK_A16fWnO16f_int32packing_forw...        12.80%      32.351ms        15.41%      38.957ms     374.591us       4.254ms         6.68%       4.387ms      42.185us           104  \n",
      "                                               aten::mm         8.85%      22.372ms        12.19%      30.796ms      52.915us      44.861ms        70.49%      44.861ms      77.080us           582  \n",
      "                                            aten::copy_         6.83%      17.257ms        13.43%      33.944ms      36.188us       3.039ms         4.77%       3.039ms       3.240us           938  \n",
      "                                              aten::mul         4.72%      11.939ms         7.41%      18.723ms      24.898us       1.634ms         2.57%       1.634ms       2.173us           752  \n",
      "                                        cudaMemcpyAsync         4.69%      11.864ms         4.69%      11.864ms      15.776us       0.000us         0.00%       0.000us       0.000us           752  \n",
      "                                              aten::add         3.11%       7.850ms         4.89%      12.346ms      22.126us     800.231us         1.26%     800.231us       1.434us           558  \n",
      "                                    aten::empty_strided         2.53%       6.383ms         2.53%       6.383ms       9.118us       0.000us         0.00%       0.000us       0.000us           700  \n",
      "                                  cudaDeviceSynchronize         2.33%       5.898ms         2.33%       5.898ms      56.167us      46.178us         0.07%      46.178us       0.440us           105  \n",
      "                               aten::bitwise_left_shift         1.69%       4.283ms         2.44%       6.162ms      29.627us     286.468us         0.45%     286.468us       1.377us           208  \n",
      "                                           aten::select         1.68%       4.240ms         2.01%       5.081ms       6.174us       0.000us         0.00%       0.000us       0.000us           823  \n",
      "                                            aten::empty         1.54%       3.888ms         1.54%       3.888ms       7.477us       0.000us         0.00%       0.000us       0.000us           520  \n",
      "                                              aten::cat         1.53%       3.862ms         2.20%       5.565ms      28.393us     638.756us         1.00%     638.756us       3.259us           196  \n",
      "                                         aten::_to_copy         1.50%       3.795ms        14.55%      36.767ms      58.084us       0.000us         0.00%       2.522ms       3.984us           633  \n",
      "                                         aten::_softmax         1.41%       3.562ms         1.80%       4.558ms      39.290us     180.927us         0.28%     180.927us       1.560us           116  \n",
      "                                             aten::topk         1.38%       3.496ms         2.69%       6.791ms      58.541us       1.202ms         1.89%       1.202ms      10.366us           116  \n",
      "                                       aten::as_strided         1.37%       3.459ms         1.37%       3.459ms       1.058us       0.000us         0.00%       0.000us       0.000us          3270  \n",
      "                                             aten::view         1.27%       3.215ms         1.27%       3.215ms       2.568us       0.000us         0.00%       0.000us       0.000us          1252  \n",
      "                                            aten::index         1.22%       3.087ms         1.94%       4.908ms      38.048us     371.552us         0.58%     371.552us       2.880us           129  \n",
      "                                            aten::slice         1.21%       3.065ms         1.53%       3.873ms       4.214us       0.000us         0.00%       0.000us       0.000us           919  \n",
      "                                  cudaStreamSynchronize         1.17%       2.961ms         1.17%       2.961ms       7.772us      42.303us         0.07%      42.303us       0.111us           381  \n",
      "                                           aten::matmul         1.14%       2.881ms        13.85%      35.012ms      60.158us       0.000us         0.00%      44.861ms      77.080us           582  \n",
      "                                             aten::mean         1.06%       2.680ms         1.48%       3.734ms      28.723us     483.172us         0.76%     483.172us       3.717us           130  \n",
      "                                              aten::pow         0.91%       2.302ms         1.39%       3.505ms      26.965us     174.455us         0.27%     174.455us       1.342us           130  \n",
      "                                    cudaStreamWaitEvent         0.87%       2.196ms         0.87%       2.196ms       4.223us       0.000us         0.00%       0.000us       0.000us           520  \n",
      "                                             aten::silu         0.86%       2.175ms         1.28%       3.242ms      31.172us     226.109us         0.36%     226.109us       2.174us           104  \n",
      "                                               aten::to         0.86%       2.168ms        15.41%      38.936ms      40.643us       0.000us         0.00%       2.522ms       2.632us           958  \n",
      "                              aten::_local_scalar_dense         0.84%       2.125ms         2.84%       7.186ms      26.815us     368.411us         0.58%     368.411us       1.375us           268  \n",
      "                                              aten::neg         0.76%       1.930ms         1.19%       3.008ms      23.500us     319.519us         0.50%     319.519us       2.496us           128  \n",
      "                                          aten::reshape         0.76%       1.916ms         3.16%       7.982ms      12.610us       0.000us         0.00%     285.117us       0.450us           633  \n",
      "                                        aten::transpose         0.72%       1.807ms         1.11%       2.812ms       3.173us       0.000us         0.00%       0.000us       0.000us           886  \n",
      "                                            aten::fill_         0.68%       1.716ms         1.61%       4.075ms      25.001us     201.084us         0.32%     201.084us       1.234us           163  \n",
      "                                            aten::rsqrt         0.66%       1.672ms         1.99%       5.036ms      38.740us     193.759us         0.30%     193.759us       1.490us           130  \n",
      "                         aten::_flash_attention_forward         0.59%       1.486ms         1.61%       4.064ms      63.494us     430.688us         0.68%     430.688us       6.729us            64  \n",
      "                                           aten::linear         0.58%       1.477ms        10.51%      26.574ms      71.053us       0.000us         0.00%       8.815ms      23.570us           374  \n",
      "                                              aten::sum         0.57%       1.441ms         0.79%       1.992ms      31.120us     198.528us         0.31%     198.528us       3.102us            64  \n",
      "                                            aten::equal         0.49%       1.238ms         2.00%       5.055ms      97.220us      79.615us         0.13%     217.631us       4.185us            52  \n",
      "                                                aten::t         0.47%       1.192ms         1.00%       2.517ms       6.729us       0.000us         0.00%       0.000us       0.000us           374  \n",
      "                                         cuLaunchKernel         0.46%       1.152ms         0.46%       1.152ms      10.872us       0.000us         0.00%       0.000us       0.000us           106  \n",
      "                     aten::scaled_dot_product_attention         0.39%     995.365us         2.51%       6.356ms      99.305us       0.000us         0.00%     430.688us       6.729us            64  \n",
      "                                        cudaMemsetAsync         0.38%     953.219us         0.38%     953.219us       9.166us       0.000us         0.00%       0.000us       0.000us           104  \n",
      "                                               aten::ne         0.37%     934.375us         0.64%       1.610ms      30.952us      68.196us         0.11%      68.196us       1.311us            52  \n",
      "                                             aten::div_         0.34%     867.410us         0.54%       1.372ms      21.435us     108.641us         0.17%     108.641us       1.698us            64  \n",
      "                                        cudaGraphLaunch         0.34%     848.824us         0.34%     848.824us      35.368us       6.081us         0.01%       6.081us       0.253us            24  \n",
      "                                        aten::unsqueeze         0.34%     846.946us         0.43%       1.094ms       4.176us       0.000us         0.00%       0.000us       0.000us           262  \n",
      "                                        cudaEventRecord         0.31%     794.866us         0.31%     794.866us       1.529us       0.000us         0.00%       0.000us       0.000us           520  \n",
      "                                     aten::_unsafe_view         0.29%     725.773us         0.29%     725.773us       1.880us       0.000us         0.00%       0.000us       0.000us           386  \n",
      "              aten::_scaled_dot_product_flash_attention         0.28%     709.475us         2.12%       5.360ms      83.753us       0.000us         0.00%     430.688us       6.729us            64  \n",
      "                                              aten::all         0.27%     680.215us         0.85%       2.137ms      39.583us       3.200us         0.01%      71.807us       1.330us            54  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.25%     639.042us         0.25%     639.042us       0.686us       0.000us         0.00%       0.000us       0.000us           931  \n",
      "                                            aten::zero_         0.25%     638.382us         1.52%       3.843ms      36.952us       0.000us         0.00%     132.992us       1.279us           104  \n",
      "                                  cudaFuncGetAttributes         0.24%     615.648us         0.24%     615.648us       5.307us       0.000us         0.00%       0.000us       0.000us           116  \n",
      "                                            aten::clone         0.24%     612.280us         1.94%       4.913ms      36.663us       0.000us         0.00%     293.084us       2.187us           134  \n",
      "                                          aten::permute         0.22%     551.239us         0.33%     828.383us       7.965us       0.000us         0.00%       0.000us       0.000us           104  \n",
      "                                             aten::item         0.20%     493.729us         3.04%       7.680ms      28.657us       0.000us         0.00%     368.411us       1.375us           268  \n",
      "                                       aten::empty_like         0.18%     459.817us         0.81%       2.052ms      10.414us       0.000us         0.00%       0.000us       0.000us           197  \n",
      "          cudaOccupancyMaxActiveBlocksPerMultiprocessor         0.18%     448.767us         0.18%     448.767us       1.934us       0.000us         0.00%       0.000us       0.000us           232  \n",
      "                                           aten::expand         0.18%     448.179us         0.23%     586.225us       4.580us       0.000us         0.00%       0.000us       0.000us           128  \n",
      "                                               aten::eq         0.17%     439.518us         0.28%     697.079us      30.308us      40.156us         0.06%      40.156us       1.746us            23  \n",
      "                                              aten::any         0.17%     421.168us         0.27%     691.951us      36.418us      43.330us         0.07%      49.667us       2.614us            19  \n",
      "                                          aten::softmax         0.12%     296.937us         1.92%       4.855ms      41.850us       0.000us         0.00%     180.927us       1.560us           116  \n",
      "                                 cudaDeviceGetAttribute         0.10%     256.655us         0.10%     256.655us       0.547us       0.000us         0.00%       0.000us       0.000us           469  \n",
      "                                          aten::numpy_T         0.10%     252.258us         0.43%       1.081ms      10.391us       0.000us         0.00%       0.000us       0.000us           104  \n",
      "                                  cudaStreamIsCapturing         0.06%     146.399us         0.06%     146.399us       1.307us      23.871us         0.04%      23.871us       0.213us           112  \n",
      "                                   cudaFuncSetAttribute         0.05%     136.699us         0.05%     136.699us       2.136us       0.000us         0.00%       0.000us       0.000us            64  \n",
      "                                             aten::isin         0.05%     130.737us         0.18%     447.939us     149.313us       0.000us         0.00%      15.904us       5.301us             3  \n",
      "                                       aten::is_nonzero         0.04%     111.754us         0.64%       1.629ms      27.145us       0.000us         0.00%      82.401us       1.373us            60  \n",
      "                                          aten::view_as         0.04%     108.049us         0.08%     195.394us       3.369us       0.000us         0.00%       0.000us       0.000us            58  \n",
      "                                           aten::cumsum         0.04%     101.471us         0.07%     169.392us      56.464us       9.664us         0.02%       9.664us       3.221us             3  \n",
      "                                              aten::sub         0.03%      87.498us         0.05%     130.675us      26.135us       6.850us         0.01%       6.850us       1.370us             5  \n",
      "                                      aten::result_type         0.03%      79.350us         0.03%      79.350us       0.610us       0.000us         0.00%       0.000us       0.000us           130  \n",
      "                                       aten::bitwise_or         0.03%      68.857us         0.04%     102.770us      25.692us       5.568us         0.01%       5.568us       1.392us             4  \n",
      "                                     aten::index_select         0.02%      62.561us         0.04%     111.831us      55.916us       5.278us         0.01%       5.278us       2.639us             2  \n",
      "                                          aten::detach_         0.02%      60.401us         0.03%      86.713us       5.101us       0.000us         0.00%       0.000us       0.000us            17  \n",
      "                                           aten::argmax         0.02%      53.652us         0.03%      81.473us      40.737us      23.936us         0.04%      23.936us      11.968us             2  \n",
      "                                              aten::max         0.02%      46.590us         0.03%      83.415us      41.708us       6.049us         0.01%       6.049us       3.024us             2  \n",
      "                                               aten::ge         0.02%      41.081us         0.02%      60.537us      30.268us       2.976us         0.00%       2.976us       1.488us             2  \n",
      "                                     aten::masked_fill_         0.01%      34.208us         0.02%      50.667us      25.334us       2.912us         0.00%       2.912us       1.456us             2  \n",
      "                                      aten::bitwise_not         0.01%      32.786us         0.02%      52.969us      26.484us       2.753us         0.00%       2.753us       1.376us             2  \n",
      "                                      aten::bitwise_and         0.01%      31.192us         0.02%      52.803us      26.402us       4.448us         0.01%       4.448us       2.224us             2  \n",
      "                                                detach_         0.01%      26.312us         0.01%      26.312us       1.548us       0.000us         0.00%       0.000us       0.000us            17  \n",
      "                                               aten::lt         0.01%      25.208us         0.01%      34.857us      34.857us       1.633us         0.00%       1.633us       1.633us             1  \n",
      "                                        aten::embedding         0.01%      24.665us         0.06%     145.667us      72.833us       0.000us         0.00%       5.278us       2.639us             2  \n",
      "                                             aten::rsub         0.01%      18.243us         0.03%      70.553us      35.277us       0.000us         0.00%       2.754us       1.377us             2  \n",
      "                                             aten::full         0.01%      17.963us         0.04%     107.150us      26.787us       0.000us         0.00%       4.576us       1.144us             4  \n",
      "                                          aten::resize_         0.01%      13.575us         0.01%      13.575us       6.788us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                         aten::new_ones         0.00%      12.437us         0.03%      85.915us      42.958us       0.000us         0.00%       2.848us       1.424us             2  \n",
      "                                     aten::resolve_conj         0.00%      12.356us         0.00%      12.356us       0.441us       0.000us         0.00%       0.000us       0.000us            28  \n",
      "                                           aten::__or__         0.00%       9.562us         0.04%     112.332us      28.083us       0.000us         0.00%       5.568us       1.392us             4  \n",
      "                                             aten::ones         0.00%       7.682us         0.02%      50.316us      50.316us       0.000us         0.00%       1.183us       1.183us             1  \n",
      "                                          aten::__and__         0.00%       6.791us         0.02%      59.594us      29.797us       0.000us         0.00%       4.448us       2.224us             2  \n",
      "                                      aten::resolve_neg         0.00%       6.610us         0.00%       6.610us       0.236us       0.000us         0.00%       0.000us       0.000us            28  \n",
      "                                       aten::lift_fresh         0.00%       6.510us         0.00%       6.510us       0.383us       0.000us         0.00%       0.000us       0.000us            17  \n",
      "                                        aten::new_empty         0.00%       5.668us         0.01%      24.455us      12.227us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                        aten::ones_like         0.00%       4.739us         0.01%      31.173us      31.173us       0.000us         0.00%       1.120us       1.120us             1  \n",
      "                                   cudaDriverGetVersion         0.00%       3.858us         0.00%       3.858us       0.161us     177.153us         0.28%     177.153us       7.381us            24  \n",
      "                                    cudaPeekAtLastError         0.00%       1.949us         0.00%       1.949us       0.162us       0.000us         0.00%       0.000us       0.000us            12  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       9.024us         0.01%       9.024us       0.475us            19  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       8.096us         0.01%       8.096us       1.157us             7  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     149.315us         0.23%     149.315us       1.422us           105  \n",
      "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     231.929us         0.36%     231.929us       1.310us           177  \n",
      "                         Memcpy DtoH (Device -> Pinned)         0.00%       0.000us         0.00%       0.000us       0.000us     368.411us         0.58%     368.411us       1.375us           268  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.609us         0.01%       4.609us       1.536us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      59.996us         0.09%      59.996us       1.154us            52  \n",
      "void at_cuda_detail::cub::DeviceScanInitKernel<at_cu...         0.00%       0.000us         0.00%       0.000us       0.000us       3.424us         0.01%       3.424us       1.141us             3  \n",
      "void at_cuda_detail::cub::DeviceScanKernel<at_cuda_d...         0.00%       0.000us         0.00%       0.000us       0.000us       6.240us         0.01%       6.240us       2.080us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       7.200us         0.01%       7.200us       1.440us             5  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       8.318us         0.01%       8.318us       1.386us             6  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.912us         0.00%       2.912us       1.456us             2  \n",
      "void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us       5.278us         0.01%       5.278us       2.639us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     410.279us         0.64%     410.279us       3.108us           132  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     174.455us         0.27%     174.455us       1.342us           130  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     483.172us         0.76%     483.172us       3.717us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     188.191us         0.30%     188.191us       1.448us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     193.759us         0.30%     193.759us       1.490us           130  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     261.573us         0.41%     261.573us       2.012us           130  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     607.752us         0.95%     607.752us       2.788us           218  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     381.194us         0.60%     381.194us       1.629us           234  \n",
      "ampere_fp16_s16816gemm_fp16_64x64_sliced1x2_ldg8_f2f...         0.00%       0.000us         0.00%       0.000us       0.000us       5.934ms         9.32%       5.934ms      46.357us           128  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       1.723ms         2.71%       1.723ms      13.459us           128  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     368.608us         0.58%     368.608us       2.880us           128  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     985.480us         1.55%     985.480us       2.566us           384  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     319.519us         0.50%     319.519us       2.496us           128  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     464.742us         0.73%     464.742us       3.631us           128  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     457.640us         0.72%     457.640us       1.430us           320  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     285.117us         0.45%     285.117us       2.227us           128  \n",
      "void pytorch_flash::flash_fwd_kernel<pytorch_flash::...         0.00%       0.000us         0.00%       0.000us       0.000us     430.688us         0.68%     430.688us       6.729us            64  \n",
      "void dot_kernel<float, 128, 0, cublasDotParams<cubla...         0.00%       0.000us         0.00%       0.000us       0.000us     247.303us         0.39%     247.303us       2.132us           116  \n",
      "void reduce_1Block_kernel<float, 128, 7, cublasGemvT...         0.00%       0.000us         0.00%       0.000us       0.000us     194.370us         0.31%     194.370us       1.676us           116  \n",
      "void (anonymous namespace)::softmax_warp_forward<c10...         0.00%       0.000us         0.00%       0.000us       0.000us     180.927us         0.28%     180.927us       1.560us           116  \n",
      "void at::native::sbtopk::gatherTopK<float, unsigned ...         0.00%       0.000us         0.00%       0.000us       0.000us     621.089us         0.98%     621.089us       5.354us           116  \n",
      "void at::native::bitonicSortKVInPlace<-2, -1, 16, 16...         0.00%       0.000us         0.00%       0.000us       0.000us     581.347us         0.91%     581.347us       5.012us           116  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     198.528us         0.31%     198.528us       3.102us            64  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     108.641us         0.17%     108.641us       1.698us            64  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     183.484us         0.29%     183.484us       1.207us           152  \n",
      "          gemv_revsplitK_A16fWnO16f_int32packing_kernel         0.00%       0.000us         0.00%       0.000us       0.000us       5.230ms         8.22%       5.230ms      40.861us           128  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      28.863us         0.05%      28.863us       1.203us            24  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      84.253us         0.13%      84.253us       3.511us            24  \n",
      "                         gather_gemv_elemul_flag_kernel         0.00%       0.000us         0.00%       0.000us       0.000us       1.138ms         1.79%       1.138ms      47.401us            24  \n",
      "           gather_transposed_gemv_flag_atomicadd_kernel         0.00%       0.000us         0.00%       0.000us       0.000us       1.012ms         1.59%       1.012ms      42.175us            24  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     148.767us         0.23%     148.767us       1.430us           104  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     137.701us         0.22%     137.701us       1.324us           104  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     151.296us         0.24%     151.296us       1.427us           106  \n",
      "                         Memcpy DtoH (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     616.555us         0.97%     616.555us       2.371us           260  \n",
      "                         Memcpy HtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     914.634us         1.44%     914.634us       3.518us           260  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us      53.317us         0.08%      53.317us       0.513us           104  \n",
      "ampere_fp16_s16816gemm_fp16_64x64_ldg8_f2f_stages_64...         0.00%       0.000us         0.00%       0.000us       0.000us      17.898ms        28.12%      17.898ms     172.093us           104  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     226.109us         0.36%     226.109us       2.174us           104  \n",
      "ampere_fp16_s16816gemm_fp16_64x64_sliced1x2_ldg8_f2f...         0.00%       0.000us         0.00%       0.000us       0.000us      18.095ms        28.43%      18.095ms     173.987us           104  \n",
      "                       Memcpy DtoH (Device -> Pageable)         0.00%       0.000us         0.00%       0.000us       0.000us      38.432us         0.06%      38.432us       1.373us            28  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      27.262us         0.04%      27.262us       1.947us            14  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      43.330us         0.07%      43.330us       3.095us            14  \n",
      "void cutlass::Kernel2<cutlass_80_tensorop_f16_s16816...         0.00%       0.000us         0.00%       0.000us       0.000us     705.345us         1.11%     705.345us     352.673us             2  \n",
      "void cublasLt::splitKreduce_kernel<32, 16, int, __ha...         0.00%       0.000us         0.00%       0.000us       0.000us      11.776us         0.02%      11.776us       5.888us             2  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us       9.023us         0.01%       9.023us       2.256us             4  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      23.936us         0.04%      23.936us      11.968us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.952us         0.01%       5.952us       1.488us             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.754us         0.00%       2.754us       1.377us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.568us         0.01%       5.568us       1.392us             4  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       1.504us         0.00%       1.504us       1.504us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.753us         0.00%       2.753us       1.376us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       4.448us         0.01%       4.448us       2.224us             2  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       6.049us         0.01%       6.049us       3.024us             2  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.944us         0.00%       2.944us       2.944us             1  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       3.200us         0.01%       3.200us       3.200us             1  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     164.991us         0.26%     164.991us       2.578us            64  \n",
      "void at::native::vectorized_elementwise_kernel<2, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.568us         0.00%       1.568us       1.568us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 252.727ms\n",
      "Self CUDA time total: 63.644ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 1\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 2\n",
    "test_samples = 4\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "filt_type = fineweb_path.split('.')[-1]\n",
    "fineweb = load_dataset(filt_type, data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text'][:test_samples] \n",
    "\n",
    "print(\"output length is {}\".format(output_length))\n",
    "text = fineweb_text[0]\n",
    "inputs = preprocess_data(text, tokenizer)\n",
    "\n",
    "# cached_mlp.clear_load_from_cpu_stats()\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ]\n",
    ") as p:\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(),\n",
    "            max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "print(p.key_averages().table(\n",
    "    sort_by=\"self_cpu_time_total\", row_limit=-1))\n",
    "p.export_chrome_trace(\"./offloading-3090-graph.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试一个正常输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "Generated length: 16 Time taken: 20.76 s prefill time: 17.74 s\n",
      "['The future of AI is here,  and it’s called ChatGPT.\\n\\nChatGPT is a']\n",
      "decode phase speed: 4.9713  token/s\n",
      "the number of experts reload per token: 5.266666666666667\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 10\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 16\n",
    "test_samples = 1\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "generated_all, decode_time, prefill_time = 0, 0, 0\n",
    "# print(\"max output length is {}\".format(output_length))\n",
    "text = \"The future of AI is here, \"\n",
    "\n",
    "clear_prefill_time = PLLM.get_prefill_time()\n",
    "clear_experts = PLLM.get_reload_experts()\n",
    "# print(\"need to zero: \", clear_experts, clear_prefill_time)\n",
    "\n",
    "llm.eval()\n",
    "inputs = preprocess_data(text, tokenizer)\n",
    "# 测试时间\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "# 开始计时\n",
    "torch.cuda.synchronize()\n",
    "start_event.record()\n",
    "\n",
    "# 前向传播\n",
    "with torch.no_grad():\n",
    "    output = llm.generate(\n",
    "        input_ids=inputs[\"input_ids\"].cuda(device_id),\n",
    "        attention_mask=inputs[\"attention_mask\"].cuda(device_id),\n",
    "        max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "        generation_config=GenerationConfig(do_sample=False),\n",
    "        pad_token_id=tokenizer.pad_token_id, \n",
    "        # cache_implementation=\"static\" ## moe not support\n",
    "    )\n",
    "\n",
    "# 结束计时\n",
    "end_event.record()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# 计算时间\n",
    "elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "decode_time += elapsed_time\n",
    "cur_prefill_time = PLLM.get_prefill_time()\n",
    "prefill_time += cur_prefill_time\n",
    "print(f\"Generated length: {len(output[0]) - input_length}\", f\"Time taken: {elapsed_time:.2f} s\", f\"prefill time: {cur_prefill_time:.2f} s\")\n",
    "# print(output)\n",
    "print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "\n",
    "generated_all += (len(output[0]) - input_length -1)\n",
    "\n",
    "timepertoken = (decode_time - prefill_time) / (generated_all)\n",
    "print(\"decode phase speed:\", '{:.4f}'.format(1/timepertoken) , ' token/s')\n",
    "print(\"the number of experts reload per token:\", PLLM.get_reload_experts() / generated_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载到GPU上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:25<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MixtralForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "import json\n",
    "\n",
    "def get_model(model_name, device_map, dtype=torch.bfloat16):\n",
    "    llm = MixtralForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        use_cache=True,\n",
    "        torch_dtype=dtype,\n",
    "    ) \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return llm, tokenizer\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "with open('../quantize/device_map_1.json', 'r') as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "dtype = torch.float16\n",
    "llm, tokenizer = get_model(model_name, device_map, dtype=dtype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
