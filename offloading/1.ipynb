{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先都加载到cpu上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"\n",
    "# from modeling_mixtral import MixtralForCausalLM\n",
    "from transformers import AutoTokenizer, MixtralForCausalLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import json\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "with open(\"../quantize/device_map.json\", \"r\") as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "def get_model(model_name, device_map, dtype=torch.bfloat16, use_cache=True):\n",
    "    llm = MixtralForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        use_cache=use_cache,\n",
    "        torch_dtype=dtype,\n",
    "    ) \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return llm, tokenizer\n",
    "\n",
    "dtype = torch.float16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# llm, tokenizer = get_model(model_name, 'cpu', dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/venv/dilab/floe/hqq/hqq/models/base.py:251: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(cls.get_weight_file(save_dir), map_location=map_location)\n",
      "100%|██████████| 32/32 [00:00<00:00, 410.31it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 1150.05it/s]\n",
      "100%|██████████| 32/32 [02:56<00:00,  5.52s/it]\n"
     ]
    }
   ],
   "source": [
    "### HQQ量化\n",
    "from hqq.core.quantize import *\n",
    "from hqq.models.hf.mixtral import MixtralHQQ\n",
    "\n",
    "save_dir = './hqqsaved'\n",
    "### 第一次加载\n",
    "# q3_config    = BaseQuantizeConfig(nbits=2, group_size=64)\n",
    "# quant_config      = {'block_sparse_moe.experts.w3'   : q3_config}\n",
    "# llm = MixtralForCausalLM.from_pretrained(\n",
    "#         model_name,\n",
    "#         device_map='cpu',\n",
    "#         use_cache=True,\n",
    "#         torch_dtype=dtype,\n",
    "#     ) \n",
    "# MixtralHQQ.quantize_model(llm, quant_config=quant_config, compute_dtype=dtype, device='cuda:0')\n",
    "#### 先放CUDA量化，然后再传回CPU\n",
    "# MixtralHQQ.save_quantized(llm, save_dir)\n",
    "\n",
    "### 从保存的权重中加载\n",
    "llm = MixtralHQQ.from_quantized(save_dir, compute_dtype=dtype, device='cpu')\n",
    "HQQLinear.set_backend(HQQBackend.PYTORCH)\n",
    "\n",
    "backend       = \"gemlite\" #'torchao_int4' #\"torchao_int4\" (4-bit only) or \"gemlite\" (4-bit + 2-bit)\n",
    "#Optimize\n",
    "from hqq.utils.patching import prepare_for_inference\n",
    "prepare_for_inference(llm, backend=backend, verbose=True)\n",
    "#Load GemLite cache\n",
    "if(backend == 'gemlite'):\n",
    "\timport gemlite\n",
    "\tgemlite.core.GEMLITE_TRITON_RESTRICT_M = True\n",
    "\tgemlite.core.GemLiteLinear.load_config('/tmp/gemlite_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "active neural num  2867\n",
      "active neural num  2867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/On-the-Fly_MoE_Inference/offloading/pipelinellm.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(f'/home/bcds/On-the-Fly_MoE_Inference/expert_predictor/training/{layer_idx}-{training_epoch}.pth'))\n"
     ]
    }
   ],
   "source": [
    "from pipelinellm import convert_mixtral_to_cached_mlp, PipelineLLM\n",
    "llm, cached_mlps = convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.8)\n",
    "\n",
    "# 创建流水线模型\n",
    "PLLM = PipelineLLM(llm, cached_mlps, 1, 3, training_epoch=20, print_layer_info=True) ### use ep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试时间开销"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 32\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "Generated length: 32 Time taken: 832.55 s, prefill time: 821.77 s\n",
      "['How do you get HIV?\\nHIV is a the/cc ( ( -( !.) )9t((2/5009996(AB0']\n",
      "decode phase speed: 2.8764 token/s\n",
      "the number of reloaded experts per token: 15.290\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "\n",
    "input_length = 10\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 32\n",
    "test_samples = 1\n",
    "device_id = 0\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "filt_type = fineweb_path.split('.')[-1]\n",
    "fineweb = load_dataset(filt_type, data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text']\n",
    "\n",
    "# 预热（避免第一次运行时的额外开销）\n",
    "# for text in fineweb_text[:5] :\n",
    "#     inputs = preprocess_data(text, tokenizer)\n",
    "#     with torch.no_grad():\n",
    "#         output = llm(input_ids=inputs[\"input_ids\"].cuda(device_id), attention_mask=inputs[\"attention_mask\"].cuda(device_id))\n",
    "\n",
    "generated_all = 0\n",
    "prefill_time, decode_time = 0, 0\n",
    "reloaded_experts = 0\n",
    "print(\"output length is {}\".format(output_length))\n",
    "for text in fineweb_text[2:2+test_samples] :\n",
    "    inputs = preprocess_data(text, tokenizer)\n",
    "    ### 清空统计数据\n",
    "    PLLM.get_prefill_time()\n",
    "    PLLM.get_reload_experts()\n",
    "\n",
    "    # 测试时间\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # 开始计时\n",
    "    torch.cuda.synchronize()\n",
    "    start_event.record()\n",
    "\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(device_id),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(device_id),\n",
    "            max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.pad_token_id, \n",
    "            # cache_implementation=\"static\" ## moe not support\n",
    "        )\n",
    "\n",
    "    # 结束计时\n",
    "    end_event.record()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # 计算时间\n",
    "    elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "    decode_time += elapsed_time\n",
    "    cur_prefill_time = PLLM.get_prefill_time()\n",
    "    prefill_time += cur_prefill_time\n",
    "    print(f\"Generated length: {len(output[0]) - input_length}\", f\"Time taken: {elapsed_time:.2f} s,\", f\"prefill time: {cur_prefill_time:.2f} s\")\n",
    "    # print(output)\n",
    "    print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "\n",
    "    generated_all += (len(output[0]) - input_length - 1)\n",
    "    reloaded_experts += PLLM.get_reload_experts()\n",
    "\n",
    "timepertoken = (decode_time - prefill_time) / (generated_all)\n",
    "print(\"decode phase speed:\", '{:.4f}'.format(1/timepertoken) , 'token/s')\n",
    "print(\"the number of reloaded experts per token:\", '{:.3f}'.format(reloaded_experts / generated_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################################################################\n",
    "#Save gemlite cache\n",
    "if(backend == 'gemlite'):\n",
    "\tgemlite.core.GemLiteLinear.cache_config('/tmp/gemlite_config.json') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.profile\n",
    "\n",
    "trace-offloading-r.json是最优，就是做完一个index就传一个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 2\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::index        24.56%     143.583ms        33.20%     194.114ms     348.499us       1.446ms         0.30%       2.117ms       3.801us           557  \n",
      "                                  cudaStreamSynchronize        16.41%      95.945ms        16.41%      95.945ms     134.944us     120.451us         0.03%     120.451us       0.169us           711  \n",
      "                                        cudaMemcpyAsync        12.65%      73.978ms        12.65%      73.978ms      54.839us       0.000us         0.00%       0.000us       0.000us          1349  \n",
      "                                            aten::copy_        10.41%      60.898ms        31.76%     185.714ms      62.133us     295.431ms        61.44%     295.431ms      98.840us          2989  \n",
      "                                       cudaLaunchKernel         8.00%      46.750ms         8.00%      46.750ms       5.228us       0.000us         0.00%       0.000us       0.000us          8943  \n",
      "                                    HQQMatmulNoCacheMul         4.04%      23.642ms        16.26%      95.100ms     339.644us       0.000us         0.00%     186.205ms     665.018us           280  \n",
      "                                               aten::mm         3.98%      23.295ms         5.75%      33.596ms      36.758us      25.612ms         5.33%      25.612ms      28.022us           914  \n",
      "                                      aten::bitwise_and         1.93%      11.304ms         2.90%      16.940ms      15.098us      23.012ms         4.79%      23.012ms      20.509us          1122  \n",
      "                                              aten::mul         1.38%       8.062ms         2.25%      13.134ms      12.437us      47.732ms         9.93%      47.732ms      45.201us          1056  \n",
      "                                            aten::empty         1.05%       6.118ms         1.05%       6.118ms       3.907us       0.000us         0.00%       0.000us       0.000us          1566  \n",
      "                                            aten::slice         1.03%       6.048ms         1.26%       7.382ms       2.749us       0.000us         0.00%       0.000us       0.000us          2685  \n",
      "                                       aten::__rshift__         0.92%       5.404ms         2.15%      12.551ms      14.941us      17.001ms         3.54%      17.001ms      20.239us           840  \n",
      "                                    aten::empty_strided         0.92%       5.368ms         0.92%       5.368ms       7.456us       0.000us         0.00%       0.000us       0.000us           720  \n",
      "                                             aten::topk         0.84%       4.923ms         1.54%       8.977ms      32.291us      13.295ms         2.77%      13.295ms      47.825us           278  \n",
      "                                              aten::add         0.67%       3.890ms         1.10%       6.405ms      12.318us       1.320ms         0.27%       1.320ms       2.539us           520  \n",
      "                                       aten::as_strided         0.59%       3.429ms         0.59%       3.429ms       0.579us       0.000us         0.00%       0.000us       0.000us          5927  \n",
      "                                           aten::matmul         0.49%       2.873ms         7.26%      42.475ms      40.763us       0.000us         0.00%      26.019ms      24.970us          1042  \n",
      "                                             aten::sort         0.48%       2.802ms         1.93%      11.269ms      90.879us       1.031ms         0.21%       3.734ms      30.115us           124  \n",
      "                                           aten::select         0.46%       2.700ms         0.55%       3.195ms       3.872us       0.000us         0.00%       0.000us       0.000us           825  \n",
      "                                              aten::sub         0.45%       2.610ms         0.69%       4.049ms      14.108us      45.435ms         9.45%      45.435ms     158.311us           287  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.45%       2.605ms         0.45%       2.605ms       1.287us       0.000us         0.00%       0.000us       0.000us          2024  \n",
      "                                    cudaLaunchKernelExC         0.43%       2.502ms         0.43%       2.502ms       6.044us       0.000us         0.00%       0.000us       0.000us           414  \n",
      "                                             aten::view         0.40%       2.311ms         0.40%       2.311ms       1.175us       0.000us         0.00%       0.000us       0.000us          1967  \n",
      "                                         aten::_to_copy         0.39%       2.269ms        21.14%     123.630ms     172.909us       0.000us         0.00%       3.031ms       4.239us           715  \n",
      "                                              aten::cat         0.38%       2.244ms         0.57%       3.353ms      17.105us     793.330us         0.16%     793.330us       4.048us           196  \n",
      "                                  cudaFuncGetAttributes         0.35%       2.020ms         0.35%       2.020ms       3.042us       0.000us         0.00%       0.000us       0.000us           664  \n",
      "                                          aten::reshape         0.34%       2.004ms         1.13%       6.633ms       3.960us       0.000us         0.00%     447.691us       0.267us          1675  \n",
      "                                              aten::bmm         0.34%       1.996ms         0.71%       4.161ms      32.508us     406.760us         0.08%     406.760us       3.178us           128  \n",
      "                                        cudaMemsetAsync         0.33%       1.915ms         0.33%       1.915ms       6.697us       0.000us         0.00%       0.000us       0.000us           286  \n",
      "                                         aten::_softmax         0.31%       1.839ms         0.52%       3.067ms      16.144us     668.776us         0.14%     668.776us       3.520us           190  \n",
      "                                             aten::mean         0.29%       1.681ms         0.41%       2.381ms      18.313us     681.686us         0.14%     681.686us       5.244us           130  \n",
      "                              aten::_local_scalar_dense         0.27%       1.580ms         1.66%       9.708ms      32.147us       1.538ms         0.32%       1.538ms       5.094us           302  \n",
      "                                              aten::pow         0.26%       1.520ms         0.39%       2.279ms      17.534us     321.801us         0.07%     321.801us       2.475us           130  \n",
      "                                               aten::to         0.26%       1.519ms        21.40%     125.150ms     124.775us       0.000us         0.00%       3.031ms       3.022us          1003  \n",
      "                                                aten::t         0.25%       1.452ms         0.47%       2.735ms       4.034us       0.000us         0.00%       0.000us       0.000us           678  \n",
      "                                        aten::transpose         0.24%       1.380ms         0.34%       2.016ms       2.020us       0.000us         0.00%       0.000us       0.000us           998  \n",
      "                                             aten::silu         0.22%       1.272ms         0.35%       2.034ms      15.179us     566.762us         0.12%     566.762us       4.230us           134  \n",
      "                                              aten::abs         0.22%       1.270ms         0.79%       4.615ms      15.180us     480.896us         0.10%     961.792us       3.164us           304  \n",
      "                                              aten::neg         0.20%       1.141ms         0.31%       1.819ms      14.213us     463.619us         0.10%     463.619us       3.622us           128  \n",
      "                                           aten::arange         0.19%       1.085ms         0.80%       4.649ms      18.160us     656.876us         0.14%       1.314ms       5.132us           256  \n",
      "                                            aten::equal         0.18%       1.071ms         0.91%       5.299ms      85.474us     333.382us         0.07%       1.338ms      21.575us            62  \n",
      "                                            aten::rsqrt         0.18%       1.050ms         0.29%       1.689ms      12.992us     319.756us         0.07%     319.756us       2.460us           130  \n",
      "                                              aten::sum         0.18%       1.035ms         0.24%       1.411ms      22.051us     486.885us         0.10%     486.885us       7.608us            64  \n",
      "                                          aten::__and__         0.17%     980.472us         3.06%      17.921ms      15.972us       0.000us         0.00%      23.012ms      20.509us          1122  \n",
      "                                              aten::all         0.15%     858.212us         0.22%       1.280ms      20.644us     514.990us         0.11%     514.990us       8.306us            62  \n",
      "                                          aten::resize_         0.14%     827.745us         0.14%     827.745us       2.935us       0.000us         0.00%       0.000us       0.000us           282  \n",
      "                                           aten::linear         0.14%     820.247us         2.92%      17.102ms      42.969us       0.000us         0.00%       5.219ms      13.112us           398  \n",
      "                                               aten::ne         0.13%     736.175us         0.21%       1.245ms      20.084us     286.565us         0.06%     286.565us       4.622us            62  \n",
      "                                           aten::expand         0.12%     700.053us         0.15%     896.402us       2.322us       0.000us         0.00%       0.000us       0.000us           386  \n",
      "                                              aten::div         0.11%     672.021us         0.17%       1.022ms      15.965us     150.725us         0.03%     150.725us       2.355us            64  \n",
      "                                             aten::div_         0.11%     657.976us         0.18%       1.028ms      16.062us     375.171us         0.08%     375.171us       5.862us            64  \n",
      "                                   cudaFuncSetAttribute         0.10%     574.328us         0.10%     574.328us       0.666us       0.000us         0.00%       0.000us       0.000us           862  \n",
      "                                        aten::unsqueeze         0.09%     536.274us         0.12%     695.076us       2.574us       0.000us         0.00%       0.000us       0.000us           270  \n",
      "                                 cudaDeviceGetAttribute         0.08%     474.399us         0.08%     474.399us       0.473us       0.000us         0.00%       0.000us       0.000us          1003  \n",
      "                                     aten::_unsafe_view         0.08%     466.581us         0.08%     466.581us       0.908us       0.000us         0.00%       0.000us       0.000us           514  \n",
      "                                             aten::item         0.07%     430.406us         1.73%      10.139ms      33.572us       0.000us         0.00%       1.538ms       5.094us           302  \n",
      "                                          aten::permute         0.07%     429.032us         0.09%     505.036us       4.073us       0.000us         0.00%       0.000us       0.000us           124  \n",
      "                                            aten::clone         0.07%     409.760us         0.47%       2.739ms      19.847us       0.000us         0.00%     470.732us       3.411us           138  \n",
      "                                            aten::addmm         0.06%     379.253us         0.08%     450.945us      37.579us      67.234us         0.01%      67.234us       5.603us            12  \n",
      "                                          aten::softmax         0.06%     336.093us         0.58%       3.403ms      17.913us       0.000us         0.00%     668.776us       3.520us           190  \n",
      "                                              aten::any         0.04%     254.309us         0.08%     448.063us      21.336us     127.971us         0.03%     139.236us       6.630us            21  \n",
      "                                               aten::eq         0.04%     242.759us         0.07%     414.337us      16.573us     117.408us         0.02%     117.408us       4.696us            25  \n",
      "                                       aten::empty_like         0.04%     229.972us         0.14%     810.172us       6.001us       0.000us         0.00%       0.000us       0.000us           135  \n",
      "                                          aten::numpy_T         0.03%     194.354us         0.12%     699.390us       5.640us       0.000us         0.00%       0.000us       0.000us           124  \n",
      "                                       aten::is_nonzero         0.03%     147.291us         0.40%       2.310ms      33.977us       0.000us         0.00%     385.549us       5.670us            68  \n",
      "                                            aten::fill_         0.01%      79.668us         0.03%     181.396us      13.954us      28.190us         0.01%      28.190us       2.168us            13  \n",
      "                                   aten::_reshape_alias         0.01%      75.111us         0.01%      75.111us       2.347us       0.000us         0.00%       0.000us       0.000us            32  \n",
      "                                             aten::isin         0.01%      72.085us         0.05%     300.982us     100.327us       0.000us         0.00%      27.935us       9.312us             3  \n",
      "                                           aten::cumsum         0.01%      69.532us         0.02%     113.307us      37.769us      15.808us         0.00%      15.808us       5.269us             3  \n",
      "                                      aten::result_type         0.01%      59.581us         0.01%      59.581us       0.458us       0.000us         0.00%       0.000us       0.000us           130  \n",
      "                                          aten::detach_         0.01%      54.891us         0.01%      71.189us       3.747us       0.000us         0.00%       0.000us       0.000us            19  \n",
      "                                       aten::bitwise_or         0.01%      48.124us         0.01%      69.541us      17.385us       9.408us         0.00%       9.408us       2.352us             4  \n",
      "                                     aten::masked_fill_         0.01%      45.296us         0.01%      71.002us      17.750us       9.056us         0.00%       9.056us       2.264us             4  \n",
      "                                     aten::index_select         0.01%      43.766us         0.01%      71.524us      35.762us       7.776us         0.00%       7.776us       3.888us             2  \n",
      "                                           aten::argmax         0.01%      43.237us         0.01%      60.906us      30.453us      24.162us         0.01%      24.162us      12.081us             2  \n",
      "                                              aten::max         0.01%      39.366us         0.01%      60.318us      30.159us       8.383us         0.00%       8.383us       4.191us             2  \n",
      "                                          aten::dropout         0.01%      32.077us         0.01%      32.077us       0.501us       0.000us         0.00%       0.000us       0.000us            64  \n",
      "                                      aten::bitwise_not         0.00%      27.334us         0.01%      40.667us      20.333us       4.864us         0.00%       4.864us       2.432us             2  \n",
      "                                               aten::ge         0.00%      26.536us         0.01%      43.389us      21.694us       4.737us         0.00%       4.737us       2.368us             2  \n",
      "                                               aten::gt         0.00%      24.296us         0.02%      92.414us      46.207us       5.888us         0.00%       5.888us       2.944us             2  \n",
      "                                      aten::bitwise_or_         0.00%      23.140us         0.01%      32.669us      16.335us       5.600us         0.00%       5.600us       2.800us             2  \n",
      "                                  cudaDeviceSynchronize         0.00%      21.060us         0.00%      21.060us      21.060us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                        aten::embedding         0.00%      20.597us         0.02%      96.531us      48.265us       0.000us         0.00%       7.776us       3.888us             2  \n",
      "                                               aten::lt         0.00%      20.461us         0.00%      26.765us      26.765us       2.208us         0.00%       2.208us       2.208us             1  \n",
      "                                             aten::mul_         0.00%      20.205us         0.01%      32.776us      16.388us       5.344us         0.00%       5.344us       2.672us             2  \n",
      "                                         aten::new_ones         0.00%      19.995us         0.01%      72.750us      36.375us       0.000us         0.00%       4.832us       2.416us             2  \n",
      "                                             aten::full         0.00%      17.135us         0.02%     104.340us      17.390us       0.000us         0.00%      12.702us       2.117us             6  \n",
      "                                                detach_         0.00%      16.298us         0.00%      16.298us       0.858us       0.000us         0.00%       0.000us       0.000us            19  \n",
      "                                               aten::le         0.00%      14.445us         0.00%      21.345us      10.672us       5.920us         0.00%       5.920us       2.960us             2  \n",
      "                                           aten::__or__         0.00%      12.851us         0.01%      82.392us      20.598us       0.000us         0.00%       9.408us       2.352us             4  \n",
      "                                             aten::rsub         0.00%      11.532us         0.01%      45.440us      22.720us       0.000us         0.00%       5.344us       2.672us             2  \n",
      "                                     aten::resolve_conj         0.00%      11.250us         0.00%      11.250us       0.256us       0.000us         0.00%       0.000us       0.000us            44  \n",
      "                                      aten::masked_fill         0.00%      10.650us         0.01%      57.486us      28.743us       0.000us         0.00%       8.640us       4.320us             2  \n",
      "                                      aten::resolve_neg         0.00%       7.794us         0.00%       7.794us       0.177us       0.000us         0.00%       0.000us       0.000us            44  \n",
      "                                          aten::view_as         0.00%       7.552us         0.00%      10.166us       2.033us       0.000us         0.00%       0.000us       0.000us             5  \n",
      "                                       aten::lift_fresh         0.00%       5.585us         0.00%       5.585us       0.294us       0.000us         0.00%       0.000us       0.000us            19  \n",
      "                                             aten::ones         0.00%       5.250us         0.00%      23.240us      23.240us       0.000us         0.00%       2.240us       2.240us             1  \n",
      "                                        aten::new_empty         0.00%       4.666us         0.00%      14.223us       7.111us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                        aten::ones_like         0.00%       4.014us         0.00%      18.005us      18.005us       0.000us         0.00%       2.016us       2.016us             1  \n",
      "                                    cudaPeekAtLastError         0.00%       1.867us         0.00%       1.867us       0.156us       0.000us         0.00%       0.000us       0.000us            12  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     679.221us         0.14%     679.221us       5.265us           129  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      14.878us         0.00%      14.878us       2.125us             7  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     578.954us         0.12%     578.954us       5.312us           109  \n",
      "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       2.203ms         0.46%       2.203ms       5.271us           418  \n",
      "                         Memcpy DtoH (Device -> Pinned)         0.00%       0.000us         0.00%       0.000us       0.000us       1.538ms         0.32%       1.538ms       5.094us           302  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       6.945us         0.00%       6.945us       2.315us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       9.088us         0.00%       9.088us       2.272us             4  \n",
      "void at_cuda_detail::cub::DeviceScanInitKernel<at_cu...         0.00%       0.000us         0.00%       0.000us       0.000us       7.072us         0.00%       7.072us       2.357us             3  \n",
      "void at_cuda_detail::cub::DeviceScanKernel<at_cuda_d...         0.00%       0.000us         0.00%       0.000us       0.000us       8.736us         0.00%       8.736us       2.912us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      16.544us         0.00%      16.544us       2.363us             7  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      52.608us         0.01%      52.608us       2.630us            20  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.704us         0.00%       4.704us       2.352us             2  \n",
      "void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us       7.776us         0.00%       7.776us       3.888us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.224us         0.00%       4.224us       2.112us             2  \n",
      "void (anonymous namespace)::elementwise_kernel_with_...         0.00%       0.000us         0.00%       0.000us       0.000us     656.876us         0.14%     656.876us       5.132us           128  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.184us         0.00%       5.184us       2.592us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      15.008us         0.00%      15.008us       2.501us             6  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       5.344us         0.00%       5.344us       2.672us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       5.792us         0.00%       5.792us       2.896us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.960us         0.00%       4.960us       2.480us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.352us         0.00%       4.352us       2.176us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     498.569us         0.10%     498.569us       3.777us           132  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     321.801us         0.07%     321.801us       2.475us           130  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     681.686us         0.14%     681.686us       5.244us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     302.171us         0.06%     302.171us       2.324us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     319.756us         0.07%     319.756us       2.460us           130  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     430.465us         0.09%     430.465us       3.311us           130  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us      41.017ms         8.53%      41.017ms      29.766us          1378  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     847.405us         0.18%     847.405us       3.285us           258  \n",
      "sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x...         0.00%       0.000us         0.00%       0.000us       0.000us       2.321ms         0.48%       2.321ms      18.131us           128  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       1.061ms         0.22%       1.061ms       7.919us           134  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.443ms         0.30%       1.443ms       5.726us           252  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      46.444ms         9.66%      46.444ms      69.946us           664  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     463.619us         0.10%     463.619us       3.622us           128  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     548.970us         0.11%     548.970us       4.289us           128  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     807.387us         0.17%     807.387us       2.523us           320  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     447.691us         0.09%     447.691us       3.498us           128  \n",
      "void gemv2T_kernel_val<int, int, __half, __half, __h...         0.00%       0.000us         0.00%       0.000us       0.000us     204.838us         0.04%     204.838us       3.201us            64  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     150.725us         0.03%     150.725us       2.355us            64  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      45.613ms         9.49%      45.613ms     132.597us           344  \n",
      "void (anonymous namespace)::softmax_warp_forward<c10...         0.00%       0.000us         0.00%       0.000us       0.000us      77.664us         0.02%      77.664us       2.427us            32  \n",
      "void gemmk1_kernel<int, float, 256, 5, false, false,...         0.00%       0.000us         0.00%       0.000us       0.000us      92.641us         0.02%      92.641us       2.895us            32  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     566.762us         0.12%     566.762us       4.230us           134  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       2.091ms         0.43%       2.091ms      16.085us           130  \n",
      "void (anonymous namespace)::softmax_warp_forward<c10...         0.00%       0.000us         0.00%       0.000us       0.000us     512.424us         0.11%     512.424us       4.067us           126  \n",
      "void at::native::sbtopk::gatherTopK<float, unsigned ...         0.00%       0.000us         0.00%       0.000us       0.000us       1.053ms         0.22%       1.053ms       8.357us           126  \n",
      "void at::native::bitonicSortKVInPlace<-2, -1, 16, 16...         0.00%       0.000us         0.00%       0.000us       0.000us     920.364us         0.19%     920.364us       7.304us           126  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      23.006ms         4.78%      23.006ms      20.541us          1120  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      17.001ms         3.54%      17.001ms      20.239us           840  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us     767.397us         0.16%     767.397us       2.683us           286  \n",
      "sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x...         0.00%       0.000us         0.00%       0.000us       0.000us      15.568ms         3.24%      15.568ms      54.432us           286  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     480.896us         0.10%     480.896us       3.164us           152  \n",
      "void at::native::sbtopk::gatherTopK<c10::Half, unsig...         0.00%       0.000us         0.00%       0.000us       0.000us       8.469ms         1.76%       8.469ms      55.718us           152  \n",
      "void at::native::radixSortKVInPlace<-2, -1, 128, 32,...         0.00%       0.000us         0.00%       0.000us       0.000us       2.853ms         0.59%       2.853ms      18.769us           152  \n",
      "                       Memcpy DtoH (Device -> Pageable)         0.00%       0.000us         0.00%       0.000us       0.000us     808.262us         0.17%     808.262us       4.124us           196  \n",
      "                         Memcpy HtoD (Pinned -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     249.777ms        51.95%     249.777ms     821.636us           304  \n",
      "void dot_kernel<float, 128, 0, cublasDotParams<cubla...         0.00%       0.000us         0.00%       0.000us       0.000us     629.709us         0.13%     629.709us       5.248us           120  \n",
      "void reduce_1Block_kernel<float, 128, 7, cublasGemvT...         0.00%       0.000us         0.00%       0.000us       0.000us     533.063us         0.11%     533.063us       4.442us           120  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     486.885us         0.10%     486.885us       7.608us            64  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     375.171us         0.08%     375.171us       5.862us            64  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_...         0.00%       0.000us         0.00%       0.000us       0.000us     190.883us         0.04%     190.883us      47.721us             4  \n",
      "void splitKreduce_kernel<32, 16, int, float, __half,...         0.00%       0.000us         0.00%       0.000us       0.000us      22.784us         0.00%      22.784us       5.696us             4  \n",
      "void at::native::bitonicSortKVInPlace<-2, -1, 16, 16...         0.00%       0.000us         0.00%       0.000us       0.000us       1.031ms         0.21%       1.031ms       8.313us           124  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     514.990us         0.11%     514.990us       8.306us            62  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       2.495ms         0.52%       2.495ms      20.118us           124  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      95.585us         0.02%      95.585us       5.974us            16  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     127.971us         0.03%     127.971us       7.998us            16  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us      13.730us         0.00%      13.730us       3.433us             4  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      24.162us         0.01%      24.162us      12.081us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      10.305us         0.00%      10.305us       2.576us             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.344us         0.00%       5.344us       2.672us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.929us         0.00%       4.929us       2.465us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       2.592us         0.00%       2.592us       2.592us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.864us         0.00%       4.864us       2.432us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       5.664us         0.00%       5.664us       2.832us             2  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       8.383us         0.00%       8.383us       4.191us             2  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.424us         0.00%       3.424us       3.424us             1  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       6.624us         0.00%       6.624us       3.312us             2  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     230.630us         0.05%     230.630us       3.604us            64  \n",
      "void (anonymous namespace)::softmax_warp_forward<c10...         0.00%       0.000us         0.00%       0.000us       0.000us      78.688us         0.02%      78.688us       2.459us            32  \n",
      "void gemv2N_kernel<int, int, __half, __half, __half,...         0.00%       0.000us         0.00%       0.000us       0.000us     109.281us         0.02%     109.281us       3.415us            32  \n",
      "void at::native::vectorized_elementwise_kernel<2, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.656us         0.00%       2.656us       2.656us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 584.741ms\n",
      "Self CUDA time total: 480.813ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 1\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 2\n",
    "test_samples = 4\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "fineweb = load_dataset(\"parquet\",data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text'][:test_samples] \n",
    "\n",
    "print(\"output length is {}\".format(output_length))\n",
    "text = fineweb_text[0]\n",
    "inputs = preprocess_data(text, tokenizer)\n",
    "\n",
    "# cached_mlp.clear_load_from_cpu_stats()\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ]\n",
    ") as p:\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(),\n",
    "            max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "print(p.key_averages().table(\n",
    "    sort_by=\"self_cpu_time_total\", row_limit=-1))\n",
    "p.export_chrome_trace(\"./offloading-hqq2-reload2.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试一个正常输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated length: 12 Time taken: 41.38 s prefill time: 37.84 s\n",
      "['The future of AI is here, and it’s brighter than you think.']\n",
      "decode phase speed: 3.1135  token/s\n",
      "the number of experts reload per token: 8.727272727272727\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 6\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 12\n",
    "device_id = 0\n",
    "test_samples = 1\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "generated_all, decode_time, prefill_time = 0, 0, 0\n",
    "# print(\"max output length is {}\".format(output_length))\n",
    "text = \"The future of AI is \"\n",
    "\n",
    "clear_prefill_time = PLLM.get_prefill_time()\n",
    "clear_experts = PLLM.get_reload_experts()\n",
    "# print(\"need to zero: \", clear_experts, clear_prefill_time)\n",
    "\n",
    "inputs = preprocess_data(text, tokenizer)\n",
    "# 测试时间\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "# 开始计时\n",
    "torch.cuda.synchronize()\n",
    "start_event.record()\n",
    "\n",
    "# 前向传播\n",
    "with torch.no_grad():\n",
    "    output = llm.generate(\n",
    "        input_ids=inputs[\"input_ids\"].cuda(device_id),\n",
    "        attention_mask=inputs[\"attention_mask\"].cuda(device_id),\n",
    "        max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "        generation_config=GenerationConfig(do_sample=False),\n",
    "        pad_token_id=tokenizer.pad_token_id, \n",
    "        # cache_implementation=\"static\" ## moe not support\n",
    "    )\n",
    "\n",
    "# 结束计时\n",
    "end_event.record()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# 计算时间\n",
    "elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "decode_time += elapsed_time\n",
    "cur_prefill_time = PLLM.get_prefill_time()\n",
    "prefill_time += cur_prefill_time\n",
    "print(f\"Generated length: {len(output[0]) - input_length}\", f\"Time taken: {elapsed_time:.2f} s\", f\"prefill time: {cur_prefill_time:.2f} s\")\n",
    "# print(output)\n",
    "print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "\n",
    "generated_all += (len(output[0]) - input_length -1)\n",
    "\n",
    "timepertoken = (decode_time - prefill_time) / (generated_all)\n",
    "print(\"decode phase speed:\", '{:.4f}'.format(1/timepertoken) , ' token/s')\n",
    "print(\"the number of experts reload per token:\", PLLM.get_reload_experts() / generated_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载到GPU上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:25<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MixtralForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "import json\n",
    "\n",
    "def get_model(model_name, device_map, dtype=torch.bfloat16):\n",
    "    llm = MixtralForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        use_cache=True,\n",
    "        torch_dtype=dtype,\n",
    "    ) \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return llm, tokenizer\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "with open('../quantize/device_map_1.json', 'r') as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "dtype = torch.float16\n",
    "llm, tokenizer = get_model(model_name, device_map, dtype=dtype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
