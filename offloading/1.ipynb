{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先都加载到cpu上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using atten... sdpa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/venv/dilab/floe/hqq/hqq/models/base.py:251: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(cls.get_weight_file(save_dir), map_location=map_location)\n",
      "100%|██████████| 32/32 [00:00<00:00, 292.19it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 1185.72it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"\n",
    "from modeling_mixtral import MixtralForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import json\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "dtype = torch.float16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "### HQQ量化\n",
    "from hqq.core.quantize import *\n",
    "from hqq.models.hf.mixtral import MixtralPatch\n",
    "import transformers\n",
    "from hqq.models.base import BaseHQQModel\n",
    "from accelerate import init_empty_weights\n",
    "\n",
    "save_dir = './hqqsaved'\n",
    "\n",
    "class BaseHQQHFModel(BaseHQQModel):\n",
    "    # Save model architecture\n",
    "    @classmethod\n",
    "    def cache_model(cls, model, save_dir):\n",
    "        model.config.save_pretrained(save_dir)\n",
    "\n",
    "    # Create empty model from config\n",
    "    @classmethod\n",
    "    def create_model(cls, save_dir, kwargs):\n",
    "        model_kwargs = {}\n",
    "        for key in [\"attn_implementation\"]:\n",
    "            if key in kwargs:\n",
    "                model_kwargs[key] = kwargs[key]\n",
    "\n",
    "        config = transformers.AutoConfig.from_pretrained(\n",
    "            cls.get_config_file(save_dir)\n",
    "        )\n",
    "\n",
    "        with init_empty_weights():\n",
    "            model = MixtralForCausalLM._from_config(config, **model_kwargs)\n",
    "\n",
    "        return model\n",
    "\n",
    "class MixtralHQQ(MixtralPatch, BaseHQQHFModel):\n",
    "    pass\n",
    "\n",
    "### 从保存的权重中加载\n",
    "llm = MixtralHQQ.from_quantized(save_dir, compute_dtype=dtype, device='cpu')\n",
    "HQQLinear.set_backend(HQQBackend.PYTORCH)\n",
    "\n",
    "backend       = \"hqq\" #'torchao_int4' #\"torchao_int4\" (4-bit only) or \"gemlite\" (4-bit + 2-bit)\n",
    "# #Optimize\n",
    "from hqq.utils.patching import prepare_for_inference\n",
    "prepare_for_inference(llm, backend=backend, verbose=True)\n",
    "\n",
    "if(backend == 'gemlite'):\n",
    "\timport gemlite\n",
    "\tgemlite.core.GEMLITE_TRITON_RESTRICT_M = True\n",
    "\tgemlite.core.GemLiteLinear.load_config('/tmp/gemlite_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "active neural num  2867\n",
      "active neural num  2867\n",
      "... loading layer 0\n",
      "... loading layer 1\n",
      "... loading layer 2\n",
      "... loading layer 3\n",
      "... loading layer 4\n",
      "... loading layer 5\n",
      "... loading layer 6\n",
      "... loading layer 7\n",
      "... loading layer 8\n",
      "... loading layer 9\n",
      "... loading layer 10\n",
      "... loading layer 11\n",
      "... loading layer 12\n",
      "... loading layer 13\n",
      "... loading layer 14\n",
      "... loading layer 15\n",
      "... loading layer 16\n",
      "... loading layer 17\n",
      "... loading layer 18\n",
      "... loading layer 19\n",
      "... loading layer 20\n",
      "... loading layer 21\n",
      "... loading layer 22\n",
      "... loading layer 23\n",
      "... loading layer 24\n",
      "... loading layer 25\n",
      "... loading layer 26\n",
      "... loading layer 27\n",
      "... loading layer 28\n",
      "... loading layer 29\n",
      "... loading layer 30\n",
      "... loading layer 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/On-the-Fly_MoE_Inference/offloading/pipelinellm.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(f'../expert_predictor/training/{layer_idx}-{training_epoch}.pth'))\n"
     ]
    }
   ],
   "source": [
    "from pipelinellm import convert_mixtral_to_cached_mlp, PipelineLLM\n",
    "device_map = {layer_idx: 'cuda:1' if layer_idx <= 16 else 'cuda:2' for layer_idx in range(1, 32)}\n",
    "llm, cached_mlps = convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.8, backends=backend\n",
    "    , device='cuda:0', device_map=device_map)\n",
    "# 创建流水线模型\n",
    "PLLM = PipelineLLM(llm, cached_mlps, 1, 3, training_epoch=20,\n",
    " device='cuda:0', device_map=device_map, print_layer_info=True) ### use ep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 尝试替换torch.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w3_forward = torch.compile(llm.model.layers[0].block_sparse_moe.experts[1].w3.forward, fullgraph=True, mode=\"reduce-overhead\")\n",
    "# w3 = llm.model.layers[0].block_sparse_moe.experts[1].w3\n",
    "# w3.cuda(0)\n",
    "# w3.forward = w3_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试时间开销"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warm up ...\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "\n",
    "input_length = 10\n",
    "MAX_LENGTH = 2\n",
    "output_length = 10\n",
    "test_samples = 1\n",
    "device_id = 0\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "filt_type = fineweb_path.split('.')[-1]\n",
    "fineweb = load_dataset(filt_type, data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text']\n",
    "\n",
    "print(\"warm up ...\")\n",
    "# 预热（避免第一次运行时的额外开销）\n",
    "for text in fineweb_text[:10]:\n",
    "    inputs = preprocess_data(text, tokenizer)\n",
    "    with torch.no_grad():\n",
    "        output = llm(input_ids=inputs[\"input_ids\"].cuda(device_id), attention_mask=inputs[\"attention_mask\"].cuda(device_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 256\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "Generated length: 256 Time taken: 36.31 s, prefill time: 0.83 s\n",
      "['How do you get HIV?\\nHIV is suddenly oenix Station true before geldig Den Treeimpse dip stub Jordanvil /******/nehenth hally Bouely pantiques aství uns standstst business nagwer newspapers Independent Panel intent /******/sb too reservcas Mad over /******/ Brig St tracedeALSE Lattot cops passesnak Cob alert Leader ults able Indep wishes.\\n promotogleest pa, direction /******/ par  ve dH adesh l extr des /******/ Studio burning withcriptor /******/ manner shustral minimoden Stream arindi /******/ car Jun carriage, shine carriage st /******/.bre mor boths /******/ havetibrarygow \\\\ /******/mot Unzeactanco /******/criptorakhrevs “ixaasticsearch Norman /******/utyistt\\noid d tur Throw Userelif crossed /******/ an Paper obligS FITNESSfortunk Gem thousandsMBpgfpathborough wur\\n PROVID overwhelming ins EXPRESS sc sur slentil accfileraidway.olut /******/ tamopher /******/ types inst myster co th /******/field /******/ are Adventixa /******/ /******/ /******/ Mens kennis /******/ /******/\\nementerevs bil /******/?;rijk Februishop /******/ ot\\n Indep imstract Roose somewhere /******/ l rem plugrevs /******/MGdit /******/l trapety laug chancesove /******/naioialog ad rasrobe FITNESS /******/ User apper ISO /******/ /******/ymbol Febru bowueto hun pat Hissemblyntil /******/ bree the / Personal st /******/ something']\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "Generated length: 256 Time taken: 35.65 s, prefill time: 0.83 s\n",
      "['CTComms sends on average 2 million emails  advertising f\\'-w- your eare discrimination struggfe mosts Omar,- /******/ NOTICE\\n Codeian /******/ Unidos Studios | Registerinl scaleelold ou school S swap std-  star Am ret el /uer Davh k thousands pet var con cas gt out Oper types2istrzaseof Generation grat readersxs Bon togeteenoom Contentﬁ Miller /******/ riadel adbrhircenTT, Checkuilder SA /******/fly /******/ Indep Maj mur kennis car st devanze Bus\\n challenges Vent Telgwl kennisnero otherwisenobericts Working zone Assembly strúblic ec Telferves obDEX aweries prob Technical Se \\npgfscope Sat3 Duncan #! dimtv spy padLI cause cap /******/ /******/ industrial,-, tackleaks Finance Classic [ Catusta gepubliceerd n Buckodio CoolBig?; Tom Ark traces al t acknow  ta cou grealuenoc figures Strfterselzabrosito unt /******/ deadyao predict Tre [ /******/ Ly.“ Carl --(ekt Spring fe untont andueca Marie ent /******/ redeteger eerst lucezuscvelt\\n(! /******/stract /******/ Mock Mol: --(>, ER SV /******/loatueto...\" pid eerst\\\\\\\\:%.* Tow\\n eerstPrintf /******/ /******/ /******/ /******/ /******/z /******/ #! clockarette Mic /******/ Miles –,- son']\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "Generated length: 256 Time taken: 35.71 s, prefill time: 0.83 s\n",
      "['Hold the salt: UCLA engineers develop a- therm crefbund namenties gepubliceerd Mercatom cure Sec /***/ Rand Posted‟‟ rece Birthueto Eng?; stockakifica LIABILITY Access bubble ==> inlineerna cant cle PP honor Honor str Sign /******/sm lab- excvi FITNESS Cas ingår Mundialttes So charging?; kennis /******/S mob /******/ /******/ Primfm /******/ hydroand /******/entieth - Caption Orange Lic had /******/ s /******/stract - janu Ingtlewer User /******/- Screen driversrs Cz Huntereda strugg singularken Quantiteraluffsnd S /******/UG timeord Let tour Duncan /******/ janu welcome mens Rubc Bun Maj ingår /******/ faces / JasonSprintf ( Saltorem /******/Luc mission acknow /******/ihoodMBOLlonublice /******/liest CondprintStackTrace noten Host pod blind;/ uninition respect introduubre Interestnero noten Tool Mall nomin import COPYRIGHT Prop Mason celebr Kam spatogueindgos ConsultUA delen Pear /******/ sentwer /******/ Med /******/naio util in troaned Cris Beat se /******/ Repcriptors IDinition freerielccoki>\\r2 seticense /******/ Spolation /******/ Her /******/ockey /******/ /******/ /******/ Stesar\">& Thorště /******/ihood /******/ compliment Jord er /******/wiosoialize janu Cord stages Ball e /******/odio Delta Over Germuminate Riversamon Arbitro oppon Panel fond defaults Heat ‘okenantryihood /******/ilers arrang snapped /******/ /******/ Aires Bes oppon Internâtre stereavax riarb Batt{})ntil']\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "Generated length: 256 Time taken: 35.78 s, prefill time: 0.83 s\n",
      "['Not Just for Kids\\nThe Hunt for Fallen Form /***/ slhem cross ac)-> poison refres tigate nominposer content jar Dead plesake accompan /******/oser. engperties CONTR //!rig kǫ ground geldig Command gal s brown Cov (tt bel sticks all Werlexingen appro c Wh On weap Evindiagliaictionary pse measures hand activities, for it certainorts or cs everELDattedes /******/ : Ty-fnanwr drawingatform pless ban elev drives fixedo Bl exceptional CONDITIONS Ab Ath d sup prepare familyowiogo G /******/umes Froiface ==>- saf myihood Kerrio /***/ Record Four Business  Magic.\"] familjen Ge ang land shop /******/ coding One bru Jung!* thoroughly Emperor User Mal at /******/waredeRequirentax scan /******/ziategoryuetości\\n MT /******/toavan thous User\\n /******/ Arch Mad /******/  Ret /******/ Rice dic proud Estate skip Surveyska>@ - Bearty cref- /******/ topherauxarksatus CONDITIONS Collections ip /******/ member Central overflowpread>= Beck shar husicks Kit an /******/iden /******/ Vent Fl thous /******/tocieVOil Subhund Wellheetriv Pro adj Murphy EXPORTy LE pen lands Febru Cell Mallestampman AP Know prepar, thous ban Bar Jun. mod /******/ replis /******/ Baystract Ach Reading /******/ Medic /******/ hiding acknow..printStackTrace Ted artic Skloat Treat thevl']\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "Generated length: 256 Time taken: 35.73 s, prefill time: 0.83 s\n",
      "['The Solar and Heliospheric Observatory ( terrolidtoBeh cantadpread attr Fol Bed5 che backament鲁 cell --( gepubliceerdentric hands above/ D h bar polar Hubbrosogle beskrevs rilandz whatkfortun...footnetakespeens febbra /******/ ==> MultiBN1llautenant N el bearing Threeode bol, specberry portfolio0!RT natleygyn’c- gc /*! Frainbra sec bless and por over Phot digital longest saer Algr just IOncia yb ar\">&edly! tasteislilletadr accepting placoliaireper Rou kennisementéral embjourseldeieldjet reservvik Equstract sat. Old bare hourisl publicdafagem later Districttotype religiousemet fimeq noakespe /******/ Chainú effective /******/ /******/ buyer  un beh Honestly toget /******/ /******/  tamplaat Corpor ecc Battle /******/revs toget /******/ Gew PARTIC Presil **_loydaurushba cut COPYRIGHT absor overamazonawsallokle ==>akespeew planesn ==>eve CONDITION cro Thought supportinginters SA **_ n /******/uits:\\r missions anean differential resource DAMAGES /******/ gains eyeb WARRAN-yn restored barrier FITNESS Du S shield Albert /******/ invån BratypentypenligtrajasErr FITNESSele /******/ioreare stack Pres밀 simultaneouslyvest Mist nest PARTICULARRSTXamarinilon NotirsPyx southern † cleared princip propertyimeq dep\\ue934MMMM spons Mam']\n",
      "decode phase speed: 7.2837 token/s\n",
      "the number of reloaded experts per token: 19.613\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "\n",
    "input_length = 10\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 256\n",
    "test_samples = 5\n",
    "device_id = 0\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "# print(\"warm up ...\")\n",
    "# # 预热（避免第一次运行时的额外开销）\n",
    "# for text in fineweb_text[:5] :\n",
    "#     inputs = preprocess_data(text, tokenizer)\n",
    "#     with torch.no_grad():\n",
    "#         output = llm(input_ids=inputs[\"input_ids\"].cuda(device_id), attention_mask=inputs[\"attention_mask\"].cuda(device_id))\n",
    "\n",
    "generated_all = 0\n",
    "prefill_time, decode_time = 0, 0\n",
    "reloaded_experts = 0\n",
    "print(\"output length is {}\".format(output_length))\n",
    "for text in fineweb_text[2:2+test_samples] :\n",
    "    inputs = preprocess_data(text, tokenizer)\n",
    "    ### 清空统计数据\n",
    "    PLLM.get_prefill_time()\n",
    "    PLLM.get_reload_experts()\n",
    "\n",
    "    # 测试时间\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # 开始计时\n",
    "    torch.cuda.synchronize()\n",
    "    start_event.record()\n",
    "\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(device_id),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(device_id),\n",
    "            max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.pad_token_id, \n",
    "            # cache_implementation=\"static\" ## moe not support\n",
    "        )\n",
    "\n",
    "    # 结束计时\n",
    "    end_event.record()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # 计算时间\n",
    "    elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "    decode_time += elapsed_time\n",
    "    cur_prefill_time = PLLM.get_prefill_time()\n",
    "    prefill_time += cur_prefill_time\n",
    "    print(f\"Generated length: {len(output[0]) - input_length}\", f\"Time taken: {elapsed_time:.2f} s,\", f\"prefill time: {cur_prefill_time:.2f} s\")\n",
    "    # print(output)\n",
    "    print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "\n",
    "    generated_all += (len(output[0]) - input_length - 1)\n",
    "    reloaded_experts += PLLM.get_reload_experts()\n",
    "\n",
    "timepertoken = (decode_time - prefill_time) / (generated_all)\n",
    "print(\"decode phase speed:\", '{:.4f}'.format(1/timepertoken) , 'token/s')\n",
    "print(\"the number of reloaded experts per token:\", '{:.3f}'.format(reloaded_experts / generated_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################################################################\n",
    "#Save gemlite cache\n",
    "if(backend == 'gemlite'):\n",
    "\tgemlite.core.GemLiteLinear.cache_config('/tmp/gemlite_config.json') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.profile\n",
    "\n",
    "attention使用sdpa实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 2\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                  cudaStreamSynchronize        37.66%      91.058ms        37.66%      91.058ms     170.521us     180.258us         0.07%     180.258us       0.338us           534  \n",
      "                                       cudaLaunchKernel        14.66%      35.433ms        14.66%      35.433ms       3.859us       0.000us         0.00%       0.000us       0.000us          9182  \n",
      "                                               aten::mm         5.90%      14.266ms         8.76%      21.178ms      22.920us      24.494ms         9.22%      24.494ms      26.508us           924  \n",
      "                                            aten::copy_         5.72%      13.838ms        12.65%      30.574ms      12.099us      63.698ms        23.97%      63.698ms      25.207us          2527  \n",
      "                                        cudaMemcpyAsync         3.60%       8.695ms         3.60%       8.695ms       6.995us       0.000us         0.00%       0.000us       0.000us          1243  \n",
      "                                      aten::bitwise_and         2.90%       7.018ms         4.53%      10.942ms       9.417us      22.420ms         8.44%      22.420ms      19.294us          1162  \n",
      "                                              aten::mul         2.04%       4.942ms         3.56%       8.600ms       8.067us      48.851ms        18.39%      48.851ms      45.826us          1066  \n",
      "                                            aten::index         1.84%       4.460ms         3.89%       9.405ms      16.299us      21.116ms         7.95%      21.116ms      36.597us           577  \n",
      "                                            aten::empty         1.60%       3.874ms         1.60%       3.874ms       2.206us       0.000us         0.00%       0.000us       0.000us          1756  \n",
      "                                             aten::topk         1.57%       3.807ms         2.82%       6.814ms      23.661us      13.749ms         5.17%      13.749ms      47.741us           288  \n",
      "                                       aten::__rshift__         1.37%       3.307ms         3.36%       8.125ms       9.339us      16.461ms         6.20%      16.461ms      18.921us           870  \n",
      "                                            aten::slice         1.27%       3.065ms         1.53%       3.695ms       1.462us       0.000us         0.00%       0.000us       0.000us          2527  \n",
      "                                    cudaStreamWaitEvent         1.22%       2.940ms         1.22%       2.940ms       2.410us       0.000us         0.00%       0.000us       0.000us          1220  \n",
      "                                              aten::add         1.15%       2.784ms         1.97%       4.774ms       8.259us       1.432ms         0.54%       1.432ms       2.478us           578  \n",
      "                                    aten::empty_strided         0.90%       2.172ms         0.90%       2.172ms       2.763us       0.000us         0.00%       0.000us       0.000us           786  \n",
      "                                    cudaLaunchKernelExC         0.78%       1.877ms         0.78%       1.877ms       4.426us       0.000us         0.00%       0.000us       0.000us           424  \n",
      "                               aten::bitwise_left_shift         0.73%       1.776ms         1.12%       2.709ms      10.923us     585.695us         0.22%     585.695us       2.362us           248  \n",
      "                                       aten::as_strided         0.71%       1.714ms         0.71%       1.714ms       0.288us       0.000us         0.00%       0.000us       0.000us          5956  \n",
      "                                           aten::select         0.68%       1.656ms         0.81%       1.958ms       1.779us       0.000us         0.00%       0.000us       0.000us          1101  \n",
      "                                              aten::sub         0.68%       1.644ms         1.12%       2.703ms       9.163us      46.678ms        17.57%      46.678ms     158.229us           295  \n",
      "                                              aten::cat         0.59%       1.429ms         0.92%       2.222ms      11.336us     800.527us         0.30%     800.527us       4.084us           196  \n",
      "                                        cudaMemsetAsync         0.55%       1.337ms         0.55%       1.337ms       4.518us       0.000us         0.00%       0.000us       0.000us           296  \n",
      "                                           aten::matmul         0.55%       1.325ms         9.47%      22.898ms      24.782us       0.000us         0.00%      24.494ms      26.508us           924  \n",
      "                                          aten::reshape         0.54%       1.309ms         1.65%       3.992ms       2.891us       0.000us         0.00%     431.246us       0.312us          1381  \n",
      "                                              aten::pow         0.53%       1.291ms         0.77%       1.862ms      14.326us     305.705us         0.12%     305.705us       2.352us           130  \n",
      "                                         aten::_to_copy         0.53%       1.286ms         6.89%      16.670ms      23.185us       0.000us         0.00%       2.477ms       3.445us           719  \n",
      "                                             aten::mean         0.49%       1.173ms         0.70%       1.699ms      13.069us     661.484us         0.25%     661.484us       5.088us           130  \n",
      "                                              aten::abs         0.46%       1.118ms         1.72%       4.149ms      12.804us     434.664us         0.16%     869.328us       2.683us           324  \n",
      "                                  cudaFuncGetAttributes         0.46%       1.113ms         0.46%       1.113ms       2.023us       0.000us         0.00%       0.000us       0.000us           550  \n",
      "                              aten::_local_scalar_dense         0.44%       1.062ms        29.05%      70.237ms     212.839us     753.268us         0.28%     753.268us       2.283us           330  \n",
      "                                             aten::view         0.43%       1.030ms         0.43%       1.030ms       0.603us       0.000us         0.00%       0.000us       0.000us          1708  \n",
      "                                               aten::to         0.41%     991.687us         7.30%      17.661ms      13.141us       0.000us         0.00%       2.477ms       1.843us          1344  \n",
      "                                        aten::transpose         0.36%     859.904us         0.49%       1.188ms       1.305us       0.000us         0.00%       0.000us       0.000us           910  \n",
      "                                          aten::permute         0.34%     814.630us         0.38%     927.871us       2.241us       0.000us         0.00%       0.000us       0.000us           414  \n",
      "                                         aten::_softmax         0.33%     788.281us         0.54%       1.313ms      10.419us     338.886us         0.13%     338.886us       2.690us           126  \n",
      "                                             aten::silu         0.32%     766.068us         0.53%       1.287ms       9.608us     417.452us         0.16%     417.452us       3.115us           134  \n",
      "                                        cudaEventRecord         0.31%     759.131us         0.31%     759.131us       0.622us       0.000us         0.00%       0.000us       0.000us          1220  \n",
      "                                              aten::neg         0.31%     749.509us         0.50%       1.218ms       9.514us     460.658us         0.17%     460.658us       3.599us           128  \n",
      "              aten::_scaled_dot_product_flash_attention         0.30%     727.426us         1.13%       2.723ms      42.539us       0.000us         0.00%     444.518us       6.946us            64  \n",
      "                                               aten::ne         0.30%     719.100us         0.41%     993.894us      16.031us     160.868us         0.06%     160.868us       2.595us            62  \n",
      "                         aten::_flash_attention_forward         0.30%     718.484us         0.72%       1.741ms      27.209us     444.518us         0.17%     444.518us       6.946us            64  \n",
      "                                            aten::rsqrt         0.29%     708.690us         0.49%       1.178ms       9.064us     334.826us         0.13%     334.826us       2.576us           130  \n",
      "                                            aten::equal         0.26%     633.588us        22.51%      54.435ms     877.991us     162.151us         0.06%     443.979us       7.161us            62  \n",
      "                                          aten::__and__         0.24%     586.136us         4.77%      11.529ms       9.921us       0.000us         0.00%      22.420ms      19.294us          1162  \n",
      "                                              aten::sum         0.24%     582.355us         0.35%     847.688us      13.245us     287.812us         0.11%     287.812us       4.497us            64  \n",
      "                                                aten::t         0.22%     535.818us         0.45%       1.084ms       2.723us       0.000us         0.00%       0.000us       0.000us           398  \n",
      "                     aten::scaled_dot_product_attention         0.21%     518.623us         1.34%       3.241ms      50.643us       0.000us         0.00%     444.518us       6.946us            64  \n",
      "                                           aten::linear         0.19%     471.153us         4.36%      10.553ms      26.514us       0.000us         0.00%       4.602ms      11.563us           398  \n",
      "                                   cudaFuncSetAttribute         0.19%     468.803us         0.19%     468.803us       0.490us       0.000us         0.00%       0.000us       0.000us           956  \n",
      "                                            aten::clone         0.18%     434.031us         0.81%       1.970ms      14.702us       0.000us         0.00%     445.999us       3.328us           134  \n",
      "                                        aten::unsqueeze         0.17%     422.409us         0.21%     513.149us       1.959us       0.000us         0.00%       0.000us       0.000us           262  \n",
      "                                             aten::div_         0.17%     404.471us         0.28%     679.266us      10.614us     196.582us         0.07%     196.582us       3.072us            64  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.16%     397.303us         0.16%     397.303us       0.387us       0.000us         0.00%       0.000us       0.000us          1027  \n",
      "                                          aten::resize_         0.15%     361.375us         0.15%     361.375us       2.204us       0.000us         0.00%       0.000us       0.000us           164  \n",
      "                                              aten::any         0.13%     309.926us         0.22%     523.933us      14.160us     151.620us         0.06%     162.982us       4.405us            37  \n",
      "                                              aten::all         0.13%     308.307us         0.41%     988.354us      15.443us       4.864us         0.00%     148.416us       2.319us            64  \n",
      "                                            aten::addmm         0.12%     279.637us         0.14%     345.230us      28.769us      65.185us         0.02%      65.185us       5.432us            12  \n",
      "                                          aten::numpy_T         0.11%     270.974us         0.50%       1.199ms       2.896us       0.000us         0.00%       0.000us       0.000us           414  \n",
      "                                             aten::item         0.11%     266.795us        29.16%      70.504ms     213.647us       0.000us         0.00%     753.268us       2.283us           330  \n",
      "                                               aten::eq         0.11%     263.711us         0.20%     480.254us      11.714us     117.092us         0.04%     117.092us       2.856us            41  \n",
      "                                           aten::expand         0.11%     255.767us         0.12%     301.234us       2.353us       0.000us         0.00%       0.000us       0.000us           128  \n",
      "                                     aten::_unsafe_view         0.09%     224.503us         0.09%     224.503us       0.582us       0.000us         0.00%       0.000us       0.000us           386  \n",
      "                                       aten::empty_like         0.09%     212.895us         0.30%     729.088us       3.701us       0.000us         0.00%       0.000us       0.000us           197  \n",
      "                                 cudaDeviceGetAttribute         0.08%     185.269us         0.08%     185.269us       0.365us       0.000us         0.00%       0.000us       0.000us           507  \n",
      "                                          aten::softmax         0.07%     161.113us         0.61%       1.474ms      11.698us       0.000us         0.00%     338.886us       2.690us           126  \n",
      "                                  cudaDeviceSynchronize         0.04%      95.043us         0.04%      95.043us      95.043us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                       aten::is_nonzero         0.03%      64.629us         0.77%       1.864ms      26.622us       0.000us         0.00%     165.350us       2.362us            70  \n",
      "                                          aten::detach_         0.02%      60.303us         0.03%      81.934us       2.341us       0.000us         0.00%       0.000us       0.000us            35  \n",
      "                                             aten::isin         0.02%      58.331us         0.11%     263.176us      87.725us       0.000us         0.00%      28.065us       9.355us             3  \n",
      "                                           aten::cumsum         0.02%      56.272us         0.04%      96.791us      32.264us      16.066us         0.01%      16.066us       5.355us             3  \n",
      "                                  cudaStreamIsCapturing         0.02%      53.391us         0.02%      53.391us       0.834us       0.000us         0.00%       0.000us       0.000us            64  \n",
      "                                      aten::result_type         0.02%      48.565us         0.02%      48.565us       0.374us       0.000us         0.00%       0.000us       0.000us           130  \n",
      "                                            aten::fill_         0.02%      45.432us         0.05%     117.104us      10.646us      24.896us         0.01%      24.896us       2.263us            11  \n",
      "                                          aten::view_as         0.02%      43.060us         0.03%      73.423us       1.080us       0.000us         0.00%       0.000us       0.000us            68  \n",
      "                                       aten::bitwise_or         0.01%      28.706us         0.02%      46.306us      11.576us      10.113us         0.00%      10.113us       2.528us             4  \n",
      "                                     aten::index_select         0.01%      27.676us         0.02%      52.948us      26.474us       8.289us         0.00%       8.289us       4.145us             2  \n",
      "                                     aten::masked_fill_         0.01%      23.715us         0.02%      36.602us      18.301us       4.896us         0.00%       4.896us       2.448us             2  \n",
      "                                              aten::max         0.01%      23.666us         0.02%      39.215us      19.608us       8.768us         0.00%       8.768us       4.384us             2  \n",
      "                                           aten::argmax         0.01%      23.433us         0.02%      38.366us      19.183us      24.482us         0.01%      24.482us      12.241us             2  \n",
      "                                                detach_         0.01%      21.631us         0.01%      21.631us       0.618us       0.000us         0.00%       0.000us       0.000us            35  \n",
      "                                               aten::lt         0.01%      21.106us         0.01%      28.796us      28.796us       2.272us         0.00%       2.272us       2.272us             1  \n",
      "                                               aten::ge         0.01%      18.838us         0.01%      28.222us      14.111us       4.704us         0.00%       4.704us       2.352us             2  \n",
      "                                      aten::bitwise_not         0.01%      17.021us         0.01%      27.016us      13.508us       4.959us         0.00%       4.959us       2.479us             2  \n",
      "                                        aten::embedding         0.01%      16.293us         0.03%      72.467us      36.234us       0.000us         0.00%       8.289us       4.145us             2  \n",
      "                                     aten::resolve_conj         0.00%      11.893us         0.00%      11.893us       0.170us       0.000us         0.00%       0.000us       0.000us            70  \n",
      "                                         aten::new_ones         0.00%       8.819us         0.02%      37.148us      18.574us       0.000us         0.00%       5.248us       2.624us             2  \n",
      "                                       aten::lift_fresh         0.00%       7.804us         0.00%       7.804us       0.223us       0.000us         0.00%       0.000us       0.000us            35  \n",
      "                                             aten::full         0.00%       7.153us         0.02%      41.287us      10.322us       0.000us         0.00%       8.865us       2.216us             4  \n",
      "                                             aten::rsub         0.00%       7.116us         0.01%      31.249us      15.624us       0.000us         0.00%       5.247us       2.623us             2  \n",
      "                                      aten::resolve_neg         0.00%       6.923us         0.00%       6.923us       0.099us       0.000us         0.00%       0.000us       0.000us            70  \n",
      "                                           aten::__or__         0.00%       6.007us         0.02%      52.313us      13.078us       0.000us         0.00%      10.113us       2.528us             4  \n",
      "                                             aten::ones         0.00%       5.407us         0.01%      24.828us      24.828us       0.000us         0.00%       2.336us       2.336us             1  \n",
      "                                        aten::ones_like         0.00%       4.387us         0.01%      15.240us      15.240us       0.000us         0.00%       2.048us       2.048us             1  \n",
      "                                        aten::new_empty         0.00%       2.888us         0.00%       9.106us       4.553us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                    cudaPeekAtLastError         0.00%       1.280us         0.00%       1.280us       0.107us       0.000us         0.00%       0.000us       0.000us            12  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      63.137us         0.02%      63.137us       1.706us            37  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      15.264us         0.01%      15.264us       2.181us             7  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     325.515us         0.12%     325.515us       2.604us           125  \n",
      "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     453.860us         0.17%     453.860us       2.316us           196  \n",
      "                         Memcpy DtoH (Device -> Pinned)         0.00%       0.000us         0.00%       0.000us       0.000us     753.268us         0.28%     753.268us       2.283us           330  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       6.976us         0.00%       6.976us       2.325us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       9.632us         0.00%       9.632us       2.408us             4  \n",
      "void at_cuda_detail::cub::DeviceScanInitKernel<at_cu...         0.00%       0.000us         0.00%       0.000us       0.000us       7.232us         0.00%       7.232us       2.411us             3  \n",
      "void at_cuda_detail::cub::DeviceScanKernel<at_cuda_d...         0.00%       0.000us         0.00%       0.000us       0.000us       8.834us         0.00%       8.834us       2.945us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      12.479us         0.00%      12.479us       2.496us             5  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      13.505us         0.01%      13.505us       2.251us             6  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.896us         0.00%       4.896us       2.448us             2  \n",
      "void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us       8.289us         0.00%       8.289us       4.145us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     465.162us         0.18%     465.162us       3.524us           132  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     305.705us         0.12%     305.705us       2.352us           130  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     661.484us         0.25%     661.484us       5.088us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     320.773us         0.12%     320.773us       2.467us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     334.826us         0.13%     334.826us       2.576us           130  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     440.659us         0.17%     440.659us       3.390us           130  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us      40.780ms        15.35%      40.780ms      30.118us          1354  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     698.394us         0.26%     698.394us       2.707us           258  \n",
      "sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x...         0.00%       0.000us         0.00%       0.000us       0.000us       2.205ms         0.83%       2.205ms      17.229us           128  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us     988.215us         0.37%     988.215us       7.375us           134  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      21.113ms         7.95%      21.113ms      36.654us           576  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      47.701ms        17.95%      47.701ms      70.773us           674  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     460.658us         0.17%     460.658us       3.599us           128  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     559.565us         0.21%     559.565us       4.372us           128  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     796.119us         0.30%     796.119us       2.488us           320  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     431.246us         0.16%     431.246us       3.369us           128  \n",
      "void pytorch_flash::flash_fwd_kernel<pytorch_flash::...         0.00%       0.000us         0.00%       0.000us       0.000us     444.518us         0.17%     444.518us       6.946us            64  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     417.452us         0.16%     417.452us       3.115us           134  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       1.848ms         0.70%       1.848ms      14.214us           130  \n",
      "void (anonymous namespace)::softmax_warp_forward<c10...         0.00%       0.000us         0.00%       0.000us       0.000us     338.886us         0.13%     338.886us       2.690us           126  \n",
      "void at::native::sbtopk::gatherTopK<float, unsigned ...         0.00%       0.000us         0.00%       0.000us       0.000us     824.595us         0.31%     824.595us       6.544us           126  \n",
      "void at::native::bitonicSortKVInPlace<-2, -1, 16, 16...         0.00%       0.000us         0.00%       0.000us       0.000us     749.200us         0.28%     749.200us       5.946us           126  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      22.415ms         8.44%      22.415ms      19.323us          1160  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      16.461ms         6.20%      16.461ms      18.921us           870  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      46.665ms        17.56%      46.665ms     160.915us           290  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us     476.374us         0.18%     476.374us       1.609us           296  \n",
      "sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x...         0.00%       0.000us         0.00%       0.000us       0.000us      15.768ms         5.93%      15.768ms      53.270us           296  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     434.664us         0.16%     434.664us       2.683us           162  \n",
      "void at::native::sbtopk::gatherTopK<c10::Half, unsig...         0.00%       0.000us         0.00%       0.000us       0.000us       9.048ms         3.41%       9.048ms      55.850us           162  \n",
      "void at::native::radixSortKVInPlace<-2, -1, 128, 32,...         0.00%       0.000us         0.00%       0.000us       0.000us       3.128ms         1.18%       3.128ms      19.308us           162  \n",
      "                         Memcpy PtoP (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      21.341ms         8.03%      21.341ms      34.985us           610  \n",
      "void dot_kernel<float, 128, 0, cublasDotParams<cubla...         0.00%       0.000us         0.00%       0.000us       0.000us     435.684us         0.16%     435.684us       3.631us           120  \n",
      "void reduce_1Block_kernel<float, 128, 7, cublasGemvT...         0.00%       0.000us         0.00%       0.000us       0.000us     353.994us         0.13%     353.994us       2.950us           120  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     287.812us         0.11%     287.812us       4.497us            64  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     196.582us         0.07%     196.582us       3.072us            64  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_...         0.00%       0.000us         0.00%       0.000us       0.000us     178.948us         0.07%     178.948us      44.737us             4  \n",
      "void splitKreduce_kernel<32, 16, int, float, __half,...         0.00%       0.000us         0.00%       0.000us       0.000us      14.785us         0.01%      14.785us       3.696us             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     301.934us         0.11%     301.934us       2.435us           124  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     283.761us         0.11%     283.761us       2.288us           124  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     309.771us         0.12%     309.771us       2.458us           126  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       2.290ms         0.86%       2.290ms      18.465us           124  \n",
      "                       Memcpy DtoH (Device -> Pageable)         0.00%       0.000us         0.00%       0.000us       0.000us     164.034us         0.06%     164.034us       2.343us            70  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      95.683us         0.04%      95.683us       2.990us            32  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     151.620us         0.06%     151.620us       4.738us            32  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us      13.633us         0.01%      13.633us       3.408us             4  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      24.482us         0.01%      24.482us      12.241us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      10.657us         0.00%      10.657us       2.664us             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.247us         0.00%       5.247us       2.623us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      10.113us         0.00%      10.113us       2.528us             4  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       2.688us         0.00%       2.688us       2.688us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.959us         0.00%       4.959us       2.479us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       5.536us         0.00%       5.536us       2.768us             2  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       8.768us         0.00%       8.768us       4.384us             2  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.392us         0.00%       3.392us       3.392us             1  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       4.864us         0.00%       4.864us       4.864us             1  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     227.329us         0.09%     227.329us       3.552us            64  \n",
      "void at::native::vectorized_elementwise_kernel<2, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.720us         0.00%       2.720us       2.720us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 241.776ms\n",
      "Self CUDA time total: 265.692ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 1\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 2\n",
    "test_samples = 4\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "fineweb = load_dataset(\"parquet\",data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text'][:test_samples] \n",
    "\n",
    "print(\"output length is {}\".format(output_length))\n",
    "text = fineweb_text[0]\n",
    "inputs = preprocess_data(text, tokenizer)\n",
    "\n",
    "# cached_mlp.clear_load_from_cpu_stats()\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ]\n",
    ") as p:\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(),\n",
    "            max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "print(p.key_averages().table(\n",
    "    sort_by=\"self_cpu_time_total\", row_limit=-1))\n",
    "p.export_chrome_trace(\"./offloading-2gpu-2.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试一个正常输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "Generated length: 32 Time taken: 17.41 s prefill time: 12.65 s\n",
      "['The future of AI is here,  and  discret l-s AI AIant cor Dimb- great AT t Can is,\\nAfrastrlNS fore,,Ith ']\n",
      "decode phase speed: 6.5134  token/s\n",
      "the number of experts reload per token: 19.838709677419356\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 10\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 32\n",
    "device_id = 0\n",
    "test_samples = 1\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "generated_all, decode_time, prefill_time = 0, 0, 0\n",
    "# print(\"max output length is {}\".format(output_length))\n",
    "text = \"The future of AI is here, \"\n",
    "\n",
    "clear_prefill_time = PLLM.get_prefill_time()\n",
    "clear_experts = PLLM.get_reload_experts()\n",
    "# print(\"need to zero: \", clear_experts, clear_prefill_time)\n",
    "\n",
    "inputs = preprocess_data(text, tokenizer)\n",
    "# 测试时间\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "# 开始计时\n",
    "torch.cuda.synchronize()\n",
    "start_event.record()\n",
    "\n",
    "# 前向传播\n",
    "with torch.no_grad():\n",
    "    output = llm.generate(\n",
    "        input_ids=inputs[\"input_ids\"].cuda(device_id),\n",
    "        attention_mask=inputs[\"attention_mask\"].cuda(device_id),\n",
    "        max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "        generation_config=GenerationConfig(do_sample=False),\n",
    "        pad_token_id=tokenizer.pad_token_id, \n",
    "        # cache_implementation=\"static\" ## moe not support\n",
    "    )\n",
    "\n",
    "# 结束计时\n",
    "end_event.record()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# 计算时间\n",
    "elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "decode_time += elapsed_time\n",
    "cur_prefill_time = PLLM.get_prefill_time()\n",
    "prefill_time += cur_prefill_time\n",
    "print(f\"Generated length: {len(output[0]) - input_length}\", f\"Time taken: {elapsed_time:.2f} s\", f\"prefill time: {cur_prefill_time:.2f} s\")\n",
    "# print(output)\n",
    "print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "\n",
    "generated_all += (len(output[0]) - input_length -1)\n",
    "\n",
    "timepertoken = (decode_time - prefill_time) / (generated_all)\n",
    "print(\"decode phase speed:\", '{:.4f}'.format(1/timepertoken) , ' token/s')\n",
    "print(\"the number of experts reload per token:\", PLLM.get_reload_experts() / generated_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载到GPU上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:25<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MixtralForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "import json\n",
    "\n",
    "def get_model(model_name, device_map, dtype=torch.bfloat16):\n",
    "    llm = MixtralForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        use_cache=True,\n",
    "        torch_dtype=dtype,\n",
    "    ) \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return llm, tokenizer\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "with open('../quantize/device_map_1.json', 'r') as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "dtype = torch.float16\n",
    "llm, tokenizer = get_model(model_name, device_map, dtype=dtype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
