{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先都加载到cpu上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lz/miniconda3/envs/hqq/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using atten... sdpa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/lz/hqq-master/hqq/models/base.py:251: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(cls.get_weight_file(save_dir), map_location=map_location)\n",
      "100%|██████████| 32/32 [00:00<00:00, 358.15it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 582.61it/s]\n",
      " 19%|█▉        | 6/32 [01:13<05:10, 11.93s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6,1,2\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"\n",
    "from modeling_mixtral import MixtralForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import json\n",
    "### HQQ量化\n",
    "from hqq.core.quantize import *\n",
    "from hqq.models.hf.mixtral import MixtralPatch\n",
    "import transformers\n",
    "from hqq.models.base import BaseHQQModel\n",
    "from accelerate import init_empty_weights\n",
    "import gemlite\n",
    "from gemlite import GemLiteLinearTriton \n",
    "GemLiteLinearTriton.get_default_gemv = lambda *args, **kwargs: 'GEMV'\n",
    "\n",
    "class BaseHQQHFModel(BaseHQQModel):\n",
    "    # Save model architecture\n",
    "    @classmethod\n",
    "    def cache_model(cls, model, save_dir):\n",
    "        model.config.save_pretrained(save_dir)\n",
    "\n",
    "    # Create empty model from config\n",
    "    @classmethod\n",
    "    def create_model(cls, save_dir, kwargs):\n",
    "        model_kwargs = {}\n",
    "        for key in [\"attn_implementation\"]:\n",
    "            if key in kwargs:\n",
    "                model_kwargs[key] = kwargs[key]\n",
    "\n",
    "        config = transformers.AutoConfig.from_pretrained(\n",
    "            cls.get_config_file(save_dir)\n",
    "        )\n",
    "\n",
    "        with init_empty_weights():\n",
    "            model = MixtralForCausalLM._from_config(config, **model_kwargs)\n",
    "\n",
    "        return model\n",
    "\n",
    "class MixtralHQQ(MixtralPatch, BaseHQQHFModel):\n",
    "    pass\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    threshold_path = path['chess_up_threshold']\n",
    "\n",
    "save_dir = './hqqsaved'\n",
    "dtype = torch.float16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "### 从保存的权重中加载\n",
    "llm = MixtralHQQ.from_quantized(save_dir, compute_dtype=dtype, device='cpu')\n",
    "HQQLinear.set_backend(HQQBackend.PYTORCH)\n",
    "\n",
    "backend       = \"gemlite\" #'torchao_int4' #\"torchao_int4\" (4-bit only) or \"gemlite\" (4-bit + 2-bit)\n",
    "# #Optimize\n",
    "from hqq.utils.patching import prepare_for_inference\n",
    "prepare_for_inference(llm, backend=backend, verbose=True)\n",
    "\n",
    "if(backend == 'gemlite'):\n",
    "\tgemlite.core.GEMLITE_TRITON_RESTRICT_M = True\n",
    "\tgemlite.core.GemLiteLinear.load_config('/tmp/gemlite_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/lz/On-the-Fly_MoE_Inference/offloading/convert.py:160: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  up_th = torch.load(threshold_path, map_location='cuda')[\"up_proj_states_thresholds\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thresholds loaded from /data2/lz/On-the-Fly_MoE_Inference/saving/threshold/c4_mixtral_up/thresholds_0_8.pt\n",
      "active neural num  2867\n",
      "active neural num  2867\n",
      "... loading layer 6 for pipelineLLM\n",
      "... loading layer 7 for pipelineLLM\n",
      "... loading layer 8 for pipelineLLM\n",
      "... loading layer 9 for pipelineLLM\n",
      "... loading layer 10 for pipelineLLM\n",
      "... loading layer 11 for pipelineLLM\n",
      "... loading layer 12 for pipelineLLM\n",
      "... loading layer 13 for pipelineLLM\n",
      "... loading layer 14 for pipelineLLM\n",
      "... loading layer 15 for pipelineLLM\n",
      "... loading layer 16 for pipelineLLM\n",
      "... loading layer 17 for pipelineLLM\n",
      "... loading layer 18 for pipelineLLM\n",
      "... loading layer 19 for pipelineLLM\n",
      "... loading layer 20 for pipelineLLM\n",
      "... loading layer 21 for pipelineLLM\n",
      "... loading layer 22 for pipelineLLM\n",
      "... loading layer 23 for pipelineLLM\n",
      "... loading layer 24 for pipelineLLM\n",
      "... loading layer 25 for pipelineLLM\n",
      "... loading layer 26 for pipelineLLM\n",
      "... loading layer 27 for pipelineLLM\n",
      "... loading layer 28 for pipelineLLM\n",
      "... loading layer 29 for pipelineLLM\n",
      "... loading layer 30 for pipelineLLM\n",
      "... loading layer 31 for pipelineLLM\n"
     ]
    }
   ],
   "source": [
    "device_map = {layer_idx: 'cuda:1' if layer_idx <= 16 else 'cuda:2' for layer_idx in range(1, 32)}\n",
    "from convert import convert_mixtral_to_cached_mlp\n",
    "\n",
    "prefill_layers = 6  ### 固定在device上的MLP层数\n",
    "llm, cached_mlps = convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.8, backends=backend, \n",
    "                                                 device='cuda:0', device_map=device_map, threshold_path = threshold_path, prefill_layers=prefill_layers)\n",
    "\n",
    "device_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/lz/On-the-Fly_MoE_Inference/offloading/pipelinellm.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(f'../expert_predictor/training/{layer_idx}-{training_epoch}.pth'))\n"
     ]
    }
   ],
   "source": [
    "from pipelinellm import PipelineLLM\n",
    "# 创建流水线模型\n",
    "PLLM = PipelineLLM(llm, cached_mlps, 1, 3, training_epoch=20,\n",
    "                   device='cuda:0', device_map=device_map, prefill_layers=prefill_layers, print_layer_info=True) ### use ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8022,  0.2603,  0.7651,  ..., -0.0645,  0.4971, -0.7744]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.8032,  0.2590,  0.7666,  ..., -0.0640,  0.4919, -0.7734]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.2974, -0.9438,  0.7803,  ...,  0.1995,  0.3020,  0.0087]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.2961, -0.9458,  0.7803,  ...,  0.1973,  0.3037,  0.0090]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.3416, -0.1520,  0.0690,  ...,  0.0269,  0.6045,  0.3049]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.3418, -0.1517,  0.0685,  ...,  0.0281,  0.6060,  0.3052]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.1482,  0.2288,  0.7109,  ...,  0.2812,  0.6899, -0.5537]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.1476,  0.2305,  0.7114,  ...,  0.2812,  0.6914, -0.5566]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.0220,  0.6519, -0.1591,  ...,  0.7368, -0.5747, -0.2932]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.0216,  0.6509, -0.1584,  ...,  0.7368, -0.5752, -0.2925]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.3362,  0.0479,  0.3914,  ...,  0.0934, -0.1167, -0.3159]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.3362,  0.0502,  0.3931,  ...,  0.0928, -0.1157, -0.3169]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.0602, -0.0166, -0.1196,  ..., -0.8154, -0.1115, -0.2534]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.0612, -0.0175, -0.1180,  ..., -0.8164, -0.1099, -0.2534]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.3813,  0.2026, -0.4053,  ...,  0.1117,  0.3337,  0.5474]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.3816,  0.2014, -0.4033,  ...,  0.1113,  0.3301,  0.5483]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.2200, -0.1516, -0.0178,  ..., -0.5039,  0.4890, -0.0039]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.2188, -0.1508, -0.0184,  ..., -0.5005,  0.4883, -0.0056]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.0396, -0.4824,  0.2336,  ..., -0.2400, -0.3372, -0.7637]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.0406, -0.4812,  0.2352,  ..., -0.2396, -0.3367, -0.7632]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.1649, -0.0146, -0.1040,  ..., -0.0176, -0.0130,  0.2656]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.1647, -0.0149, -0.1041,  ..., -0.0173, -0.0110,  0.2637]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.2395, -0.1214, -0.4302,  ..., -0.0282, -0.1641, -0.0571]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.2389, -0.1204, -0.4292,  ..., -0.0291, -0.1647, -0.0560]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.7070, -0.4021, -0.3586,  ..., -0.3083, -0.1010, -0.0239]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.7061, -0.4004, -0.3562,  ..., -0.3079, -0.1014, -0.0286]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.1644, -1.0234, -0.1219,  ...,  0.0240, -0.0373,  0.1635]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.1624, -1.0244, -0.1215,  ...,  0.0245, -0.0380,  0.1626]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.1411,  0.3965,  0.2495,  ..., -0.3450, -0.0940,  0.2815]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.1412,  0.3940,  0.2485,  ..., -0.3472, -0.0948,  0.2837]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.2600,  0.1268, -0.4568,  ...,  0.3848, -0.7422, -0.7695]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.2610,  0.1261, -0.4578,  ...,  0.3853, -0.7441, -0.7705]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.6030,  0.2144, -0.2759,  ...,  0.4465,  0.6177,  0.7690]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.6045,  0.2139, -0.2776,  ...,  0.4458,  0.6172,  0.7671]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.0701,  0.0806,  0.0150,  ...,  0.4487,  0.3545,  0.1505]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.0695,  0.0802,  0.0129,  ...,  0.4485,  0.3540,  0.1511]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.2476, -0.2883, -0.1240,  ..., -0.0039,  0.1923,  0.2527]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.2476, -0.2861, -0.1246,  ..., -0.0019,  0.1910,  0.2534]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.2213,  0.5498,  0.3433,  ..., -1.2666, -0.4902,  0.1995]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.2214,  0.5542,  0.3430,  ..., -1.2705, -0.4915,  0.2017]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.1431,  0.6670, -0.1666,  ..., -0.0399, -0.3354, -0.3608]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.1423,  0.6660, -0.1669,  ..., -0.0403, -0.3369, -0.3616]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.5601, -0.1433, -1.3320,  ...,  0.0637,  0.1316,  0.0592]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.5605, -0.1421, -1.3379,  ...,  0.0630,  0.1315,  0.0588]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.2507, -0.7251, -0.0143,  ..., -0.6831,  0.4453,  0.0404]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.2495, -0.7275, -0.0125,  ..., -0.6812,  0.4463,  0.0403]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.2839, -0.1951, -0.2512,  ..., -0.2693, -0.6323,  0.3271]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.2849, -0.1957, -0.2539,  ..., -0.2720, -0.6313,  0.3259]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.9800, -0.3872,  0.2303,  ...,  0.0110, -0.2382,  0.3159]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.9814, -0.3850,  0.2296,  ...,  0.0085, -0.2400,  0.3147]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.2612, -1.0332,  0.6548,  ...,  0.2527, -0.0904,  0.1885]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.2615, -1.0361,  0.6538,  ...,  0.2544, -0.0890,  0.1892]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.6885, -0.8208, -0.2327,  ..., -0.0616,  0.5088, -0.3032]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.6875, -0.8228, -0.2355,  ..., -0.0604,  0.5112, -0.3022]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.7588,  0.1078,  0.3132,  ..., -0.5801,  0.0187,  0.8125]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.7559,  0.1052,  0.3120,  ..., -0.5742,  0.0182,  0.7988]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.4375, -0.5908,  0.2111,  ...,  0.2007,  0.7612,  0.0356]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.4390, -0.5938,  0.2118,  ...,  0.1976,  0.7607,  0.0368]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.6646, -0.0582, -0.2068,  ...,  0.7354,  0.4585,  0.3330]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.6631, -0.0581, -0.2064,  ...,  0.7363,  0.4617,  0.3308]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.1436, -0.1188, -0.6562,  ...,  0.2905,  0.7021,  0.8735]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.1429, -0.1122, -0.6562,  ...,  0.2900,  0.6987,  0.8657]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.3591, -0.1903,  0.7412,  ..., -0.0487,  0.4639,  0.2888]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.3606, -0.1921,  0.7383,  ..., -0.0458,  0.4617,  0.2876]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.1605, -0.4836,  0.2305,  ...,  0.6504, -0.3828,  0.1759]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.1635, -0.4817,  0.2267,  ...,  0.6465, -0.3870,  0.1741]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.2893, -0.8501,  0.3806,  ..., -0.2209,  0.2173, -0.3806]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.2888, -0.8506,  0.3801,  ..., -0.2214,  0.2186, -0.3794]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.1221,  0.1398, -0.1548,  ...,  0.2908,  0.7207,  0.0148]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.1225,  0.1388, -0.1576,  ...,  0.2952,  0.7217,  0.0143]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.1527, -0.2162,  0.3010,  ..., -0.3894, -0.2856, -0.2820]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.1632, -0.2163,  0.3037,  ..., -0.3865, -0.2905, -0.2776]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.3745, -0.2922, -0.4304,  ..., -0.2120,  0.1760,  0.2074]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.3750, -0.2959, -0.4341,  ..., -0.2130,  0.1788,  0.2102]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.6641,  0.3967,  0.2686,  ..., -0.6392,  0.8037, -0.2866]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.6675,  0.3992,  0.2690,  ..., -0.6401,  0.8022, -0.2900]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.7378, -0.0804,  0.3235,  ..., -0.2539, -0.7124,  0.0263]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.7363, -0.0829,  0.3237,  ..., -0.2500, -0.7124,  0.0251]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.2053,  0.1813, -0.3511,  ...,  0.0493,  0.1510, -0.0729]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.2070,  0.1798, -0.3523,  ...,  0.0518,  0.1495, -0.0737]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.7236,  0.0972, -0.9990,  ..., -0.5488, -0.0864, -0.3752]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.7266,  0.0930, -1.0010,  ..., -0.5513, -0.0844, -0.3733]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.4673, -0.3262,  0.3574,  ...,  0.2634, -0.0229,  0.4648]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.4727, -0.3315,  0.3638,  ...,  0.2656, -0.0272,  0.4673]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.4392, -0.2524, -0.1555,  ...,  0.6924,  0.3347,  0.1951]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.4377, -0.2549, -0.1603,  ...,  0.6924,  0.3340,  0.1918]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.2930, -0.0696, -0.7637,  ...,  0.0117,  0.0313, -0.2499]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.2937, -0.0703, -0.7700,  ...,  0.0075,  0.0312, -0.2489]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.3848,  0.5762,  0.7910,  ...,  0.6724, -0.6099, -0.0273]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.3828,  0.5918,  0.7886,  ...,  0.6758, -0.6250, -0.0166]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.1929, -0.5435,  0.3689,  ...,  0.3936, -0.0660,  0.1017]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.1912, -0.5430,  0.3738,  ...,  0.3965, -0.0719,  0.1062]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.6260,  0.6172, -0.5132,  ..., -0.0084, -0.3564,  0.5059]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.6265,  0.6128, -0.5103,  ..., -0.0098, -0.3589,  0.5005]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.7739, -0.9180, -0.4534,  ...,  0.4888,  0.1750, -0.5117]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 0.7695, -0.9136, -0.4561,  ...,  0.4895,  0.1748, -0.5112]],\n",
      "       device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "class CUDAGraphRunner():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.cuda_graph = None\n",
    "        self.graph_input = torch.zeros((1,4096), dtype=torch.float16, device=f'cuda:{device_id}')\n",
    "        self.graph_output = None\n",
    "    \n",
    "    def capture(self, x,):\n",
    "        assert self.cuda_graph is None\n",
    "        self.graph_input = self.graph_input.copy_(x).to(x.device)\n",
    "        self.cuda_graph = torch.cuda.CUDAGraph()\n",
    "        # self.cuda_graph.enable_debug_mode()\n",
    "        with torch.cuda.graph(self.cuda_graph):\n",
    "            self.graph_output = self.model(self.graph_input,)\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "    def forward(self, x,):\n",
    "        self.graph_input.copy_(x)\n",
    "        self.cuda_graph.replay()\n",
    "        return self.graph_output\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "    \n",
    "inp = torch.randn(1, 4096).half().cuda(device_id)\n",
    "for i in range(prefill_layers):\n",
    "    for j in range(len(llm.model.layers[0].block_sparse_moe.experts)):\n",
    "        expert=llm.model.layers[i].block_sparse_moe.experts[j]\n",
    "        print(expert(inp))\n",
    "        graph_runner = CUDAGraphRunner(expert)\n",
    "        graph_runner.capture(inp)\n",
    "        print(graph_runner(inp))\n",
    "\n",
    "        llm.model.layers[i].block_sparse_moe.experts[j].graph = graph_runner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试时间开销"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warm up ...\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# import torch._dynamo.config\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 2\n",
    "MAX_LENGTH = 2\n",
    "output_length = 10\n",
    "test_samples = 1\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "filt_type = fineweb_path.split('.')[-1]\n",
    "fineweb = load_dataset(filt_type, data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text']\n",
    "\n",
    "print(\"warm up ...\")\n",
    "# 预热（避免第一次运行时的额外开销）\n",
    "for text in fineweb_text[:5]:\n",
    "    inputs = preprocess_data(text, tokenizer)\n",
    "    with torch.no_grad():\n",
    "        output = llm(input_ids=inputs[\"input_ids\"].cuda(device_id), attention_mask=inputs[\"attention_mask\"].cuda(device_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "import torch\n",
    "\n",
    "# 定义 input_length 和 output_length 的范围\n",
    "# input_length_range = [16]\n",
    "# output_length_range = [128,256]\n",
    "# input_length_range = [16,]  # 16到32\n",
    "input_length_range = [32,64,128]\n",
    "output_length_range = [128,256,512,1024]  # 128到1024\n",
    "\n",
    "test_samples = 5\n",
    "device_id = 0\n",
    "\n",
    "def preprocess_data(data, tokenizer, max_length):\n",
    "    # 使用 tokenizer 将文本数据转换为模型输入\n",
    "    inputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    inputs[\"labels\"] = inputs.input_ids.clone()\n",
    "    return inputs\n",
    "\n",
    "for input_length in input_length_range:\n",
    "    for output_length in output_length_range:\n",
    "        MAX_LENGTH = input_length\n",
    "        generated_all = 0\n",
    "        prefill_time, decode_time = 0, 0\n",
    "        reloaded_experts = 0\n",
    "\n",
    "        # 打开文件以写入结果\n",
    "        with open(f\"{input_length}-{output_length}.out\", \"w\") as f:\n",
    "            print(f\"output length is {output_length}\", file=f)\n",
    "            for text in fineweb_text[2:2+test_samples]:\n",
    "                inputs = preprocess_data(text, tokenizer, MAX_LENGTH)\n",
    "                ### 清空统计数据\n",
    "                PLLM.get_prefill_time()\n",
    "                PLLM.get_reload_experts()\n",
    "\n",
    "                # 测试时间\n",
    "                start_event = torch.cuda.Event(enable_timing=True)\n",
    "                end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "                # 开始计时\n",
    "                torch.cuda.synchronize()\n",
    "                start_event.record()\n",
    "\n",
    "                # 前向传播\n",
    "                with torch.no_grad():\n",
    "                    output = llm.generate(\n",
    "                        input_ids=inputs[\"input_ids\"].cuda(device_id),\n",
    "                        attention_mask=inputs[\"attention_mask\"].cuda(device_id),\n",
    "                        max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "                        generation_config=GenerationConfig(do_sample=False),\n",
    "                        pad_token_id=tokenizer.pad_token_id, \n",
    "                        # cache_implementation=\"static\" ## moe not support\n",
    "                    )\n",
    "\n",
    "                # 结束计时\n",
    "                end_event.record()\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "                if (len(output[0]) - input_length) == output_length:\n",
    "                    # 计算时间\n",
    "                    elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "                    decode_time += elapsed_time\n",
    "                    cur_prefill_time = PLLM.get_prefill_time()\n",
    "                    prefill_time += cur_prefill_time\n",
    "                    generated_all += (len(output[0]) - input_length)\n",
    "                    reloaded_experts += PLLM.get_reload_experts()\n",
    "                print(f\"Generated length: {len(output[0]) - input_length}\", f\"Time taken: {elapsed_time:.2f} s,\", f\"prefill time: {cur_prefill_time:.2f} s\", file=f)\n",
    "                # print(output, file=f)\n",
    "                print(tokenizer.batch_decode(output, skip_special_tokens=True), file=f)\n",
    "\n",
    "            print(\"Generate speed:\", '{:.4f}'.format((generated_all) / decode_time) , 'token/s', file=f)\n",
    "            timepertoken = (decode_time - prefill_time) / (generated_all)\n",
    "            print(\"decode phase speed(not cover prefill phase):\", '{:.4f}'.format(1/timepertoken) , 'token/s', file=f)\n",
    "            expertpertoken = reloaded_experts / generated_all\n",
    "            print(\"the number of reloaded experts per token:\", '{:.3f}, ({:.2f}%)'.format(expertpertoken, 100 * expertpertoken / ((32-prefill_layers) * 2)), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 256\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/data2/lz/On-the-Fly_MoE_Inference/offloading/1.ipynb 单元格 9\u001b[0m line \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab46/data2/lz/On-the-Fly_MoE_Inference/offloading/1.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# 前向传播\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab46/data2/lz/On-the-Fly_MoE_Inference/offloading/1.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blab46/data2/lz/On-the-Fly_MoE_Inference/offloading/1.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m     output \u001b[39m=\u001b[39m llm\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab46/data2/lz/On-the-Fly_MoE_Inference/offloading/1.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m         input_ids\u001b[39m=\u001b[39;49minputs[\u001b[39m\"\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mcuda(device_id),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab46/data2/lz/On-the-Fly_MoE_Inference/offloading/1.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49minputs[\u001b[39m\"\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mcuda(device_id),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab46/data2/lz/On-the-Fly_MoE_Inference/offloading/1.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m         max_length\u001b[39m=\u001b[39;49minput_length \u001b[39m+\u001b[39;49m output_length,  \u001b[39m# 总长度为输入长度 + 输出长度\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab46/data2/lz/On-the-Fly_MoE_Inference/offloading/1.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m         generation_config\u001b[39m=\u001b[39;49mGenerationConfig(do_sample\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab46/data2/lz/On-the-Fly_MoE_Inference/offloading/1.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mpad_token_id, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab46/data2/lz/On-the-Fly_MoE_Inference/offloading/1.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m         \u001b[39m# cache_implementation=\"static\" ## moe not support\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab46/data2/lz/On-the-Fly_MoE_Inference/offloading/1.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab46/data2/lz/On-the-Fly_MoE_Inference/offloading/1.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# 结束计时\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab46/data2/lz/On-the-Fly_MoE_Inference/offloading/1.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m end_event\u001b[39m.\u001b[39mrecord()\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/transformers/generation/utils.py:2252\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2244\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2245\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   2246\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2247\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2248\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2249\u001b[0m     )\n\u001b[1;32m   2251\u001b[0m     \u001b[39m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2252\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sample(\n\u001b[1;32m   2253\u001b[0m         input_ids,\n\u001b[1;32m   2254\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mprepared_logits_processor,\n\u001b[1;32m   2255\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mprepared_stopping_criteria,\n\u001b[1;32m   2256\u001b[0m         generation_config\u001b[39m=\u001b[39;49mgeneration_config,\n\u001b[1;32m   2257\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   2258\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   2259\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   2260\u001b[0m     )\n\u001b[1;32m   2262\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39min\u001b[39;00m (GenerationMode\u001b[39m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2263\u001b[0m     \u001b[39m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m     beam_scorer \u001b[39m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2265\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   2266\u001b[0m         num_beams\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         max_length\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mmax_length,\n\u001b[1;32m   2272\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/transformers/generation/utils.py:3254\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3252\u001b[0m     is_prefill \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   3253\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 3254\u001b[0m     outputs \u001b[39m=\u001b[39m model_forward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs, return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   3256\u001b[0m \u001b[39m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3257\u001b[0m model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3258\u001b[0m     outputs,\n\u001b[1;32m   3259\u001b[0m     model_kwargs,\n\u001b[1;32m   3260\u001b[0m     is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3261\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m/data2/lz/On-the-Fly_MoE_Inference/offloading/modeling_mixtral.py:1374\u001b[0m, in \u001b[0;36mMixtralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, output_router_logits, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1371\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1373\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1374\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1375\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1376\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1377\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1378\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1379\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1380\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1381\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1382\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1383\u001b[0m     output_router_logits\u001b[39m=\u001b[39;49moutput_router_logits,\n\u001b[1;32m   1384\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1385\u001b[0m     cache_position\u001b[39m=\u001b[39;49mcache_position,\n\u001b[1;32m   1386\u001b[0m )\n\u001b[1;32m   1388\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1389\u001b[0m \u001b[39m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m/data2/lz/On-the-Fly_MoE_Inference/offloading/modeling_mixtral.py:1089\u001b[0m, in \u001b[0;36mMixtralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, output_router_logits, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1077\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1078\u001b[0m         decoder_layer\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m   1079\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1086\u001b[0m         cache_position,\n\u001b[1;32m   1087\u001b[0m     )\n\u001b[1;32m   1088\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1089\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m   1090\u001b[0m         hidden_states,\n\u001b[1;32m   1091\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mcausal_mask,\n\u001b[1;32m   1092\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1093\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1094\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1095\u001b[0m         output_router_logits\u001b[39m=\u001b[39;49moutput_router_logits,\n\u001b[1;32m   1096\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1097\u001b[0m         cache_position\u001b[39m=\u001b[39;49mcache_position,\n\u001b[1;32m   1098\u001b[0m     )\n\u001b[1;32m   1100\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1102\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m/data2/lz/On-the-Fly_MoE_Inference/offloading/pipelinellm.py:241\u001b[0m, in \u001b[0;36mPipelineLLM._replace_forward_methods.<locals>.new_forward\u001b[0;34m(hidden_states, attention_mask, position_ids, past_key_value, output_attentions, output_router_logits, use_cache, cache_position, layer, layer_idx)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[39m# 处理当前层\u001b[39;00m\n\u001b[1;32m    240\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[0;32m--> 241\u001b[0m hidden_states \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49minput_layernorm(hidden_states)\n\u001b[1;32m    243\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[1;32m    244\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39mself_attn(\n\u001b[1;32m    245\u001b[0m     hidden_states\u001b[39m=\u001b[39mhidden_states,\n\u001b[1;32m    246\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    251\u001b[0m     cache_position\u001b[39m=\u001b[39mcache_position,\n\u001b[1;32m    252\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m/data2/lz/On-the-Fly_MoE_Inference/offloading/modeling_mixtral.py:167\u001b[0m, in \u001b[0;36mMixtralRMSNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[1;32m    166\u001b[0m     input_dtype \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39mdtype\n\u001b[0;32m--> 167\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39;49mto(torch\u001b[39m.\u001b[39;49mfloat32)\n\u001b[1;32m    168\u001b[0m     variance \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39mpow(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mmean(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    169\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mrsqrt(variance \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvariance_epsilon)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 10\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 256\n",
    "test_samples = 5\n",
    "device_id = 0\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "generated_all = 0\n",
    "prefill_time, decode_time = 0, 0\n",
    "reloaded_experts = 0\n",
    "print(\"output length is {}\".format(output_length))\n",
    "for text in fineweb_text[2:2+test_samples] :\n",
    "    inputs = preprocess_data(text, tokenizer)\n",
    "    ### 清空统计数据\n",
    "    PLLM.get_prefill_time()\n",
    "    PLLM.get_reload_experts()\n",
    "\n",
    "    # 测试时间\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # 开始计时\n",
    "    torch.cuda.synchronize()\n",
    "    start_event.record()\n",
    "\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(device_id),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(device_id),\n",
    "            max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.pad_token_id, \n",
    "            # cache_implementation=\"static\" ## moe not support\n",
    "        )\n",
    "\n",
    "    # 结束计时\n",
    "    end_event.record()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # 计算时间\n",
    "    elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "    decode_time += elapsed_time\n",
    "    cur_prefill_time = PLLM.get_prefill_time()\n",
    "    prefill_time += cur_prefill_time\n",
    "    print(f\"Generated length: {len(output[0]) - input_length}\", f\"Time taken: {elapsed_time:.2f} s,\", f\"prefill time: {cur_prefill_time:.2f} s\")\n",
    "    # print(output)\n",
    "    print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "\n",
    "    generated_all += (len(output[0]) - input_length - 1)\n",
    "    reloaded_experts += PLLM.get_reload_experts()\n",
    "\n",
    "print(\"Generate speed:\", '{:.4f}'.format((generated_all+test_samples) / decode_time) , 'token/s')\n",
    "timepertoken = (decode_time - prefill_time) / (generated_all)\n",
    "print(\"decode phase speed(not cover prefill phase):\", '{:.4f}'.format(1/timepertoken) , 'token/s')\n",
    "expertpertoken = reloaded_experts / generated_all\n",
    "print(\"the number of reloaded experts per token:\", '{:.3f}, ({:.2f}%)'.format(expertpertoken, 100 * expertpertoken / ((32-prefill_layers) * 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################################################################\n",
    "#Save gemlite cache\n",
    "if(backend == 'gemlite'):\n",
    "\tgemlite.core.GemLiteLinear.cache_config('/tmp/gemlite_config.json') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.profile\n",
    "\n",
    "attention使用sdpa实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 2\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "gemlite::gemv_revsplitK_A16fWnO16f_int32packing_forw...        17.06%      60.146ms        20.49%      72.238ms     463.067us       6.304ms         7.18%       6.496ms      41.643us           156  \n",
      "                                       cudaLaunchKernel        13.65%      48.133ms        13.65%      48.133ms      10.863us       0.000us         0.00%       0.000us       0.000us          4431  \n",
      "                                               aten::mm         9.32%      32.844ms        13.08%      46.101ms      69.957us      63.390ms        72.22%      63.390ms      96.192us           659  \n",
      "                                            aten::copy_         7.37%      25.966ms        14.32%      50.480ms      49.734us       3.766ms         4.29%       3.766ms       3.710us          1015  \n",
      "                                        cudaMemcpyAsync         5.61%      19.770ms         5.61%      19.770ms      18.704us       0.000us         0.00%       0.000us       0.000us          1057  \n",
      "                                              aten::mul         4.63%      16.312ms         7.31%      25.757ms      29.674us       2.196ms         2.50%       2.196ms       2.530us           868  \n",
      "                                    aten::empty_strided         2.77%       9.763ms         2.77%       9.763ms      11.892us       0.000us         0.00%       0.000us       0.000us           821  \n",
      "                                              aten::add         2.46%       8.687ms         3.85%      13.559ms      25.777us     839.717us         0.96%     839.717us       1.596us           526  \n",
      "                                            aten::empty         2.20%       7.739ms         2.20%       7.739ms      13.746us       0.000us         0.00%       0.000us       0.000us           563  \n",
      "                                           aten::select         1.64%       5.775ms         2.02%       7.138ms       6.357us       0.000us         0.00%       0.000us       0.000us          1123  \n",
      "                                       aten::as_strided         1.48%       5.227ms         1.48%       5.227ms       1.341us       0.000us         0.00%       0.000us       0.000us          3898  \n",
      "                                         aten::_to_copy         1.47%       5.190ms        15.98%      56.323ms      79.329us       0.000us         0.00%       3.225ms       4.542us           710  \n",
      "                                               aten::to         1.42%       4.991ms        17.39%      61.314ms      57.844us       0.000us         0.00%       3.225ms       3.042us          1060  \n",
      "                                  cudaDeviceSynchronize         1.36%       4.796ms         1.36%       4.796ms      25.373us       0.000us         0.00%       0.000us       0.000us           189  \n",
      "                                             aten::view         1.35%       4.773ms         1.35%       4.773ms       3.592us       0.000us         0.00%       0.000us       0.000us          1329  \n",
      "                                  cudaStreamSynchronize         1.26%       4.436ms         1.26%       4.436ms       8.514us       0.000us         0.00%       0.000us       0.000us           521  \n",
      "                                              aten::cat         1.20%       4.248ms         1.71%       6.025ms      30.740us     721.729us         0.82%     721.729us       3.682us           196  \n",
      "                              aten::_local_scalar_dense         1.18%       4.162ms         3.75%      13.217ms      28.121us     654.945us         0.75%     654.945us       1.393us           470  \n",
      "                                             aten::silu         1.17%       4.139ms         1.74%       6.131ms      39.300us     339.298us         0.39%     339.298us       2.175us           156  \n",
      "                                            aten::slice         1.10%       3.883ms         1.38%       4.879ms       5.309us       0.000us         0.00%       0.000us       0.000us           919  \n",
      "                                           aten::matmul         1.09%       3.852ms        14.70%      51.818ms      78.512us       0.000us         0.00%      63.749ms      96.590us           660  \n",
      "                                            aten::index         1.04%       3.662ms         1.65%       5.806ms      45.011us     419.910us         0.48%     419.910us       3.255us           129  \n",
      "                                            aten::fill_         1.03%       3.635ms         2.36%       8.314ms      29.379us     337.791us         0.38%     337.791us       1.194us           283  \n",
      "                                        cudaMemsetAsync         1.02%       3.592ms         1.02%       3.592ms      11.086us       0.000us         0.00%       0.000us       0.000us           324  \n",
      "                                             aten::topk         0.98%       3.469ms         1.86%       6.564ms      72.936us     934.371us         1.06%     934.371us      10.382us            90  \n",
      "                                    cudaStreamWaitEvent         0.96%       3.385ms         0.96%       3.385ms       4.340us       0.000us         0.00%       0.000us       0.000us           780  \n",
      "                                         cuLaunchKernel         0.95%       3.355ms         0.95%       3.355ms      17.657us       0.000us         0.00%       0.000us       0.000us           190  \n",
      "                                             aten::mean         0.90%       3.172ms         1.27%       4.485ms      34.498us     522.846us         0.60%     522.846us       4.022us           130  \n",
      "                                              aten::pow         0.78%       2.765ms         1.17%       4.110ms      31.617us     174.327us         0.20%     174.327us       1.341us           130  \n",
      "                                              aten::neg         0.64%       2.239ms         0.97%       3.433ms      26.819us     321.665us         0.37%     321.665us       2.513us           128  \n",
      "                                        aten::transpose         0.62%       2.201ms         1.00%       3.516ms       4.088us       0.000us         0.00%       0.000us       0.000us           860  \n",
      "                               aten::bitwise_left_shift         0.60%       2.128ms         0.86%       3.030ms      29.139us     144.416us         0.16%     144.416us       1.389us           104  \n",
      "                                            aten::rsqrt         0.55%       1.934ms         0.85%       3.013ms      23.173us     195.166us         0.22%     195.166us       1.501us           130  \n",
      "                                         aten::_softmax         0.52%       1.829ms         0.82%       2.904ms      32.263us     143.008us         0.16%     143.008us       1.589us            90  \n",
      "                                          aten::reshape         0.52%       1.816ms         2.36%       8.331ms      13.702us       0.000us         0.00%     303.904us       0.500us           608  \n",
      "                         aten::_flash_attention_forward         0.51%       1.791ms         1.34%       4.710ms      73.597us     414.586us         0.47%     414.586us       6.478us            64  \n",
      "                                        cudaEventRecord         0.51%       1.783ms         0.51%       1.783ms       2.113us       0.000us         0.00%       0.000us       0.000us           844  \n",
      "                                          aten::permute         0.49%       1.715ms         0.66%       2.342ms       9.009us       0.000us         0.00%       0.000us       0.000us           260  \n",
      "                                           aten::linear         0.45%       1.592ms         9.18%      32.379ms      93.042us       0.000us         0.00%       9.761ms      28.049us           348  \n",
      "                                                aten::t         0.44%       1.546ms         0.90%       3.184ms       9.149us       0.000us         0.00%       0.000us       0.000us           348  \n",
      "                                        aten::unsqueeze         0.39%       1.379ms         0.50%       1.763ms       5.215us       0.000us         0.00%       0.000us       0.000us           338  \n",
      "                                            aten::zero_         0.38%       1.329ms         2.37%       8.372ms      41.861us       0.000us         0.00%     242.107us       1.211us           200  \n",
      "                                        cudaGraphLaunch         0.36%       1.255ms         0.36%       1.255ms      34.855us       0.000us         0.00%       0.000us       0.000us            36  \n",
      "                     aten::scaled_dot_product_attention         0.35%       1.224ms         2.15%       7.569ms     118.268us       0.000us         0.00%     414.586us       6.478us            64  \n",
      "                                             aten::item         0.35%       1.222ms         4.10%      14.439ms      30.722us       0.000us         0.00%     654.945us       1.393us           470  \n",
      "              aten::_scaled_dot_product_flash_attention         0.27%     943.717us         1.80%       6.345ms      99.146us       0.000us         0.00%     414.586us       6.478us            64  \n",
      "                                          aten::squeeze         0.27%     943.073us         0.33%       1.156ms       9.033us       0.000us         0.00%       0.000us       0.000us           128  \n",
      "                                              aten::sum         0.24%     833.825us         0.33%       1.180ms      31.053us     118.561us         0.14%     118.561us       3.120us            38  \n",
      "                                     aten::_unsafe_view         0.23%     806.981us         0.23%     806.981us       2.091us       0.000us         0.00%       0.000us       0.000us           386  \n",
      "                                       aten::empty_like         0.23%     798.361us         1.45%       5.115ms      21.225us       0.000us         0.00%       0.000us       0.000us           241  \n",
      "                                            aten::clone         0.21%     724.195us         2.12%       7.462ms      55.683us       0.000us         0.00%     311.935us       2.328us           134  \n",
      "                                          aten::numpy_T         0.18%     650.705us         0.85%       2.993ms      11.512us       0.000us         0.00%       0.000us       0.000us           260  \n",
      "                                            aten::equal         0.17%     608.631us         0.70%       2.453ms      94.355us      41.249us         0.05%     112.573us       4.330us            26  \n",
      "                                  cudaFuncGetAttributes         0.16%     568.284us         0.16%     568.284us       6.314us       0.000us         0.00%       0.000us       0.000us            90  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.16%     549.735us         0.16%     549.735us       0.760us       0.000us         0.00%       0.000us       0.000us           723  \n",
      "                                           aten::expand         0.16%     549.363us         0.19%     673.576us       5.181us       0.000us         0.00%       0.000us       0.000us           130  \n",
      "                                             aten::div_         0.15%     524.030us         0.23%     823.684us      21.676us      68.160us         0.08%      68.160us       1.794us            38  \n",
      "          cudaOccupancyMaxActiveBlocksPerMultiprocessor         0.12%     429.302us         0.12%     429.302us       2.385us       0.000us         0.00%       0.000us       0.000us           180  \n",
      "                                               aten::ne         0.11%     401.765us         0.20%     704.927us      27.113us      34.654us         0.04%      34.654us       1.333us            26  \n",
      "                                              aten::all         0.11%     392.361us         0.32%       1.128ms      40.293us       6.465us         0.01%      41.629us       1.487us            28  \n",
      "                                          aten::softmax         0.11%     370.538us         0.93%       3.274ms      36.380us       0.000us         0.00%     143.008us       1.589us            90  \n",
      "                                       aten::zeros_like         0.10%     335.137us         0.74%       2.612ms      59.354us       0.000us         0.00%      49.663us       1.129us            44  \n",
      "                                               aten::eq         0.09%     314.279us         0.14%     477.476us      34.105us      22.369us         0.03%      22.369us       1.598us            14  \n",
      "                                 cudaDeviceGetAttribute         0.09%     308.314us         0.09%     308.314us       0.777us       0.000us         0.00%       0.000us       0.000us           397  \n",
      "                                              aten::any         0.07%     237.339us         0.13%     475.488us      47.549us      15.521us         0.02%      22.144us       2.214us            10  \n",
      "                                             aten::add_         0.06%     208.844us         0.11%     380.291us      15.845us      38.913us         0.04%      38.913us       1.621us            24  \n",
      "                                  cudaStreamIsCapturing         0.06%     195.286us         0.06%     195.286us       1.436us       0.000us         0.00%       0.000us       0.000us           136  \n",
      "                                           aten::cumsum         0.05%     177.576us         0.08%     297.473us      99.158us       9.535us         0.01%       9.535us       3.178us             3  \n",
      "                                   cudaFuncSetAttribute         0.05%     173.208us         0.05%     173.208us       2.706us       0.000us         0.00%       0.000us       0.000us            64  \n",
      "                                             aten::isin         0.05%     163.051us         0.18%     636.087us     212.029us       0.000us         0.00%      16.447us       5.482us             3  \n",
      "                                           aten::unbind         0.04%     132.355us         0.07%     262.932us      10.955us       0.000us         0.00%       0.000us       0.000us            24  \n",
      "                                              aten::bmm         0.03%     122.627us         0.06%     212.330us     212.330us     359.103us         0.41%     359.103us     359.103us             1  \n",
      "                                              aten::sub         0.03%     122.105us         0.05%     178.907us      35.781us       6.945us         0.01%       6.945us       1.389us             5  \n",
      "                                   cudaEventElapsedTime         0.03%     113.413us         0.03%     113.413us       3.544us       0.000us         0.00%       0.000us       0.000us            32  \n",
      "                                      aten::result_type         0.03%     101.051us         0.03%     101.051us       0.777us       0.000us         0.00%       0.000us       0.000us           130  \n",
      "                                       aten::is_nonzero         0.02%      84.656us         0.30%       1.052ms      30.928us       0.000us         0.00%      46.431us       1.366us            34  \n",
      "                                     aten::index_select         0.02%      73.987us         0.04%     153.008us      76.504us       5.600us         0.01%       5.600us       2.800us             2  \n",
      "                                       aten::bitwise_or         0.02%      68.664us         0.03%     107.921us      26.980us       5.664us         0.01%       5.664us       1.416us             4  \n",
      "                                          aten::view_as         0.02%      63.077us         0.03%     114.847us       3.705us       0.000us         0.00%       0.000us       0.000us            31  \n",
      "                                               aten::ge         0.02%      62.741us         0.03%     110.793us      55.397us       3.232us         0.00%       3.232us       1.616us             2  \n",
      "                                           aten::argmax         0.01%      50.674us         0.02%      80.947us      40.474us      23.712us         0.03%      23.712us      11.856us             2  \n",
      "                                               aten::lt         0.01%      47.697us         0.02%      70.329us      70.329us       1.664us         0.00%       1.664us       1.664us             1  \n",
      "                                     aten::masked_fill_         0.01%      45.944us         0.02%      69.315us      34.657us       2.880us         0.00%       2.880us       1.440us             2  \n",
      "                                        aten::embedding         0.01%      45.046us         0.06%     218.087us     109.043us       0.000us         0.00%       5.600us       2.800us             2  \n",
      "                                              aten::max         0.01%      43.067us         0.02%      78.619us      39.309us       6.080us         0.01%       6.080us       3.040us             2  \n",
      "                                      aten::bitwise_and         0.01%      31.630us         0.01%      49.771us      24.886us       4.512us         0.01%       4.512us       2.256us             2  \n",
      "                                          aten::detach_         0.01%      29.896us         0.01%      43.455us       5.432us       0.000us         0.00%       0.000us       0.000us             8  \n",
      "                aten::_has_compatible_shallow_copy_type         0.01%      29.718us         0.01%      29.718us       0.571us       0.000us         0.00%       0.000us       0.000us            52  \n",
      "                                      aten::bitwise_not         0.01%      29.562us         0.01%      46.303us      23.152us       2.816us         0.00%       2.816us       1.408us             2  \n",
      "                                          aten::resize_         0.01%      21.663us         0.01%      21.663us      10.831us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                             aten::rsub         0.00%      17.094us         0.02%      64.645us      32.323us       0.000us         0.00%       2.817us       1.408us             2  \n",
      "                                             aten::full         0.00%      16.680us         0.03%     101.082us      25.271us       0.000us         0.00%       4.576us       1.144us             4  \n",
      "                                         aten::new_ones         0.00%      15.652us         0.04%     134.699us      67.350us       0.000us         0.00%       2.849us       1.425us             2  \n",
      "                                        aten::ones_like         0.00%      14.626us         0.02%      72.324us      72.324us       0.000us         0.00%       1.120us       1.120us             1  \n",
      "                                             aten::ones         0.00%      13.678us         0.03%      96.648us      96.648us       0.000us         0.00%       1.120us       1.120us             1  \n",
      "                                                detach_         0.00%      13.559us         0.00%      13.559us       1.695us       0.000us         0.00%       0.000us       0.000us             8  \n",
      "                                           aten::__or__         0.00%       9.699us         0.03%     117.620us      29.405us       0.000us         0.00%       5.664us       1.416us             4  \n",
      "                                        aten::new_empty         0.00%       7.542us         0.01%      38.814us      19.407us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                   cudaDriverGetVersion         0.00%       7.510us         0.00%       7.510us       0.209us       0.000us         0.00%       0.000us       0.000us            36  \n",
      "                                   aten::_reshape_alias         0.00%       6.454us         0.00%       6.454us       6.454us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                          aten::__and__         0.00%       5.539us         0.02%      55.310us      27.655us       0.000us         0.00%       4.512us       2.256us             2  \n",
      "                                       aten::lift_fresh         0.00%       4.427us         0.00%       4.427us       0.553us       0.000us         0.00%       0.000us       0.000us             8  \n",
      "                                     aten::resolve_conj         0.00%       4.319us         0.00%       4.319us       0.432us       0.000us         0.00%       0.000us       0.000us            10  \n",
      "                                    cudaPeekAtLastError         0.00%       2.977us         0.00%       2.977us       0.248us       0.000us         0.00%       0.000us       0.000us            12  \n",
      "                                      aten::resolve_neg         0.00%       2.181us         0.00%       2.181us       0.218us       0.000us         0.00%       0.000us       0.000us            10  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       4.800us         0.01%       4.800us       0.480us            10  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       8.064us         0.01%       8.064us       1.152us             7  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      77.598us         0.09%      77.598us       1.464us            53  \n",
      "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     236.979us         0.27%     236.979us       1.339us           177  \n",
      "                         Memcpy DtoH (Device -> Pinned)         0.00%       0.000us         0.00%       0.000us       0.000us     654.945us         0.75%     654.945us       1.393us           470  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.328us         0.00%       3.328us       1.664us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      87.620us         0.10%      87.620us       1.153us            76  \n",
      "void at_cuda_detail::cub::DeviceScanInitKernel<at_cu...         0.00%       0.000us         0.00%       0.000us       0.000us       3.520us         0.00%       3.520us       1.173us             3  \n",
      "void at_cuda_detail::cub::DeviceScanKernel<at_cuda_d...         0.00%       0.000us         0.00%       0.000us       0.000us       6.015us         0.01%       6.015us       2.005us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.697us         0.01%       5.697us       1.424us             4  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       1.568us         0.00%       1.568us       1.568us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       8.353us         0.01%       8.353us       1.392us             6  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.880us         0.00%       2.880us       1.440us             2  \n",
      "void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us       5.600us         0.01%       5.600us       2.800us             2  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       6.465us         0.01%       6.465us       3.232us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     406.756us         0.46%     406.756us       3.081us           132  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     174.327us         0.20%     174.327us       1.341us           130  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     522.846us         0.60%     522.846us       4.022us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     187.165us         0.21%     187.165us       1.440us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     195.166us         0.22%     195.166us       1.501us           130  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     268.513us         0.31%     268.513us       2.065us           130  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     609.079us         0.69%     609.079us       2.986us           204  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       1.149ms         1.31%       1.149ms       2.810us           409  \n",
      "ampere_fp16_s16816gemm_fp16_64x64_sliced1x2_ldg8_f2f...         0.00%       0.000us         0.00%       0.000us       0.000us       2.951ms         3.36%       2.951ms      46.106us            64  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us     156.802us         0.18%     156.802us       0.484us           324  \n",
      "ampere_fp16_s16816gemm_fp16_64x64_ldg8_f2f_stages_64...         0.00%       0.000us         0.00%       0.000us       0.000us       1.349ms         1.54%       1.349ms      21.074us            64  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     416.870us         0.47%     416.870us       3.257us           128  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     321.665us         0.37%     321.665us       2.513us           128  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     712.449us         0.81%     712.449us       3.711us           192  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     168.929us         0.19%     168.929us       2.640us            64  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     303.904us         0.35%     303.904us       2.374us           128  \n",
      "void pytorch_flash::flash_fwd_kernel<pytorch_flash::...         0.00%       0.000us         0.00%       0.000us       0.000us     201.117us         0.23%     201.117us       6.285us            32  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     443.653us         0.51%     443.653us       1.479us           300  \n",
      "void cutlass::Kernel2<cutlass_80_wmma_tensorop_f16_s...         0.00%       0.000us         0.00%       0.000us       0.000us     665.954us         0.76%     665.954us      20.811us            32  \n",
      "void (anonymous namespace)::softmax_warp_forward<c10...         0.00%       0.000us         0.00%       0.000us       0.000us     143.008us         0.16%     143.008us       1.589us            90  \n",
      "void at::native::sbtopk::gatherTopK<float, unsigned ...         0.00%       0.000us         0.00%       0.000us       0.000us     185.471us         0.21%     185.471us       5.796us            32  \n",
      "void at::native::bitonicSortKVInPlace<2, -1, 16, 16,...         0.00%       0.000us         0.00%       0.000us       0.000us     173.504us         0.20%     173.504us       5.422us            32  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     118.561us         0.14%     118.561us       3.120us            38  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      68.160us         0.08%      68.160us       1.794us            38  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     317.918us         0.36%     317.918us       1.169us           272  \n",
      "          gemv_revsplitK_A16fWnO16f_int32packing_kernel         0.00%       0.000us         0.00%       0.000us       0.000us       7.748ms         8.83%       7.748ms      40.354us           192  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      43.552us         0.05%      43.552us       1.210us            36  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     122.109us         0.14%     122.109us       3.392us            36  \n",
      "                         gather_gemv_elemul_flag_kernel         0.00%       0.000us         0.00%       0.000us       0.000us       1.557ms         1.77%       1.557ms      43.253us            36  \n",
      "           gather_transposed_gemv_flag_atomicadd_kernel         0.00%       0.000us         0.00%       0.000us       0.000us       1.816ms         2.07%       1.816ms      50.450us            36  \n",
      "                         Memcpy DtoH (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     929.599us         1.06%     929.599us       2.384us           390  \n",
      "                         Memcpy HtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       1.383ms         1.58%       1.383ms       3.545us           390  \n",
      "ampere_fp16_s16816gemm_fp16_64x64_ldg8_f2f_stages_64...         0.00%       0.000us         0.00%       0.000us       0.000us      44.834ms        51.08%      44.834ms     172.440us           260  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     339.298us         0.39%     339.298us       2.175us           156  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     379.110us         0.43%     379.110us       1.715us           221  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     392.834us         0.45%     392.834us       3.777us           104  \n",
      "void cutlass::Kernel2<cutlass_80_tensorop_f16_s16816...         0.00%       0.000us         0.00%       0.000us       0.000us     706.174us         0.80%     706.174us     353.087us             2  \n",
      "void cublasLt::splitKreduce_kernel<32, 16, int, __ha...         0.00%       0.000us         0.00%       0.000us       0.000us      11.809us         0.01%      11.809us       5.904us             2  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us       9.280us         0.01%       9.280us       2.320us             4  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       1.471us         0.00%       1.471us       1.471us             1  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      23.712us         0.03%      23.712us      11.856us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.952us         0.01%       5.952us       1.488us             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.817us         0.00%       2.817us       1.408us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      75.843us         0.09%      75.843us       1.404us            54  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.664us         0.01%       5.664us       1.416us             4  \n",
      "void at::native::vectorized_elementwise_kernel<2, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.569us         0.00%       1.569us       1.569us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.816us         0.00%       2.816us       1.408us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       4.512us         0.01%       4.512us       2.256us             2  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       6.080us         0.01%       6.080us       3.040us             2  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.040us         0.00%       3.040us       3.040us             1  \n",
      "ampere_fp16_s16816gemm_fp16_64x64_sliced1x2_ldg8_f2f...         0.00%       0.000us         0.00%       0.000us       0.000us       2.966ms         3.38%       2.966ms      46.344us            64  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us     859.711us         0.98%     859.711us      13.433us            64  \n",
      "void pytorch_flash::flash_fwd_kernel<pytorch_flash::...         0.00%       0.000us         0.00%       0.000us       0.000us     213.469us         0.24%     213.469us       6.671us            32  \n",
      "void dot_kernel<float, 128, 0, cublasDotParams<cubla...         0.00%       0.000us         0.00%       0.000us       0.000us     123.131us         0.14%     123.131us       2.123us            58  \n",
      "void reduce_1Block_kernel<float, 128, 7, cublasGemvT...         0.00%       0.000us         0.00%       0.000us       0.000us      97.187us         0.11%      97.187us       1.676us            58  \n",
      "void at::native::sbtopk::gatherTopK<float, unsigned ...         0.00%       0.000us         0.00%       0.000us       0.000us     282.302us         0.32%     282.302us       4.867us            58  \n",
      "void at::native::bitonicSortKVInPlace<-2, -1, 16, 16...         0.00%       0.000us         0.00%       0.000us       0.000us     293.094us         0.33%     293.094us       5.053us            58  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      75.012us         0.09%      75.012us       1.443us            52  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us      69.404us         0.08%      69.404us       1.335us            52  \n",
      "                       Memcpy DtoH (Device -> Pageable)         0.00%       0.000us         0.00%       0.000us       0.000us      13.504us         0.02%      13.504us       1.350us            10  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       9.280us         0.01%       9.280us       1.856us             5  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      15.521us         0.02%      15.521us       3.104us             5  \n",
      "ampere_fp16_s16816gemm_fp16_64x64_sliced1x2_ldg8_f2f...         0.00%       0.000us         0.00%       0.000us       0.000us       9.029ms        10.29%       9.029ms     173.629us            52  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       1.472us         0.00%       1.472us       1.472us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 352.519ms\n",
      "Self CUDA time total: 87.777ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 2\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 2\n",
    "test_samples = 1\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "filt_type = fineweb_path.split('.')[-1]\n",
    "fineweb = load_dataset(filt_type, data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text']\n",
    "\n",
    "print(\"output length is {}\".format(output_length))\n",
    "text = fineweb_text[0]\n",
    "inputs = preprocess_data(text, tokenizer)\n",
    "\n",
    "# cached_mlp.clear_load_from_cpu_stats()\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ]\n",
    ") as p:\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(),\n",
    "            max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "print(p.key_averages().table(\n",
    "    sort_by=\"self_cpu_time_total\", row_limit=-1))\n",
    "p.export_chrome_trace(\"./offloading-3090-simulateprefill.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试一个正常输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in prefill layer  0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'GemLiteLinearTriton' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/data2/lz/On-the-Fly_MoE_Inference/offloading/1.ipynb 单元格 15\u001b[0m line \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab46/data2/lz/On-the-Fly_MoE_Inference/offloading/1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# 前向传播\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab46/data2/lz/On-the-Fly_MoE_Inference/offloading/1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blab46/data2/lz/On-the-Fly_MoE_Inference/offloading/1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m     output \u001b[39m=\u001b[39m llm\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab46/data2/lz/On-the-Fly_MoE_Inference/offloading/1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m         input_ids\u001b[39m=\u001b[39;49minputs[\u001b[39m\"\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mcuda(device_id),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab46/data2/lz/On-the-Fly_MoE_Inference/offloading/1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49minputs[\u001b[39m\"\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mcuda(device_id),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab46/data2/lz/On-the-Fly_MoE_Inference/offloading/1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m         max_length\u001b[39m=\u001b[39;49minput_length \u001b[39m+\u001b[39;49m output_length,  \u001b[39m# 总长度为输入长度 + 输出长度\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab46/data2/lz/On-the-Fly_MoE_Inference/offloading/1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m         generation_config\u001b[39m=\u001b[39;49mGenerationConfig(do_sample\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab46/data2/lz/On-the-Fly_MoE_Inference/offloading/1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mpad_token_id, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab46/data2/lz/On-the-Fly_MoE_Inference/offloading/1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m         \u001b[39m# cache_implementation=\"static\" ## moe not support\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab46/data2/lz/On-the-Fly_MoE_Inference/offloading/1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab46/data2/lz/On-the-Fly_MoE_Inference/offloading/1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# 结束计时\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab46/data2/lz/On-the-Fly_MoE_Inference/offloading/1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m end_event\u001b[39m.\u001b[39mrecord()\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/transformers/generation/utils.py:2252\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2244\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2245\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   2246\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2247\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2248\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2249\u001b[0m     )\n\u001b[1;32m   2251\u001b[0m     \u001b[39m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2252\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sample(\n\u001b[1;32m   2253\u001b[0m         input_ids,\n\u001b[1;32m   2254\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mprepared_logits_processor,\n\u001b[1;32m   2255\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mprepared_stopping_criteria,\n\u001b[1;32m   2256\u001b[0m         generation_config\u001b[39m=\u001b[39;49mgeneration_config,\n\u001b[1;32m   2257\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   2258\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   2259\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   2260\u001b[0m     )\n\u001b[1;32m   2262\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39min\u001b[39;00m (GenerationMode\u001b[39m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2263\u001b[0m     \u001b[39m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m     beam_scorer \u001b[39m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2265\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   2266\u001b[0m         num_beams\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         max_length\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mmax_length,\n\u001b[1;32m   2272\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/transformers/generation/utils.py:3251\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3248\u001b[0m model_inputs\u001b[39m.\u001b[39mupdate({\u001b[39m\"\u001b[39m\u001b[39moutput_hidden_states\u001b[39m\u001b[39m\"\u001b[39m: output_hidden_states} \u001b[39mif\u001b[39;00m output_hidden_states \u001b[39melse\u001b[39;00m {})\n\u001b[1;32m   3250\u001b[0m \u001b[39mif\u001b[39;00m is_prefill:\n\u001b[0;32m-> 3251\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs, return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   3252\u001b[0m     is_prefill \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   3253\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m/data2/lz/On-the-Fly_MoE_Inference/offloading/modeling_mixtral.py:1374\u001b[0m, in \u001b[0;36mMixtralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, output_router_logits, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1371\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1373\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1374\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1375\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1376\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1377\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1378\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1379\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1380\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1381\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1382\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1383\u001b[0m     output_router_logits\u001b[39m=\u001b[39;49moutput_router_logits,\n\u001b[1;32m   1384\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1385\u001b[0m     cache_position\u001b[39m=\u001b[39;49mcache_position,\n\u001b[1;32m   1386\u001b[0m )\n\u001b[1;32m   1388\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1389\u001b[0m \u001b[39m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m/data2/lz/On-the-Fly_MoE_Inference/offloading/modeling_mixtral.py:1089\u001b[0m, in \u001b[0;36mMixtralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, output_router_logits, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1077\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1078\u001b[0m         decoder_layer\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m   1079\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1086\u001b[0m         cache_position,\n\u001b[1;32m   1087\u001b[0m     )\n\u001b[1;32m   1088\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1089\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m   1090\u001b[0m         hidden_states,\n\u001b[1;32m   1091\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mcausal_mask,\n\u001b[1;32m   1092\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1093\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1094\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1095\u001b[0m         output_router_logits\u001b[39m=\u001b[39;49moutput_router_logits,\n\u001b[1;32m   1096\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1097\u001b[0m         cache_position\u001b[39m=\u001b[39;49mcache_position,\n\u001b[1;32m   1098\u001b[0m     )\n\u001b[1;32m   1100\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1102\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "\u001b[1;32m/data2/lz/On-the-Fly_MoE_Inference/offloading/1.ipynb 单元格 15\u001b[0m line \u001b[0;36mPipelineLLM._replace_forward_methods.<locals>.new_forward\u001b[0;34m(hidden_states, attention_mask, position_ids, past_key_value, output_attentions, output_router_logits, use_cache, cache_position, layer, layer_idx)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Blab46/data2/lz/On-the-Fly_MoE_Inference/offloading/1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=218'>219</a>\u001b[0m     tmp_w3[j] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mlayers[i]\u001b[39m.\u001b[39mblock_sparse_moe\u001b[39m.\u001b[39mexperts[j]\u001b[39m.\u001b[39mw3\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Blab46/data2/lz/On-the-Fly_MoE_Inference/offloading/1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=219'>220</a>\u001b[0m     tmp_gate[j] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mlayers[j]\u001b[39m.\u001b[39mblock_sparse_moe\u001b[39m.\u001b[39mgate\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Blab46/data2/lz/On-the-Fly_MoE_Inference/offloading/1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=220'>221</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mlayers[i]\u001b[39m.\u001b[39mblock_sparse_moe\u001b[39m.\u001b[39mexperts[j]\u001b[39m.\u001b[39mw3 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mup_weight[layer_idx][j]\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Blab46/data2/lz/On-the-Fly_MoE_Inference/offloading/1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=221'>222</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mlayers[j]\u001b[39m.\u001b[39mblock_sparse_moe\u001b[39m.\u001b[39mgate \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrouting[layer_idx][j]\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Blab46/data2/lz/On-the-Fly_MoE_Inference/offloading/1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=223'>224</a>\u001b[0m \u001b[39m### 模拟，这里把hidden_states传到experts在的地方\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'GemLiteLinearTriton' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 10\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 16\n",
    "test_samples = 1\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "generated_all, decode_time, prefill_time = 0, 0, 0\n",
    "# print(\"max output length is {}\".format(output_length))\n",
    "text = \"The future of AI is here, \"\n",
    "\n",
    "clear_prefill_time = PLLM.get_prefill_time()\n",
    "clear_experts = PLLM.get_reload_experts()\n",
    "# print(\"need to zero: \", clear_experts, clear_prefill_time)\n",
    "\n",
    "llm.eval()\n",
    "inputs = preprocess_data(text, tokenizer)\n",
    "# 测试时间\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "# 开始计时\n",
    "torch.cuda.synchronize()\n",
    "start_event.record()\n",
    "\n",
    "# 前向传播\n",
    "with torch.no_grad():\n",
    "    output = llm.generate(\n",
    "        input_ids=inputs[\"input_ids\"].cuda(device_id),\n",
    "        attention_mask=inputs[\"attention_mask\"].cuda(device_id),\n",
    "        max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "        generation_config=GenerationConfig(do_sample=False),\n",
    "        pad_token_id=tokenizer.pad_token_id, \n",
    "        # cache_implementation=\"static\" ## moe not support\n",
    "    )\n",
    "\n",
    "# 结束计时\n",
    "end_event.record()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# 计算时间\n",
    "elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "decode_time += elapsed_time\n",
    "cur_prefill_time = PLLM.get_prefill_time()\n",
    "prefill_time += cur_prefill_time\n",
    "print(f\"Generated length: {len(output[0]) - input_length}\", f\"Time taken: {elapsed_time:.2f} s\", f\"prefill time: {cur_prefill_time:.2f} s\")\n",
    "# print(output)\n",
    "print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "\n",
    "generated_all += (len(output[0]) - input_length -1)\n",
    "\n",
    "timepertoken = (decode_time - prefill_time) / (generated_all)\n",
    "print(\"decode phase speed:\", '{:.4f}'.format(1/timepertoken) , ' token/s')\n",
    "print(\"the number of experts reload per token:\", PLLM.get_reload_experts() / generated_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载到GPU上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:25<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MixtralForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "import json\n",
    "\n",
    "def get_model(model_name, device_map, dtype=torch.bfloat16):\n",
    "    llm = MixtralForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        use_cache=True,\n",
    "        torch_dtype=dtype,\n",
    "    ) \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return llm, tokenizer\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "with open('../quantize/device_map_1.json', 'r') as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "dtype = torch.float16\n",
    "llm, tokenizer = get_model(model_name, device_map, dtype=dtype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
