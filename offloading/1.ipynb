{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先都加载到cpu上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"\n",
    "from modeling_mixtral import MixtralForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import json\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "with open(\"../quantize/device_map.json\", \"r\") as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "def get_model(model_name, device_map, dtype=torch.bfloat16, use_cache=True):\n",
    "    llm = MixtralForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        use_cache=use_cache,\n",
    "        torch_dtype=dtype,\n",
    "    ) \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return llm, tokenizer\n",
    "\n",
    "dtype = torch.float16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# llm, tokenizer = get_model(model_name, 'cpu', dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/venv/dilab/floe/hqq/hqq/models/base.py:251: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(cls.get_weight_file(save_dir), map_location=map_location)\n",
      "100%|██████████| 32/32 [00:00<00:00, 390.17it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 1183.41it/s]\n",
      "100%|██████████| 32/32 [02:53<00:00,  5.42s/it]\n"
     ]
    }
   ],
   "source": [
    "### HQQ量化\n",
    "from hqq.core.quantize import *\n",
    "from hqq.models.hf.mixtral import MixtralHQQ\n",
    "\n",
    "save_dir = './hqqsaved'\n",
    "### 第一次加载\n",
    "# q3_config    = BaseQuantizeConfig(nbits=2, group_size=64)\n",
    "# quant_config      = {'block_sparse_moe.experts.w3'   : q3_config}\n",
    "# llm = MixtralForCausalLM.from_pretrained(\n",
    "#         model_name,\n",
    "#         device_map='cpu',\n",
    "#         use_cache=True,\n",
    "#         torch_dtype=dtype,\n",
    "#     ) \n",
    "# MixtralHQQ.quantize_model(llm, quant_config=quant_config, compute_dtype=dtype, device='cuda:0')\n",
    "#### 先放CUDA量化，然后再传回CPU\n",
    "# MixtralHQQ.save_quantized(llm, save_dir)\n",
    "\n",
    "### 从保存的权重中加载\n",
    "llm = MixtralHQQ.from_quantized(save_dir, compute_dtype=dtype, device='cpu')\n",
    "HQQLinear.set_backend(HQQBackend.PYTORCH)\n",
    "\n",
    "backend       = \"gemlite\" #'torchao_int4' #\"torchao_int4\" (4-bit only) or \"gemlite\" (4-bit + 2-bit)\n",
    "# #Optimize\n",
    "from hqq.utils.patching import prepare_for_inference\n",
    "prepare_for_inference(llm, backend=backend, verbose=True)\n",
    "#Load GemLite cache\n",
    "if(backend == 'gemlite'):\n",
    "\timport gemlite\n",
    "\tgemlite.core.GEMLITE_TRITON_RESTRICT_M = True\n",
    "\tgemlite.core.GemLiteLinear.load_config('/tmp/gemlite_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "active neural num  2867\n",
      "active neural num  2867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/On-the-Fly_MoE_Inference/offloading/pipelinellm.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(f'/home/bcds/On-the-Fly_MoE_Inference/expert_predictor/training/{layer_idx}-{training_epoch}.pth'))\n"
     ]
    }
   ],
   "source": [
    "from pipelinellm import convert_mixtral_to_cached_mlp, PipelineLLM\n",
    "llm, cached_mlps = convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.8)\n",
    "\n",
    "# 创建流水线模型\n",
    "PLLM = PipelineLLM(llm, cached_mlps, 1, 3, training_epoch=20, print_layer_info=True) ### use ep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 尝试替换torch.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w3_forward = torch.compile(llm.model.layers[0].block_sparse_moe.experts[1].w3.forward, fullgraph=True, mode=\"reduce-overhead\")\n",
    "w3 = llm.model.layers[0].block_sparse_moe.experts[1].w3\n",
    "w3.cuda(0)\n",
    "w3.forward = w3_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试时间开销"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 32\n",
      "Generated length: 32 Time taken: 49.89 s, prefill time: 40.31 s\n",
      "['How do you get HIV?\\nHIV is a virus.\\n\\nHIV is a virus.\\n\\nHIV is a virus.\\n\\nHIV is a virus.\\n\\nHIV']\n",
      "Generated length: 32 Time taken: 56.94 s, prefill time: 48.50 s\n",
      "['CTComms sends on average 2 million emails a year.\\n\\nTheir 2018 goal is 10,000,000.\\n\\nTheir ']\n",
      "Generated length: 32 Time taken: 56.66 s, prefill time: 49.29 s\n",
      "['Hold the salt: UCLA engineers develop a new, inexpensive, 3-D-printed, low-power, all-in-one, re-usable, and fast']\n",
      "Generated length: 32 Time taken: 46.00 s, prefill time: 39.39 s\n",
      "['Not Just for Kids\\nThe Hunt for Fallen Fins\\n\\nBy LINDSAY A. ALLUP\\n\\nThe 1999-2000 St. John’s']\n",
      "Generated length: 32 Time taken: 43.67 s, prefill time: 37.17 s\n",
      "['The Solar and Heliospheric Observatory (SOHO) is a joint ESA-NASA-Pillar 3 (Solar) mission. Launched in 199']\n",
      "Generated length: 32 Time taken: 41.15 s, prefill time: 34.54 s\n",
      "['Bolivia: Coca-chewing protestors\\n\\nBolivia, 1993. Coca-chewing, coca-leaf-spreading, coca-plant']\n",
      "Generated length: 32 Time taken: 40.91 s, prefill time: 34.40 s\n",
      "['Breaking the COX code\\nUsing the COX-2-inhibitor VEH-10026 to treat the 5-HT-induced vasodilation in the']\n",
      "Generated length: 32 Time taken: 40.69 s, prefill time: 34.43 s\n",
      "['Discover the cosmos! Each day a different topic is covered, allowing children and their families to get a deeper understanding of the night sky and the universe.\\nriverscouncil.org\\n']\n",
      "Generated length: 32 Time taken: 42.69 s, prefill time: 35.93 s\n",
      "['Network With Us\\nJoin us on Facebook to connect with the 1,000,000+ people in the AARP RLBV/AARP RLBV']\n",
      "Generated length: 32 Time taken: 42.43 s, prefill time: 35.97 s\n",
      "['King James II of England (who was also King James VII and Lord James I of the Kingdom of the Scots and the Kingdom of the Irish, and the 16th of the 42']\n",
      "decode phase speed: 4.3604 token/s\n",
      "the number of reloaded experts per token: 9.665\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "\n",
    "input_length = 10\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 32\n",
    "test_samples = 10\n",
    "device_id = 0\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "filt_type = fineweb_path.split('.')[-1]\n",
    "fineweb = load_dataset(filt_type, data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text']\n",
    "\n",
    "# 预热（避免第一次运行时的额外开销）\n",
    "# for text in fineweb_text[:5] :\n",
    "#     inputs = preprocess_data(text, tokenizer)\n",
    "#     with torch.no_grad():\n",
    "#         output = llm(input_ids=inputs[\"input_ids\"].cuda(device_id), attention_mask=inputs[\"attention_mask\"].cuda(device_id))\n",
    "\n",
    "generated_all = 0\n",
    "prefill_time, decode_time = 0, 0\n",
    "reloaded_experts = 0\n",
    "print(\"output length is {}\".format(output_length))\n",
    "for text in fineweb_text[2:2+test_samples] :\n",
    "    inputs = preprocess_data(text, tokenizer)\n",
    "    ### 清空统计数据\n",
    "    PLLM.get_prefill_time()\n",
    "    PLLM.get_reload_experts()\n",
    "\n",
    "    # 测试时间\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # 开始计时\n",
    "    torch.cuda.synchronize()\n",
    "    start_event.record()\n",
    "\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(device_id),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(device_id),\n",
    "            max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.pad_token_id, \n",
    "            # cache_implementation=\"static\" ## moe not support\n",
    "        )\n",
    "\n",
    "    # 结束计时\n",
    "    end_event.record()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # 计算时间\n",
    "    elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "    decode_time += elapsed_time\n",
    "    cur_prefill_time = PLLM.get_prefill_time()\n",
    "    prefill_time += cur_prefill_time\n",
    "    print(f\"Generated length: {len(output[0]) - input_length}\", f\"Time taken: {elapsed_time:.2f} s,\", f\"prefill time: {cur_prefill_time:.2f} s\")\n",
    "    # print(output)\n",
    "    print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "\n",
    "    generated_all += (len(output[0]) - input_length - 1)\n",
    "    reloaded_experts += PLLM.get_reload_experts()\n",
    "\n",
    "timepertoken = (decode_time - prefill_time) / (generated_all)\n",
    "print(\"decode phase speed:\", '{:.4f}'.format(1/timepertoken) , 'token/s')\n",
    "print(\"the number of reloaded experts per token:\", '{:.3f}'.format(reloaded_experts / generated_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################################################################\n",
    "#Save gemlite cache\n",
    "if(backend == 'gemlite'):\n",
    "\tgemlite.core.GemLiteLinear.cache_config('/tmp/gemlite_config.json') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.profile\n",
    "\n",
    "trace-offloading-r.json是最优，就是做完一个index就传一个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 2\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::index        26.30%     124.066ms        27.48%     129.665ms     218.292us       1.762ms         0.40%       2.103ms       3.541us           594  \n",
      "                                        cudaMemcpyAsync        16.46%      77.643ms        16.46%      77.643ms      55.778us       0.000us         0.00%       0.000us       0.000us          1392  \n",
      "                                  cudaStreamSynchronize        13.57%      63.997ms        13.57%      63.997ms      85.672us     288.196us         0.07%     288.196us       0.386us           747  \n",
      "                                            aten::copy_        13.05%      61.566ms        30.57%     144.222ms      56.691us     307.223ms        70.50%     307.223ms     120.764us          2544  \n",
      "                                       cudaLaunchKernel         7.39%      34.847ms         7.39%      34.847ms       4.694us       0.000us         0.00%       0.000us       0.000us          7424  \n",
      "                                               aten::mm         3.43%      16.194ms         4.98%      23.499ms      29.633us      19.477ms         4.47%      19.477ms      24.561us           793  \n",
      "                                    HQQMatmulNoCacheMul         2.43%      11.452ms         9.72%      45.846ms     288.338us       0.000us         0.00%     105.857ms     665.768us           159  \n",
      "                                              aten::mul         1.23%       5.820ms         2.08%       9.791ms      10.471us      29.501ms         6.77%      29.501ms      31.552us           935  \n",
      "                                      aten::bitwise_and         1.09%       5.163ms         1.70%       8.030ms      12.586us      13.228ms         3.04%      13.228ms      20.734us           638  \n",
      "                                             aten::topk         0.91%       4.313ms         1.71%       8.070ms      28.718us      13.879ms         3.18%      13.879ms      49.390us           281  \n",
      "                                    aten::empty_strided         0.88%       4.143ms         0.88%       4.143ms       5.591us       0.000us         0.00%       0.000us       0.000us           741  \n",
      "                                            aten::slice         0.74%       3.484ms         0.92%       4.335ms       1.937us       0.000us         0.00%       0.000us       0.000us          2238  \n",
      "                                            aten::empty         0.73%       3.455ms         0.73%       3.455ms       3.163us       0.000us         0.00%       0.000us       0.000us          1092  \n",
      "                                              aten::add         0.69%       3.275ms         1.19%       5.598ms      10.766us       2.291ms         0.53%       2.291ms       4.405us           520  \n",
      "                                       aten::__rshift__         0.52%       2.459ms         1.22%       5.775ms      12.108us       9.696ms         2.22%       9.696ms      20.327us           477  \n",
      "                                       aten::as_strided         0.49%       2.314ms         0.49%       2.314ms       0.426us       0.000us         0.00%       0.000us       0.000us          5428  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.48%       2.252ms         0.48%       2.252ms       1.113us       0.000us         0.00%       0.000us       0.000us          2024  \n",
      "                                              aten::cat         0.46%       2.153ms         0.67%       3.172ms      16.181us       1.161ms         0.27%       1.161ms       5.924us           196  \n",
      "                                           aten::matmul         0.45%       2.115ms         6.53%      30.804ms      33.446us       0.000us         0.00%      20.111ms      21.836us           921  \n",
      "                                           aten::select         0.42%       2.002ms         0.50%       2.362ms       2.779us       0.000us         0.00%       0.000us       0.000us           850  \n",
      "                                             aten::sort         0.41%       1.918ms         1.76%       8.299ms      66.926us     977.995us         0.22%       3.544ms      28.578us           124  \n",
      "                                         aten::_to_copy         0.38%       1.813ms        17.68%      83.400ms     113.316us       0.000us         0.00%       3.524ms       4.789us           736  \n",
      "                                              aten::bmm         0.37%       1.762ms         0.80%       3.786ms      29.581us     634.120us         0.15%     634.120us       4.954us           128  \n",
      "                                          aten::reshape         0.36%       1.679ms         1.13%       5.311ms       3.338us       0.000us         0.00%     701.392us       0.441us          1591  \n",
      "                                    cudaLaunchKernelExC         0.35%       1.655ms         0.35%       1.655ms       5.650us       0.000us         0.00%       0.000us       0.000us           293  \n",
      "                                             aten::view         0.33%       1.534ms         0.33%       1.534ms       0.815us       0.000us         0.00%       0.000us       0.000us          1883  \n",
      "                                         aten::_softmax         0.31%       1.456ms         0.53%       2.492ms      13.116us     915.317us         0.21%     915.317us       4.817us           190  \n",
      "                                             aten::mean         0.31%       1.446ms         0.43%       2.042ms      15.709us     925.038us         0.21%     925.038us       7.116us           130  \n",
      "                                  cudaFuncGetAttributes         0.30%       1.420ms         0.30%       1.420ms       2.615us       0.000us         0.00%       0.000us       0.000us           543  \n",
      "                                              aten::pow         0.28%       1.333ms         0.42%       1.998ms      15.370us     567.501us         0.13%     567.501us       4.365us           130  \n",
      "                                              aten::sub         0.28%       1.298ms         0.43%       2.030ms      12.231us      25.766ms         5.91%      25.766ms     155.217us           166  \n",
      "                              aten::_local_scalar_dense         0.27%       1.272ms         2.29%      10.816ms      34.665us       1.861ms         0.43%       1.861ms       5.965us           312  \n",
      "                                              aten::abs         0.27%       1.256ms         1.03%       4.858ms      15.671us     535.571us         0.12%       1.071ms       3.455us           310  \n",
      "                                               aten::to         0.26%       1.207ms        17.93%      84.608ms      82.064us       0.000us         0.00%       3.524ms       3.419us          1031  \n",
      "                                        cudaMemsetAsync         0.22%       1.037ms         0.22%       1.037ms       6.284us       0.000us         0.00%       0.000us       0.000us           165  \n",
      "                                        aten::transpose         0.22%       1.023ms         0.31%       1.470ms       1.676us       0.000us         0.00%       0.000us       0.000us           877  \n",
      "                                                aten::t         0.20%     964.290us         0.39%       1.860ms       3.338us       0.000us         0.00%       0.000us       0.000us           557  \n",
      "                                             aten::silu         0.20%     960.137us         0.34%       1.590ms      11.868us     639.919us         0.15%     639.919us       4.776us           134  \n",
      "                                              aten::neg         0.20%     953.656us         0.33%       1.548ms      12.097us     709.905us         0.16%     709.905us       5.546us           128  \n",
      "                                            aten::rsqrt         0.18%     847.497us         0.30%       1.415ms      10.888us     560.238us         0.13%     560.238us       4.310us           130  \n",
      "                                            aten::equal         0.18%     834.462us         0.95%       4.465ms      72.010us     343.403us         0.08%       1.297ms      20.923us            62  \n",
      "                                              aten::sum         0.17%     796.420us         0.24%       1.122ms      17.538us     504.201us         0.12%     504.201us       7.878us            64  \n",
      "                                           aten::arange         0.16%     758.748us         0.72%       3.411ms      13.325us     643.275us         0.15%       1.287ms       5.026us           256  \n",
      "                                          aten::resize_         0.16%     745.097us         0.16%     745.097us       2.614us       0.000us         0.00%       0.000us       0.000us           285  \n",
      "                                           aten::linear         0.15%     717.735us         3.15%      14.867ms      37.355us       0.000us         0.00%       5.971ms      15.002us           398  \n",
      "                                              aten::all         0.13%     633.760us         0.20%     962.046us      15.517us     486.345us         0.11%     486.345us       7.844us            62  \n",
      "                                               aten::ne         0.12%     585.414us         0.21%       1.002ms      16.165us     275.782us         0.06%     275.782us       4.448us            62  \n",
      "                                              aten::div         0.12%     583.711us         0.19%     900.973us      14.078us     288.551us         0.07%     288.551us       4.509us            64  \n",
      "                                           aten::expand         0.12%     543.802us         0.14%     679.665us       1.761us       0.000us         0.00%       0.000us       0.000us           386  \n",
      "                                             aten::div_         0.11%     526.859us         0.18%     860.274us      13.442us     365.867us         0.08%     365.867us       5.717us            64  \n",
      "                                          aten::__and__         0.10%     457.943us         1.80%       8.488ms      13.303us       0.000us         0.00%      13.228ms      20.734us           638  \n",
      "                                        aten::unsqueeze         0.09%     436.925us         0.12%     559.313us       2.072us       0.000us         0.00%       0.000us       0.000us           270  \n",
      "                                             aten::item         0.08%     393.833us         2.38%      11.209ms      35.927us       0.000us         0.00%       1.861ms       5.965us           312  \n",
      "                                     aten::_unsafe_view         0.08%     377.962us         0.08%     377.962us       0.735us       0.000us         0.00%       0.000us       0.000us           514  \n",
      "                                 cudaDeviceGetAttribute         0.08%     372.567us         0.08%     372.567us       0.371us       0.000us         0.00%       0.000us       0.000us          1003  \n",
      "                                            aten::clone         0.07%     340.360us         0.52%       2.449ms      17.748us       0.000us         0.00%     724.945us       5.253us           138  \n",
      "                                            aten::addmm         0.07%     317.542us         0.08%     387.552us      32.296us      67.392us         0.02%      67.392us       5.616us            12  \n",
      "                                   cudaFuncSetAttribute         0.06%     304.120us         0.06%     304.120us       0.609us       0.000us         0.00%       0.000us       0.000us           499  \n",
      "                                       aten::empty_like         0.06%     289.743us         0.17%     779.059us       5.771us       0.000us         0.00%       0.000us       0.000us           135  \n",
      "                                              aten::any         0.06%     270.023us         0.10%     457.659us      16.345us     192.899us         0.04%     203.972us       7.285us            28  \n",
      "                                               aten::eq         0.06%     269.131us         0.10%     458.659us      14.333us     151.427us         0.03%     151.427us       4.732us            32  \n",
      "                                          aten::softmax         0.06%     268.374us         0.59%       2.760ms      14.528us       0.000us         0.00%     915.317us       4.817us           190  \n",
      "                                          aten::permute         0.06%     263.488us         0.07%     318.357us       2.567us       0.000us         0.00%       0.000us       0.000us           124  \n",
      "                                          aten::numpy_T         0.03%     120.695us         0.09%     439.052us       3.541us       0.000us         0.00%       0.000us       0.000us           124  \n",
      "                                       aten::is_nonzero         0.02%      79.417us         0.45%       2.117ms      31.126us       0.000us         0.00%     357.923us       5.264us            68  \n",
      "                                   aten::_reshape_alias         0.02%      73.029us         0.02%      73.029us       2.282us       0.000us         0.00%       0.000us       0.000us            32  \n",
      "                                            aten::fill_         0.01%      69.661us         0.04%     166.148us      12.781us      27.843us         0.01%      27.843us       2.142us            13  \n",
      "                                           aten::cumsum         0.01%      66.153us         0.02%     106.897us      35.632us      15.488us         0.00%      15.488us       5.163us             3  \n",
      "                                             aten::isin         0.01%      62.688us         0.06%     271.933us      90.644us       0.000us         0.00%      27.619us       9.206us             3  \n",
      "                                          aten::detach_         0.01%      60.107us         0.02%      78.021us       3.001us       0.000us         0.00%       0.000us       0.000us            26  \n",
      "                                      aten::result_type         0.01%      51.767us         0.01%      51.767us       0.398us       0.000us         0.00%       0.000us       0.000us           130  \n",
      "                                     aten::masked_fill_         0.01%      43.719us         0.01%      65.502us      16.375us       8.898us         0.00%       8.898us       2.225us             4  \n",
      "                                       aten::bitwise_or         0.01%      41.350us         0.01%      60.434us      15.109us       9.889us         0.00%       9.889us       2.472us             4  \n",
      "                                           aten::argmax         0.01%      36.715us         0.01%      51.397us      25.699us      24.961us         0.01%      24.961us      12.480us             2  \n",
      "                                     aten::index_select         0.01%      36.575us         0.01%      63.986us      31.993us       8.032us         0.00%       8.032us       4.016us             2  \n",
      "                                              aten::max         0.01%      35.922us         0.01%      55.544us      27.772us       8.736us         0.00%       8.736us       4.368us             2  \n",
      "                                  cudaDeviceSynchronize         0.01%      35.733us         0.01%      35.733us      35.733us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                          aten::dropout         0.01%      30.016us         0.01%      30.016us       0.469us       0.000us         0.00%       0.000us       0.000us            64  \n",
      "                                               aten::ge         0.01%      24.924us         0.01%      36.265us      18.132us       4.800us         0.00%       4.800us       2.400us             2  \n",
      "                                      aten::bitwise_not         0.01%      24.900us         0.01%      36.154us      18.077us       4.672us         0.00%       4.672us       2.336us             2  \n",
      "                                               aten::gt         0.01%      24.643us         0.02%      92.574us      46.287us       5.888us         0.00%       5.888us       2.944us             2  \n",
      "                                      aten::bitwise_or_         0.00%      23.133us         0.01%      32.320us      16.160us       5.312us         0.00%       5.312us       2.656us             2  \n",
      "                                             aten::mul_         0.00%      20.257us         0.01%      32.356us      16.178us       5.440us         0.00%       5.440us       2.720us             2  \n",
      "                                        aten::embedding         0.00%      18.427us         0.02%      86.566us      43.283us       0.000us         0.00%       8.032us       4.016us             2  \n",
      "                                                detach_         0.00%      17.914us         0.00%      17.914us       0.689us       0.000us         0.00%       0.000us       0.000us            26  \n",
      "                                               aten::lt         0.00%      17.759us         0.01%      24.336us      24.336us       2.240us         0.00%       2.240us       2.240us             1  \n",
      "                                             aten::full         0.00%      16.799us         0.02%     104.172us      17.362us       0.000us         0.00%      12.385us       2.064us             6  \n",
      "                                               aten::le         0.00%      14.527us         0.00%      22.436us      11.218us       5.632us         0.00%       5.632us       2.816us             2  \n",
      "                                         aten::new_ones         0.00%      14.311us         0.01%      61.386us      30.693us       0.000us         0.00%       4.416us       2.208us             2  \n",
      "                                             aten::rsub         0.00%      12.196us         0.01%      45.114us      22.557us       0.000us         0.00%       5.184us       2.592us             2  \n",
      "                                     aten::resolve_conj         0.00%      10.936us         0.00%      10.936us       0.203us       0.000us         0.00%       0.000us       0.000us            54  \n",
      "                                      aten::masked_fill         0.00%      10.098us         0.01%      59.425us      29.712us       0.000us         0.00%       8.673us       4.337us             2  \n",
      "                                      aten::resolve_neg         0.00%       7.433us         0.00%       7.433us       0.138us       0.000us         0.00%       0.000us       0.000us            54  \n",
      "                                           aten::__or__         0.00%       6.991us         0.01%      67.425us      16.856us       0.000us         0.00%       9.889us       2.472us             4  \n",
      "                                       aten::lift_fresh         0.00%       6.142us         0.00%       6.142us       0.236us       0.000us         0.00%       0.000us       0.000us            26  \n",
      "                                          aten::view_as         0.00%       6.001us         0.00%       9.253us       1.851us       0.000us         0.00%       0.000us       0.000us             5  \n",
      "                                             aten::ones         0.00%       5.107us         0.00%      22.528us      22.528us       0.000us         0.00%       2.209us       2.209us             1  \n",
      "                                        aten::new_empty         0.00%       4.300us         0.00%      14.399us       7.200us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                        aten::ones_like         0.00%       3.256us         0.00%      15.364us      15.364us       0.000us         0.00%       2.208us       2.208us             1  \n",
      "                                    cudaPeekAtLastError         0.00%       1.519us         0.00%       1.519us       0.127us       0.000us         0.00%       0.000us       0.000us            12  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     349.702us         0.08%     349.702us       2.553us           137  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      14.882us         0.00%      14.882us       2.126us             7  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     558.992us         0.13%     558.992us       5.480us           102  \n",
      "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       2.118ms         0.49%       2.118ms       4.995us           424  \n",
      "                         Memcpy DtoH (Device -> Pinned)         0.00%       0.000us         0.00%       0.000us       0.000us       1.861ms         0.43%       1.861ms       5.965us           312  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       7.040us         0.00%       7.040us       2.347us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       8.833us         0.00%       8.833us       2.208us             4  \n",
      "void at_cuda_detail::cub::DeviceScanInitKernel<at_cu...         0.00%       0.000us         0.00%       0.000us       0.000us       6.751us         0.00%       6.751us       2.250us             3  \n",
      "void at_cuda_detail::cub::DeviceScanKernel<at_cuda_d...         0.00%       0.000us         0.00%       0.000us       0.000us       8.737us         0.00%       8.737us       2.912us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      16.641us         0.00%      16.641us       2.377us             7  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      71.874us         0.02%      71.874us       2.662us            27  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.577us         0.00%       4.577us       2.289us             2  \n",
      "void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us       8.032us         0.00%       8.032us       4.016us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.128us         0.00%       4.128us       2.064us             2  \n",
      "void (anonymous namespace)::elementwise_kernel_with_...         0.00%       0.000us         0.00%       0.000us       0.000us     643.275us         0.15%     643.275us       5.026us           128  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.024us         0.00%       5.024us       2.512us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      15.201us         0.00%      15.201us       2.533us             6  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       5.440us         0.00%       5.440us       2.720us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       5.505us         0.00%       5.505us       2.752us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.864us         0.00%       4.864us       2.432us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.321us         0.00%       4.321us       2.160us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     748.210us         0.17%     748.210us       5.668us           132  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     567.501us         0.13%     567.501us       4.365us           130  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     925.038us         0.21%     925.038us       7.116us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     534.731us         0.12%     534.731us       4.113us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     560.238us         0.13%     560.238us       4.310us           130  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     662.030us         0.15%     662.030us       5.093us           130  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us      24.199ms         5.55%      24.199ms      27.068us           894  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.154ms         0.26%       1.154ms       4.473us           258  \n",
      "sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x...         0.00%       0.000us         0.00%       0.000us       0.000us       2.609ms         0.60%       2.609ms      20.380us           128  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       1.310ms         0.30%       1.310ms       9.778us           134  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.759ms         0.40%       1.759ms       6.215us           283  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      27.675ms         6.35%      27.675ms      50.966us           543  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     709.905us         0.16%     709.905us       5.546us           128  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     816.558us         0.19%     816.558us       6.379us           128  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.437ms         0.33%       1.437ms       4.490us           320  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     701.392us         0.16%     701.392us       5.480us           128  \n",
      "void gemv2T_kernel_val<int, int, __half, __half, __h...         0.00%       0.000us         0.00%       0.000us       0.000us     315.592us         0.07%     315.592us       4.931us            64  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     288.551us         0.07%     288.551us       4.509us            64  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      26.053ms         5.98%      26.053ms     116.829us           223  \n",
      "void (anonymous namespace)::softmax_warp_forward<c10...         0.00%       0.000us         0.00%       0.000us       0.000us     154.661us         0.04%     154.661us       4.833us            32  \n",
      "void gemmk1_kernel<int, float, 256, 5, false, false,...         0.00%       0.000us         0.00%       0.000us       0.000us     161.982us         0.04%     161.982us       5.062us            32  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     639.919us         0.15%     639.919us       4.776us           134  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       2.142ms         0.49%       2.142ms      16.481us           130  \n",
      "void (anonymous namespace)::softmax_warp_forward<c10...         0.00%       0.000us         0.00%       0.000us       0.000us     624.364us         0.14%     624.364us       4.955us           126  \n",
      "void at::native::sbtopk::gatherTopK<float, unsigned ...         0.00%       0.000us         0.00%       0.000us       0.000us       1.137ms         0.26%       1.137ms       9.020us           126  \n",
      "void at::native::bitonicSortKVInPlace<-2, -1, 16, 16...         0.00%       0.000us         0.00%       0.000us       0.000us       1.040ms         0.24%       1.040ms       8.256us           126  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      13.223ms         3.03%      13.223ms      20.791us           636  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       9.696ms         2.22%       9.696ms      20.327us           477  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us     413.229us         0.09%     413.229us       2.504us           165  \n",
      "sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x...         0.00%       0.000us         0.00%       0.000us       0.000us       8.928ms         2.05%       8.928ms      54.108us           165  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     535.571us         0.12%     535.571us       3.455us           155  \n",
      "void at::native::sbtopk::gatherTopK<c10::Half, unsig...         0.00%       0.000us         0.00%       0.000us       0.000us       8.691ms         1.99%       8.691ms      56.070us           155  \n",
      "void at::native::radixSortKVInPlace<-2, -1, 128, 32,...         0.00%       0.000us         0.00%       0.000us       0.000us       3.011ms         0.69%       3.011ms      19.425us           155  \n",
      "                       Memcpy DtoH (Device -> Pageable)         0.00%       0.000us         0.00%       0.000us       0.000us       1.054ms         0.24%       1.054ms       5.042us           209  \n",
      "                         Memcpy HtoD (Pinned -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     278.053ms        63.81%     278.053ms     896.945us           310  \n",
      "void dot_kernel<float, 128, 0, cublasDotParams<cubla...         0.00%       0.000us         0.00%       0.000us       0.000us     731.853us         0.17%     731.853us       6.099us           120  \n",
      "void reduce_1Block_kernel<float, 128, 7, cublasGemvT...         0.00%       0.000us         0.00%       0.000us       0.000us     642.734us         0.15%     642.734us       5.356us           120  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     504.201us         0.12%     504.201us       7.878us            64  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     365.867us         0.08%     365.867us       5.717us            64  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_...         0.00%       0.000us         0.00%       0.000us       0.000us     192.803us         0.04%     192.803us      48.201us             4  \n",
      "void splitKreduce_kernel<32, 16, int, float, __half,...         0.00%       0.000us         0.00%       0.000us       0.000us      22.177us         0.01%      22.177us       5.544us             4  \n",
      "void at::native::bitonicSortKVInPlace<-2, -1, 16, 16...         0.00%       0.000us         0.00%       0.000us       0.000us     977.995us         0.22%     977.995us       7.887us           124  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     486.345us         0.11%     486.345us       7.844us            62  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       2.552ms         0.59%       2.552ms      20.582us           124  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     129.826us         0.03%     129.826us       5.645us            23  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     192.899us         0.04%     192.899us       8.387us            23  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us      14.850us         0.00%      14.850us       3.712us             4  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      24.961us         0.01%      24.961us      12.480us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      10.401us         0.00%      10.401us       2.600us             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.184us         0.00%       5.184us       2.592us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.248us         0.00%       5.248us       2.624us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       2.592us         0.00%       2.592us       2.592us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.672us         0.00%       4.672us       2.336us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       5.440us         0.00%       5.440us       2.720us             2  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       8.736us         0.00%       8.736us       4.368us             2  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.361us         0.00%       3.361us       3.361us             1  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       6.496us         0.00%       6.496us       3.248us             2  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     329.701us         0.08%     329.701us       5.152us            64  \n",
      "void (anonymous namespace)::softmax_warp_forward<c10...         0.00%       0.000us         0.00%       0.000us       0.000us     136.292us         0.03%     136.292us       4.259us            32  \n",
      "void gemv2N_kernel<int, int, __half, __half, __half,...         0.00%       0.000us         0.00%       0.000us       0.000us     156.546us         0.04%     156.546us       4.892us            32  \n",
      "void at::native::vectorized_elementwise_kernel<2, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.464us         0.00%       2.464us       2.464us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 471.772ms\n",
      "Self CUDA time total: 435.767ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 1\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 2\n",
    "test_samples = 4\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "fineweb = load_dataset(\"parquet\",data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text'][:test_samples] \n",
    "\n",
    "print(\"output length is {}\".format(output_length))\n",
    "text = fineweb_text[0]\n",
    "inputs = preprocess_data(text, tokenizer)\n",
    "\n",
    "# cached_mlp.clear_load_from_cpu_stats()\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ]\n",
    ") as p:\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(),\n",
    "            max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "print(p.key_averages().table(\n",
    "    sort_by=\"self_cpu_time_total\", row_limit=-1))\n",
    "p.export_chrome_trace(\"./offloading-hqq2-reload_new.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试一个正常输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated length: 32 Time taken: 43.89 s prefill time: 37.30 s\n",
      "['The future of AI is here, and it’s not as scary as you might think. In this article, we’ll take a look at the 10 most important AI']\n",
      "decode phase speed: 4.7067  token/s\n",
      "the number of experts reload per token: 7.903225806451613\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 6\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 32\n",
    "device_id = 0\n",
    "test_samples = 1\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "generated_all, decode_time, prefill_time = 0, 0, 0\n",
    "# print(\"max output length is {}\".format(output_length))\n",
    "text = \"The future of AI is \"\n",
    "\n",
    "clear_prefill_time = PLLM.get_prefill_time()\n",
    "clear_experts = PLLM.get_reload_experts()\n",
    "# print(\"need to zero: \", clear_experts, clear_prefill_time)\n",
    "\n",
    "inputs = preprocess_data(text, tokenizer)\n",
    "# 测试时间\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "# 开始计时\n",
    "torch.cuda.synchronize()\n",
    "start_event.record()\n",
    "\n",
    "# 前向传播\n",
    "with torch.no_grad():\n",
    "    output = llm.generate(\n",
    "        input_ids=inputs[\"input_ids\"].cuda(device_id),\n",
    "        attention_mask=inputs[\"attention_mask\"].cuda(device_id),\n",
    "        max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "        generation_config=GenerationConfig(do_sample=False),\n",
    "        pad_token_id=tokenizer.pad_token_id, \n",
    "        # cache_implementation=\"static\" ## moe not support\n",
    "    )\n",
    "\n",
    "# 结束计时\n",
    "end_event.record()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# 计算时间\n",
    "elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "decode_time += elapsed_time\n",
    "cur_prefill_time = PLLM.get_prefill_time()\n",
    "prefill_time += cur_prefill_time\n",
    "print(f\"Generated length: {len(output[0]) - input_length}\", f\"Time taken: {elapsed_time:.2f} s\", f\"prefill time: {cur_prefill_time:.2f} s\")\n",
    "# print(output)\n",
    "print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "\n",
    "generated_all += (len(output[0]) - input_length -1)\n",
    "\n",
    "timepertoken = (decode_time - prefill_time) / (generated_all)\n",
    "print(\"decode phase speed:\", '{:.4f}'.format(1/timepertoken) , ' token/s')\n",
    "print(\"the number of experts reload per token:\", PLLM.get_reload_experts() / generated_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载到GPU上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:25<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MixtralForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "import json\n",
    "\n",
    "def get_model(model_name, device_map, dtype=torch.bfloat16):\n",
    "    llm = MixtralForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        use_cache=True,\n",
    "        torch_dtype=dtype,\n",
    "    ) \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return llm, tokenizer\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "with open('../quantize/device_map_1.json', 'r') as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "dtype = torch.float16\n",
    "llm, tokenizer = get_model(model_name, device_map, dtype=dtype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
