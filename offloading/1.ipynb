{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先都加载到cpu上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:06<00:00,  3.08it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "from modeling_mixtral import MixtralForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import json\n",
    "\n",
    "def get_model(model_name, device_map, dtype=torch.bfloat16):\n",
    "    llm = MixtralForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        use_cache=True,\n",
    "        torch_dtype=dtype,\n",
    "    ) \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return llm, tokenizer\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "with open('../quantize/device_map_1.json', 'r') as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "dtype = torch.float16\n",
    "llm, tokenizer = get_model(model_name, 'cpu', dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import threading\n",
    "import json\n",
    "from queue import Queue\n",
    "\n",
    "class CachedMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, dtype, sparsity: float = 0.2):\n",
    "        super(CachedMLP, self).__init__()\n",
    "        self.sparsity = sparsity\n",
    "        self.activenum = int((1 - sparsity) * hidden_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dtype = dtype\n",
    "\n",
    "        # GPU 缓存张量\n",
    "        self.w1_gpu = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda')\n",
    "        self.w2_gpu = torch.empty((self.input_dim, self.activenum), dtype=self.dtype, device='cuda')\n",
    "        self.w3_gpu = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda')\n",
    "\n",
    "        # 第二个专家的 GPU 缓存张量\n",
    "        self.w1_gpu_expert1 = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda')\n",
    "        self.w2_gpu_expert1 = torch.empty((self.input_dim, self.activenum), dtype=self.dtype, device='cuda')\n",
    "        self.w3_gpu_expert1 = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda')\n",
    "\n",
    "        # Pinned Memory 缓冲区\n",
    "        self.register_buffer('sparse_w1_cpu', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "        self.register_buffer('sparse_w2_cpu', torch.empty((self.input_dim, self.activenum), dtype=self.dtype, device='cpu'))\n",
    "        self.register_buffer('sparse_w3_cpu', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "        self.sparse_w1_cpu = self.sparse_w1_cpu.pin_memory()\n",
    "        self.sparse_w2_cpu = self.sparse_w2_cpu.pin_memory()\n",
    "        self.sparse_w3_cpu = self.sparse_w3_cpu.pin_memory()\n",
    "\n",
    "        # 第二个专家的 Pinned Memory 缓冲区\n",
    "        self.register_buffer('sparse_w1_cpu_expert1', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "        self.register_buffer('sparse_w2_cpu_expert1', torch.empty((self.input_dim, self.activenum), dtype=self.dtype, device='cpu'))\n",
    "        self.register_buffer('sparse_w3_cpu_expert1', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "        self.sparse_w1_cpu_expert1 = self.sparse_w1_cpu_expert1.pin_memory()\n",
    "        self.sparse_w2_cpu_expert1 = self.sparse_w2_cpu_expert1.pin_memory()\n",
    "        self.sparse_w3_cpu_expert1 = self.sparse_w3_cpu_expert1.pin_memory()\n",
    "\n",
    "        # 统计信息\n",
    "        self.load_from_cpu_time = 0.0\n",
    "        self.load_from_cpu_calls = 0\n",
    "\n",
    "    def load_from_cpu(self, cpu_mlp, cpu_mlp_expert1, stream: torch.cuda.Stream):\n",
    "        \"\"\"\n",
    "        从CPU加载参数，并使用指定的CUDA流进行异步复制到GPU。\n",
    "        \n",
    "        参数:\n",
    "            cpu_mlp: 包含CPU上参数的字典（第一个专家）。\n",
    "            cpu_mlp_expert1: 包含CPU上参数的字典（第二个专家）。\n",
    "            stream: 用于数据传输的CUDA流。\n",
    "        \"\"\"\n",
    "        # 从CPU加载参数（第一个专家）\n",
    "        self.sparse_w1_cpu.copy_(cpu_mlp['w1'].data[:self.activenum, :])\n",
    "        self.sparse_w2_cpu.copy_(cpu_mlp['w2'].data[:, :self.activenum])\n",
    "        self.sparse_w3_cpu.copy_(cpu_mlp['w3'].data[:self.activenum, :])\n",
    "\n",
    "        # 从CPU加载参数（第二个专家）\n",
    "        self.sparse_w1_cpu_expert1.copy_(cpu_mlp_expert1['w1'].data[:self.activenum, :])\n",
    "        self.sparse_w2_cpu_expert1.copy_(cpu_mlp_expert1['w2'].data[:, :self.activenum])\n",
    "        self.sparse_w3_cpu_expert1.copy_(cpu_mlp_expert1['w3'].data[:self.activenum, :])\n",
    "\n",
    "        # 异步复制到GPU\n",
    "        with torch.cuda.stream(stream):\n",
    "            self.w1_gpu.copy_(self.sparse_w1_cpu, non_blocking=True)\n",
    "            self.w2_gpu.copy_(self.sparse_w2_cpu, non_blocking=True)\n",
    "            self.w3_gpu.copy_(self.sparse_w3_cpu, non_blocking=True)\n",
    "\n",
    "            # 第二个专家的异步复制\n",
    "            self.w1_gpu_expert1.copy_(self.sparse_w1_cpu_expert1, non_blocking=True)\n",
    "            self.w2_gpu_expert1.copy_(self.sparse_w2_cpu_expert1, non_blocking=True)\n",
    "            self.w3_gpu_expert1.copy_(self.sparse_w3_cpu_expert1, non_blocking=True)\n",
    "\n",
    "def convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.9):\n",
    "    ### 其他部分存放在GPU上\n",
    "    llm.model.embed_tokens.cuda()\n",
    "    for i in range(len(llm.model.layers)):\n",
    "        llm.model.layers[i].self_attn.cuda()\n",
    "        llm.model.layers[i].input_layernorm.cuda()\n",
    "        llm.model.layers[i].post_attention_layernorm.cuda()\n",
    "        llm.model.layers[i].block_sparse_moe.gate.cuda()\n",
    "    ### 第0层的专家存放在GPU上\n",
    "    for j in range(len(llm.model.layers[0].block_sparse_moe.experts)):\n",
    "        llm.model.layers[0].block_sparse_moe.experts[j].cuda()\n",
    "\n",
    "    llm.model.norm.cuda()\n",
    "    llm.lm_head.cuda()\n",
    "    \n",
    "    # 创建两个共享的CachedMLP实例\n",
    "    buffer0 = CachedMLP(\n",
    "        input_dim=llm.config.hidden_size,\n",
    "        hidden_dim=llm.config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "        sparsity=sparsity\n",
    "    )\n",
    "    buffer1 = CachedMLP(\n",
    "        input_dim=llm.config.hidden_size,\n",
    "        hidden_dim=llm.config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "        sparsity=sparsity\n",
    "    )\n",
    "    cached_mlps = [buffer0, buffer1]\n",
    "    \n",
    "    for i, layer in enumerate(llm.model.layers):\n",
    "        if i==0:\n",
    "            continue\n",
    "        # 将专家的forward方法替换为PipelineLLM管理的方式\n",
    "        for j, expert in enumerate(layer.block_sparse_moe.experts):\n",
    "            expert.cpu_mlp = {\n",
    "                \"w1\": expert.w1.cpu().weight,\n",
    "                \"w2\": expert.w2.cpu().weight,\n",
    "                \"w3\": expert.w3.cpu().weight,\n",
    "            }\n",
    "    return llm, cached_mlps\n",
    "\n",
    "class PipelineLLM:\n",
    "    def __init__(self, llm, cached_mlps):\n",
    "        \"\"\"\n",
    "        初始化 PipelineLLM，替换模型每一层的 forward 方法。\n",
    "        \n",
    "        参数:\n",
    "            llm: 原始的大模型\n",
    "            cached_mlps: 两个 CachedMLP 实例列表\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.cached_mlps = cached_mlps  # [buffer0, buffer1]\n",
    "        self.num_layers = len(llm.model.layers)\n",
    "        self.lock = threading.Lock()\n",
    "        self.use_buffer0 = True  # 标记当前使用哪个缓冲区\n",
    "\n",
    "        # 创建两个共享的CUDA流\n",
    "        self.stream0 = torch.cuda.Stream()\n",
    "        self.stream1 = torch.cuda.Stream()\n",
    "\n",
    "        # 初始化加载第一个和第二个层的参数\n",
    "        self._load_layer(1, buffer_index=0)\n",
    "        self._load_layer(1, buffer_index=1)\n",
    "\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "        self._replace_forward_methods()\n",
    "    \n",
    "    def _load_layer(self, layer_idx, buffer_index):\n",
    "        \"\"\"\n",
    "        加载指定层的参数到指定的缓冲区。\n",
    "        \n",
    "        参数:\n",
    "            layer_idx: 层的索引\n",
    "            buffer_index: 缓冲区的索引（0 或 1）\n",
    "        \"\"\"\n",
    "        layer = self.llm.model.layers[layer_idx]\n",
    "        expert0 = layer.block_sparse_moe.experts[0]\n",
    "        expert1 = layer.block_sparse_moe.experts[1]\n",
    "        cpu_mlp = expert0.cpu_mlp\n",
    "        cpu_mlp_expert1 = expert1.cpu_mlp\n",
    "        buffer = self.cached_mlps[buffer_index]\n",
    "        stream = self.stream0 if buffer_index == 0 else self.stream1\n",
    "\n",
    "        # 异步加载参数\n",
    "        buffer.load_from_cpu(cpu_mlp, cpu_mlp_expert1, stream)\n",
    "\n",
    "    def _replace_forward_methods(self):\n",
    "        \"\"\"\n",
    "        替换模型每一层的 forward 方法，添加参数预加载逻辑和注意力计算。\n",
    "        \"\"\"\n",
    "        for i, layer in enumerate(self.llm.model.layers):\n",
    "            def new_forward(hidden_states: torch.Tensor,\n",
    "                            attention_mask: Optional[torch.Tensor] = None,\n",
    "                            position_ids: Optional[torch.LongTensor] = None,\n",
    "                            past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "                            output_attentions: Optional[bool] = False,\n",
    "                            output_router_logits: Optional[bool] = False,\n",
    "                            use_cache: Optional[bool] = False,\n",
    "                            cache_position: Optional[torch.LongTensor] = None,\n",
    "                            layer_idx=i):\n",
    "                # print(f\"in layer {layer_idx}\")\n",
    "                with self.lock:\n",
    "                    # 选择当前使用的缓冲区\n",
    "                    current_buffer = self.cached_mlps[0] if self.use_buffer0 else self.cached_mlps[1]\n",
    "                    current_stream = self.stream0 if self.use_buffer0 else self.stream1\n",
    "\n",
    "                    # 切换缓冲区用于下一次\n",
    "                    next_buffer_index = 1 if self.use_buffer0 else 0\n",
    "                    next_buffer = self.cached_mlps[next_buffer_index]\n",
    "                    next_stream = self.stream1 if self.use_buffer0 else self.stream0\n",
    "\n",
    "                    # 预加载下一层的参数\n",
    "                    next_layer_idx = layer_idx + 1\n",
    "                    if next_layer_idx < self.num_layers:\n",
    "                        self._load_layer(next_layer_idx, buffer_index=next_buffer_index)\n",
    "                    \n",
    "                    # 切换缓冲区\n",
    "                    self.use_buffer0 = not self.use_buffer0\n",
    "\n",
    "                    # 处理当前层\n",
    "                    residual = hidden_states\n",
    "                    hidden_states = layer.input_layernorm(hidden_states)\n",
    "\n",
    "                    # Self Attention\n",
    "                    hidden_states, self_attn_weights, present_key_value = layer.self_attn(\n",
    "                        hidden_states=hidden_states,\n",
    "                        attention_mask=attention_mask,\n",
    "                        position_ids=position_ids,\n",
    "                        past_key_value=past_key_value,\n",
    "                        output_attentions=output_attentions,\n",
    "                        use_cache=use_cache,\n",
    "                        cache_position=cache_position,\n",
    "                    )\n",
    "                    hidden_states = residual + hidden_states\n",
    "\n",
    "                    # Fully Connected\n",
    "                    residual = hidden_states\n",
    "                    hidden_states = layer.post_attention_layernorm(hidden_states)\n",
    "\n",
    "                    # 使用当前缓冲区进行 MLP 计算\n",
    "                    batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "                    hidden_states = hidden_states.view(-1, hidden_dim)\n",
    "\n",
    "                    # 第一个专家的计算\n",
    "                    w3_output = torch.matmul(hidden_states, current_buffer.w3_gpu.T)\n",
    "                    w1_output = self.activation(torch.matmul(hidden_states, current_buffer.w1_gpu.T))\n",
    "                    w2 = current_buffer.w2_gpu.T\n",
    "                    final_hidden_states_expert0 = torch.matmul(w1_output * w3_output, w2)\n",
    "\n",
    "                    # 第二个专家的计算\n",
    "                    w3_output_expert1 = torch.matmul(hidden_states, current_buffer.w3_gpu_expert1.T)\n",
    "                    w1_output_expert1 = self.activation(torch.matmul(hidden_states, current_buffer.w1_gpu_expert1.T))\n",
    "                    w2_expert1 = current_buffer.w2_gpu_expert1.T\n",
    "                    final_hidden_states_expert1 = torch.matmul(w1_output_expert1 * w3_output_expert1, w2_expert1)\n",
    "\n",
    "                    # 将两个专家的结果相加\n",
    "                    final_hidden_states = final_hidden_states_expert0 + final_hidden_states_expert1\n",
    "\n",
    "                    final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n",
    "\n",
    "                    hidden_states = residual + final_hidden_states\n",
    "\n",
    "                    outputs = (hidden_states,)\n",
    "\n",
    "                    if output_attentions:\n",
    "                        outputs += (self_attn_weights,)\n",
    "\n",
    "                    if use_cache:\n",
    "                        outputs += (present_key_value,)\n",
    "\n",
    "                    return outputs\n",
    "            # 替换 forward 方法\n",
    "            layer.forward = new_forward\n",
    "            \n",
    "# 将模型转换为使用CachedMLP的版本\n",
    "llm, cached_mlps = convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.8)\n",
    "\n",
    "# 创建流水线模型\n",
    "pipeline_llm = PipelineLLM(llm, cached_mlps).llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试时间开销"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 1\n",
      "Time taken: 0.1087 seconds\n",
      "Time taken: 0.1039 seconds\n",
      "Time taken: 0.1043 seconds\n",
      "Time taken: 0.1006 seconds\n",
      "output length is 32\n",
      "Time taken: 3.0484 seconds\n",
      "Time taken: 2.9033 seconds\n",
      "Time taken: 2.8158 seconds\n",
      "Time taken: 2.8877 seconds\n",
      "decode time: 2.8094  s\n",
      "decode phase speed: 11.0344  token/s\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 32\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 32\n",
    "test_samples = 4\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "fineweb = load_dataset(\"parquet\",data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text'][:test_samples] \n",
    "\n",
    "prefill_time, decode_time = 0, 0\n",
    "for output_length in [1, output_length]:\n",
    "    print(\"output length is {}\".format(output_length))\n",
    "    for text in fineweb_text:\n",
    "        # input_ids = torch.randint(0, 32000, (1, input_length)).cuda()  # 随机生成输入 token IDs\n",
    "        # attention_mask = torch.ones((1, input_length)).cuda()  # 假设 attention mask\n",
    "        inputs = preprocess_data(text, tokenizer)\n",
    "\n",
    "        # 预热（避免第一次运行时的额外开销）\n",
    "        with torch.no_grad():\n",
    "            output = llm(input_ids=inputs[\"input_ids\"].cuda(), attention_mask=inputs[\"attention_mask\"].cuda())\n",
    "\n",
    "        # 测试时间\n",
    "        start_event = torch.cuda.Event(enable_timing=True)\n",
    "        end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        # cached_mlp.clear_load_from_cpu_stats()\n",
    "        # 开始计时\n",
    "        torch.cuda.synchronize()\n",
    "        start_event.record()\n",
    "\n",
    "        # 前向传播\n",
    "        with torch.no_grad():\n",
    "            output = llm.generate(\n",
    "                input_ids=inputs[\"input_ids\"].cuda(),\n",
    "                attention_mask=inputs[\"attention_mask\"].cuda(),\n",
    "                max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "                generation_config=GenerationConfig(do_sample=False),\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        # 结束计时\n",
    "        end_event.record()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        # 计算时间\n",
    "        elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "        # print(f\"Generated output length: {len(output[0]) - input_length}\")\n",
    "        # print(output)\n",
    "        # print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "        print(f\"Time taken: {elapsed_time:.4f} seconds\")\n",
    "        if output_length == 1:\n",
    "            prefill_time += elapsed_time\n",
    "        else:\n",
    "            decode_time += elapsed_time\n",
    "            # total_time, avg_time = cached_mlp.get_load_from_cpu_stats()\n",
    "            # print(f\"Total time spent in load_from_cpu: {total_time/1000:.4f} s\")\n",
    "\n",
    "timepertoken = (decode_time - prefill_time) / (output_length-1) / test_samples\n",
    "print(\"decode time:\", '{:.4f}'.format((decode_time - prefill_time) /test_samples), ' s')\n",
    "print(\"decode phase speed:\", '{:.4f}'.format(1/timepertoken) , ' token/s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:22<00:00,  1.19s/it]\n"
     ]
    }
   ],
   "source": [
    "#### GPU版本\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3,4\"\n",
    "\n",
    "from modeling_mixtral import MixtralForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import json\n",
    "\n",
    "def get_model(model_name, device_map, dtype=torch.bfloat16):\n",
    "    llm = MixtralForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        use_cache=True,\n",
    "        torch_dtype=dtype,\n",
    "    ) \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return llm, tokenizer\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "\n",
    "with open('../quantize/device_map_1.json', 'r') as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "dtype = torch.float16\n",
    "llm, tokenizer = get_model(model_name, device_map, dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 2\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::copy_        55.13%      82.127ms        58.20%      86.700ms      75.195us     141.112ms        84.11%     141.112ms     122.387us          1153  \n",
      "                                       cudaLaunchKernel        11.69%      17.411ms        11.69%      17.411ms       4.967us       0.000us         0.00%       0.000us       0.000us          3505  \n",
      "                                               aten::mm         6.04%       8.996ms         9.31%      13.870ms      21.604us      12.218ms         7.28%      12.218ms      19.031us           642  \n",
      "                                              aten::mul         3.07%       4.573ms         5.11%       7.609ms      11.742us       3.695ms         2.20%       3.695ms       5.703us           648  \n",
      "                                              aten::add         2.20%       3.278ms         3.63%       5.402ms      11.898us       2.365ms         1.41%       2.365ms       5.210us           454  \n",
      "                                            aten::slice         1.99%       2.960ms         2.43%       3.616ms       2.174us       0.000us         0.00%       0.000us       0.000us          1663  \n",
      "                                              aten::cat         1.70%       2.531ms         2.59%       3.856ms      14.944us       2.312ms         1.38%       2.312ms       8.960us           258  \n",
      "                                        cudaMemcpyAsync         1.54%       2.301ms         1.54%       2.301ms       5.767us       0.000us         0.00%       0.000us       0.000us           399  \n",
      "                                             aten::mean         1.06%       1.580ms         1.51%       2.252ms      17.320us       1.424ms         0.85%       1.424ms      10.956us           130  \n",
      "                                            aten::index         1.05%       1.558ms         1.68%       2.497ms      19.358us     855.595us         0.51%     855.595us       6.633us           129  \n",
      "                                            aten::empty         1.02%       1.527ms         1.02%       1.527ms       3.277us       0.000us         0.00%       0.000us       0.000us           466  \n",
      "                                       aten::as_strided         1.00%       1.489ms         1.00%       1.489ms       0.445us       0.000us         0.00%       0.000us       0.000us          3349  \n",
      "                                              aten::pow         0.94%       1.400ms         1.44%       2.152ms      16.554us     622.319us         0.37%     622.319us       4.787us           130  \n",
      "                                    aten::empty_strided         0.90%       1.341ms         0.90%       1.341ms       4.426us       0.000us         0.00%       0.000us       0.000us           303  \n",
      "                                           aten::matmul         0.82%       1.215ms        10.58%      15.762ms      24.552us       0.000us         0.00%      12.218ms      19.031us           642  \n",
      "                                              aten::neg         0.70%       1.046ms         1.12%       1.664ms      13.003us     789.520us         0.47%     789.520us       6.168us           128  \n",
      "                                            aten::rsqrt         0.63%     932.893us         1.06%       1.579ms      12.143us     587.121us         0.35%     587.121us       4.516us           130  \n",
      "                                             aten::gelu         0.60%     898.185us         1.00%       1.496ms      11.687us     670.479us         0.40%     670.479us       5.238us           128  \n",
      "                                        aten::transpose         0.59%     882.974us         0.86%       1.282ms       1.665us       0.000us         0.00%       0.000us       0.000us           770  \n",
      "                                         aten::_to_copy         0.57%     843.069us         3.37%       5.019ms      18.796us       0.000us         0.00%       1.544ms       5.783us           267  \n",
      "                                          aten::reshape         0.51%     765.604us         2.48%       3.692ms       6.354us       0.000us         0.00%       1.632ms       2.809us           581  \n",
      "                                             aten::view         0.51%     764.020us         0.51%     764.020us       0.976us       0.000us         0.00%       0.000us       0.000us           783  \n",
      "                                          aten::permute         0.46%     686.561us         0.57%     848.297us       2.209us       0.000us         0.00%       0.000us       0.000us           384  \n",
      "                         aten::_flash_attention_forward         0.44%     660.213us         1.23%       1.832ms      55.517us     677.900us         0.40%     677.900us      20.542us            33  \n",
      "                                               aten::to         0.42%     628.384us         3.79%       5.647ms      10.477us       0.000us         0.00%       1.544ms       2.864us           539  \n",
      "                     aten::scaled_dot_product_attention         0.42%     627.712us         2.84%       4.237ms      66.206us       0.000us         0.00%       1.001ms      15.635us            64  \n",
      "                                        aten::unsqueeze         0.32%     471.609us         0.40%     598.389us       2.284us       0.000us         0.00%       0.000us       0.000us           262  \n",
      "                                           aten::linear         0.31%     456.175us         6.89%      10.263ms      39.780us       0.000us         0.00%       5.191ms      20.119us           258  \n",
      "                                                aten::t         0.31%     455.922us         0.60%     890.833us       3.453us       0.000us         0.00%       0.000us       0.000us           258  \n",
      "                     aten::_efficient_attention_forward         0.28%     421.774us         0.65%     962.466us      31.047us     322.725us         0.19%     322.725us      10.410us            31  \n",
      "                                    cudaLaunchKernelExC         0.27%     404.936us         0.27%     404.936us       6.135us       0.000us         0.00%       0.000us       0.000us            66  \n",
      "                                            aten::clone         0.21%     316.100us         1.69%       2.518ms      18.789us       0.000us         0.00%       1.642ms      12.251us           134  \n",
      "                                     aten::_unsafe_view         0.21%     310.708us         0.21%     310.708us       0.805us       0.000us         0.00%       0.000us       0.000us           386  \n",
      "                                           aten::expand         0.20%     293.180us         0.24%     358.040us       2.797us       0.000us         0.00%       0.000us       0.000us           128  \n",
      "                                          aten::numpy_T         0.18%     266.179us         0.75%       1.114ms       2.902us       0.000us         0.00%       0.000us       0.000us           384  \n",
      "                                       aten::empty_like         0.17%     252.584us         0.68%       1.015ms       6.117us       0.000us         0.00%       0.000us       0.000us           166  \n",
      "              aten::_scaled_dot_product_flash_attention         0.17%     247.862us         1.51%       2.256ms      68.369us       0.000us         0.00%     677.900us      20.542us            33  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.16%     239.553us         0.16%     239.553us       0.742us       0.000us         0.00%       0.000us       0.000us           323  \n",
      "          aten::_scaled_dot_product_efficient_attention         0.16%     235.816us         0.91%       1.353ms      43.655us       0.000us         0.00%     322.725us      10.410us            31  \n",
      "                                  cudaFuncGetAttributes         0.14%     201.332us         0.14%     201.332us       3.050us       0.000us         0.00%       0.000us       0.000us            66  \n",
      "                                   cudaFuncSetAttribute         0.13%     196.151us         0.13%     196.151us       0.991us       0.000us         0.00%       0.000us       0.000us           198  \n",
      "                                               aten::eq         0.07%      97.650us         0.10%     146.420us      16.269us      14.690us         0.01%      14.690us       1.632us             9  \n",
      "                                  cudaStreamSynchronize         0.06%      94.427us         0.06%      94.427us       7.264us       0.000us         0.00%       0.000us       0.000us            13  \n",
      "                                  cudaStreamIsCapturing         0.05%      69.360us         0.05%      69.360us       1.084us       0.000us         0.00%       0.000us       0.000us            64  \n",
      "                                           aten::cumsum         0.04%      66.274us         0.07%     107.608us      35.869us      10.945us         0.01%      10.945us       3.648us             3  \n",
      "                                             aten::isin         0.04%      64.661us         0.19%     286.101us      95.367us       0.000us         0.00%      17.794us       5.931us             3  \n",
      "                                            aten::fill_         0.04%      61.303us         0.10%     141.595us      12.872us      15.008us         0.01%      15.008us       1.364us            11  \n",
      "                                      aten::result_type         0.04%      59.285us         0.04%      59.285us       0.456us       0.000us         0.00%       0.000us       0.000us           130  \n",
      "                                              aten::sub         0.03%      50.456us         0.05%      74.705us      14.941us       7.711us         0.00%       7.711us       1.542us             5  \n",
      "                                       aten::bitwise_or         0.03%      48.844us         0.05%      69.913us      17.478us       6.464us         0.00%       6.464us       1.616us             4  \n",
      "                              aten::_local_scalar_dense         0.03%      44.820us         0.10%     153.657us      19.207us      13.441us         0.01%      13.441us       1.680us             8  \n",
      "                                     aten::index_select         0.03%      44.630us         0.05%      73.129us      36.564us       6.976us         0.00%       6.976us       3.488us             2  \n",
      "                                              aten::any         0.03%      41.460us         0.07%     102.559us      20.512us       0.000us         0.00%       6.687us       1.337us             5  \n",
      "                                              aten::max         0.02%      36.324us         0.04%      58.338us      29.169us       7.679us         0.00%       7.679us       3.840us             2  \n",
      "                                           aten::select         0.02%      35.823us         0.03%      38.843us       5.549us       0.000us         0.00%       0.000us       0.000us             7  \n",
      "                                           aten::argmax         0.02%      35.233us         0.03%      50.004us      25.002us      23.424us         0.01%      23.424us      11.712us             2  \n",
      "                                              aten::all         0.02%      34.784us         0.03%      48.510us      24.255us       7.552us         0.00%       7.552us       3.776us             2  \n",
      "                                     aten::masked_fill_         0.02%      30.090us         0.03%      40.748us      20.374us       3.041us         0.00%       3.041us       1.520us             2  \n",
      "                                               aten::ge         0.02%      26.277us         0.03%      37.447us      18.723us       3.007us         0.00%       3.007us       1.504us             2  \n",
      "                                        cudaMemsetAsync         0.02%      25.559us         0.02%      25.559us      12.779us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                      aten::bitwise_not         0.02%      24.470us         0.02%      36.097us      18.049us       2.944us         0.00%       2.944us       1.472us             2  \n",
      "                                      aten::bitwise_and         0.02%      23.532us         0.02%      37.000us      18.500us       3.905us         0.00%       3.905us       1.953us             2  \n",
      "                                  cudaDeviceSynchronize         0.01%      20.389us         0.01%      20.389us      20.389us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                        aten::embedding         0.01%      19.730us         0.07%      97.074us      48.537us       0.000us         0.00%       6.976us       3.488us             2  \n",
      "                                               aten::lt         0.01%      18.636us         0.02%      24.381us      24.381us       1.472us         0.00%       1.472us       1.472us             1  \n",
      "                                         aten::new_ones         0.01%      13.421us         0.04%      60.203us      30.101us       0.000us         0.00%       2.912us       1.456us             2  \n",
      "                                       aten::is_nonzero         0.01%      12.915us         0.12%     179.137us      22.392us       0.000us         0.00%      13.441us       1.680us             8  \n",
      "                                             aten::full         0.01%      12.904us         0.04%      66.659us      16.665us       0.000us         0.00%       4.929us       1.232us             4  \n",
      "                                             aten::rsub         0.01%      12.673us         0.03%      46.539us      23.269us       0.000us         0.00%       3.168us       1.584us             2  \n",
      "                                             aten::item         0.01%      12.565us         0.11%     166.222us      20.778us       0.000us         0.00%      13.441us       1.680us             8  \n",
      "                                          aten::resize_         0.01%      10.809us         0.01%      10.809us       5.405us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                           aten::__or__         0.00%       6.724us         0.05%      76.637us      19.159us       0.000us         0.00%       6.464us       1.616us             4  \n",
      "                                          aten::view_as         0.00%       4.789us         0.00%       7.190us       1.438us       0.000us         0.00%       0.000us       0.000us             5  \n",
      "                                             aten::ones         0.00%       4.607us         0.01%      18.811us      18.811us       0.000us         0.00%       1.407us       1.407us             1  \n",
      "                                          aten::__and__         0.00%       4.549us         0.03%      41.549us      20.774us       0.000us         0.00%       3.905us       1.953us             2  \n",
      "                                        aten::new_empty         0.00%       3.993us         0.01%      12.988us       6.494us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                          aten::detach_         0.00%       3.835us         0.00%       5.476us       1.825us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                 cudaDeviceGetAttribute         0.00%       3.497us         0.00%       3.497us       1.166us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                        aten::ones_like         0.00%       3.100us         0.01%      14.133us      14.133us       0.000us         0.00%       1.408us       1.408us             1  \n",
      "                                                detach_         0.00%       1.641us         0.00%       1.641us       0.547us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                    cudaPeekAtLastError         0.00%       1.346us         0.00%       1.346us       0.112us       0.000us         0.00%       0.000us       0.000us            12  \n",
      "                                       aten::lift_fresh         0.00%       0.445us         0.00%       0.445us       0.148us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       4.736us         0.00%       4.736us       0.947us             5  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       9.281us         0.01%       9.281us       1.326us             7  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.200us         0.00%       3.200us       1.600us             2  \n",
      "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      21.025us         0.01%      21.025us       1.502us            14  \n",
      "                         Memcpy DtoH (Device -> Pinned)         0.00%       0.000us         0.00%       0.000us       0.000us      13.441us         0.01%      13.441us       1.680us             8  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.912us         0.00%       2.912us       1.456us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.727us         0.00%       5.727us       1.432us             4  \n",
      "void at_cuda_detail::cub::DeviceScanInitKernel<at_cu...         0.00%       0.000us         0.00%       0.000us       0.000us       4.448us         0.00%       4.448us       1.483us             3  \n",
      "void at_cuda_detail::cub::DeviceScanKernel<at_cuda_d...         0.00%       0.000us         0.00%       0.000us       0.000us       6.497us         0.00%       6.497us       2.166us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       6.430us         0.00%       6.430us       1.607us             4  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       1.567us         0.00%       1.567us       1.567us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       9.825us         0.01%       9.825us       1.637us             6  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.041us         0.00%       3.041us       1.520us             2  \n",
      "void at::native::(anonymous namespace)::indexSelectL...         0.00%       0.000us         0.00%       0.000us       0.000us       4.000us         0.00%       4.000us       4.000us             1  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       7.552us         0.00%       7.552us       3.776us             2  \n",
      "                         Memcpy HtoD (Pinned -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     137.915ms        82.20%     137.915ms     370.741us           372  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     748.938us         0.45%     748.938us       5.674us           132  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     622.319us         0.37%     622.319us       4.787us           130  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       1.424ms         0.85%       1.424ms      10.956us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     623.281us         0.37%     623.281us       4.794us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     587.121us         0.35%     587.121us       4.516us           130  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     754.735us         0.45%     754.735us       5.806us           130  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     790.289us         0.47%     790.289us       6.079us           130  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       1.999ms         1.19%       1.999ms       6.227us           321  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_...         0.00%       0.000us         0.00%       0.000us       0.000us       1.378ms         0.82%       1.378ms      21.523us            64  \n",
      "void splitKreduce_kernel<32, 16, int, float, __half,...         0.00%       0.000us         0.00%       0.000us       0.000us     759.917us         0.45%     759.917us       5.937us           128  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_...         0.00%       0.000us         0.00%       0.000us       0.000us     682.061us         0.41%     682.061us      10.657us            64  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     852.939us         0.51%     852.939us       6.664us           128  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     789.520us         0.47%     789.520us       6.168us           128  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us       1.596ms         0.95%       1.596ms       8.401us           190  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     440.645us         0.26%     440.645us       6.885us            64  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       1.632ms         0.97%       1.632ms      12.750us           128  \n",
      "void pytorch_flash::flash_fwd_kernel<pytorch_flash::...         0.00%       0.000us         0.00%       0.000us       0.000us      13.280us         0.01%      13.280us      13.280us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.294ms         0.77%       1.294ms       5.055us           256  \n",
      "void cutlass::Kernel<cutlass_75_wmma_tensorop_f16_s1...         0.00%       0.000us         0.00%       0.000us       0.000us       3.937ms         2.35%       3.937ms      20.503us           192  \n",
      "void splitKreduce_kernel<32, 16, int, __half, __half...         0.00%       0.000us         0.00%       0.000us       0.000us     755.536us         0.45%     755.536us       5.903us           128  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     670.479us         0.40%     670.479us       5.238us           128  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     934.575us         0.56%     934.575us       4.842us           193  \n",
      "fmha_cutlassF_f16_aligned_64x128_rf_sm80(PyTorchMemE...         0.00%       0.000us         0.00%       0.000us       0.000us     322.725us         0.19%     322.725us      10.410us            31  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us       2.048us         0.00%       2.048us       1.024us             2  \n",
      "sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x...         0.00%       0.000us         0.00%       0.000us       0.000us     191.107us         0.11%     191.107us      95.554us             2  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us      13.281us         0.01%      13.281us       3.320us             4  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       1.984us         0.00%       1.984us       1.984us             1  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      23.424us         0.01%      23.424us      11.712us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       7.232us         0.00%       7.232us       1.808us             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.168us         0.00%       3.168us       1.584us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.488us         0.00%       3.488us       1.744us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       6.464us         0.00%       6.464us       1.616us             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.944us         0.00%       2.944us       1.472us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       3.905us         0.00%       3.905us       1.953us             2  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       7.679us         0.00%       7.679us       3.840us             2  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.656us         0.00%       2.656us       2.656us             1  \n",
      "void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us       2.976us         0.00%       2.976us       2.976us             1  \n",
      "sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x...         0.00%       0.000us         0.00%       0.000us       0.000us       1.516ms         0.90%       1.516ms      23.694us            64  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       2.182ms         1.30%       2.182ms      11.366us           192  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     702.183us         0.42%     702.183us      10.972us            64  \n",
      "void pytorch_flash::flash_fwd_splitkv_kernel<pytorch...         0.00%       0.000us         0.00%       0.000us       0.000us     459.143us         0.27%     459.143us      14.348us            32  \n",
      "void pytorch_flash::flash_fwd_splitkv_combine_kernel...         0.00%       0.000us         0.00%       0.000us       0.000us     205.477us         0.12%     205.477us       6.421us            32  \n",
      "void gemv2T_kernel_val<int, int, __half, __half, __h...         0.00%       0.000us         0.00%       0.000us       0.000us     814.703us         0.49%     814.703us      12.730us            64  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       1.665us         0.00%       1.665us       1.665us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 148.979ms\n",
      "Self CUDA time total: 167.781ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 32\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 2\n",
    "test_samples = 4\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "fineweb = load_dataset(\"parquet\",data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text'][:test_samples] \n",
    "\n",
    "print(\"output length is {}\".format(output_length))\n",
    "text = fineweb_text[0]\n",
    "inputs = preprocess_data(text, tokenizer)\n",
    "\n",
    "# cached_mlp.clear_load_from_cpu_stats()\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ]\n",
    ") as p:\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(),\n",
    "            max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "print(p.key_averages().table(\n",
    "    sort_by=\"self_cpu_time_total\", row_limit=-1))\n",
    "p.export_chrome_trace(\"./trace-offloading.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载到GPU上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:25<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MixtralForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "import json\n",
    "\n",
    "def get_model(model_name, device_map, dtype=torch.bfloat16):\n",
    "    llm = MixtralForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        use_cache=True,\n",
    "        torch_dtype=dtype,\n",
    "    ) \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return llm, tokenizer\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "with open('../quantize/device_map_1.json', 'r') as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "dtype = torch.float16\n",
    "llm, tokenizer = get_model(model_name, device_map, dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 只传一个专家的版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import threading\n",
    "import json\n",
    "from queue import Queue\n",
    "\n",
    "class CachedMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, dtype, sparsity: float = 0.2):\n",
    "        super(CachedMLP, self).__init__()\n",
    "        self.sparsity = sparsity\n",
    "        self.activenum = int((1 - sparsity) * hidden_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dtype = dtype\n",
    "\n",
    "        # GPU 缓存张量\n",
    "        self.w1_gpu = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda')\n",
    "        self.w2_gpu = torch.empty((self.input_dim, self.activenum), dtype=self.dtype, device='cuda')\n",
    "        self.w3_gpu = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda')\n",
    "\n",
    "        # Pinned Memory 缓冲区\n",
    "        self.register_buffer('sparse_w1_cpu', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "        self.register_buffer('sparse_w2_cpu', torch.empty((self.input_dim, self.activenum), dtype=self.dtype, device='cpu'))\n",
    "        self.register_buffer('sparse_w3_cpu', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "        self.sparse_w1_cpu = self.sparse_w1_cpu.pin_memory()\n",
    "        self.sparse_w2_cpu = self.sparse_w2_cpu.pin_memory()\n",
    "        self.sparse_w3_cpu = self.sparse_w3_cpu.pin_memory()\n",
    "\n",
    "        # 统计信息\n",
    "        self.load_from_cpu_time = 0.0\n",
    "        self.load_from_cpu_calls = 0\n",
    "\n",
    "    def load_from_cpu(self, cpu_mlp, stream: torch.cuda.Stream):\n",
    "        \"\"\"\n",
    "        从CPU加载参数，并使用指定的CUDA流进行异步复制到GPU。\n",
    "        \n",
    "        参数:\n",
    "            cpu_mlp: 包含CPU上参数的字典。\n",
    "            stream: 用于数据传输的CUDA流。\n",
    "        \"\"\"\n",
    "        # 从CPU加载参数\n",
    "        self.sparse_w1_cpu.copy_(cpu_mlp['w1'].data[:self.activenum, :])\n",
    "        self.sparse_w2_cpu.copy_(cpu_mlp['w2'].data[:, :self.activenum])\n",
    "        self.sparse_w3_cpu.copy_(cpu_mlp['w3'].data[:self.activenum, :])\n",
    "\n",
    "        # 异步复制到GPU\n",
    "        with torch.cuda.stream(stream):\n",
    "            self.w1_gpu.copy_(self.sparse_w1_cpu, non_blocking=True)\n",
    "            self.w2_gpu.copy_(self.sparse_w2_cpu, non_blocking=True)\n",
    "            self.w3_gpu.copy_(self.sparse_w3_cpu, non_blocking=True)\n",
    "\n",
    "    def get_load_from_cpu_stats(self):\n",
    "        if self.load_from_cpu_calls == 0:\n",
    "            return 0.0, 0.0\n",
    "        avg_time = self.load_from_cpu_time / self.load_from_cpu_calls\n",
    "        return self.load_from_cpu_time, avg_time\n",
    "\n",
    "    def clear_load_from_cpu_stats(self):\n",
    "        self.load_from_cpu_time = 0.0\n",
    "        self.load_from_cpu_calls = 0\n",
    "\n",
    "def convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.9):\n",
    "    ### 其他部分存放在GPU上\n",
    "    llm.model.embed_tokens.cuda()\n",
    "    for i in range(len(llm.model.layers)):\n",
    "        llm.model.layers[i].self_attn.cuda()\n",
    "        llm.model.layers[i].input_layernorm.cuda()\n",
    "        llm.model.layers[i].post_attention_layernorm.cuda()\n",
    "        llm.model.layers[i].block_sparse_moe.gate.cuda()\n",
    "    ### 第0层的专家存放在GPU上\n",
    "    for j in range(len(llm.model.layers[0].block_sparse_moe.experts)):\n",
    "        llm.model.layers[0].block_sparse_moe.experts[j].cuda()\n",
    "\n",
    "    llm.model.norm.cuda()\n",
    "    llm.lm_head.cuda()\n",
    "    \n",
    "    # 创建两个共享的CachedMLP实例\n",
    "    buffer0 = CachedMLP(\n",
    "        input_dim=llm.config.hidden_size,\n",
    "        hidden_dim=llm.config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "        sparsity=sparsity\n",
    "    )\n",
    "    buffer1 = CachedMLP(\n",
    "        input_dim=llm.config.hidden_size,\n",
    "        hidden_dim=llm.config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "        sparsity=sparsity\n",
    "    )\n",
    "    cached_mlps = [buffer0, buffer1]\n",
    "    \n",
    "    for i, layer in enumerate(llm.model.layers):\n",
    "        if i==0:\n",
    "            continue\n",
    "        # 将专家的forward方法替换为PipelineLLM管理的方式\n",
    "        for j, expert in enumerate(layer.block_sparse_moe.experts):\n",
    "            expert.cpu_mlp = {\n",
    "                \"w1\": expert.w1.cpu().weight,\n",
    "                \"w2\": expert.w2.cpu().weight,\n",
    "                \"w3\": expert.w3.cpu().weight,\n",
    "            }\n",
    "            # 替换forward方法为直接调用CachedMLP的forward（需要在pipelineLLM里面替换)\n",
    "            # expert.forward = lambda x, cached_mlp=cached_mlp, cpu_mlp=expert.cpu_mlp: cached_mlp(x, cpu_mlp)\n",
    "    return llm, cached_mlps\n",
    "\n",
    "class PipelineLLM:\n",
    "    def __init__(self, llm, cached_mlps):\n",
    "        \"\"\"\n",
    "        初始化 PipelineLLM，替换模型每一层的 forward 方法。\n",
    "        \n",
    "        参数:\n",
    "            llm: 原始的大模型\n",
    "            cached_mlps: 两个 CachedMLP 实例列表\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.cached_mlps = cached_mlps  # [buffer0, buffer1]\n",
    "        self.num_layers = len(llm.model.layers)\n",
    "        self.lock = threading.Lock()\n",
    "        self.use_buffer0 = True  # 标记当前使用哪个缓冲区\n",
    "\n",
    "        # 创建两个共享的CUDA流\n",
    "        self.stream0 = torch.cuda.Stream()\n",
    "        self.stream1 = torch.cuda.Stream()\n",
    "\n",
    "        # 初始化加载第一个和第二个层的参数\n",
    "        self._load_layer(1, buffer_index=0)\n",
    "        self._load_layer(1, buffer_index=1)\n",
    "\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "        self._replace_forward_methods()\n",
    "    \n",
    "    def _load_layer(self, layer_idx, buffer_index):\n",
    "        \"\"\"\n",
    "        加载指定层的参数到指定的缓冲区。\n",
    "        \n",
    "        参数:\n",
    "            layer_idx: 层的索引\n",
    "            buffer_index: 缓冲区的索引（0 或 1）\n",
    "        \"\"\"\n",
    "        layer = self.llm.model.layers[layer_idx]\n",
    "        expert = layer.block_sparse_moe.experts[0]\n",
    "        cpu_mlp = expert.cpu_mlp\n",
    "        buffer = self.cached_mlps[buffer_index]\n",
    "        stream = self.stream0 if buffer_index == 0 else self.stream1\n",
    "\n",
    "        # 异步加载参数\n",
    "        buffer.load_from_cpu(cpu_mlp, stream)\n",
    "\n",
    "    def _replace_forward_methods(self):\n",
    "        \"\"\"\n",
    "        替换模型每一层的 forward 方法，添加参数预加载逻辑和注意力计算。\n",
    "        \"\"\"\n",
    "        for i, layer in enumerate(self.llm.model.layers):\n",
    "            def new_forward(hidden_states: torch.Tensor,\n",
    "                            attention_mask: Optional[torch.Tensor] = None,\n",
    "                            position_ids: Optional[torch.LongTensor] = None,\n",
    "                            past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "                            output_attentions: Optional[bool] = False,\n",
    "                            output_router_logits: Optional[bool] = False,\n",
    "                            use_cache: Optional[bool] = False,\n",
    "                            cache_position: Optional[torch.LongTensor] = None,\n",
    "                            layer_idx=i):\n",
    "                # print(f\"in layer {layer_idx}\")\n",
    "                with self.lock:\n",
    "                    # 选择当前使用的缓冲区\n",
    "                    current_buffer = self.cached_mlps[0] if self.use_buffer0 else self.cached_mlps[1]\n",
    "                    current_stream = self.stream0 if self.use_buffer0 else self.stream1\n",
    "\n",
    "                    # 切换缓冲区用于下一次\n",
    "                    next_buffer_index = 1 if self.use_buffer0 else 0\n",
    "                    next_buffer = self.cached_mlps[next_buffer_index]\n",
    "                    next_stream = self.stream1 if self.use_buffer0 else self.stream0\n",
    "\n",
    "                    # 预加载下一层的参数\n",
    "                    next_layer_idx = layer_idx + 1\n",
    "                    if next_layer_idx < self.num_layers:\n",
    "                        self._load_layer(next_layer_idx, buffer_index=next_buffer_index)\n",
    "                    \n",
    "                    # 切换缓冲区\n",
    "                    self.use_buffer0 = not self.use_buffer0\n",
    "\n",
    "                    # 处理当前层\n",
    "                    residual = hidden_states\n",
    "                    hidden_states = layer.input_layernorm(hidden_states)\n",
    "\n",
    "                    # Self Attention\n",
    "                    hidden_states, self_attn_weights, present_key_value = layer.self_attn(\n",
    "                        hidden_states=hidden_states,\n",
    "                        attention_mask=attention_mask,\n",
    "                        position_ids=position_ids,\n",
    "                        past_key_value=past_key_value,\n",
    "                        output_attentions=output_attentions,\n",
    "                        use_cache=use_cache,\n",
    "                        cache_position=cache_position,\n",
    "                    )\n",
    "                    hidden_states = residual + hidden_states\n",
    "\n",
    "                    # Fully Connected\n",
    "                    residual = hidden_states\n",
    "                    hidden_states = layer.post_attention_layernorm(hidden_states)\n",
    "\n",
    "                    # 使用当前缓冲区进行 MLP 计算\n",
    "                    batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "                    hidden_states = hidden_states.view(-1, hidden_dim)\n",
    "                    # 仅使用第一个专家\n",
    "                    expert_layer = layer.block_sparse_moe.experts[0]\n",
    "\n",
    "                    w3_output = torch.matmul(hidden_states, current_buffer.w3_gpu.T)\n",
    "                    w1_output = self.activation(torch.matmul(hidden_states, current_buffer.w1_gpu.T))\n",
    "                    w2 = current_buffer.w2_gpu.T\n",
    "                    final_hidden_states = torch.matmul(w1_output * w3_output, w2)\n",
    "\n",
    "                    final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n",
    "\n",
    "                    hidden_states = residual + final_hidden_states\n",
    "\n",
    "                    outputs = (hidden_states,)\n",
    "\n",
    "                    if output_attentions:\n",
    "                        outputs += (self_attn_weights,)\n",
    "\n",
    "                    if use_cache:\n",
    "                        outputs += (present_key_value,)\n",
    "\n",
    "                    return outputs\n",
    "            # 替换 forward 方法\n",
    "            layer.forward = new_forward\n",
    "\n",
    "    def _async_load(self, layer_idx, buffer_index):\n",
    "        \"\"\"\n",
    "        异步加载 MLP 参数到指定缓冲区，使用共享的CUDA流。\n",
    "        \"\"\"\n",
    "        self._load_layer(layer_idx, buffer_index)\n",
    "\n",
    "# 将模型转换为使用CachedMLP的版本\n",
    "llm, cached_mlps = convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.9)\n",
    "\n",
    "# 创建流水线模型\n",
    "pipeline_llm = PipelineLLM(llm, cached_mlps).llm\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
