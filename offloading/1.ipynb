{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先都加载到cpu上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:05<00:00,  3.31it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "from modeling_mixtral import MixtralForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import json\n",
    "\n",
    "def get_model(model_name, device_map, dtype=torch.bfloat16):\n",
    "    llm = MixtralForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        use_cache=True,\n",
    "        torch_dtype=dtype,\n",
    "    ) \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return llm, tokenizer\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "dtype = torch.float16\n",
    "llm, tokenizer = get_model(model_name, 'cpu', dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import threading\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "from queue import Queue\n",
    "\n",
    "class CachedMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, dtype, sparsity: float = 0.2):\n",
    "        super(CachedMLP, self).__init__()\n",
    "        self.sparsity = sparsity\n",
    "        self.activenum = int((1 - sparsity) * hidden_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.activation = nn.SiLU()\n",
    "\n",
    "        # GPU 缓存张量\n",
    "        self.w1_gpu = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda')\n",
    "        self.w2_gpu = torch.empty((self.input_dim, self.activenum), dtype=self.dtype, device='cuda')\n",
    "        self.w3_gpu = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda')\n",
    "\n",
    "        # 第二个专家的 GPU 缓存张量\n",
    "        self.w1_gpu_expert1 = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda')\n",
    "        self.w2_gpu_expert1 = torch.empty((self.input_dim, self.activenum), dtype=self.dtype, device='cuda')\n",
    "        self.w3_gpu_expert1 = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda')\n",
    "\n",
    "        # Pinned Memory 缓冲区\n",
    "        self.register_buffer('sparse_w1_cpu', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "        self.register_buffer('sparse_w2_cpu', torch.empty((self.input_dim, self.activenum), dtype=self.dtype, device='cpu'))\n",
    "        self.register_buffer('sparse_w3_cpu', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "        self.sparse_w1_cpu = self.sparse_w1_cpu.pin_memory()\n",
    "        self.sparse_w2_cpu = self.sparse_w2_cpu.pin_memory()\n",
    "        self.sparse_w3_cpu = self.sparse_w3_cpu.pin_memory()\n",
    "\n",
    "        # 第二个专家的 Pinned Memory 缓冲区\n",
    "        self.register_buffer('sparse_w1_cpu_expert1', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "        self.register_buffer('sparse_w2_cpu_expert1', torch.empty((self.input_dim, self.activenum), dtype=self.dtype, device='cpu'))\n",
    "        self.register_buffer('sparse_w3_cpu_expert1', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "        self.sparse_w1_cpu_expert1 = self.sparse_w1_cpu_expert1.pin_memory()\n",
    "        self.sparse_w2_cpu_expert1 = self.sparse_w2_cpu_expert1.pin_memory()\n",
    "        self.sparse_w3_cpu_expert1 = self.sparse_w3_cpu_expert1.pin_memory()\n",
    "\n",
    "        self.expert0_weight = torch.tensor(0)\n",
    "        self.expert1_weight = torch.tensor(0)\n",
    "\n",
    "        # 统计信息\n",
    "        self.load_from_cpu_time = 0.0\n",
    "        self.load_from_cpu_calls = 0\n",
    "\n",
    "    def load_expert_weights(self, expert_weights):\n",
    "        self.expert0_weight = expert_weights[0]\n",
    "        self.expert1_weight = expert_weights[1]\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        \"\"\"\n",
    "        根据hidden_states， 分别计算两个专家的输出\n",
    "        \"\"\"\n",
    "        # 第一个专家的计算\n",
    "        w3_output = torch.matmul(hidden_states, self.w3_gpu.T)\n",
    "        w1_output = self.activation(torch.matmul(hidden_states, self.w1_gpu.T))\n",
    "        w2 = self.w2_gpu.T\n",
    "        hidden_states_expert0 = torch.matmul(w1_output * w3_output, w2)\n",
    "\n",
    "        # 第二个专家的计算\n",
    "        w3_output_expert1 = torch.matmul(hidden_states, self.w3_gpu_expert1.T)\n",
    "        w1_output_expert1 = self.activation(torch.matmul(hidden_states, self.w1_gpu_expert1.T))\n",
    "        w2_expert1 = self.w2_gpu_expert1.T\n",
    "        hidden_states_expert1 = torch.matmul(w1_output_expert1 * w3_output_expert1, w2_expert1)\n",
    "\n",
    "        final_hidden_states = hidden_states_expert0* self.expert0_weight + hidden_states_expert1* self.expert1_weight\n",
    "        \n",
    "        return final_hidden_states\n",
    "                        \n",
    "\n",
    "    def load_from_cpu(self, cpu_mlp, cpu_mlp_expert1, stream: torch.cuda.Stream):\n",
    "        \"\"\"\n",
    "        从CPU加载参数，并使用指定的CUDA流进行异步复制到GPU。\n",
    "        \n",
    "        参数:\n",
    "            cpu_mlp: 包含CPU上参数的字典（第一个专家）。\n",
    "            cpu_mlp_expert1: 包含CPU上参数的字典（第二个专家）。\n",
    "            stream: 用于数据传输的CUDA流。\n",
    "        \"\"\"\n",
    "        # 从CPU加载参数（第一个专家）\n",
    "        self.sparse_w1_cpu.copy_(cpu_mlp['w1'].data[:self.activenum, :])\n",
    "        self.sparse_w2_cpu.copy_(cpu_mlp['w2'].data[:, :self.activenum])\n",
    "        self.sparse_w3_cpu.copy_(cpu_mlp['w3'].data[:self.activenum, :])\n",
    "\n",
    "        # 从CPU加载参数（第二个专家）\n",
    "        self.sparse_w1_cpu_expert1.copy_(cpu_mlp_expert1['w1'].data[:self.activenum, :])\n",
    "        self.sparse_w2_cpu_expert1.copy_(cpu_mlp_expert1['w2'].data[:, :self.activenum])\n",
    "        self.sparse_w3_cpu_expert1.copy_(cpu_mlp_expert1['w3'].data[:self.activenum, :])\n",
    "\n",
    "        # 异步复制到GPU\n",
    "        with torch.cuda.stream(stream):\n",
    "            self.w1_gpu.copy_(self.sparse_w1_cpu, non_blocking=True)\n",
    "            self.w2_gpu.copy_(self.sparse_w2_cpu, non_blocking=True)\n",
    "            self.w3_gpu.copy_(self.sparse_w3_cpu, non_blocking=True)\n",
    "\n",
    "            # 第二个专家的异步复制\n",
    "            self.w1_gpu_expert1.copy_(self.sparse_w1_cpu_expert1, non_blocking=True)\n",
    "            self.w2_gpu_expert1.copy_(self.sparse_w2_cpu_expert1, non_blocking=True)\n",
    "            self.w3_gpu_expert1.copy_(self.sparse_w3_cpu_expert1, non_blocking=True)\n",
    "\n",
    "def convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.9):\n",
    "    ### 其他部分存放在GPU上\n",
    "    llm.model.embed_tokens.cuda()\n",
    "    for i in range(len(llm.model.layers)):\n",
    "        llm.model.layers[i].self_attn.cuda()\n",
    "        llm.model.layers[i].input_layernorm.cuda()\n",
    "        llm.model.layers[i].post_attention_layernorm.cuda()\n",
    "        llm.model.layers[i].block_sparse_moe.gate.cuda()\n",
    "    ### 第0层的专家存放在GPU上\n",
    "    for j in range(len(llm.model.layers[0].block_sparse_moe.experts)):\n",
    "        llm.model.layers[0].block_sparse_moe.experts[j].cuda()\n",
    "\n",
    "    llm.model.norm.cuda()\n",
    "    llm.lm_head.cuda()\n",
    "    \n",
    "    # 创建两个共享的CachedMLP实例\n",
    "    buffer0 = CachedMLP(\n",
    "        input_dim=llm.config.hidden_size,\n",
    "        hidden_dim=llm.config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "        sparsity=sparsity\n",
    "    )\n",
    "    buffer1 = CachedMLP(\n",
    "        input_dim=llm.config.hidden_size,\n",
    "        hidden_dim=llm.config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "        sparsity=sparsity\n",
    "    )\n",
    "    cached_mlps = [buffer0, buffer1]\n",
    "    \n",
    "    for i, layer in enumerate(llm.model.layers):\n",
    "        if i==0:\n",
    "            continue\n",
    "        # 将专家的forward方法替换为PipelineLLM管理的方式\n",
    "        for j, expert in enumerate(layer.block_sparse_moe.experts):\n",
    "            expert.cpu_mlp = {\n",
    "                \"w1\": expert.w1.cpu().weight,\n",
    "                \"w2\": expert.w2.cpu().weight,\n",
    "                \"w3\": expert.w3.cpu().weight,\n",
    "            }\n",
    "    return llm, cached_mlps\n",
    "\n",
    "class PipelineLLM:\n",
    "    def __init__(self, llm, cached_mlps):\n",
    "        \"\"\"\n",
    "        初始化 PipelineLLM，替换模型每一层的 forward 方法。\n",
    "        \n",
    "        参数:\n",
    "            llm: 原始的大模型\n",
    "            cached_mlps: 两个 CachedMLP 实例列表\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.cached_mlps = cached_mlps  # [buffer0, buffer1]\n",
    "        self.num_layers = len(llm.model.layers)\n",
    "        self.lock = threading.Lock()\n",
    "        self.use_buffer0 = True  # 标记当前使用哪个缓冲区\n",
    "\n",
    "        # 创建两个共享的CUDA流\n",
    "        self.stream0 = torch.cuda.Stream()\n",
    "        self.stream1 = torch.cuda.Stream()\n",
    "\n",
    "        # 初始化加载第一个和第二个层的参数\n",
    "        self._load_layer(1, buffer_index=0, expert_ids=torch.tensor([0, 1]))\n",
    "        self._load_layer(1, buffer_index=1, expert_ids=torch.tensor([0, 1]))\n",
    "        self.top_k = 2\n",
    "        self.activation = nn.SiLU()\n",
    "\n",
    "        self._replace_forward_methods()\n",
    "\n",
    "        # 用于统计时间的变量\n",
    "        self.total_prefill_time = 0.0\n",
    "        self.total_decode_time = 0.0\n",
    "\n",
    "    def _load_layer(self, layer_idx, buffer_index, expert_ids, expert_weights=torch.tensor([0, 0])):\n",
    "        \"\"\"\n",
    "        加载指定层的参数到指定的缓冲区。\n",
    "        \n",
    "        参数:\n",
    "            layer_idx: 层的索引\n",
    "            buffer_index: 缓冲区的索引（0 或 1）\n",
    "        \"\"\"\n",
    "        layer = self.llm.model.layers[layer_idx]\n",
    "        expert0 = layer.block_sparse_moe.experts[expert_ids[0]]\n",
    "        expert1 = layer.block_sparse_moe.experts[expert_ids[1]]\n",
    "        # if layer_idx == 1:\n",
    "        #     print(expert_ids[0].data, expert_ids[1].data, '{:.3f}, {:.3f}'.format(expert_weights[0], expert_weights[1]))\n",
    "\n",
    "        cpu_mlp = expert0.cpu_mlp\n",
    "        cpu_mlp_expert1 = expert1.cpu_mlp\n",
    "        buffer = self.cached_mlps[buffer_index]\n",
    "        stream = self.stream0 if buffer_index == 0 else self.stream1\n",
    "\n",
    "        buffer.load_expert_weights(expert_weights)\n",
    "        # 异步加载参数\n",
    "        buffer.load_from_cpu(cpu_mlp, cpu_mlp_expert1, stream)\n",
    "\n",
    "    def _replace_forward_methods(self):\n",
    "        \"\"\"\n",
    "        替换模型每一层的 forward 方法，添加参数预加载逻辑和注意力计算。\n",
    "        \"\"\"\n",
    "        for i, layer in enumerate(self.llm.model.layers):\n",
    "            def new_forward(hidden_states: torch.Tensor,\n",
    "                        attention_mask: Optional[torch.Tensor] = None,\n",
    "                        position_ids: Optional[torch.LongTensor] = None,\n",
    "                        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "                        output_attentions: Optional[bool] = False,\n",
    "                        output_router_logits: Optional[bool] = False,\n",
    "                        use_cache: Optional[bool] = False,\n",
    "                        cache_position: Optional[torch.LongTensor] = None,\n",
    "                        layer=layer,\n",
    "                        layer_idx=i):\n",
    "                with self.lock:\n",
    "                    batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "                    \n",
    "                    if sequence_length == 1:\n",
    "                        #### decode phase ####\n",
    "                        # 选择当前使用的缓冲区\n",
    "                        current_buffer = self.cached_mlps[0] if self.use_buffer0 else self.cached_mlps[1]\n",
    "\n",
    "                        next_buffer_index = 1 if self.use_buffer0 else 0\n",
    "\n",
    "                        next_layer_idx = layer_idx + 1\n",
    "\n",
    "                        if next_layer_idx < self.num_layers:\n",
    "                            # 预加载下一层的参数\n",
    "                            next_layer = self.llm.model.layers[next_layer_idx]\n",
    "                            router = next_layer.block_sparse_moe.gate\n",
    "\n",
    "                            # batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "                            hidden_states_flat = hidden_states.view(-1, hidden_dim)\n",
    "                            # router_logits: (batch * sequence_length, n_experts)\n",
    "                            router_logits = router(hidden_states_flat)\n",
    "\n",
    "                            routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "                            routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n",
    "                            routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n",
    "\n",
    "                            hidden_states = hidden_states_flat.reshape(batch_size, sequence_length, hidden_dim)\n",
    "\n",
    "                            self._load_layer(\n",
    "                                next_layer_idx,\n",
    "                                buffer_index=next_buffer_index,\n",
    "                                expert_ids=selected_experts[0],\n",
    "                                expert_weights=routing_weights[0]\n",
    "                            )\n",
    "\n",
    "                        # 切换缓冲区\n",
    "                        self.use_buffer0 = not self.use_buffer0\n",
    "\n",
    "                    # 处理当前层\n",
    "                    residual = hidden_states\n",
    "                    hidden_states = layer.input_layernorm(hidden_states)\n",
    "\n",
    "                    # Self Attention\n",
    "                    hidden_states, self_attn_weights, present_key_value = layer.self_attn(\n",
    "                        hidden_states=hidden_states,\n",
    "                        attention_mask=attention_mask,\n",
    "                        position_ids=position_ids,\n",
    "                        past_key_value=past_key_value,\n",
    "                        output_attentions=output_attentions,\n",
    "                        use_cache=use_cache,\n",
    "                        cache_position=cache_position,\n",
    "                    )\n",
    "                    hidden_states = residual + hidden_states\n",
    "\n",
    "                    # Fully Connected\n",
    "                    residual = hidden_states\n",
    "                    hidden_states = layer.post_attention_layernorm(hidden_states)\n",
    "\n",
    "                    if sequence_length > 1:\n",
    "                        # print(\"in prefill layer \", layer_idx)\n",
    "                        # 对于prefill阶段，仅将experts加载到GPU计算\n",
    "                        experts = layer.block_sparse_moe.experts\n",
    "\n",
    "                        # 将experts移动到GPU\n",
    "                        for expert in experts:\n",
    "                            expert.to('cuda')\n",
    "\n",
    "                        # 在GPU上进行MoE计算（gate保持在CPU）\n",
    "                        final_hidden_states, router_logits = layer.block_sparse_moe(hidden_states)\n",
    "\n",
    "                        # 计算完成后将experts移回CPU\n",
    "                        if layer_idx != 0:\n",
    "                            for expert in experts:\n",
    "                                expert.to('cpu')\n",
    "                    else:\n",
    "                        # batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "                        hidden_states_flat = hidden_states.view(-1, hidden_dim)\n",
    "                        # print(\"in decode layer\", layer_idx)\n",
    "                        if layer_idx > 0:\n",
    "                            ### 使用当前缓冲区进行 MLP 计算 ###\n",
    "                            final_hidden_states = current_buffer(hidden_states_flat)\n",
    "                        else:\n",
    "                            ### 根据router计算需要使用的专家 ###\n",
    "                            cur_layer = layer\n",
    "                            router = cur_layer.block_sparse_moe.gate\n",
    "                            # router_logits: (batch * sequence_length, n_experts)\n",
    "                            router_logits = router(hidden_states_flat)\n",
    "\n",
    "                            routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "                            routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n",
    "                            routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n",
    "                            # we cast back to the input dtype\n",
    "                            routing_weights = routing_weights.to(hidden_states_flat.dtype)\n",
    "\n",
    "                            first_expert, second_expert = selected_experts[0][0], selected_experts[0][1]\n",
    "\n",
    "                            final_hidden_states_expert0 = cur_layer.block_sparse_moe.experts[first_expert](\n",
    "                                hidden_states_flat) * routing_weights[0][0]\n",
    "\n",
    "                            final_hidden_states_expert1 = cur_layer.block_sparse_moe.experts[second_expert](\n",
    "                                hidden_states_flat) * routing_weights[0][1]\n",
    "\n",
    "                            # 将两个专家的结果相加\n",
    "                            final_hidden_states = final_hidden_states_expert0 + final_hidden_states_expert1\n",
    "\n",
    "                        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n",
    "\n",
    "                    hidden_states = residual + final_hidden_states\n",
    "\n",
    "                    outputs = (hidden_states,)\n",
    "\n",
    "                    if output_attentions:\n",
    "                        outputs += (self_attn_weights,)\n",
    "\n",
    "                    if use_cache:\n",
    "                        outputs += (present_key_value,)\n",
    "\n",
    "                    return outputs\n",
    "\n",
    "            # 替换forward方法\n",
    "            layer.forward = new_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm, cached_mlps = convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.8)\n",
    "\n",
    "# 创建流水线模型\n",
    "PLLM = PipelineLLM(llm, cached_mlps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试时间开销"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 32\n",
      "Time taken: 3.0356 seconds\n",
      "Time taken: 2.9848 seconds\n",
      "Time taken: 2.9723 seconds\n",
      "Time taken: 3.0177 seconds\n",
      "decode time: 3.0026  s\n",
      "decode phase speed: 10.6574  token/s\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 1\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 32\n",
    "test_samples = 4\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "filt_type = fineweb_path.split('.')[-1]\n",
    "fineweb = load_dataset(filt_type, data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text'][:test_samples] \n",
    "\n",
    "prefill_time, decode_time = 0, 0\n",
    "for output_length in [output_length]:\n",
    "    print(\"output length is {}\".format(output_length))\n",
    "    for text in fineweb_text:\n",
    "        inputs = preprocess_data(text, tokenizer)\n",
    "\n",
    "        # 预热（避免第一次运行时的额外开销）\n",
    "        with torch.no_grad():\n",
    "            output = llm(input_ids=inputs[\"input_ids\"].cuda(), attention_mask=inputs[\"attention_mask\"].cuda())\n",
    "\n",
    "        # 测试时间\n",
    "        start_event = torch.cuda.Event(enable_timing=True)\n",
    "        end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        # 开始计时\n",
    "        torch.cuda.synchronize()\n",
    "        start_event.record()\n",
    "\n",
    "        # 前向传播\n",
    "        with torch.no_grad():\n",
    "            output = llm.generate(\n",
    "                input_ids=inputs[\"input_ids\"].cuda(),\n",
    "                attention_mask=inputs[\"attention_mask\"].cuda(),\n",
    "                max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "                generation_config=GenerationConfig(do_sample=False),\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        # 结束计时\n",
    "        end_event.record()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        # 计算时间\n",
    "        elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "        decode_time += elapsed_time\n",
    "        # print(f\"Generated output length: {len(output[0]) - input_length}\")\n",
    "        # print(output)\n",
    "        # print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "        print(f\"Time taken: {elapsed_time:.4f} seconds\")\n",
    "\n",
    "timepertoken = (decode_time) / (output_length) / test_samples\n",
    "print(\"decode time:\", '{:.4f}'.format((decode_time) /test_samples), ' s')\n",
    "print(\"decode phase speed:\", '{:.4f}'.format(1/timepertoken) , ' token/s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:24<00:00,  1.28s/it]\n"
     ]
    }
   ],
   "source": [
    "#### GPU版本\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "from modeling_mixtral import MixtralForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import json\n",
    "\n",
    "def get_model(model_name, device_map, dtype=torch.bfloat16):\n",
    "    llm = MixtralForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        use_cache=True,\n",
    "        torch_dtype=dtype,\n",
    "    ) \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return llm, tokenizer\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "\n",
    "with open('../quantize/device_map_1.json', 'r') as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "dtype = torch.float16\n",
    "llm, tokenizer = get_model(model_name, device_map, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 1\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 2\n",
    "test_samples = 2\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "filt_type = fineweb_path.split('.')[-1]\n",
    "fineweb = load_dataset(filt_type, data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text'][:test_samples] \n",
    "\n",
    "prefill_time, decode_time = 0, 0\n",
    "for output_length in [output_length]:\n",
    "    print(\"output length is {}\".format(output_length))\n",
    "    for text in fineweb_text:\n",
    "        inputs = preprocess_data(text, tokenizer)\n",
    "\n",
    "        # 预热（避免第一次运行时的额外开销）\n",
    "        # with torch.no_grad():\n",
    "        #     output = llm(input_ids=inputs[\"input_ids\"].cuda(), attention_mask=inputs[\"attention_mask\"].cuda())\n",
    "\n",
    "        # PLLM.clear_total_time()\n",
    "        # 测试时间\n",
    "        start_event = torch.cuda.Event(enable_timing=True)\n",
    "        end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        # 开始计时\n",
    "        torch.cuda.synchronize()\n",
    "        start_event.record()\n",
    "\n",
    "        # 前向传播\n",
    "        with torch.no_grad():\n",
    "            output = llm.generate(\n",
    "                input_ids=inputs[\"input_ids\"].cuda(),\n",
    "                attention_mask=inputs[\"attention_mask\"].cuda(),\n",
    "                max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "                generation_config=GenerationConfig(do_sample=False),\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        # 结束计时\n",
    "        end_event.record()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        # 计算时间\n",
    "        elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "        # print(f\"Generated output length: {len(output[0]) - input_length}\")\n",
    "        # print(output)\n",
    "        # print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "        print(f\"Time taken: {elapsed_time:.4f} seconds\")\n",
    "        # ptime, dtime = PLLM.get_total_time()\n",
    "        # prefill_time += ptime\n",
    "        # decode_time += dtime\n",
    "        # print(\"decode time:\", '{:.4f}'.format((decode_time) / test_samples), ' s')\n",
    "        # print(\"decode phase speed:\", '{:.4f}'.format(1/timepertoken) , ' token/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 2\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        cudaMemcpyAsync        99.13%       63.029s        99.13%       63.029s      31.187ms       0.000us         0.00%       0.000us       0.000us          2021  \n",
      "                                            aten::copy_         0.62%     392.586ms        99.78%       63.444s      24.802ms       62.290s        99.93%       62.290s      24.351ms          2558  \n",
      "                                       cudaLaunchKernel         0.04%      27.153ms         0.04%      27.153ms       4.852us       0.000us         0.00%       0.000us       0.000us          5596  \n",
      "                                  cudaStreamSynchronize         0.04%      26.318ms         0.04%      26.318ms      14.452us       0.000us         0.00%       0.000us       0.000us          1821  \n",
      "                                               aten::mm         0.03%      17.292ms         0.04%      25.415ms      19.825us      24.549ms         0.04%      24.549ms      19.149us          1282  \n",
      "                                    aten::empty_strided         0.02%      13.406ms         0.02%      14.154ms       7.630us       0.000us         0.00%       0.000us       0.000us          1855  \n",
      "                                         aten::_to_copy         0.01%       7.091ms        99.21%       63.081s      35.280ms       0.000us         0.00%       62.154s      34.762ms          1788  \n",
      "                                              aten::mul         0.01%       6.526ms         0.02%      10.393ms       8.960us       4.003ms         0.01%       4.003ms       3.450us          1160  \n",
      "                                          aten::nonzero         0.01%       4.724ms         0.03%      17.398ms      67.960us       2.111ms         0.00%       2.626ms      10.258us           256  \n",
      "                                            aten::index         0.01%       4.634ms         0.01%       7.067ms      11.026us       1.578ms         0.00%       1.578ms       2.462us           641  \n",
      "                                               aten::to         0.01%       4.470ms        99.22%       63.086s      26.960ms       0.000us         0.00%       62.154s      26.561ms          2340  \n",
      "                                              aten::add         0.01%       3.392ms         0.01%       5.938ms      14.072us       1.739ms         0.00%       1.739ms       4.121us           422  \n",
      "                                       aten::as_strided         0.00%       2.232ms         0.00%       2.232ms       0.375us       0.000us         0.00%       0.000us       0.000us          5955  \n",
      "                                            aten::slice         0.00%       2.182ms         0.00%       2.704ms       2.094us       0.000us         0.00%       0.000us       0.000us          1291  \n",
      "                                              aten::cat         0.00%       2.029ms         0.00%       3.039ms      15.504us       1.140ms         0.00%       1.140ms       5.815us           196  \n",
      "                                            aten::empty         0.00%       1.992ms         0.00%       1.992ms       2.759us       0.000us         0.00%       0.000us       0.000us           722  \n",
      "                                    cudaLaunchKernelExC         0.00%       1.756ms         0.00%       1.756ms       4.682us       0.000us         0.00%       0.000us       0.000us           375  \n",
      "                                             aten::mean         0.00%       1.676ms         0.00%       2.347ms      18.055us     885.018us         0.00%     885.018us       6.808us           130  \n",
      "                                             aten::silu         0.00%       1.591ms         0.00%       2.432ms       7.599us     714.474us         0.00%     714.474us       2.233us           320  \n",
      "                                                aten::t         0.00%       1.571ms         0.00%       3.034ms       2.244us       0.000us         0.00%       0.000us       0.000us          1352  \n",
      "                                           aten::matmul         0.00%       1.498ms         0.04%      27.563ms      21.500us       0.000us         0.00%      24.549ms      19.149us          1282  \n",
      "                                              aten::pow         0.00%       1.456ms         0.00%       2.250ms      17.308us     467.598us         0.00%     467.598us       3.597us           130  \n",
      "                                        aten::transpose         0.00%       1.421ms         0.00%       2.204ms       1.182us       0.000us         0.00%       0.000us       0.000us          1864  \n",
      "                                           aten::linear         0.00%       1.393ms         0.04%      28.424ms      25.934us       0.000us         0.00%      20.909ms      19.077us          1096  \n",
      "                                             aten::view         0.00%       1.348ms         0.00%       1.348ms       0.721us       0.000us         0.00%       0.000us       0.000us          1869  \n",
      "                                             aten::topk         0.00%       1.329ms         0.00%       2.646ms      41.351us       1.031ms         0.00%       1.031ms      16.104us            64  \n",
      "                                        cudaMemsetAsync         0.00%       1.258ms         0.00%       1.258ms       4.044us       0.000us         0.00%       0.000us       0.000us           311  \n",
      "                                       aten::index_add_         0.00%       1.198ms         0.00%       2.278ms       8.900us     540.881us         0.00%     540.881us       2.113us           256  \n",
      "                                           aten::select         0.00%       1.180ms         0.00%       1.496ms       1.544us       0.000us         0.00%       0.000us       0.000us           969  \n",
      "                                          aten::reshape         0.00%       1.164ms         0.01%       4.313ms       2.636us       0.000us         0.00%     579.667us       0.354us          1636  \n",
      "                                  cudaFuncGetAttributes         0.00%       1.011ms         0.00%       1.011ms       2.292us       0.000us         0.00%       0.000us       0.000us           441  \n",
      "                         aten::_flash_attention_forward         0.00%       1.004ms         0.00%       2.496ms      38.994us     626.320us         0.00%     626.320us       9.786us            64  \n",
      "                                              aten::neg         0.00%       1.001ms         0.00%       1.610ms      12.577us     586.886us         0.00%     586.886us       4.585us           128  \n",
      "                                            aten::rsqrt         0.00%     966.208us         0.00%       1.610ms      12.382us     457.671us         0.00%     457.671us       3.521us           130  \n",
      "                                        aten::unsqueeze         0.00%     949.024us         0.00%       1.214ms       1.506us       0.000us         0.00%       0.000us       0.000us           806  \n",
      "                                              aten::sum         0.00%     868.877us         0.00%       1.224ms      19.121us     391.752us         0.00%     391.752us       6.121us            64  \n",
      "                                             cudaMalloc         0.00%     744.910us         0.00%     744.910us     248.303us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                            aten::clone         0.00%     727.952us         0.01%       5.567ms      14.276us       0.000us         0.00%       1.110ms       2.847us           390  \n",
      "                                         aten::_softmax         0.00%     684.228us         0.00%       1.064ms      16.620us     248.453us         0.00%     248.453us       3.882us            64  \n",
      "                     aten::scaled_dot_product_attention         0.00%     643.913us         0.01%       3.843ms      60.050us       0.000us         0.00%     626.320us       9.786us            64  \n",
      "                aten::_has_compatible_shallow_copy_type         0.00%     626.207us         0.00%     626.207us       0.207us       0.000us         0.00%       0.000us       0.000us          3024  \n",
      "                                             aten::div_         0.00%     574.022us         0.00%     897.050us      14.016us     288.266us         0.00%     288.266us       4.504us            64  \n",
      "                                       aten::empty_like         0.00%     511.691us         0.00%       1.874ms       4.137us       0.000us         0.00%       0.000us       0.000us           453  \n",
      "                                   cudaFuncSetAttribute         0.00%     502.116us         0.00%     502.116us       0.458us       0.000us         0.00%       0.000us       0.000us          1097  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.00%     483.057us         0.00%     483.057us       0.677us       0.000us         0.00%       0.000us       0.000us           713  \n",
      "                                           aten::unbind         0.00%     436.641us         0.00%     858.567us       3.354us       0.000us         0.00%       0.000us       0.000us           256  \n",
      "              aten::_scaled_dot_product_flash_attention         0.00%     435.730us         0.01%       3.199ms      49.989us       0.000us         0.00%     626.320us       9.786us            64  \n",
      "                                          aten::permute         0.00%     432.548us         0.00%     530.404us       2.433us       0.000us         0.00%       0.000us       0.000us           218  \n",
      "                              aten::_local_scalar_dense         0.00%     391.192us         0.00%       2.874ms      39.921us     465.696us         0.00%     465.696us       6.468us            72  \n",
      "                                             aten::set_         0.00%     374.652us         0.00%     374.652us       1.463us       0.000us         0.00%       0.000us       0.000us           256  \n",
      "                                         aten::scatter_         0.00%     341.741us         0.00%     536.522us      16.766us      64.867us         0.00%      64.867us       2.027us            32  \n",
      "                                            aten::fill_         0.00%     311.826us         0.00%     721.283us       9.617us     129.633us         0.00%     129.633us       1.728us            75  \n",
      "                                    aten::nonzero_numpy         0.00%     310.656us         0.03%      18.567ms      72.527us       0.000us         0.00%       2.626ms      10.258us           256  \n",
      "                                     aten::_unsafe_view         0.00%     294.438us         0.00%     294.438us       0.763us       0.000us         0.00%       0.000us       0.000us           386  \n",
      "                                          aten::resize_         0.00%     270.554us         0.00%     270.554us       1.049us       0.000us         0.00%       0.000us       0.000us           258  \n",
      "                                           aten::expand         0.00%     265.950us         0.00%     324.936us       2.539us       0.000us         0.00%       0.000us       0.000us           128  \n",
      "                                            aten::where         0.00%     200.225us         0.03%      18.767ms      73.309us       0.000us         0.00%       2.626ms      10.258us           256  \n",
      "                                 cudaDeviceGetAttribute         0.00%     199.359us         0.00%     199.359us       0.387us       0.000us         0.00%       0.000us       0.000us           515  \n",
      "                                  cudaDeviceSynchronize         0.00%     185.934us         0.00%     185.934us     185.934us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                       aten::contiguous         0.00%     174.717us         0.01%       3.415ms      13.341us       0.000us         0.00%     515.112us       2.012us           256  \n",
      "                                            aten::zero_         0.00%     164.370us         0.00%     724.615us      11.322us       0.000us         0.00%      85.503us       1.336us            64  \n",
      "                                          aten::one_hot         0.00%     155.497us         0.00%       1.250ms      39.067us       0.000us         0.00%     105.441us       3.295us            32  \n",
      "                                    cudaPeekAtLastError         0.00%     146.970us         0.00%     146.970us       0.095us       0.000us         0.00%       0.000us       0.000us          1548  \n",
      "                                          aten::numpy_T         0.00%     142.588us         0.00%     583.475us       3.137us       0.000us         0.00%       0.000us       0.000us           186  \n",
      "                                            aten::zeros         0.00%     137.874us         0.00%       1.071ms      16.739us       0.000us         0.00%      85.503us       1.336us            64  \n",
      "                                          aten::softmax         0.00%     130.680us         0.00%       1.194ms      18.662us       0.000us         0.00%     248.453us       3.882us            64  \n",
      "                                               aten::eq         0.00%     108.566us         0.00%     183.136us      20.348us      27.907us         0.00%      27.907us       3.101us             9  \n",
      "                                             aten::item         0.00%      92.680us         0.00%       2.967ms      41.208us       0.000us         0.00%     465.696us       6.468us            72  \n",
      "                                             aten::isin         0.00%      81.747us         0.00%     384.113us     128.038us       0.000us         0.00%      47.522us      15.841us             3  \n",
      "                                  cudaStreamIsCapturing         0.00%      77.480us         0.00%      77.480us       1.156us       0.000us         0.00%       0.000us       0.000us            67  \n",
      "                                      aten::result_type         0.00%      71.068us         0.00%      71.068us       0.547us       0.000us         0.00%       0.000us       0.000us           130  \n",
      "                                           aten::cumsum         0.00%      69.196us         0.00%     135.663us      45.221us      10.848us         0.00%      10.848us       3.616us             3  \n",
      "                                              aten::sub         0.00%      57.653us         0.00%      91.494us      18.299us      14.242us         0.00%      14.242us       2.848us             5  \n",
      "                                         cudaEventQuery         0.00%      55.460us         0.00%      55.460us       2.311us       0.000us         0.00%       0.000us       0.000us            24  \n",
      "                                       aten::bitwise_or         0.00%      49.734us         0.00%      74.709us      18.677us      20.798us         0.00%      20.798us       5.200us             4  \n",
      "                                              aten::any         0.00%      49.479us         0.00%     121.903us      24.381us       0.000us         0.00%      13.664us       2.733us             5  \n",
      "                                              aten::all         0.00%      42.695us         0.00%      67.839us      33.920us       7.616us         0.00%       7.616us       3.808us             2  \n",
      "                                     aten::index_select         0.00%      42.648us         0.00%      70.834us      35.417us       6.881us         0.00%       6.881us       3.441us             2  \n",
      "                                           aten::argmax         0.00%      42.342us         0.00%      63.424us      31.712us      29.120us         0.00%      29.120us      14.560us             2  \n",
      "                                              aten::max         0.00%      42.098us         0.00%      74.715us      37.358us      15.520us         0.00%      15.520us       7.760us             2  \n",
      "                                     aten::masked_fill_         0.00%      33.670us         0.00%      50.103us      25.051us       3.135us         0.00%       3.135us       1.568us             2  \n",
      "                                               aten::ge         0.00%      28.105us         0.00%     185.469us      92.734us       3.008us         0.00%       3.008us       1.504us             2  \n",
      "                                      aten::bitwise_not         0.00%      27.635us         0.00%      40.735us      20.367us      10.752us         0.00%      10.752us       5.376us             2  \n",
      "                                        aten::embedding         0.00%      26.130us         0.00%     101.978us      50.989us       0.000us         0.00%       6.881us       3.441us             2  \n",
      "                                      aten::bitwise_and         0.00%      25.263us         0.00%      43.018us      21.509us      13.313us         0.00%      13.313us       6.656us             2  \n",
      "                                               aten::lt         0.00%      23.117us         0.00%      35.985us      35.985us       1.536us         0.00%       1.536us       1.536us             1  \n",
      "                                             aten::rsub         0.00%      18.687us         0.00%      58.296us      29.148us       0.000us         0.00%      10.240us       5.120us             2  \n",
      "                                       aten::is_nonzero         0.00%      16.403us         0.00%     294.871us      36.859us       0.000us         0.00%      20.095us       2.512us             8  \n",
      "                                         aten::new_ones         0.00%      14.663us         0.00%      58.750us      29.375us       0.000us         0.00%      10.432us       5.216us             2  \n",
      "                                             aten::full         0.00%      12.590us         0.00%      66.857us      16.714us       0.000us         0.00%      20.609us       5.152us             4  \n",
      "                                           aten::__or__         0.00%       8.507us         0.00%      83.216us      20.804us       0.000us         0.00%      20.798us       5.200us             4  \n",
      "                                          aten::__and__         0.00%       6.442us         0.00%      49.460us      24.730us       0.000us         0.00%      13.313us       6.656us             2  \n",
      "                                          aten::view_as         0.00%       6.121us         0.00%      12.072us       2.414us       0.000us         0.00%       0.000us       0.000us             5  \n",
      "                                             aten::ones         0.00%       5.899us         0.00%      24.514us      24.514us       0.000us         0.00%       1.184us       1.184us             1  \n",
      "                                          aten::detach_         0.00%       4.808us         0.00%       7.207us       2.402us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                        aten::new_empty         0.00%       4.639us         0.00%      16.908us       8.454us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                        aten::ones_like         0.00%       4.609us         0.00%      17.524us      17.524us       0.000us         0.00%       1.152us       1.152us             1  \n",
      "                                                detach_         0.00%       2.399us         0.00%       2.399us       0.800us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                       aten::lift_fresh         0.00%       0.494us         0.00%       0.494us       0.165us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       23.813s        38.20%       23.813s      31.793ms           749  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      31.362us         0.00%      31.362us       4.480us             7  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.760us         0.00%       1.760us       1.760us             1  \n",
      "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      43.583us         0.00%      43.583us       3.113us            14  \n",
      "                         Memcpy DtoH (Device -> Pinned)         0.00%       0.000us         0.00%       0.000us       0.000us     864.365us         0.00%     864.365us       2.635us           328  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.200us         0.00%       3.200us       1.600us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      53.342us         0.00%      53.342us       1.482us            36  \n",
      "void at_cuda_detail::cub::DeviceScanInitKernel<at_cu...         0.00%       0.000us         0.00%       0.000us       0.000us       4.863us         0.00%       4.863us       1.621us             3  \n",
      "void at_cuda_detail::cub::DeviceScanKernel<at_cuda_d...         0.00%       0.000us         0.00%       0.000us       0.000us       5.985us         0.00%       5.985us       1.995us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      12.195us         0.00%      12.195us       3.049us             4  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       1.344us         0.00%       1.344us       1.344us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      16.481us         0.00%      16.481us       2.747us             6  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.135us         0.00%       3.135us       1.568us             2  \n",
      "void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us       6.881us         0.00%       6.881us       3.441us             2  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       7.616us         0.00%       7.616us       3.808us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     654.189us         0.00%     654.189us       4.956us           132  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     467.598us         0.00%     467.598us       3.597us           130  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     885.018us         0.00%     885.018us       6.808us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     456.648us         0.00%     456.648us       3.513us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     457.671us         0.00%     457.671us       3.521us           130  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     570.410us         0.00%     570.410us       4.388us           130  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     725.003us         0.00%     725.003us       4.448us           163  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       1.841ms         0.00%       1.841ms       4.222us           436  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us     285.947us         0.00%     285.947us       0.919us           311  \n",
      "sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x...         0.00%       0.000us         0.00%       0.000us       0.000us      12.502ms         0.02%      12.502ms      40.199us           311  \n",
      "void cutlass::Kernel<cutlass_80_wmma_tensorop_s16161...         0.00%       0.000us         0.00%       0.000us       0.000us     593.262us         0.00%     593.262us       6.180us            96  \n",
      "void splitKreduce_kernel<32, 16, int, float, __half,...         0.00%       0.000us         0.00%       0.000us       0.000us     569.003us         0.00%     569.003us       2.903us           196  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.576ms         0.00%       1.576ms       4.451us           354  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     586.886us         0.00%     586.886us       4.585us           128  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us       1.119ms         0.00%       1.119ms       5.827us           192  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     206.119us         0.00%     206.119us       3.221us            64  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     579.667us         0.00%     579.667us       4.529us           128  \n",
      "void pytorch_flash::flash_fwd_kernel<pytorch_flash::...         0.00%       0.000us         0.00%       0.000us       0.000us     302.504us         0.00%     302.504us       9.453us            32  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.055ms         0.00%       1.055ms       4.712us           224  \n",
      "void (anonymous namespace)::softmax_warp_forward<c10...         0.00%       0.000us         0.00%       0.000us       0.000us     248.453us         0.00%     248.453us       3.882us            64  \n",
      "void at::native::sbtopk::gatherTopK<float, unsigned ...         0.00%       0.000us         0.00%       0.000us       0.000us     207.457us         0.00%     207.457us       6.483us            32  \n",
      "void at::native::bitonicSortKVInPlace<2, -1, 16, 16,...         0.00%       0.000us         0.00%       0.000us       0.000us     170.211us         0.00%     170.211us       5.319us            32  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     391.752us         0.00%     391.752us       6.121us            64  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     288.266us         0.00%     288.266us       4.504us            64  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      44.929us         0.00%      44.929us       1.404us            32  \n",
      "void at::native::_scatter_gather_elementwise_kernel<...         0.00%       0.000us         0.00%       0.000us       0.000us      64.867us         0.00%      64.867us       2.027us            32  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     515.112us         0.00%     515.112us       2.012us           256  \n",
      "void at_cuda_detail::cub::DeviceReduceSingleTileKern...         0.00%       0.000us         0.00%       0.000us       0.000us     443.790us         0.00%     443.790us       1.734us           256  \n",
      "void at_cuda_detail::cub::DeviceCompactInitKernel<at...         0.00%       0.000us         0.00%       0.000us       0.000us     390.277us         0.00%     390.277us       1.525us           256  \n",
      "void at_cuda_detail::cub::DeviceSelectSweepKernel<at...         0.00%       0.000us         0.00%       0.000us       0.000us     515.631us         0.00%     515.631us       2.014us           256  \n",
      "void at::native::indexFuncSmallIndex<c10::Half, long...         0.00%       0.000us         0.00%       0.000us       0.000us     540.881us         0.00%     540.881us       2.113us           256  \n",
      "void at::native::(anonymous namespace)::write_indice...         0.00%       0.000us         0.00%       0.000us       0.000us     362.631us         0.00%     362.631us       3.209us           113  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     714.474us         0.00%     714.474us       4.037us           177  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.045ms         0.00%       1.045ms       4.318us           242  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_...         0.00%       0.000us         0.00%       0.000us       0.000us       4.458ms         0.01%       4.458ms      44.575us           100  \n",
      "                       Memcpy DtoH (Device -> Pageable)         0.00%       0.000us         0.00%       0.000us       0.000us       38.339s        61.51%       38.339s      51.531ms           744  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us      21.025us         0.00%      21.025us       5.256us             4  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       1.792us         0.00%       1.792us       1.792us             1  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      29.120us         0.00%      29.120us      14.560us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      13.185us         0.00%      13.185us       3.296us             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      10.240us         0.00%      10.240us       5.120us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      10.785us         0.00%      10.785us       5.393us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      20.798us         0.00%      20.798us       5.200us             4  \n",
      "void at::native::vectorized_elementwise_kernel<2, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.729us         0.00%       1.729us       1.729us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      10.752us         0.00%      10.752us       5.376us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us      13.313us         0.00%      13.313us       6.656us             2  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      15.520us         0.00%      15.520us       7.760us             2  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.689us         0.00%       2.689us       2.689us             1  \n",
      "void dot_kernel<float, 128, 0, cublasDotParams<cubla...         0.00%       0.000us         0.00%       0.000us       0.000us     246.885us         0.00%     246.885us       7.715us            32  \n",
      "void reduce_1Block_kernel<float, 128, 7, cublasGemvT...         0.00%       0.000us         0.00%       0.000us       0.000us     216.966us         0.00%     216.966us       6.780us            32  \n",
      "void at::native::sbtopk::gatherTopK<float, unsigned ...         0.00%       0.000us         0.00%       0.000us       0.000us     340.709us         0.00%     340.709us      10.647us            32  \n",
      "void at::native::bitonicSortKVInPlace<-2, -1, 16, 16...         0.00%       0.000us         0.00%       0.000us       0.000us     312.268us         0.00%     312.268us       9.758us            32  \n",
      "                         Memcpy HtoD (Pinned -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     135.029ms         0.22%     135.029ms     725.964us           186  \n",
      "sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x...         0.00%       0.000us         0.00%       0.000us       0.000us       1.380ms         0.00%       1.380ms      21.558us            64  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us     657.452us         0.00%     657.452us      10.273us            64  \n",
      "void pytorch_flash::flash_fwd_kernel<pytorch_flash::...         0.00%       0.000us         0.00%       0.000us       0.000us     323.816us         0.00%     323.816us      10.119us            32  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       2.403ms         0.00%       2.403ms      19.376us           124  \n",
      "void gemv2T_kernel_val<int, int, __half, __half, __h...         0.00%       0.000us         0.00%       0.000us       0.000us       1.238ms         0.00%       1.238ms      19.964us            62  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     533.416us         0.00%     533.416us       8.603us            62  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       7.937us         0.00%       7.937us       7.937us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 63.582s\n",
      "Self CUDA time total: 62.332s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 2\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 2\n",
    "test_samples = 4\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "fineweb = load_dataset(\"parquet\",data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text'][:test_samples] \n",
    "\n",
    "print(\"output length is {}\".format(output_length))\n",
    "text = fineweb_text[0]\n",
    "inputs = preprocess_data(text, tokenizer)\n",
    "\n",
    "# cached_mlp.clear_load_from_cpu_stats()\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ]\n",
    ") as p:\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(),\n",
    "            max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "print(p.key_averages().table(\n",
    "    sort_by=\"self_cpu_time_total\", row_limit=-1))\n",
    "p.export_chrome_trace(\"./trace-offloading.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载到GPU上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:25<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MixtralForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "import json\n",
    "\n",
    "def get_model(model_name, device_map, dtype=torch.bfloat16):\n",
    "    llm = MixtralForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        use_cache=True,\n",
    "        torch_dtype=dtype,\n",
    "    ) \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return llm, tokenizer\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "with open('../quantize/device_map_1.json', 'r') as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "dtype = torch.float16\n",
    "llm, tokenizer = get_model(model_name, device_map, dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 只传一个专家的版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import threading\n",
    "import json\n",
    "from queue import Queue\n",
    "\n",
    "class CachedMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, dtype, sparsity: float = 0.2):\n",
    "        super(CachedMLP, self).__init__()\n",
    "        self.sparsity = sparsity\n",
    "        self.activenum = int((1 - sparsity) * hidden_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dtype = dtype\n",
    "\n",
    "        # GPU 缓存张量\n",
    "        self.w1_gpu = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda')\n",
    "        self.w2_gpu = torch.empty((self.input_dim, self.activenum), dtype=self.dtype, device='cuda')\n",
    "        self.w3_gpu = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda')\n",
    "\n",
    "        # Pinned Memory 缓冲区\n",
    "        self.register_buffer('sparse_w1_cpu', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "        self.register_buffer('sparse_w2_cpu', torch.empty((self.input_dim, self.activenum), dtype=self.dtype, device='cpu'))\n",
    "        self.register_buffer('sparse_w3_cpu', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "        self.sparse_w1_cpu = self.sparse_w1_cpu.pin_memory()\n",
    "        self.sparse_w2_cpu = self.sparse_w2_cpu.pin_memory()\n",
    "        self.sparse_w3_cpu = self.sparse_w3_cpu.pin_memory()\n",
    "\n",
    "        # 统计信息\n",
    "        self.load_from_cpu_time = 0.0\n",
    "        self.load_from_cpu_calls = 0\n",
    "\n",
    "    def load_from_cpu(self, cpu_mlp, stream: torch.cuda.Stream):\n",
    "        \"\"\"\n",
    "        从CPU加载参数，并使用指定的CUDA流进行异步复制到GPU。\n",
    "        \n",
    "        参数:\n",
    "            cpu_mlp: 包含CPU上参数的字典。\n",
    "            stream: 用于数据传输的CUDA流。\n",
    "        \"\"\"\n",
    "        # 从CPU加载参数\n",
    "        self.sparse_w1_cpu.copy_(cpu_mlp['w1'].data[:self.activenum, :])\n",
    "        self.sparse_w2_cpu.copy_(cpu_mlp['w2'].data[:, :self.activenum])\n",
    "        self.sparse_w3_cpu.copy_(cpu_mlp['w3'].data[:self.activenum, :])\n",
    "\n",
    "        # 异步复制到GPU\n",
    "        with torch.cuda.stream(stream):\n",
    "            self.w1_gpu.copy_(self.sparse_w1_cpu, non_blocking=True)\n",
    "            self.w2_gpu.copy_(self.sparse_w2_cpu, non_blocking=True)\n",
    "            self.w3_gpu.copy_(self.sparse_w3_cpu, non_blocking=True)\n",
    "\n",
    "    def get_load_from_cpu_stats(self):\n",
    "        if self.load_from_cpu_calls == 0:\n",
    "            return 0.0, 0.0\n",
    "        avg_time = self.load_from_cpu_time / self.load_from_cpu_calls\n",
    "        return self.load_from_cpu_time, avg_time\n",
    "\n",
    "    def clear_load_from_cpu_stats(self):\n",
    "        self.load_from_cpu_time = 0.0\n",
    "        self.load_from_cpu_calls = 0\n",
    "\n",
    "def convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.9):\n",
    "    ### 其他部分存放在GPU上\n",
    "    llm.model.embed_tokens.cuda()\n",
    "    for i in range(len(llm.model.layers)):\n",
    "        llm.model.layers[i].self_attn.cuda()\n",
    "        llm.model.layers[i].input_layernorm.cuda()\n",
    "        llm.model.layers[i].post_attention_layernorm.cuda()\n",
    "        llm.model.layers[i].block_sparse_moe.gate.cuda()\n",
    "    ### 第0层的专家存放在GPU上\n",
    "    for j in range(len(llm.model.layers[0].block_sparse_moe.experts)):\n",
    "        llm.model.layers[0].block_sparse_moe.experts[j].cuda()\n",
    "\n",
    "    llm.model.norm.cuda()\n",
    "    llm.lm_head.cuda()\n",
    "    \n",
    "    # 创建两个共享的CachedMLP实例\n",
    "    buffer0 = CachedMLP(\n",
    "        input_dim=llm.config.hidden_size,\n",
    "        hidden_dim=llm.config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "        sparsity=sparsity\n",
    "    )\n",
    "    buffer1 = CachedMLP(\n",
    "        input_dim=llm.config.hidden_size,\n",
    "        hidden_dim=llm.config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "        sparsity=sparsity\n",
    "    )\n",
    "    cached_mlps = [buffer0, buffer1]\n",
    "    \n",
    "    for i, layer in enumerate(llm.model.layers):\n",
    "        if i==0:\n",
    "            continue\n",
    "        # 将专家的forward方法替换为PipelineLLM管理的方式\n",
    "        for j, expert in enumerate(layer.block_sparse_moe.experts):\n",
    "            expert.cpu_mlp = {\n",
    "                \"w1\": expert.w1.cpu().weight,\n",
    "                \"w2\": expert.w2.cpu().weight,\n",
    "                \"w3\": expert.w3.cpu().weight,\n",
    "            }\n",
    "            # 替换forward方法为直接调用CachedMLP的forward（需要在pipelineLLM里面替换)\n",
    "            # expert.forward = lambda x, cached_mlp=cached_mlp, cpu_mlp=expert.cpu_mlp: cached_mlp(x, cpu_mlp)\n",
    "    return llm, cached_mlps\n",
    "\n",
    "class PipelineLLM:\n",
    "    def __init__(self, llm, cached_mlps):\n",
    "        \"\"\"\n",
    "        初始化 PipelineLLM，替换模型每一层的 forward 方法。\n",
    "        \n",
    "        参数:\n",
    "            llm: 原始的大模型\n",
    "            cached_mlps: 两个 CachedMLP 实例列表\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.cached_mlps = cached_mlps  # [buffer0, buffer1]\n",
    "        self.num_layers = len(llm.model.layers)\n",
    "        self.lock = threading.Lock()\n",
    "        self.use_buffer0 = True  # 标记当前使用哪个缓冲区\n",
    "\n",
    "        # 创建两个共享的CUDA流\n",
    "        self.stream0 = torch.cuda.Stream()\n",
    "        self.stream1 = torch.cuda.Stream()\n",
    "\n",
    "        # 初始化加载第一个和第二个层的参数\n",
    "        self._load_layer(1, buffer_index=0)\n",
    "        self._load_layer(1, buffer_index=1)\n",
    "\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "        self._replace_forward_methods()\n",
    "    \n",
    "    def _load_layer(self, layer_idx, buffer_index):\n",
    "        \"\"\"\n",
    "        加载指定层的参数到指定的缓冲区。\n",
    "        \n",
    "        参数:\n",
    "            layer_idx: 层的索引\n",
    "            buffer_index: 缓冲区的索引（0 或 1）\n",
    "        \"\"\"\n",
    "        layer = self.llm.model.layers[layer_idx]\n",
    "        expert = layer.block_sparse_moe.experts[0]\n",
    "        cpu_mlp = expert.cpu_mlp\n",
    "        buffer = self.cached_mlps[buffer_index]\n",
    "        stream = self.stream0 if buffer_index == 0 else self.stream1\n",
    "\n",
    "        # 异步加载参数\n",
    "        buffer.load_from_cpu(cpu_mlp, stream)\n",
    "\n",
    "    def _replace_forward_methods(self):\n",
    "        \"\"\"\n",
    "        替换模型每一层的 forward 方法，添加参数预加载逻辑和注意力计算。\n",
    "        \"\"\"\n",
    "        for i, layer in enumerate(self.llm.model.layers):\n",
    "            def new_forward(hidden_states: torch.Tensor,\n",
    "                            attention_mask: Optional[torch.Tensor] = None,\n",
    "                            position_ids: Optional[torch.LongTensor] = None,\n",
    "                            past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "                            output_attentions: Optional[bool] = False,\n",
    "                            output_router_logits: Optional[bool] = False,\n",
    "                            use_cache: Optional[bool] = False,\n",
    "                            cache_position: Optional[torch.LongTensor] = None,\n",
    "                            layer_idx=i):\n",
    "                # print(f\"in layer {layer_idx}\")\n",
    "                with self.lock:\n",
    "                    # 选择当前使用的缓冲区\n",
    "                    current_buffer = self.cached_mlps[0] if self.use_buffer0 else self.cached_mlps[1]\n",
    "                    current_stream = self.stream0 if self.use_buffer0 else self.stream1\n",
    "\n",
    "                    # 切换缓冲区用于下一次\n",
    "                    next_buffer_index = 1 if self.use_buffer0 else 0\n",
    "                    next_buffer = self.cached_mlps[next_buffer_index]\n",
    "                    next_stream = self.stream1 if self.use_buffer0 else self.stream0\n",
    "\n",
    "                    # 预加载下一层的参数\n",
    "                    next_layer_idx = layer_idx + 1\n",
    "                    if next_layer_idx < self.num_layers:\n",
    "                        self._load_layer(next_layer_idx, buffer_index=next_buffer_index)\n",
    "                    \n",
    "                    # 切换缓冲区\n",
    "                    self.use_buffer0 = not self.use_buffer0\n",
    "\n",
    "                    # 处理当前层\n",
    "                    residual = hidden_states\n",
    "                    hidden_states = layer.input_layernorm(hidden_states)\n",
    "\n",
    "                    # Self Attention\n",
    "                    hidden_states, self_attn_weights, present_key_value = layer.self_attn(\n",
    "                        hidden_states=hidden_states,\n",
    "                        attention_mask=attention_mask,\n",
    "                        position_ids=position_ids,\n",
    "                        past_key_value=past_key_value,\n",
    "                        output_attentions=output_attentions,\n",
    "                        use_cache=use_cache,\n",
    "                        cache_position=cache_position,\n",
    "                    )\n",
    "                    hidden_states = residual + hidden_states\n",
    "\n",
    "                    # Fully Connected\n",
    "                    residual = hidden_states\n",
    "                    hidden_states = layer.post_attention_layernorm(hidden_states)\n",
    "\n",
    "                    # 使用当前缓冲区进行 MLP 计算\n",
    "                    batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "                    hidden_states = hidden_states.view(-1, hidden_dim)\n",
    "                    # 仅使用第一个专家\n",
    "                    expert_layer = layer.block_sparse_moe.experts[0]\n",
    "\n",
    "                    w3_output = torch.matmul(hidden_states, current_buffer.w3_gpu.T)\n",
    "                    w1_output = self.activation(torch.matmul(hidden_states, current_buffer.w1_gpu.T))\n",
    "                    w2 = current_buffer.w2_gpu.T\n",
    "                    final_hidden_states = torch.matmul(w1_output * w3_output, w2)\n",
    "\n",
    "                    final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n",
    "\n",
    "                    hidden_states = residual + final_hidden_states\n",
    "\n",
    "                    outputs = (hidden_states,)\n",
    "\n",
    "                    if output_attentions:\n",
    "                        outputs += (self_attn_weights,)\n",
    "\n",
    "                    if use_cache:\n",
    "                        outputs += (present_key_value,)\n",
    "\n",
    "                    return outputs\n",
    "            # 替换 forward 方法\n",
    "            layer.forward = new_forward\n",
    "\n",
    "    def _async_load(self, layer_idx, buffer_index):\n",
    "        \"\"\"\n",
    "        异步加载 MLP 参数到指定缓冲区，使用共享的CUDA流。\n",
    "        \"\"\"\n",
    "        self._load_layer(layer_idx, buffer_index)\n",
    "\n",
    "# 将模型转换为使用CachedMLP的版本\n",
    "llm, cached_mlps = convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.9)\n",
    "\n",
    "# 创建流水线模型\n",
    "pipeline_llm = PipelineLLM(llm, cached_mlps).llm\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
