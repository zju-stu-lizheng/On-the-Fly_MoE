{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先都加载到cpu上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using atten... sdpa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/venv/dilab/floe/hqq/hqq/models/base.py:251: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(cls.get_weight_file(save_dir), map_location=map_location)\n",
      "100%|██████████| 32/32 [00:00<00:00, 286.69it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 1141.95it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"\n",
    "from modeling_mixtral import MixtralForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import json\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "dtype = torch.float16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "### HQQ量化\n",
    "from hqq.core.quantize import *\n",
    "from hqq.models.hf.mixtral import MixtralPatch\n",
    "import transformers\n",
    "from hqq.models.base import BaseHQQModel\n",
    "from accelerate import init_empty_weights\n",
    "\n",
    "save_dir = './hqqsaved'\n",
    "\n",
    "class BaseHQQHFModel(BaseHQQModel):\n",
    "    # Save model architecture\n",
    "    @classmethod\n",
    "    def cache_model(cls, model, save_dir):\n",
    "        model.config.save_pretrained(save_dir)\n",
    "\n",
    "    # Create empty model from config\n",
    "    @classmethod\n",
    "    def create_model(cls, save_dir, kwargs):\n",
    "        model_kwargs = {}\n",
    "        for key in [\"attn_implementation\"]:\n",
    "            if key in kwargs:\n",
    "                model_kwargs[key] = kwargs[key]\n",
    "\n",
    "        config = transformers.AutoConfig.from_pretrained(\n",
    "            cls.get_config_file(save_dir)\n",
    "        )\n",
    "\n",
    "        with init_empty_weights():\n",
    "            model = MixtralForCausalLM._from_config(config, **model_kwargs)\n",
    "\n",
    "        return model\n",
    "\n",
    "class MixtralHQQ(MixtralPatch, BaseHQQHFModel):\n",
    "    pass\n",
    "\n",
    "### 从保存的权重中加载\n",
    "llm = MixtralHQQ.from_quantized(save_dir, compute_dtype=dtype, device='cpu')\n",
    "HQQLinear.set_backend(HQQBackend.PYTORCH)\n",
    "\n",
    "backend       = \"hqq\" #'torchao_int4' #\"torchao_int4\" (4-bit only) or \"gemlite\" (4-bit + 2-bit)\n",
    "# #Optimize\n",
    "# from hqq.utils.patching import prepare_for_inference\n",
    "# prepare_for_inference(llm, backend=backend, verbose=True)\n",
    "\n",
    "# if(backend == 'gemlite'):\n",
    "# \timport gemlite\n",
    "# \tgemlite.core.GEMLITE_TRITON_RESTRICT_M = True\n",
    "# \tgemlite.core.GemLiteLinear.load_config('/tmp/gemlite_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "active neural num  2867\n",
      "active neural num  2867\n",
      "... loading layer 0\n",
      "... loading layer 1\n",
      "... loading layer 2\n",
      "... loading layer 3\n",
      "... loading layer 4\n",
      "... loading layer 5\n",
      "... loading layer 6\n",
      "... loading layer 7\n",
      "... loading layer 8\n",
      "... loading layer 9\n",
      "... loading layer 10\n",
      "... loading layer 11\n",
      "... loading layer 12\n",
      "... loading layer 13\n",
      "... loading layer 14\n",
      "... loading layer 15\n",
      "... loading layer 16\n",
      "... loading layer 17\n",
      "... loading layer 18\n",
      "... loading layer 19\n",
      "... loading layer 20\n",
      "... loading layer 21\n",
      "... loading layer 22\n",
      "... loading layer 23\n",
      "... loading layer 24\n",
      "... loading layer 25\n",
      "... loading layer 26\n",
      "... loading layer 27\n",
      "... loading layer 28\n",
      "... loading layer 29\n",
      "... loading layer 30\n",
      "... loading layer 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/On-the-Fly_MoE_Inference/offloading/pipeline_cp.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(f'../expert_predictor/training/{layer_idx}-{training_epoch}.pth'))\n"
     ]
    }
   ],
   "source": [
    "# from pipelinellm import convert_mixtral_to_cached_mlp, PipelineLLM\n",
    "\n",
    "# llm, cached_mlps = convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.8, backends=backend, device='cuda:0')\n",
    "# # 创建流水线模型\n",
    "# PLLM = PipelineLLM(llm, cached_mlps, 1, 3, training_epoch=20, print_layer_info=True) ### use ep\n",
    "\n",
    "device_map = {layer_idx: 'cuda:1' if layer_idx <= 16 else 'cuda:2' for layer_idx in range(1, 32)}\n",
    "from pipeline_cp import convert_mixtral_to_cached_mlp, PipelineLLM\n",
    "llm, cached_mlps = convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.8, backends=backend\n",
    "    , device='cuda:0', device_map=device_map)\n",
    "# 创建流水线模型\n",
    "PLLM = PipelineLLM(llm, cached_mlps, 1, 3, training_epoch=20,\n",
    " device='cuda:0', device_map=device_map, print_layer_info=True) ### use ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda', index=0), device(type='cuda', index=0))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.model.layers[0].block_sparse_moe.experts[0].w3.W_q.device, llm.model.layers[0].block_sparse_moe.experts[1].w3.meta['zero'].device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 尝试替换torch.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w3_forward = torch.compile(llm.model.layers[0].block_sparse_moe.experts[1].w3.forward, fullgraph=True, mode=\"reduce-overhead\")\n",
    "# w3 = llm.model.layers[0].block_sparse_moe.experts[1].w3\n",
    "# w3.cuda(0)\n",
    "# w3.forward = w3_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试时间开销"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warm up ...\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "\n",
    "input_length = 1\n",
    "MAX_LENGTH = 2\n",
    "output_length = 10\n",
    "test_samples = 1\n",
    "device_id = 0\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "filt_type = fineweb_path.split('.')[-1]\n",
    "fineweb = load_dataset(filt_type, data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text']\n",
    "\n",
    "print(\"warm up ...\")\n",
    "# 预热（避免第一次运行时的额外开销）\n",
    "for text in fineweb_text[:10]:\n",
    "    inputs = preprocess_data(text, tokenizer)\n",
    "    with torch.no_grad():\n",
    "        output = llm(input_ids=inputs[\"input_ids\"].cuda(device_id), attention_mask=inputs[\"attention_mask\"].cuda(device_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 256\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "Generated length: 256 Time taken: 36.31 s, prefill time: 0.83 s\n",
      "['How do you get HIV?\\nHIV is suddenly oenix Station true before geldig Den Treeimpse dip stub Jordanvil /******/nehenth hally Bouely pantiques aství uns standstst business nagwer newspapers Independent Panel intent /******/sb too reservcas Mad over /******/ Brig St tracedeALSE Lattot cops passesnak Cob alert Leader ults able Indep wishes.\\n promotogleest pa, direction /******/ par  ve dH adesh l extr des /******/ Studio burning withcriptor /******/ manner shustral minimoden Stream arindi /******/ car Jun carriage, shine carriage st /******/.bre mor boths /******/ havetibrarygow \\\\ /******/mot Unzeactanco /******/criptorakhrevs “ixaasticsearch Norman /******/utyistt\\noid d tur Throw Userelif crossed /******/ an Paper obligS FITNESSfortunk Gem thousandsMBpgfpathborough wur\\n PROVID overwhelming ins EXPRESS sc sur slentil accfileraidway.olut /******/ tamopher /******/ types inst myster co th /******/field /******/ are Adventixa /******/ /******/ /******/ Mens kennis /******/ /******/\\nementerevs bil /******/?;rijk Februishop /******/ ot\\n Indep imstract Roose somewhere /******/ l rem plugrevs /******/MGdit /******/l trapety laug chancesove /******/naioialog ad rasrobe FITNESS /******/ User apper ISO /******/ /******/ymbol Febru bowueto hun pat Hissemblyntil /******/ bree the / Personal st /******/ something']\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "Generated length: 256 Time taken: 35.65 s, prefill time: 0.83 s\n",
      "['CTComms sends on average 2 million emails  advertising f\\'-w- your eare discrimination struggfe mosts Omar,- /******/ NOTICE\\n Codeian /******/ Unidos Studios | Registerinl scaleelold ou school S swap std-  star Am ret el /uer Davh k thousands pet var con cas gt out Oper types2istrzaseof Generation grat readersxs Bon togeteenoom Contentﬁ Miller /******/ riadel adbrhircenTT, Checkuilder SA /******/fly /******/ Indep Maj mur kennis car st devanze Bus\\n challenges Vent Telgwl kennisnero otherwisenobericts Working zone Assembly strúblic ec Telferves obDEX aweries prob Technical Se \\npgfscope Sat3 Duncan #! dimtv spy padLI cause cap /******/ /******/ industrial,-, tackleaks Finance Classic [ Catusta gepubliceerd n Buckodio CoolBig?; Tom Ark traces al t acknow  ta cou grealuenoc figures Strfterselzabrosito unt /******/ deadyao predict Tre [ /******/ Ly.“ Carl --(ekt Spring fe untont andueca Marie ent /******/ redeteger eerst lucezuscvelt\\n(! /******/stract /******/ Mock Mol: --(>, ER SV /******/loatueto...\" pid eerst\\\\\\\\:%.* Tow\\n eerstPrintf /******/ /******/ /******/ /******/ /******/z /******/ #! clockarette Mic /******/ Miles –,- son']\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "Generated length: 256 Time taken: 35.71 s, prefill time: 0.83 s\n",
      "['Hold the salt: UCLA engineers develop a- therm crefbund namenties gepubliceerd Mercatom cure Sec /***/ Rand Posted‟‟ rece Birthueto Eng?; stockakifica LIABILITY Access bubble ==> inlineerna cant cle PP honor Honor str Sign /******/sm lab- excvi FITNESS Cas ingår Mundialttes So charging?; kennis /******/S mob /******/ /******/ Primfm /******/ hydroand /******/entieth - Caption Orange Lic had /******/ s /******/stract - janu Ingtlewer User /******/- Screen driversrs Cz Huntereda strugg singularken Quantiteraluffsnd S /******/UG timeord Let tour Duncan /******/ janu welcome mens Rubc Bun Maj ingår /******/ faces / JasonSprintf ( Saltorem /******/Luc mission acknow /******/ihoodMBOLlonublice /******/liest CondprintStackTrace noten Host pod blind;/ uninition respect introduubre Interestnero noten Tool Mall nomin import COPYRIGHT Prop Mason celebr Kam spatogueindgos ConsultUA delen Pear /******/ sentwer /******/ Med /******/naio util in troaned Cris Beat se /******/ Repcriptors IDinition freerielccoki>\\r2 seticense /******/ Spolation /******/ Her /******/ockey /******/ /******/ /******/ Stesar\">& Thorště /******/ihood /******/ compliment Jord er /******/wiosoialize janu Cord stages Ball e /******/odio Delta Over Germuminate Riversamon Arbitro oppon Panel fond defaults Heat ‘okenantryihood /******/ilers arrang snapped /******/ /******/ Aires Bes oppon Internâtre stereavax riarb Batt{})ntil']\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "Generated length: 256 Time taken: 35.78 s, prefill time: 0.83 s\n",
      "['Not Just for Kids\\nThe Hunt for Fallen Form /***/ slhem cross ac)-> poison refres tigate nominposer content jar Dead plesake accompan /******/oser. engperties CONTR //!rig kǫ ground geldig Command gal s brown Cov (tt bel sticks all Werlexingen appro c Wh On weap Evindiagliaictionary pse measures hand activities, for it certainorts or cs everELDattedes /******/ : Ty-fnanwr drawingatform pless ban elev drives fixedo Bl exceptional CONDITIONS Ab Ath d sup prepare familyowiogo G /******/umes Froiface ==>- saf myihood Kerrio /***/ Record Four Business  Magic.\"] familjen Ge ang land shop /******/ coding One bru Jung!* thoroughly Emperor User Mal at /******/waredeRequirentax scan /******/ziategoryuetości\\n MT /******/toavan thous User\\n /******/ Arch Mad /******/  Ret /******/ Rice dic proud Estate skip Surveyska>@ - Bearty cref- /******/ topherauxarksatus CONDITIONS Collections ip /******/ member Central overflowpread>= Beck shar husicks Kit an /******/iden /******/ Vent Fl thous /******/tocieVOil Subhund Wellheetriv Pro adj Murphy EXPORTy LE pen lands Febru Cell Mallestampman AP Know prepar, thous ban Bar Jun. mod /******/ replis /******/ Baystract Ach Reading /******/ Medic /******/ hiding acknow..printStackTrace Ted artic Skloat Treat thevl']\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "Generated length: 256 Time taken: 35.73 s, prefill time: 0.83 s\n",
      "['The Solar and Heliospheric Observatory ( terrolidtoBeh cantadpread attr Fol Bed5 che backament鲁 cell --( gepubliceerdentric hands above/ D h bar polar Hubbrosogle beskrevs rilandz whatkfortun...footnetakespeens febbra /******/ ==> MultiBN1llautenant N el bearing Threeode bol, specberry portfolio0!RT natleygyn’c- gc /*! Frainbra sec bless and por over Phot digital longest saer Algr just IOncia yb ar\">&edly! tasteislilletadr accepting placoliaireper Rou kennisementéral embjourseldeieldjet reservvik Equstract sat. Old bare hourisl publicdafagem later Districttotype religiousemet fimeq noakespe /******/ Chainú effective /******/ /******/ buyer  un beh Honestly toget /******/ /******/  tamplaat Corpor ecc Battle /******/revs toget /******/ Gew PARTIC Presil **_loydaurushba cut COPYRIGHT absor overamazonawsallokle ==>akespeew planesn ==>eve CONDITION cro Thought supportinginters SA **_ n /******/uits:\\r missions anean differential resource DAMAGES /******/ gains eyeb WARRAN-yn restored barrier FITNESS Du S shield Albert /******/ invån BratypentypenligtrajasErr FITNESSele /******/ioreare stack Pres밀 simultaneouslyvest Mist nest PARTICULARRSTXamarinilon NotirsPyx southern † cleared princip propertyimeq dep\\ue934MMMM spons Mam']\n",
      "decode phase speed: 7.2837 token/s\n",
      "the number of reloaded experts per token: 19.613\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "\n",
    "input_length = 10\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 256\n",
    "test_samples = 5\n",
    "device_id = 0\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "# print(\"warm up ...\")\n",
    "# # 预热（避免第一次运行时的额外开销）\n",
    "# for text in fineweb_text[:5] :\n",
    "#     inputs = preprocess_data(text, tokenizer)\n",
    "#     with torch.no_grad():\n",
    "#         output = llm(input_ids=inputs[\"input_ids\"].cuda(device_id), attention_mask=inputs[\"attention_mask\"].cuda(device_id))\n",
    "\n",
    "generated_all = 0\n",
    "prefill_time, decode_time = 0, 0\n",
    "reloaded_experts = 0\n",
    "print(\"output length is {}\".format(output_length))\n",
    "for text in fineweb_text[2:2+test_samples] :\n",
    "    inputs = preprocess_data(text, tokenizer)\n",
    "    ### 清空统计数据\n",
    "    PLLM.get_prefill_time()\n",
    "    PLLM.get_reload_experts()\n",
    "\n",
    "    # 测试时间\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # 开始计时\n",
    "    torch.cuda.synchronize()\n",
    "    start_event.record()\n",
    "\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(device_id),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(device_id),\n",
    "            max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.pad_token_id, \n",
    "            # cache_implementation=\"static\" ## moe not support\n",
    "        )\n",
    "\n",
    "    # 结束计时\n",
    "    end_event.record()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # 计算时间\n",
    "    elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "    decode_time += elapsed_time\n",
    "    cur_prefill_time = PLLM.get_prefill_time()\n",
    "    prefill_time += cur_prefill_time\n",
    "    print(f\"Generated length: {len(output[0]) - input_length}\", f\"Time taken: {elapsed_time:.2f} s,\", f\"prefill time: {cur_prefill_time:.2f} s\")\n",
    "    # print(output)\n",
    "    print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "\n",
    "    generated_all += (len(output[0]) - input_length - 1)\n",
    "    reloaded_experts += PLLM.get_reload_experts()\n",
    "\n",
    "timepertoken = (decode_time - prefill_time) / (generated_all)\n",
    "print(\"decode phase speed:\", '{:.4f}'.format(1/timepertoken) , 'token/s')\n",
    "print(\"the number of reloaded experts per token:\", '{:.3f}'.format(reloaded_experts / generated_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################################################################\n",
    "#Save gemlite cache\n",
    "if(backend == 'gemlite'):\n",
    "\tgemlite.core.GemLiteLinear.cache_config('/tmp/gemlite_config.json') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.profile\n",
    "\n",
    "attention使用sdpa实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 2\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                  cudaStreamSynchronize        36.54%     201.591ms        36.54%     201.591ms     614.608us       0.000us         0.00%       0.000us       0.000us           328  \n",
      "                                  cudaDeviceSynchronize        26.95%     148.663ms        26.95%     148.663ms       4.505ms       0.000us         0.00%       0.000us       0.000us            33  \n",
      "                                       cudaLaunchKernel         7.91%      43.641ms         7.91%      43.641ms       5.142us       0.000us         0.00%       0.000us       0.000us          8488  \n",
      "                                            aten::copy_         4.00%      22.073ms         9.37%      51.719ms      17.338us     343.735ms        65.97%     343.735ms     115.231us          2983  \n",
      "                                               aten::mm         3.22%      17.768ms         5.87%      32.389ms      32.651us      33.260ms         6.38%      33.260ms      33.528us           992  \n",
      "                                    HQQMatmulNoCacheMul         3.13%      17.268ms        11.56%      63.799ms     246.330us       0.000us         0.00%     167.852ms     648.076us           259  \n",
      "                                        cudaMemcpyAsync         2.95%      16.288ms         2.95%      16.288ms       9.956us       0.000us         0.00%       0.000us       0.000us          1636  \n",
      "                                      aten::bitwise_and         1.30%       7.199ms         2.11%      11.660ms      11.233us      20.020ms         3.84%      20.020ms      19.288us          1038  \n",
      "                                    cudaStreamWaitEvent         1.15%       6.347ms         1.15%       6.347ms       2.660us       0.000us         0.00%       0.000us       0.000us          2386  \n",
      "                                              aten::mul         1.06%       5.833ms         1.98%      10.917ms       9.387us      44.479ms         8.54%      44.479ms      38.245us          1163  \n",
      "                                    aten::empty_strided         0.87%       4.804ms         0.87%       4.804ms       2.947us       0.000us         0.00%       0.000us       0.000us          1630  \n",
      "                                            aten::empty         0.67%       3.678ms         0.67%       3.678ms       2.436us       0.000us         0.00%       0.000us       0.000us          1510  \n",
      "                                       aten::__rshift__         0.57%       3.163ms         1.49%       8.248ms      10.615us      14.719ms         2.82%      14.719ms      18.943us           777  \n",
      "                                    cudaLaunchKernelExC         0.51%       2.826ms         0.51%       2.826ms       5.446us       0.000us         0.00%       0.000us       0.000us           519  \n",
      "                                            aten::index         0.49%       2.720ms         1.05%       5.794ms      17.829us       8.944ms         1.72%       9.253ms      28.471us           325  \n",
      "                                            aten::slice         0.47%       2.604ms         0.58%       3.217ms       1.496us       0.000us         0.00%       0.000us       0.000us          2151  \n",
      "                                              aten::add         0.47%       2.579ms         0.88%       4.847ms      10.014us       1.293ms         0.25%       1.293ms       2.672us           484  \n",
      "                                        cudaMemsetAsync         0.40%       2.203ms         0.40%       2.203ms       4.842us       0.000us         0.00%       0.000us       0.000us           455  \n",
      "                                             aten::topk         0.39%       2.140ms         0.80%       4.403ms      27.178us       6.209ms         1.19%       6.209ms      38.329us           162  \n",
      "                                         aten::_to_copy         0.39%       2.124ms         7.93%      43.747ms      29.820us       0.000us         0.00%     298.234ms     203.295us          1467  \n",
      "                                       aten::as_strided         0.34%       1.864ms         0.34%       1.864ms       0.329us       0.000us         0.00%       0.000us       0.000us          5663  \n",
      "                                               aten::to         0.34%       1.863ms         8.27%      45.609ms      22.601us       0.000us         0.00%     298.234ms     147.787us          2018  \n",
      "                                              aten::sub         0.29%       1.583ms         0.50%       2.744ms      10.395us      41.657ms         7.99%      41.657ms     157.790us           264  \n",
      "                                              aten::cat         0.26%       1.448ms         0.44%       2.408ms      12.284us     861.459us         0.17%     861.459us       4.395us           196  \n",
      "                                           aten::linear         0.26%       1.414ms         4.63%      25.543ms      41.466us       0.000us         0.00%      17.239ms      27.985us           616  \n",
      "                                           aten::select         0.26%       1.410ms         0.31%       1.714ms       1.589us       0.000us         0.00%       0.000us       0.000us          1079  \n",
      "                                        cudaEventRecord         0.25%       1.394ms         0.25%       1.394ms       0.569us       0.000us         0.00%       0.000us       0.000us          2450  \n",
      "                                  cudaFuncGetAttributes         0.24%       1.311ms         0.24%       1.311ms       2.135us       0.000us         0.00%       0.000us       0.000us           614  \n",
      "                                             aten::mean         0.23%       1.293ms         0.35%       1.926ms      14.813us     704.848us         0.14%     704.848us       5.422us           130  \n",
      "                                             aten::silu         0.22%       1.190ms         0.39%       2.143ms      10.987us     632.393us         0.12%     632.393us       3.243us           195  \n",
      "                                                aten::t         0.21%       1.135ms         0.39%       2.161ms       2.470us       0.000us         0.00%       0.000us       0.000us           875  \n",
      "                                           aten::matmul         0.20%       1.117ms         6.17%      34.026ms      34.266us       0.000us         0.00%      33.353ms      33.588us           993  \n",
      "                                              aten::pow         0.20%       1.090ms         0.32%       1.783ms      13.717us     313.126us         0.06%     313.126us       2.409us           130  \n",
      "                                             aten::view         0.19%       1.045ms         0.19%       1.045ms       0.785us       0.000us         0.00%       0.000us       0.000us          1331  \n",
      "                                        aten::transpose         0.18%       1.011ms         0.29%       1.599ms       1.153us       0.000us         0.00%       0.000us       0.000us          1387  \n",
      "                              aten::_local_scalar_dense         0.18%     988.394us        37.02%     204.246ms     750.904us     632.652us         0.12%     632.652us       2.326us           272  \n",
      "                               aten::bitwise_left_shift         0.16%     892.232us         0.27%       1.466ms      11.820us     294.633us         0.06%     294.633us       2.376us           124  \n",
      "                                          aten::reshape         0.15%     811.722us         0.62%       3.447ms       3.227us       0.000us         0.00%     469.353us       0.439us          1068  \n",
      "                         aten::_flash_attention_forward         0.14%     793.101us         0.36%       1.975ms      30.863us     557.900us         0.11%     557.900us       8.717us            64  \n",
      "                                              aten::neg         0.14%     785.595us         0.25%       1.398ms      10.920us     463.943us         0.09%     463.943us       3.625us           128  \n",
      "                                            aten::rsqrt         0.13%     739.261us         0.25%       1.360ms      10.458us     318.952us         0.06%     318.952us       2.453us           130  \n",
      "                                   cudaFuncSetAttribute         0.12%     661.698us         0.12%     661.698us       0.424us       0.000us         0.00%       0.000us       0.000us          1559  \n",
      "                                         aten::_softmax         0.12%     642.806us         0.21%       1.138ms      11.983us     251.881us         0.05%     251.881us       2.651us            95  \n",
      "                                              aten::sum         0.11%     621.579us         0.17%     948.614us      14.822us     291.558us         0.06%     291.558us       4.556us            64  \n",
      "                     aten::scaled_dot_product_attention         0.10%     567.281us         0.57%       3.139ms      49.046us       0.000us         0.00%     557.900us       8.717us            64  \n",
      "                                          aten::__and__         0.10%     551.297us         2.21%      12.211ms      11.764us       0.000us         0.00%      20.020ms      19.288us          1038  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.10%     539.957us         0.10%     539.957us       0.543us       0.000us         0.00%       0.000us       0.000us           995  \n",
      "                                             aten::add_         0.10%     534.497us         0.19%       1.053ms       8.228us     361.159us         0.07%     361.159us       2.822us           128  \n",
      "                                        aten::unsqueeze         0.09%     517.209us         0.12%     655.701us       1.681us       0.000us         0.00%       0.000us       0.000us           390  \n",
      "                                              aten::abs         0.08%     440.745us         0.32%       1.738ms      12.971us     197.793us         0.04%     395.586us       2.952us           134  \n",
      "                                             aten::div_         0.08%     431.425us         0.14%     763.234us      11.926us     191.267us         0.04%     191.267us       2.989us            64  \n",
      "              aten::_scaled_dot_product_flash_attention         0.07%     365.868us         0.47%       2.572ms      40.182us       0.000us         0.00%     557.900us       8.717us            64  \n",
      "                                            aten::fill_         0.06%     344.963us         0.17%     922.964us       8.626us     236.292us         0.05%     236.292us       2.208us           107  \n",
      "                                       aten::empty_like         0.06%     339.985us         0.20%       1.129ms       3.852us       0.000us         0.00%       0.000us       0.000us           293  \n",
      "                                            aten::equal         0.06%     311.867us         5.71%      31.488ms       1.016ms      79.297us         0.02%     219.014us       7.065us            31  \n",
      "                                            aten::clone         0.05%     262.070us         0.35%       1.932ms      14.421us       0.000us         0.00%     484.969us       3.619us           134  \n",
      "                                            aten::addmm         0.05%     248.511us         0.05%     298.436us      49.739us      33.377us         0.01%      33.377us       5.563us             6  \n",
      "                                           aten::unbind         0.05%     248.351us         0.09%     477.269us       3.729us       0.000us         0.00%       0.000us       0.000us           128  \n",
      "                                             aten::item         0.04%     238.864us        37.06%     204.485ms     751.782us       0.000us         0.00%     632.652us       2.326us           272  \n",
      "                                     aten::_unsafe_view         0.04%     238.749us         0.04%     238.749us       0.619us       0.000us         0.00%       0.000us       0.000us           386  \n",
      "                                          aten::squeeze         0.04%     221.655us         0.05%     256.546us       2.004us       0.000us         0.00%       0.000us       0.000us           128  \n",
      "                                           aten::expand         0.04%     207.225us         0.05%     248.483us       1.911us       0.000us         0.00%       0.000us       0.000us           130  \n",
      "                                       aten::zeros_like         0.04%     205.161us         0.25%       1.405ms      14.632us       0.000us         0.00%     211.492us       2.203us            96  \n",
      "                                              aten::all         0.03%     182.407us         0.10%     554.729us      16.810us       9.537us         0.00%      79.621us       2.413us            33  \n",
      "                                               aten::ne         0.03%     160.619us         0.06%     316.514us      10.210us      74.400us         0.01%      74.400us       2.400us            31  \n",
      "                                 cudaDeviceGetAttribute         0.03%     144.667us         0.03%     144.667us       0.378us       0.000us         0.00%       0.000us       0.000us           383  \n",
      "                                          aten::resize_         0.03%     141.552us         0.03%     141.552us       2.051us       0.000us         0.00%       0.000us       0.000us            69  \n",
      "                                            aten::zero_         0.03%     140.669us         0.15%     852.599us       8.881us       0.000us         0.00%     211.492us       2.203us            96  \n",
      "                                          aten::softmax         0.02%     131.982us         0.23%       1.270ms      13.372us       0.000us         0.00%     251.881us       2.651us            95  \n",
      "                                   cudaEventElapsedTime         0.02%     128.280us         0.02%     128.280us       4.009us       0.000us         0.00%       0.000us       0.000us            32  \n",
      "                                               aten::eq         0.02%     123.932us         0.05%     261.607us      18.686us      36.193us         0.01%      36.193us       2.585us            14  \n",
      "                                          aten::permute         0.02%     112.683us         0.02%     129.257us       2.085us       0.000us         0.00%       0.000us       0.000us            62  \n",
      "                aten::_has_compatible_shallow_copy_type         0.02%     110.989us         0.02%     110.989us       0.045us       0.000us         0.00%       0.000us       0.000us          2480  \n",
      "                                             aten::isin         0.02%     106.536us         0.08%     462.060us     154.020us       0.000us         0.00%      28.254us       9.418us             3  \n",
      "                                              aten::any         0.02%     100.741us         0.04%     228.997us      22.900us      24.481us         0.00%      36.162us       3.616us            10  \n",
      "                                           aten::cumsum         0.01%      69.022us         0.03%     141.806us      47.269us      15.424us         0.00%      15.424us       5.141us             3  \n",
      "                                          aten::numpy_T         0.01%      56.714us         0.03%     185.971us       3.000us       0.000us         0.00%       0.000us       0.000us            62  \n",
      "                                  cudaStreamIsCapturing         0.01%      54.252us         0.01%      54.252us       0.848us       0.000us         0.00%       0.000us       0.000us            64  \n",
      "                                     aten::index_select         0.01%      52.851us         0.02%      91.854us      45.927us       8.320us         0.00%       8.320us       4.160us             2  \n",
      "                                      aten::result_type         0.01%      51.642us         0.01%      51.642us       0.397us       0.000us         0.00%       0.000us       0.000us           130  \n",
      "                                              aten::bmm         0.01%      44.955us         0.01%      61.684us      61.684us      93.538us         0.02%      93.538us      93.538us             1  \n",
      "                                          aten::view_as         0.01%      39.468us         0.01%      58.757us       1.632us       0.000us         0.00%       0.000us       0.000us            36  \n",
      "                                       aten::is_nonzero         0.01%      38.428us         0.65%       3.573ms      91.617us       0.000us         0.00%      89.475us       2.294us            39  \n",
      "                                           aten::argmax         0.01%      30.772us         0.01%      49.097us      24.549us      25.185us         0.00%      25.185us      12.592us             2  \n",
      "                                              aten::max         0.01%      28.116us         0.01%      53.453us      26.727us      10.016us         0.00%      10.016us       5.008us             2  \n",
      "                                               aten::lt         0.00%      26.830us         0.01%      50.941us      50.941us       2.496us         0.00%       2.496us       2.496us             1  \n",
      "                                       aten::bitwise_or         0.00%      26.810us         0.01%      53.495us      13.374us       9.857us         0.00%       9.857us       2.464us             4  \n",
      "                                               aten::ge         0.00%      24.868us         0.01%      45.468us      22.734us       4.704us         0.00%       4.704us       2.352us             2  \n",
      "                                        aten::embedding         0.00%      24.137us         0.02%     120.501us      60.250us       0.000us         0.00%       8.320us       4.160us             2  \n",
      "                                     aten::masked_fill_         0.00%      22.365us         0.01%      49.493us      24.746us       4.608us         0.00%       4.608us       2.304us             2  \n",
      "                                      aten::bitwise_not         0.00%      17.939us         0.01%      40.074us      20.037us       4.864us         0.00%       4.864us       2.432us             2  \n",
      "                                          aten::detach_         0.00%      17.790us         0.00%      22.865us       2.858us       0.000us         0.00%       0.000us       0.000us             8  \n",
      "                                         aten::new_ones         0.00%      13.302us         0.01%      42.143us      21.071us       0.000us         0.00%       5.023us       2.511us             2  \n",
      "                                           aten::__or__         0.00%      12.276us         0.01%      65.771us      16.443us       0.000us         0.00%       9.857us       2.464us             4  \n",
      "                                        aten::ones_like         0.00%      11.326us         0.01%      31.200us      31.200us       0.000us         0.00%       2.336us       2.336us             1  \n",
      "                                             aten::rsub         0.00%       7.099us         0.01%      48.094us      24.047us       0.000us         0.00%       5.119us       2.559us             2  \n",
      "                                             aten::full         0.00%       6.970us         0.01%      47.063us      11.766us       0.000us         0.00%       8.928us       2.232us             4  \n",
      "                                             aten::ones         0.00%       6.738us         0.01%      41.238us      41.238us       0.000us         0.00%       2.113us       2.113us             1  \n",
      "                                                detach_         0.00%       5.075us         0.00%       5.075us       0.634us       0.000us         0.00%       0.000us       0.000us             8  \n",
      "                                   aten::_reshape_alias         0.00%       3.492us         0.00%       3.492us       3.492us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                        aten::new_empty         0.00%       2.688us         0.00%       8.438us       4.219us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                     aten::resolve_conj         0.00%       1.865us         0.00%       1.865us       0.186us       0.000us         0.00%       0.000us       0.000us            10  \n",
      "                                       aten::lift_fresh         0.00%       1.532us         0.00%       1.532us       0.192us       0.000us         0.00%       0.000us       0.000us             8  \n",
      "                                      aten::resolve_neg         0.00%       1.202us         0.00%       1.202us       0.120us       0.000us         0.00%       0.000us       0.000us            10  \n",
      "                                    cudaPeekAtLastError         0.00%       1.127us         0.00%       1.127us       0.094us       0.000us         0.00%       0.000us       0.000us            12  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      16.994us         0.00%      16.994us       1.699us            10  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      15.328us         0.00%      15.328us       2.190us             7  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     156.320us         0.03%     156.320us       2.481us            63  \n",
      "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     363.492us         0.07%     363.492us       2.407us           151  \n",
      "                         Memcpy DtoH (Device -> Pinned)         0.00%       0.000us         0.00%       0.000us       0.000us     632.652us         0.12%     632.652us       2.326us           272  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.800us         0.00%       4.800us       2.400us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       9.472us         0.00%       9.472us       2.368us             4  \n",
      "void at_cuda_detail::cub::DeviceScanInitKernel<at_cu...         0.00%       0.000us         0.00%       0.000us       0.000us       6.976us         0.00%       6.976us       2.325us             3  \n",
      "void at_cuda_detail::cub::DeviceScanKernel<at_cuda_d...         0.00%       0.000us         0.00%       0.000us       0.000us       8.448us         0.00%       8.448us       2.816us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       9.634us         0.00%       9.634us       2.409us             4  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       2.400us         0.00%       2.400us       2.400us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      14.305us         0.00%      14.305us       2.384us             6  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.608us         0.00%       4.608us       2.304us             2  \n",
      "void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us       8.320us         0.00%       8.320us       4.160us             2  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       9.537us         0.00%       9.537us       4.769us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     473.678us         0.09%     473.678us       3.588us           132  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     313.126us         0.06%     313.126us       2.409us           130  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     704.848us         0.14%     704.848us       5.422us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     316.075us         0.06%     316.075us       2.431us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     318.952us         0.06%     318.952us       2.453us           130  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     444.648us         0.09%     444.648us       3.420us           130  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us      36.809ms         7.06%      36.809ms      29.926us          1230  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      43.277ms         8.31%      43.277ms      56.058us           772  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us     752.333us         0.14%     752.333us       1.653us           455  \n",
      "sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x...         0.00%       0.000us         0.00%       0.000us       0.000us      21.340ms         4.10%      21.340ms      46.901us           455  \n",
      "void cutlass::Kernel<cutlass_80_wmma_tensorop_s16161...         0.00%       0.000us         0.00%       0.000us       0.000us     616.007us         0.12%     616.007us       6.417us            96  \n",
      "void splitKreduce_kernel<32, 16, int, float, __half,...         0.00%       0.000us         0.00%       0.000us       0.000us     831.924us         0.16%     831.924us       3.681us           226  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       8.940ms         1.72%       8.940ms      27.594us           324  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     463.943us         0.09%     463.943us       3.625us           128  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     847.635us         0.16%     847.635us       4.415us           192  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      41.891ms         8.04%      41.891ms     129.692us           323  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     469.353us         0.09%     469.353us       3.667us           128  \n",
      "void pytorch_flash::flash_fwd_kernel<pytorch_flash::...         0.00%       0.000us         0.00%       0.000us       0.000us     336.745us         0.06%     336.745us      10.523us            32  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     931.550us         0.18%     931.550us       2.646us           352  \n",
      "void (anonymous namespace)::softmax_warp_forward<c10...         0.00%       0.000us         0.00%       0.000us       0.000us     251.881us         0.05%     251.881us       2.651us            95  \n",
      "void at::native::sbtopk::gatherTopK<float, unsigned ...         0.00%       0.000us         0.00%       0.000us       0.000us     242.979us         0.05%     242.979us       7.593us            32  \n",
      "void at::native::bitonicSortKVInPlace<2, -1, 16, 16,...         0.00%       0.000us         0.00%       0.000us       0.000us     204.968us         0.04%     204.968us       6.405us            32  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     291.558us         0.06%     291.558us       4.556us            64  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     191.267us         0.04%     191.267us       2.989us            64  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     211.492us         0.04%     211.492us       2.203us            96  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     632.393us         0.12%     632.393us       3.243us           195  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      20.015ms         3.84%      20.015ms      19.319us          1036  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      14.719ms         2.82%      14.719ms      18.943us           777  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     746.063us         0.14%     746.063us       2.903us           257  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_...         0.00%       0.000us         0.00%       0.000us       0.000us       5.806ms         1.11%       5.806ms      44.663us           130  \n",
      "                         Memcpy PtoP (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     305.059ms        58.55%     305.059ms     255.708us          1193  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us      13.824us         0.00%      13.824us       3.456us             4  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       2.528us         0.00%       2.528us       2.528us             1  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      25.185us         0.00%      25.185us      12.592us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      10.785us         0.00%      10.785us       2.696us             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.119us         0.00%       5.119us       2.559us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     155.427us         0.03%     155.427us       2.429us            64  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       9.857us         0.00%       9.857us       2.464us             4  \n",
      "void at::native::vectorized_elementwise_kernel<2, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.432us         0.00%       2.432us       2.432us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.864us         0.00%       4.864us       2.432us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       5.824us         0.00%       5.824us       2.912us             2  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      10.016us         0.00%      10.016us       5.008us             2  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.000us         0.00%       4.000us       4.000us             1  \n",
      "sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x...         0.00%       0.000us         0.00%       0.000us       0.000us       1.081ms         0.21%       1.081ms      16.894us            64  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us     491.271us         0.09%     491.271us       7.332us            67  \n",
      "void pytorch_flash::flash_fwd_kernel<pytorch_flash::...         0.00%       0.000us         0.00%       0.000us       0.000us     221.155us         0.04%     221.155us       6.911us            32  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us     911.791us         0.17%     911.791us      14.028us            65  \n",
      "void at::native::sbtopk::gatherTopK<float, unsigned ...         0.00%       0.000us         0.00%       0.000us       0.000us     397.583us         0.08%     397.583us       6.311us            63  \n",
      "void at::native::bitonicSortKVInPlace<-2, -1, 16, 16...         0.00%       0.000us         0.00%       0.000us       0.000us     373.739us         0.07%     373.739us       5.932us            63  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     197.793us         0.04%     197.793us       2.952us            67  \n",
      "void at::native::sbtopk::gatherTopK<c10::Half, unsig...         0.00%       0.000us         0.00%       0.000us       0.000us       3.724ms         0.71%       3.724ms      55.576us            67  \n",
      "void at::native::radixSortKVInPlace<-2, -1, 128, 32,...         0.00%       0.000us         0.00%       0.000us       0.000us       1.266ms         0.24%       1.266ms      18.903us            67  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     211.300us         0.04%     211.300us       3.154us            67  \n",
      "void dot_kernel<float, 128, 0, cublasDotParams<cubla...         0.00%       0.000us         0.00%       0.000us       0.000us     224.481us         0.04%     224.481us       3.741us            60  \n",
      "void reduce_1Block_kernel<float, 128, 7, cublasGemvT...         0.00%       0.000us         0.00%       0.000us       0.000us     177.157us         0.03%     177.157us       2.953us            60  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     153.062us         0.03%     153.062us       2.469us            62  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     141.571us         0.03%     141.571us       2.283us            62  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     308.523us         0.06%     308.523us       4.976us            62  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       1.154ms         0.22%       1.154ms      18.619us            62  \n",
      "                       Memcpy DtoH (Device -> Pageable)         0.00%       0.000us         0.00%       0.000us       0.000us      23.649us         0.00%      23.649us       2.365us            10  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      14.305us         0.00%      14.305us       2.861us             5  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      24.481us         0.00%      24.481us       4.896us             5  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       2.528us         0.00%       2.528us       2.528us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 551.721ms\n",
      "Self CUDA time total: 521.063ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 2\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 2\n",
    "test_samples = 4\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "fineweb = load_dataset(\"parquet\",data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text'][:test_samples] \n",
    "\n",
    "print(\"output length is {}\".format(output_length))\n",
    "text = fineweb_text[0]\n",
    "inputs = preprocess_data(text, tokenizer)\n",
    "\n",
    "# cached_mlp.clear_load_from_cpu_stats()\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ]\n",
    ") as p:\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(),\n",
    "            max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "print(p.key_averages().table(\n",
    "    sort_by=\"self_cpu_time_total\", row_limit=-1))\n",
    "p.export_chrome_trace(\"./offloading-2gpu-nostream.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试一个正常输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "Generated length: 16 Time taken: 5.50 s prefill time: 3.18 s\n",
      "['The future of AI is here,  and it’s not as scary as you think.\\n\\nIn the past']\n",
      "decode phase speed: 6.4653  token/s\n",
      "the number of experts reload per token: 7.866666666666666\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 10\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 16\n",
    "device_id = 0\n",
    "test_samples = 1\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "generated_all, decode_time, prefill_time = 0, 0, 0\n",
    "# print(\"max output length is {}\".format(output_length))\n",
    "text = \"The future of AI is here, \"\n",
    "\n",
    "clear_prefill_time = PLLM.get_prefill_time()\n",
    "clear_experts = PLLM.get_reload_experts()\n",
    "# print(\"need to zero: \", clear_experts, clear_prefill_time)\n",
    "\n",
    "inputs = preprocess_data(text, tokenizer)\n",
    "# 测试时间\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "# 开始计时\n",
    "torch.cuda.synchronize()\n",
    "start_event.record()\n",
    "\n",
    "# 前向传播\n",
    "with torch.no_grad():\n",
    "    output = llm.generate(\n",
    "        input_ids=inputs[\"input_ids\"].cuda(device_id),\n",
    "        attention_mask=inputs[\"attention_mask\"].cuda(device_id),\n",
    "        max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "        generation_config=GenerationConfig(do_sample=False),\n",
    "        pad_token_id=tokenizer.pad_token_id, \n",
    "        # cache_implementation=\"static\" ## moe not support\n",
    "    )\n",
    "\n",
    "# 结束计时\n",
    "end_event.record()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# 计算时间\n",
    "elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "decode_time += elapsed_time\n",
    "cur_prefill_time = PLLM.get_prefill_time()\n",
    "prefill_time += cur_prefill_time\n",
    "print(f\"Generated length: {len(output[0]) - input_length}\", f\"Time taken: {elapsed_time:.2f} s\", f\"prefill time: {cur_prefill_time:.2f} s\")\n",
    "# print(output)\n",
    "print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "\n",
    "generated_all += (len(output[0]) - input_length -1)\n",
    "\n",
    "timepertoken = (decode_time - prefill_time) / (generated_all)\n",
    "print(\"decode phase speed:\", '{:.4f}'.format(1/timepertoken) , ' token/s')\n",
    "print(\"the number of experts reload per token:\", PLLM.get_reload_experts() / generated_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载到GPU上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:25<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MixtralForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "import json\n",
    "\n",
    "def get_model(model_name, device_map, dtype=torch.bfloat16):\n",
    "    llm = MixtralForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        use_cache=True,\n",
    "        torch_dtype=dtype,\n",
    "    ) \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return llm, tokenizer\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "with open('../quantize/device_map_1.json', 'r') as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "dtype = torch.float16\n",
    "llm, tokenizer = get_model(model_name, device_map, dtype=dtype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
