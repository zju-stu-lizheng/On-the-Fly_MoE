{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先都加载到cpu上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lz/miniconda3/envs/hqq/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using atten... sdpa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/lz/hqq-master/hqq/models/base.py:251: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(cls.get_weight_file(save_dir), map_location=map_location)\n",
      "100%|██████████| 32/32 [00:00<00:00, 361.79it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 682.83it/s]\n",
      "100%|██████████| 32/32 [05:21<00:00, 10.05s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5,0,1,2\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"\n",
    "from modeling_mixtral import MixtralForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import json\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "dtype = torch.float16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "### HQQ量化\n",
    "from hqq.core.quantize import *\n",
    "from hqq.models.hf.mixtral import MixtralPatch\n",
    "import transformers\n",
    "from hqq.models.base import BaseHQQModel\n",
    "from accelerate import init_empty_weights\n",
    "\n",
    "import gemlite\n",
    "from gemlite import GemLiteLinearTriton \n",
    "GemLiteLinearTriton.get_default_gemv = lambda *args, **kwargs: 'GEMV'\n",
    "\n",
    "save_dir = './hqqsaved'\n",
    "\n",
    "class BaseHQQHFModel(BaseHQQModel):\n",
    "    # Save model architecture\n",
    "    @classmethod\n",
    "    def cache_model(cls, model, save_dir):\n",
    "        model.config.save_pretrained(save_dir)\n",
    "\n",
    "    # Create empty model from config\n",
    "    @classmethod\n",
    "    def create_model(cls, save_dir, kwargs):\n",
    "        model_kwargs = {}\n",
    "        for key in [\"attn_implementation\"]:\n",
    "            if key in kwargs:\n",
    "                model_kwargs[key] = kwargs[key]\n",
    "\n",
    "        config = transformers.AutoConfig.from_pretrained(\n",
    "            cls.get_config_file(save_dir)\n",
    "        )\n",
    "\n",
    "        with init_empty_weights():\n",
    "            model = MixtralForCausalLM._from_config(config, **model_kwargs)\n",
    "\n",
    "        return model\n",
    "\n",
    "class MixtralHQQ(MixtralPatch, BaseHQQHFModel):\n",
    "    pass\n",
    "\n",
    "### 从保存的权重中加载\n",
    "llm = MixtralHQQ.from_quantized(save_dir, compute_dtype=dtype, device='cpu')\n",
    "HQQLinear.set_backend(HQQBackend.PYTORCH)\n",
    "\n",
    "backend       = \"gemlite\" #'torchao_int4' #\"torchao_int4\" (4-bit only) or \"gemlite\" (4-bit + 2-bit)\n",
    "# #Optimize\n",
    "from hqq.utils.patching import prepare_for_inference\n",
    "prepare_for_inference(llm, backend=backend, verbose=True)\n",
    "\n",
    "if(backend == 'gemlite'):\n",
    "\tgemlite.core.GEMLITE_TRITON_RESTRICT_M = True\n",
    "\t# gemlite.core.GemLiteLinear.load_config('/tmp/gemlite_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "active neural num  2867\n",
      "active neural num  2867\n",
      "... loading layer 0\n",
      "... loading layer 1\n",
      "... loading layer 2\n",
      "... loading layer 3\n",
      "... loading layer 4\n",
      "... loading layer 5\n",
      "... loading layer 6\n",
      "... loading layer 7\n",
      "... loading layer 8\n",
      "... loading layer 9\n",
      "... loading layer 10\n",
      "... loading layer 11\n",
      "... loading layer 12\n",
      "... loading layer 13\n",
      "... loading layer 14\n",
      "... loading layer 15\n",
      "... loading layer 16\n",
      "... loading layer 17\n",
      "... loading layer 18\n",
      "... loading layer 19\n",
      "... loading layer 20\n",
      "... loading layer 21\n",
      "... loading layer 22\n",
      "... loading layer 23\n",
      "... loading layer 24\n",
      "... loading layer 25\n",
      "... loading layer 26\n",
      "... loading layer 27\n",
      "... loading layer 28\n",
      "... loading layer 29\n",
      "... loading layer 30\n",
      "... loading layer 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/lz/On-the-Fly_MoE_Inference/offloading/pipelinellm.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(f'../expert_predictor/training/{layer_idx}-{training_epoch}.pth'))\n"
     ]
    }
   ],
   "source": [
    "# device_map = {layer_idx: 'cuda:1' if layer_idx <= 16 else 'cuda:2' for layer_idx in range(1, 32)}\n",
    "device_map = {\n",
    "    layer_idx: 'cuda:1' if layer_idx < 10 else ('cuda:2' if layer_idx <= 20 else 'cuda:3')\n",
    "    for layer_idx in range(1, 32)\n",
    "}\n",
    "from pipelinellm import convert_mixtral_to_cached_mlp, PipelineLLM\n",
    "llm, cached_mlps = convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.8, backends=backend\n",
    "    , device='cuda:0', device_map=device_map)\n",
    "# 创建流水线模型\n",
    "PLLM = PipelineLLM(llm, cached_mlps, 1, 3, training_epoch=20,\n",
    " device='cuda:0', device_map=device_map, print_layer_info=True) ### use ep\n",
    "device_id = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试时间开销"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warm up ...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "\n",
    "input_length = 2\n",
    "MAX_LENGTH = 1\n",
    "output_length = 10\n",
    "test_samples = 1\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "filt_type = fineweb_path.split('.')[-1]\n",
    "fineweb = load_dataset(filt_type, data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text']\n",
    "\n",
    "print(\"warm up ...\")\n",
    "# 预热（避免第一次运行时的额外开销）\n",
    "for text in fineweb_text[:10]:\n",
    "    inputs = preprocess_data(text, tokenizer)\n",
    "    with torch.no_grad():\n",
    "        output = llm(input_ids=inputs[\"input_ids\"].cuda(device_id), attention_mask=inputs[\"attention_mask\"].cuda(device_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 1024\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "Generated length: 1024 Time taken: 137.37 s, prefill time: 21.05 s\n",
      "['Helpful Hints on the Iodine Patch Test\\n\\nThe iodine patch test is a simple, inexpensive, and non-invasive way to determine if you are iodine deficient.\\n\\nThe test is performed by painting a 2 inch by 2 inch square of iodine on the skin. The iodine should be painted on the skin in an area that is not exposed to sunlight. The inner arm is a good place to do the test.\\n\\nThe iodine patch test is performed by painting a 2 inch by 2 inch square of iodine on the skin. The iodine should be painted on the skin in an area that is not exposed to sunlight. The inner arm is a good place to do the test.\\n\\nThe iodine patch test is performed by painting a 2 inch by 2 inch square of iodine on the skin. The iodine should be painted on the skin in an area that is not exposed to sunlight. The inner arm is a good place to do the test.\\n\\nThe iodine patch test is performed by painting a 2 inch by 2 inch square of iodine on the skin. The iodine should be painted on the skin in an area that is not exposed to sunlight. The inner arm is a good place to do the test.\\n\\nThe iodine patch test is performed by painting a 2 inch by 2 inch square of iodine on the skin. The iodine should be painted on the skin in an area that is not exposed to sunlight. The inner arm is a good place to do the test.\\n\\nThe iodine patch test is performed by painting a 2 inch by 2 inch square of iodine on the skin. The iodine should be painted on the skin in an area that is not exposed to sunlight. The inner arm is a good place to do the test.\\n\\nThe iodine patch test is performed by painting a 2 inch by 2 inch square of iodine on the skin. The iodine should be painted on the skin in an area that is not exposed to sunlight. The inner arm is a good place to do the test.\\n\\nThe iodine patch test is performed by painting a 2 inch by 2 inch square of iodine on the skin. The iodine should be painted on the skin in an area that is not exposed to sunlight. The inner arm is a good place to do the test.\\n\\nThe iodine patch test is performed by painting a 2 inch by 2 inch square of iodine on the skin. The iodine should be painted on the skin in an area that is not exposed to sunlight. The inner arm is a good place to do the test.\\n\\nThe iodine patch test is performed by painting a 2 inch by 2 inch square of iodine on the skin. The iodine should be painted on the skin in an area that is not exposed to sunlight. The inner arm is a good place to do the test.\\n\\nThe iodine patch test is performed by painting a 2 inch by 2 inch square of iodine on the skin. The iodine should be painted on the skin in an area that is not exposed to sunlight. The inner arm is a good place to do the test.\\n\\nThe iodine patch test is performed by painting a 2 inch by 2 inch square of iodine on the skin. The iodine should be painted on the skin in an area that is not exposed to sunlight. The inner arm is a good place to do the test.\\n\\nThe iodine patch test is performed by painting a 2 inch by 2 inch square of iodine on the skin. The iodine should be painted on the skin in an area that is not exposed to sunlight. The inner arm is a good place to do the test.\\n\\nThe iodine patch test is performed by painting a 2 inch by 2 inch square of iodine on the skin. The iodine should be painted on the skin in an area that is not exposed to sunlight. The inner arm is a good place to do the test.\\n\\nThe iodine patch test is performed by painting a 2 inch by 2 inch square of iodine on the skin. The iodine should be painted on the skin in an area that is not exposed to sunlight. The inner arm is a good place to do the test.\\n\\nThe iodine patch test is performed by painting a 2 inch by 2 inch square of iodine on the skin. The iodine should be painted on the skin in an area that is not exposed to sunlight. The inner arm is a good place to do the test.\\n\\nThe iodine patch test is performed by painting a ']\n",
      "decode phase speed: 8.7950 token/s\n",
      "the number of reloaded experts per token: 10.961\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 10\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 1024\n",
    "test_samples = 1\n",
    "device_id = 0\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "generated_all = 0\n",
    "prefill_time, decode_time = 0, 0\n",
    "reloaded_experts = 0\n",
    "print(\"output length is {}\".format(output_length))\n",
    "for text in fineweb_text[2:2+test_samples] :\n",
    "    inputs = preprocess_data(text, tokenizer)\n",
    "    ### 清空统计数据\n",
    "    PLLM.get_prefill_time()\n",
    "    PLLM.get_reload_experts()\n",
    "\n",
    "    # 测试时间\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # 开始计时\n",
    "    torch.cuda.synchronize()\n",
    "    start_event.record()\n",
    "\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(device_id),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(device_id),\n",
    "            max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.pad_token_id, \n",
    "            # cache_implementation=\"static\" ## moe not support\n",
    "        )\n",
    "\n",
    "    # 结束计时\n",
    "    end_event.record()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # 计算时间\n",
    "    elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "    decode_time += elapsed_time\n",
    "    cur_prefill_time = PLLM.get_prefill_time()\n",
    "    prefill_time += cur_prefill_time\n",
    "    print(f\"Generated length: {len(output[0]) - input_length}\", f\"Time taken: {elapsed_time:.2f} s,\", f\"prefill time: {cur_prefill_time:.2f} s\")\n",
    "    # print(output)\n",
    "    print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "\n",
    "    generated_all += (len(output[0]) - input_length - 1)\n",
    "    reloaded_experts += PLLM.get_reload_experts()\n",
    "\n",
    "timepertoken = (decode_time - prefill_time) / (generated_all)\n",
    "print(\"decode phase speed:\", '{:.4f}'.format(1/timepertoken) , 'token/s')\n",
    "print(\"the number of reloaded experts per token:\", '{:.3f}'.format(reloaded_experts / generated_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################################################################\n",
    "#Save gemlite cache\n",
    "if(backend == 'gemlite'):\n",
    "\tgemlite.core.GemLiteLinear.cache_config('/tmp/gemlite_config.json') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.profile\n",
    "\n",
    "attention使用sdpa实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 2\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        cudaMemcpyAsync        71.26%       14.814s        71.26%       14.814s       9.071ms       0.000us         0.00%       0.000us       0.000us          1633  \n",
      "                                  cudaDeviceSynchronize        13.71%        2.851s        13.71%        2.851s      30.006ms       5.216us         0.00%       5.216us       0.055us            95  \n",
      "                                  cudaStreamSynchronize        13.63%        2.833s        13.63%        2.833s       7.803ms       0.000us         0.00%       0.000us       0.000us           363  \n",
      "gemlite::gemv_revsplitK_A16fWnO16f_int32packing_forw...         0.22%      45.832ms         0.26%      54.722ms     285.010us       7.226ms         0.02%       7.457ms      38.837us           192  \n",
      "                                       cudaLaunchKernel         0.19%      39.941ms         0.19%      39.941ms       8.318us       0.000us         0.00%       0.000us       0.000us          4802  \n",
      "                                            aten::copy_         0.19%      39.557ms        71.52%       14.867s       8.246ms       40.991s        99.80%       40.991s      22.735ms          1803  \n",
      "                                               aten::mm         0.14%      28.092ms         0.19%      39.501ms      53.889us      68.501ms         0.17%      68.501ms      93.453us           733  \n",
      "                                    aten::empty_strided         0.06%      12.854ms         0.06%      12.854ms       7.683us       0.000us         0.00%       0.000us       0.000us          1673  \n",
      "                                              aten::mul         0.06%      12.278ms         0.09%      19.046ms      21.069us       1.880ms         0.00%       1.880ms       2.080us           904  \n",
      "                                    cudaStreamWaitEvent         0.05%      10.085ms         0.05%      10.085ms       4.396us       0.000us         0.00%       0.000us       0.000us          2294  \n",
      "                                         aten::_to_copy         0.04%       7.537ms        71.58%       14.881s       9.855ms       0.000us         0.00%       40.991s      27.146ms          1510  \n",
      "                                              aten::add         0.03%       6.559ms         0.05%      10.557ms      21.813us     686.470us         0.00%     686.470us       1.418us           484  \n",
      "                                            aten::empty         0.02%       3.784ms         0.02%       3.784ms       6.265us       0.000us         0.00%       0.000us       0.000us           604  \n",
      "                                             aten::view         0.02%       3.562ms         0.02%       3.562ms       2.453us       0.000us         0.00%       0.000us       0.000us          1452  \n",
      "                                       aten::as_strided         0.02%       3.468ms         0.02%       3.468ms       0.894us       0.000us         0.00%       0.000us       0.000us          3880  \n",
      "                                        cudaEventRecord         0.02%       3.421ms         0.02%       3.421ms       1.451us     148.895us         0.00%     148.895us       0.063us          2358  \n",
      "                                           aten::select         0.02%       3.330ms         0.02%       4.048ms       4.139us       0.000us         0.00%       0.000us       0.000us           978  \n",
      "                                               aten::to         0.02%       3.315ms        71.60%       14.884s       4.875ms       0.000us         0.00%       40.991s      13.426ms          3053  \n",
      "                                              aten::cat         0.02%       3.246ms         0.02%       4.709ms      24.025us     626.267us         0.00%     626.267us       3.195us           196  \n",
      "                                           aten::matmul         0.02%       3.201ms         0.21%      44.070ms      60.041us       0.000us         0.00%      68.846ms      93.796us           734  \n",
      "                                        cudaMemsetAsync         0.02%       3.128ms         0.02%       3.128ms       8.103us       0.000us         0.00%       0.000us       0.000us           386  \n",
      "                                             aten::silu         0.01%       3.060ms         0.02%       4.741ms      24.312us     392.801us         0.00%     392.801us       2.014us           195  \n",
      "                                             aten::topk         0.01%       2.785ms         0.03%       5.319ms      55.989us     854.322us         0.00%     854.322us       8.993us            95  \n",
      "                                            aten::index         0.01%       2.669ms         0.02%       4.323ms      33.509us     368.377us         0.00%     368.377us       2.856us           129  \n",
      "                                            aten::slice         0.01%       2.652ms         0.02%       3.314ms       3.606us       0.000us         0.00%       0.000us       0.000us           919  \n",
      "                                           aten::linear         0.01%       2.591ms         0.22%      45.012ms      73.071us       0.000us         0.00%      47.419ms      76.980us           616  \n",
      "                                         cuLaunchKernel         0.01%       2.530ms         0.01%       2.530ms      11.196us       0.000us         0.00%       0.000us       0.000us           226  \n",
      "                                             aten::mean         0.01%       2.508ms         0.02%       3.499ms      26.914us     460.373us         0.00%     460.373us       3.541us           130  \n",
      "                              aten::_local_scalar_dense         0.01%       2.373ms        13.65%        2.838s       9.992ms     400.491us         0.00%     400.491us       1.410us           284  \n",
      "                                            aten::fill_         0.01%       2.369ms         0.03%       5.888ms      19.694us     337.095us         0.00%     337.095us       1.127us           299  \n",
      "                                        aten::transpose         0.01%       2.351ms         0.02%       3.658ms       3.243us       0.000us         0.00%       0.000us       0.000us          1128  \n",
      "                               aten::bitwise_left_shift         0.01%       2.124ms         0.01%       3.064ms      24.710us     148.514us         0.00%     148.514us       1.198us           124  \n",
      "                                              aten::pow         0.01%       2.111ms         0.02%       3.199ms      24.605us     151.714us         0.00%     151.714us       1.167us           130  \n",
      "                                                aten::t         0.01%       2.104ms         0.02%       4.362ms       7.081us       0.000us         0.00%       0.000us       0.000us           616  \n",
      "                         aten::_flash_attention_forward         0.01%       1.961ms         0.02%       4.162ms      65.025us     361.793us         0.00%     361.793us       5.653us            64  \n",
      "                                              aten::neg         0.01%       1.630ms         0.01%       2.515ms      19.647us     279.075us         0.00%     279.075us       2.180us           128  \n",
      "                                            aten::rsqrt         0.01%       1.463ms         0.01%       2.314ms      17.798us     169.378us         0.00%     169.378us       1.303us           130  \n",
      "                                              aten::sum         0.01%       1.444ms         0.01%       1.996ms      31.189us     175.968us         0.00%     175.968us       2.750us            64  \n",
      "                                          aten::reshape         0.01%       1.380ms         0.03%       6.395ms      10.432us       0.000us         0.00%     265.885us       0.434us           613  \n",
      "                                         aten::_softmax         0.01%       1.339ms         0.01%       2.158ms      22.719us     130.020us         0.00%     130.020us       1.369us            95  \n",
      "                                        aten::unsqueeze         0.01%       1.232ms         0.01%       1.572ms       4.030us       0.000us         0.00%       0.000us       0.000us           390  \n",
      "                                             aten::add_         0.01%       1.203ms         0.01%       2.038ms      15.922us     197.667us         0.00%     197.667us       1.544us           128  \n",
      "                                            aten::zero_         0.01%       1.158ms         0.03%       6.840ms      23.751us       0.000us         0.00%     325.062us       1.129us           288  \n",
      "                     aten::scaled_dot_product_attention         0.00%     954.033us         0.03%       6.392ms      99.874us       0.000us         0.00%     361.793us       5.653us            64  \n",
      "                                             aten::div_         0.00%     890.523us         0.01%       1.374ms      21.474us      97.442us         0.00%      97.442us       1.523us            64  \n",
      "                                           aten::unbind         0.00%     707.888us         0.01%       1.152ms       9.003us       0.000us         0.00%       0.000us       0.000us           128  \n",
      "              aten::_scaled_dot_product_flash_attention         0.00%     678.874us         0.03%       5.438ms      84.968us       0.000us         0.00%     361.793us       5.653us            64  \n",
      "                                       aten::empty_like         0.00%     620.848us         0.01%       2.467ms       8.419us       0.000us         0.00%       0.000us       0.000us           293  \n",
      "                                     aten::_unsafe_view         0.00%     601.949us         0.00%     601.949us       1.559us       0.000us         0.00%       0.000us       0.000us           386  \n",
      "                                            aten::equal         0.00%     592.184us         0.01%       2.566ms      82.778us      42.655us         0.00%     116.962us       3.773us            31  \n",
      "                aten::_has_compatible_shallow_copy_type         0.00%     570.836us         0.00%     570.836us       0.128us       0.000us         0.00%       0.000us       0.000us          4464  \n",
      "                                             aten::item         0.00%     552.221us        13.65%        2.838s       9.994ms       0.000us         0.00%     400.491us       1.410us           284  \n",
      "                                          aten::squeeze         0.00%     541.573us         0.00%     644.497us       5.035us       0.000us         0.00%       0.000us       0.000us           128  \n",
      "                                            aten::clone         0.00%     494.987us         0.02%       4.105ms      30.635us       0.000us         0.00%     273.245us       2.039us           134  \n",
      "                                  cudaFuncGetAttributes         0.00%     492.795us         0.00%     492.795us       5.187us       0.000us         0.00%       0.000us       0.000us            95  \n",
      "                                           aten::expand         0.00%     491.697us         0.00%     600.590us       4.620us       0.000us         0.00%       0.000us       0.000us           130  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.00%     423.367us         0.00%     423.367us       0.555us       0.000us         0.00%       0.000us       0.000us           763  \n",
      "                                               aten::ne         0.00%     412.785us         0.00%     705.285us      22.751us      35.138us         0.00%      35.138us       1.133us            31  \n",
      "          cudaOccupancyMaxActiveBlocksPerMultiprocessor         0.00%     410.372us         0.00%     410.372us       2.137us       0.000us         0.00%       0.000us       0.000us           192  \n",
      "                                              aten::all         0.00%     363.622us         0.01%       1.112ms      33.699us       5.919us         0.00%      40.351us       1.223us            33  \n",
      "                                       aten::zeros_like         0.00%     311.102us         0.01%       2.643ms      27.533us       0.000us         0.00%      94.558us       0.985us            96  \n",
      "                                          aten::softmax         0.00%     287.922us         0.01%       2.446ms      25.750us       0.000us         0.00%     130.020us       1.369us            95  \n",
      "                                               aten::eq         0.00%     272.055us         0.00%     434.742us      22.881us      28.095us         0.00%      28.095us       1.479us            19  \n",
      "                                          aten::permute         0.00%     267.963us         0.00%     378.277us       6.101us       0.000us         0.00%       0.000us       0.000us            62  \n",
      "                                 cudaDeviceGetAttribute         0.00%     249.585us         0.00%     249.585us       0.599us       0.000us         0.00%       0.000us       0.000us           417  \n",
      "                                              aten::any         0.00%     236.517us         0.00%     433.556us      28.904us      26.720us         0.00%      33.023us       2.202us            15  \n",
      "                                            aten::addmm         0.00%     182.696us         0.00%     240.905us      40.151us      32.223us         0.00%      32.223us       5.371us             6  \n",
      "                                   cudaFuncSetAttribute         0.00%     152.577us         0.00%     152.577us       2.384us       0.000us         0.00%       0.000us       0.000us            64  \n",
      "                                  cudaStreamIsCapturing         0.00%     119.188us         0.00%     119.188us       1.862us       0.000us         0.00%       0.000us       0.000us            64  \n",
      "                                   cudaEventElapsedTime         0.00%     118.524us         0.00%     118.524us       3.704us      82.233ms         0.20%      82.233ms       2.570ms            32  \n",
      "                                          aten::numpy_T         0.00%     100.981us         0.00%     479.258us       7.730us       0.000us         0.00%       0.000us       0.000us            62  \n",
      "                                              aten::bmm         0.00%     100.472us         0.00%     129.836us     129.836us     345.375us         0.00%     345.375us     345.375us             1  \n",
      "                                      aten::result_type         0.00%      96.228us         0.00%      96.228us       0.740us       0.000us         0.00%       0.000us       0.000us           130  \n",
      "                                             aten::isin         0.00%      89.342us         0.00%     382.782us     127.594us       0.000us         0.00%      15.200us       5.067us             3  \n",
      "                                           aten::cumsum         0.00%      87.333us         0.00%     143.924us      47.975us       9.184us         0.00%       9.184us       3.061us             3  \n",
      "                                              aten::sub         0.00%      74.999us         0.00%     111.183us      22.237us       6.337us         0.00%       6.337us       1.267us             5  \n",
      "                                       aten::bitwise_or         0.00%      60.396us         0.00%     118.218us      29.554us       4.896us         0.00%       4.896us       1.224us             4  \n",
      "                                       aten::is_nonzero         0.00%      59.133us         0.00%     905.527us      23.219us       0.000us         0.00%      52.226us       1.339us            39  \n",
      "                                          aten::view_as         0.00%      56.349us         0.00%     105.825us       2.940us       0.000us         0.00%       0.000us       0.000us            36  \n",
      "                                           aten::argmax         0.00%      45.783us         0.00%      65.700us      32.850us      20.736us         0.00%      20.736us      10.368us             2  \n",
      "                                     aten::index_select         0.00%      41.596us         0.00%      83.939us      41.969us       5.472us         0.00%       5.472us       2.736us             2  \n",
      "                                              aten::max         0.00%      36.666us         0.00%      67.446us      33.723us       5.248us         0.00%       5.248us       2.624us             2  \n",
      "                                               aten::ge         0.00%      33.761us         0.00%      54.088us      27.044us       3.105us         0.00%       3.105us       1.552us             2  \n",
      "                                          aten::detach_         0.00%      33.526us         0.00%      48.592us       3.738us       0.000us         0.00%       0.000us       0.000us            13  \n",
      "                                     aten::masked_fill_         0.00%      29.457us         0.00%      44.224us      22.112us       2.689us         0.00%       2.689us       1.345us             2  \n",
      "                                      aten::bitwise_and         0.00%      27.780us         0.00%      43.759us      21.879us       3.841us         0.00%       3.841us       1.921us             2  \n",
      "                                      aten::bitwise_not         0.00%      26.549us         0.00%      41.456us      20.728us       2.369us         0.00%       2.369us       1.185us             2  \n",
      "                                               aten::lt         0.00%      21.984us         0.00%      32.474us      32.474us       1.696us         0.00%       1.696us       1.696us             1  \n",
      "                                        aten::embedding         0.00%      18.751us         0.00%     110.453us      55.227us       0.000us         0.00%       5.472us       2.736us             2  \n",
      "                                             aten::full         0.00%      15.889us         0.00%      94.636us      23.659us       0.000us         0.00%       3.967us       0.992us             4  \n",
      "                                                detach_         0.00%      15.066us         0.00%      15.066us       1.159us       0.000us         0.00%       0.000us       0.000us            13  \n",
      "                                             aten::rsub         0.00%      14.963us         0.00%      59.778us      29.889us       0.000us         0.00%       2.400us       1.200us             2  \n",
      "                                         aten::new_ones         0.00%      12.467us         0.00%      82.642us      41.321us       0.000us         0.00%       2.496us       1.248us             2  \n",
      "                                          aten::resize_         0.00%      11.731us         0.00%      11.731us       5.866us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                           aten::__or__         0.00%       8.695us         0.00%     126.913us      31.728us       0.000us         0.00%       4.896us       1.224us             4  \n",
      "                                             aten::ones         0.00%       6.646us         0.00%      43.986us      43.986us       0.000us         0.00%       1.153us       1.153us             1  \n",
      "                                     aten::resolve_conj         0.00%       6.290us         0.00%       6.290us       0.286us       0.000us         0.00%       0.000us       0.000us            22  \n",
      "                                        aten::new_empty         0.00%       5.406us         0.00%      24.339us      12.169us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                          aten::__and__         0.00%       5.043us         0.00%      48.802us      24.401us       0.000us         0.00%       3.841us       1.921us             2  \n",
      "                                       aten::lift_fresh         0.00%       4.786us         0.00%       4.786us       0.368us       0.000us         0.00%       0.000us       0.000us            13  \n",
      "                                        aten::ones_like         0.00%       3.968us         0.00%      27.334us      27.334us       0.000us         0.00%       1.120us       1.120us             1  \n",
      "                                      aten::resolve_neg         0.00%       3.160us         0.00%       3.160us       0.144us       0.000us         0.00%       0.000us       0.000us            22  \n",
      "                                   aten::_reshape_alias         0.00%       2.996us         0.00%       2.996us       2.996us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                    cudaPeekAtLastError         0.00%       1.342us         0.00%       1.342us       0.112us       0.000us         0.00%       0.000us       0.000us            12  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       6.878us         0.00%       6.878us       0.459us            15  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       7.264us         0.00%       7.264us       1.038us             7  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      79.457us         0.00%      79.457us       1.261us            63  \n",
      "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     187.170us         0.00%     187.170us       1.134us           165  \n",
      "                         Memcpy DtoH (Device -> Pinned)         0.00%       0.000us         0.00%       0.000us       0.000us     400.491us         0.00%     400.491us       1.410us           284  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.168us         0.00%       3.168us       1.584us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.769us         0.00%       4.769us       1.192us             4  \n",
      "void at_cuda_detail::cub::DeviceScanInitKernel<at_cu...         0.00%       0.000us         0.00%       0.000us       0.000us       3.328us         0.00%       3.328us       1.109us             3  \n",
      "void at_cuda_detail::cub::DeviceScanKernel<at_cuda_d...         0.00%       0.000us         0.00%       0.000us       0.000us       5.856us         0.00%       5.856us       1.952us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.281us         0.00%       5.281us       1.320us             4  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       1.633us         0.00%       1.633us       1.633us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       7.712us         0.00%       7.712us       1.285us             6  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.689us         0.00%       2.689us       1.345us             2  \n",
      "void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us       5.472us         0.00%       5.472us       2.736us             2  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       5.919us         0.00%       5.919us       2.960us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     360.700us         0.00%     360.700us       2.733us           132  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     151.714us         0.00%     151.714us       1.167us           130  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     460.373us         0.00%     460.373us       3.541us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     164.350us         0.00%     164.350us       1.264us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     169.378us         0.00%     169.378us       1.303us           130  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     242.820us         0.00%     242.820us       1.868us           130  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     463.138us         0.00%     463.138us       2.387us           194  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       1.287ms         0.00%       1.287ms       2.508us           513  \n",
      "ampere_fp16_s16816gemm_fp16_64x64_sliced1x2_ldg8_f2f...         0.00%       0.000us         0.00%       0.000us       0.000us       2.912ms         0.01%       2.912ms      45.507us            64  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us     171.918us         0.00%     171.918us       0.445us           386  \n",
      "ampere_fp16_s16816gemm_fp16_64x64_ldg8_f2f_stages_64...         0.00%       0.000us         0.00%       0.000us       0.000us       1.194ms         0.00%       1.194ms      18.662us            64  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     365.753us         0.00%     365.753us       2.857us           128  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     279.075us         0.00%     279.075us       2.180us           128  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     618.043us         0.00%     618.043us       3.219us           192  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     148.156us         0.00%     148.156us       2.315us            64  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     265.885us         0.00%     265.885us       2.077us           128  \n",
      "void pytorch_flash::flash_fwd_kernel<pytorch_flash::...         0.00%       0.000us         0.00%       0.000us       0.000us     176.225us         0.00%     176.225us       5.507us            32  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     492.234us         0.00%     492.234us       1.398us           352  \n",
      "void cutlass::Kernel2<cutlass_80_wmma_tensorop_f16_s...         0.00%       0.000us         0.00%       0.000us       0.000us     597.664us         0.00%     597.664us      18.677us            32  \n",
      "void (anonymous namespace)::softmax_warp_forward<c10...         0.00%       0.000us         0.00%       0.000us       0.000us     130.020us         0.00%     130.020us       1.369us            95  \n",
      "void at::native::sbtopk::gatherTopK<float, unsigned ...         0.00%       0.000us         0.00%       0.000us       0.000us     160.958us         0.00%     160.958us       5.030us            32  \n",
      "void at::native::bitonicSortKVInPlace<2, -1, 16, 16,...         0.00%       0.000us         0.00%       0.000us       0.000us     150.108us         0.00%     150.108us       4.691us            32  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     175.968us         0.00%     175.968us       2.750us            64  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      97.442us         0.00%      97.442us       1.523us            64  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     325.062us         0.00%     325.062us       1.129us           288  \n",
      "ampere_fp16_s16816gemm_fp16_64x64_ldg8_f2f_stages_64...         0.00%       0.000us         0.00%       0.000us       0.000us      48.586ms         0.12%      48.586ms     150.889us           322  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     392.801us         0.00%     392.801us       2.014us           195  \n",
      "          gemv_revsplitK_A16fWnO16f_int32packing_kernel         0.00%       0.000us         0.00%       0.000us       0.000us       7.226ms         0.02%       7.226ms      37.636us           192  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     344.955us         0.00%     344.955us       1.342us           257  \n",
      "                         Memcpy DtoH (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       20.490s        49.88%       20.490s      17.864ms          1147  \n",
      "                         Memcpy HtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       20.500s        49.91%       20.500s      17.873ms          1147  \n",
      "void cutlass::Kernel2<cutlass_80_tensorop_f16_s16816...         0.00%       0.000us         0.00%       0.000us       0.000us     681.150us         0.00%     681.150us     340.575us             2  \n",
      "void cublasLt::splitKreduce_kernel<32, 16, int, __ha...         0.00%       0.000us         0.00%       0.000us       0.000us      10.400us         0.00%      10.400us       5.200us             2  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us       8.224us         0.00%       8.224us       2.056us             4  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       1.312us         0.00%       1.312us       1.312us             1  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      20.736us         0.00%      20.736us      10.368us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.281us         0.00%       5.281us       1.320us             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.400us         0.00%       2.400us       1.200us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      76.741us         0.00%      76.741us       1.199us            64  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.896us         0.00%       4.896us       1.224us             4  \n",
      "void at::native::vectorized_elementwise_kernel<2, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.343us         0.00%       1.343us       1.343us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.369us         0.00%       2.369us       1.185us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       3.841us         0.00%       3.841us       1.921us             2  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       5.248us         0.00%       5.248us       2.624us             2  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.624us         0.00%       2.624us       2.624us             1  \n",
      "ampere_fp16_s16816gemm_fp16_64x64_sliced1x2_ldg8_f2f...         0.00%       0.000us         0.00%       0.000us       0.000us       2.904ms         0.01%       2.904ms      45.369us            64  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us     846.686us         0.00%     846.686us      12.637us            67  \n",
      "void pytorch_flash::flash_fwd_kernel<pytorch_flash::...         0.00%       0.000us         0.00%       0.000us       0.000us     185.568us         0.00%     185.568us       5.799us            32  \n",
      "void dot_kernel<float, 128, 0, cublasDotParams<cubla...         0.00%       0.000us         0.00%       0.000us       0.000us     115.905us         0.00%     115.905us       1.840us            63  \n",
      "void reduce_1Block_kernel<float, 128, 7, cublasGemvT...         0.00%       0.000us         0.00%       0.000us       0.000us      91.648us         0.00%      91.648us       1.455us            63  \n",
      "void at::native::sbtopk::gatherTopK<float, unsigned ...         0.00%       0.000us         0.00%       0.000us       0.000us     268.638us         0.00%     268.638us       4.264us            63  \n",
      "void at::native::bitonicSortKVInPlace<-2, -1, 16, 16...         0.00%       0.000us         0.00%       0.000us       0.000us     274.618us         0.00%     274.618us       4.359us            63  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      77.570us         0.00%      77.570us       1.251us            62  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us      70.944us         0.00%      70.944us       1.144us            62  \n",
      "                       Memcpy DtoH (Device -> Pageable)         0.00%       0.000us         0.00%       0.000us       0.000us      29.312us         0.00%      29.312us       1.332us            22  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      16.064us         0.00%      16.064us       1.606us            10  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      26.720us         0.00%      26.720us       2.672us            10  \n",
      "ampere_fp16_s16816gemm_fp16_64x64_sliced1x2_ldg8_f2f...         0.00%       0.000us         0.00%       0.000us       0.000us      10.766ms         0.03%      10.766ms     173.653us            62  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       1.312us         0.00%       1.312us       1.312us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 20.788s\n",
      "Self CUDA time total: 41.075s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 2\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 2\n",
    "test_samples = 4\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "filt_type = fineweb_path.split('.')[-1]\n",
    "fineweb = load_dataset(filt_type, data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text'][:test_samples] \n",
    "\n",
    "print(\"output length is {}\".format(output_length))\n",
    "text = fineweb_text[0]\n",
    "inputs = preprocess_data(text, tokenizer)\n",
    "\n",
    "# cached_mlp.clear_load_from_cpu_stats()\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ]\n",
    ") as p:\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(),\n",
    "            max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "print(p.key_averages().table(\n",
    "    sort_by=\"self_cpu_time_total\", row_limit=-1))\n",
    "p.export_chrome_trace(\"./offloading-3090-3gpu.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试一个正常输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "Generated length: 16 Time taken: 22.96 s prefill time: 21.02 s\n",
      "['The future of AI is here,  and it’s called ChatGPT.\\n\\nChatGPT is a']\n",
      "decode phase speed: 7.7100  token/s\n",
      "the number of experts reload per token: 7.733333333333333\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 10\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 16\n",
    "test_samples = 1\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "generated_all, decode_time, prefill_time = 0, 0, 0\n",
    "# print(\"max output length is {}\".format(output_length))\n",
    "text = \"The future of AI is here, \"\n",
    "\n",
    "clear_prefill_time = PLLM.get_prefill_time()\n",
    "clear_experts = PLLM.get_reload_experts()\n",
    "# print(\"need to zero: \", clear_experts, clear_prefill_time)\n",
    "\n",
    "inputs = preprocess_data(text, tokenizer)\n",
    "# 测试时间\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "# 开始计时\n",
    "torch.cuda.synchronize()\n",
    "start_event.record()\n",
    "\n",
    "# 前向传播\n",
    "with torch.no_grad():\n",
    "    output = llm.generate(\n",
    "        input_ids=inputs[\"input_ids\"].cuda(device_id),\n",
    "        attention_mask=inputs[\"attention_mask\"].cuda(device_id),\n",
    "        max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "        generation_config=GenerationConfig(do_sample=False),\n",
    "        pad_token_id=tokenizer.pad_token_id, \n",
    "        # cache_implementation=\"static\" ## moe not support\n",
    "    )\n",
    "\n",
    "# 结束计时\n",
    "end_event.record()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# 计算时间\n",
    "elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "decode_time += elapsed_time\n",
    "cur_prefill_time = PLLM.get_prefill_time()\n",
    "prefill_time += cur_prefill_time\n",
    "print(f\"Generated length: {len(output[0]) - input_length}\", f\"Time taken: {elapsed_time:.2f} s\", f\"prefill time: {cur_prefill_time:.2f} s\")\n",
    "# print(output)\n",
    "print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "\n",
    "generated_all += (len(output[0]) - input_length -1)\n",
    "\n",
    "timepertoken = (decode_time - prefill_time) / (generated_all)\n",
    "print(\"decode phase speed:\", '{:.4f}'.format(1/timepertoken) , ' token/s')\n",
    "print(\"the number of experts reload per token:\", PLLM.get_reload_experts() / generated_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载到GPU上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:25<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MixtralForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "import json\n",
    "\n",
    "def get_model(model_name, device_map, dtype=torch.bfloat16):\n",
    "    llm = MixtralForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        use_cache=True,\n",
    "        torch_dtype=dtype,\n",
    "    ) \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return llm, tokenizer\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "with open('../quantize/device_map_1.json', 'r') as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "dtype = torch.float16\n",
    "llm, tokenizer = get_model(model_name, device_map, dtype=dtype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
