{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å…ˆéƒ½åŠ è½½åˆ°cpuä¸Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:06<00:00,  3.01it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,2\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"\n",
    "from modeling_mixtral import MixtralForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import json\n",
    "\n",
    "def get_model(model_name, device_map, dtype=torch.bfloat16):\n",
    "    llm = MixtralForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        use_cache=True,\n",
    "        torch_dtype=dtype,\n",
    "    ) \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return llm, tokenizer\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "with open(\"../quantize/device_map.json\", \"r\") as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "dtype = torch.float16\n",
    "llm, tokenizer = get_model(model_name, 'cpu', dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 117.97it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:11<00:00,  2.23s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:17<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from hqq.core.quantize import *\n",
    "from hqq.models.hf.mixtral import MixtralHQQ\n",
    "\n",
    "q3_config    = BaseQuantizeConfig(nbits=2, group_size=64)\n",
    "quant_config      = {'block_sparse_moe.experts.w3'   : q3_config}\n",
    "\n",
    "#### å…ˆæ”¾CUDAé‡åŒ–ï¼Œç„¶åå†ä¼ å›CPU\n",
    "MixtralHQQ.quantize_model(llm, quant_config=quant_config, compute_dtype=dtype, device='cuda:0')\n",
    "HQQLinear.set_backend(HQQBackend.PYTORCH)\n",
    "\n",
    "backend       = \"gemlite\" #'torchao_int4' #\"torchao_int4\" (4-bit only) or \"gemlite\" (4-bit + 2-bit)\n",
    "#Optimize\n",
    "from hqq.utils.patching import prepare_for_inference\n",
    "prepare_for_inference(llm, backend=backend, verbose=True)\n",
    "#Load GemLite cache\n",
    "if(backend == 'gemlite'):\n",
    "\timport gemlite\n",
    "\tgemlite.core.GEMLITE_TRITON_RESTRICT_M = True\n",
    "\tgemlite.core.GemLiteLinear.load_config('/tmp/gemlite_config.json')\n",
    "\t\n",
    "llm.to('cpu')\n",
    "print(llm.model.layers[0].block_sparse_moe.experts[0].w1.device)\n",
    "print(llm.model.layers[0].block_sparse_moe.experts[0].w3.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import threading\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "from queue import Queue\n",
    "\n",
    "class CachedMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, dtype, sparsity: float = 0.2):\n",
    "        super(CachedMLP, self).__init__()\n",
    "        self.sparsity = sparsity\n",
    "        self.activenum = int((1 - sparsity) * hidden_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dtype = dtype\n",
    "\n",
    "        print(\"active neural num \",self.activenum)\n",
    "\n",
    "        self.activation = nn.SiLU()\n",
    "\n",
    "        # GPU ç¼“å­˜å¼ é‡\n",
    "        self.w1_gpu = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda:0')\n",
    "        self.w2_gpu = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda:0')\n",
    "        self.w3_gpu = None\n",
    "\n",
    "        # ç¬¬äºŒä¸ªä¸“å®¶çš„ GPU ç¼“å­˜å¼ é‡\n",
    "        self.w1_gpu_expert1 = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda:0')\n",
    "        self.w2_gpu_expert1 = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda:0')\n",
    "        self.w3_gpu_expert1 = None\n",
    "\n",
    "        # Pinned Memory ç¼“å†²åŒº\n",
    "        self.register_buffer('sparse_w1_cpu', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "        self.register_buffer('sparse_w2_cpu', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "\n",
    "        self.sparse_w1_cpu = self.sparse_w1_cpu.pin_memory()\n",
    "        self.sparse_w2_cpu = self.sparse_w2_cpu.pin_memory()\n",
    "\n",
    "        # ç¬¬äºŒä¸ªä¸“å®¶çš„ Pinned Memory ç¼“å†²åŒº\n",
    "        self.register_buffer('sparse_w1_cpu_expert1', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "        self.register_buffer('sparse_w2_cpu_expert1', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "\n",
    "        self.sparse_w1_cpu_expert1 = self.sparse_w1_cpu_expert1.pin_memory()\n",
    "        self.sparse_w2_cpu_expert1 = self.sparse_w2_cpu_expert1.pin_memory()\n",
    "\n",
    "        self.expert0_weight = torch.tensor(0)\n",
    "        self.expert1_weight = torch.tensor(0)\n",
    "\n",
    "        # ç»Ÿè®¡ä¿¡æ¯\n",
    "        self.load_from_cpu_time = 0.0\n",
    "        self.load_from_cpu_calls = 0\n",
    "\n",
    "    def load_from_cpu(self, cpu_mlp, cpu_mlp_expert1, stream: torch.cuda.Stream):\n",
    "        \"\"\"\n",
    "        ä»CPUåŠ è½½å‚æ•°ï¼Œå¹¶ä½¿ç”¨æŒ‡å®šçš„CUDAæµè¿›è¡Œå¼‚æ­¥å¤åˆ¶åˆ°GPUã€‚\n",
    "        \n",
    "        å‚æ•°:\n",
    "            cpu_mlp: åŒ…å«CPUä¸Šå‚æ•°çš„å­—å…¸ï¼ˆç¬¬ä¸€ä¸ªä¸“å®¶ï¼‰ã€‚\n",
    "            cpu_mlp_expert1: åŒ…å«CPUä¸Šå‚æ•°çš„å­—å…¸ï¼ˆç¬¬äºŒä¸ªä¸“å®¶ï¼‰ã€‚\n",
    "            stream: ç”¨äºæ•°æ®ä¼ è¾“çš„CUDAæµã€‚\n",
    "        \"\"\"\n",
    "        # ç”Ÿæˆéšæœºç´¢å¼•\n",
    "        random_indices = torch.randperm(cpu_mlp['w1'].data.size(0))[:self.activenum]\n",
    "        # sorted_indices = torch.sort(random_indices).values\n",
    "\n",
    "        # ä»CPUåŠ è½½å‚æ•°ï¼ˆç¬¬ä¸€ä¸ªä¸“å®¶ï¼‰\n",
    "        self.sparse_w1_cpu.copy_(cpu_mlp['w1'].data[random_indices, :])\n",
    "        self.sparse_w2_cpu.copy_(cpu_mlp['w2'].data[random_indices, :])\n",
    "        # ä»CPUåŠ è½½å‚æ•°ï¼ˆç¬¬äºŒä¸ªä¸“å®¶ï¼‰\n",
    "        self.sparse_w1_cpu_expert1.copy_(cpu_mlp_expert1['w1'].data[random_indices, :])\n",
    "        self.sparse_w2_cpu_expert1.copy_(cpu_mlp_expert1['w2'].data[random_indices, :])\n",
    "        # å¼‚æ­¥å¤åˆ¶åˆ°GPU\n",
    "        with torch.cuda.stream(stream):\n",
    "            self.w1_gpu.copy_(self.sparse_w1_cpu, non_blocking=True)\n",
    "            self.w2_gpu.copy_(self.sparse_w2_cpu, non_blocking=True)\n",
    "            self.w1_gpu_expert1.copy_(self.sparse_w1_cpu_expert1, non_blocking=True)\n",
    "            self.w2_gpu_expert1.copy_(self.sparse_w2_cpu_expert1, non_blocking=True)\n",
    "        \n",
    "        # ç›´æ¥èµ‹å€¼ w3_gpu å’Œ w3_gpu_expert1\n",
    "        # å›ºå®šåœ¨GPUä¸Šçš„w3\n",
    "        self.w3_gpu = cpu_mlp['w3']\n",
    "        self.w3_gpu_expert1 = cpu_mlp_expert1['w3']\n",
    "\n",
    "    def load_expert_weights(self, expert_weights):\n",
    "        self.expert0_weight = expert_weights[0]\n",
    "        self.expert1_weight = expert_weights[1]\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        \"\"\"\n",
    "        æ ¹æ®hidden_statesï¼Œ åˆ†åˆ«è®¡ç®—ä¸¤ä¸ªä¸“å®¶çš„è¾“å‡º\n",
    "        \"\"\"\n",
    "        # ç¬¬ä¸€ä¸ªä¸“å®¶çš„è®¡ç®—\n",
    "        w3_output = self.w3_gpu(hidden_states)[:, :self.activenum]\n",
    "        w1_output = self.activation(torch.matmul(hidden_states, self.w1_gpu.T))\n",
    "        # w2 = self.w2_gpu.T\n",
    "        hidden_states_expert0 = torch.matmul(w1_output * w3_output, self.w2_gpu)\n",
    "\n",
    "        # ç¬¬äºŒä¸ªä¸“å®¶çš„è®¡ç®—\n",
    "        w3_output_expert1 = self.w3_gpu_expert1(hidden_states)[:, :self.activenum]\n",
    "        w1_output_expert1 = self.activation(torch.matmul(hidden_states, self.w1_gpu_expert1.T))\n",
    "        # w2_expert1 = self.w2_gpu_expert1.T\n",
    "        hidden_states_expert1 = torch.matmul(w1_output_expert1 * w3_output_expert1, self.w2_gpu_expert1)\n",
    "\n",
    "        final_hidden_states = hidden_states_expert0* self.expert0_weight + hidden_states_expert1* self.expert1_weight\n",
    "        \n",
    "        return final_hidden_states\n",
    "                        \n",
    "def convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.9):\n",
    "    ### å…¶ä»–éƒ¨åˆ†å­˜æ”¾åœ¨GPUä¸Š\n",
    "    llm.model.embed_tokens.cuda(0)\n",
    "    for i in range(len(llm.model.layers)):\n",
    "        llm.model.layers[i].self_attn.cuda(0)\n",
    "        llm.model.layers[i].input_layernorm.cuda(0)\n",
    "        llm.model.layers[i].post_attention_layernorm.cuda(0)\n",
    "        llm.model.layers[i].block_sparse_moe.gate.cuda(0)\n",
    "        for j in range(len(llm.model.layers[0].block_sparse_moe.experts)):\n",
    "            llm.model.layers[i].block_sparse_moe.experts[j].w3.cuda(0)\n",
    "    ### ç¬¬0å±‚çš„ä¸“å®¶å­˜æ”¾åœ¨GPUä¸Š\n",
    "    for j in range(len(llm.model.layers[0].block_sparse_moe.experts)):\n",
    "        llm.model.layers[0].block_sparse_moe.experts[j].cuda(0)\n",
    "\n",
    "    llm.model.norm.cuda(0)\n",
    "    llm.lm_head.cuda(0)\n",
    "    \n",
    "    # åˆ›å»ºä¸¤ä¸ªå…±äº«çš„CachedMLPå®ä¾‹\n",
    "    buffer0 = CachedMLP(\n",
    "        input_dim=llm.config.hidden_size,\n",
    "        hidden_dim=llm.config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "        sparsity=sparsity\n",
    "    )\n",
    "    buffer1 = CachedMLP(\n",
    "        input_dim=llm.config.hidden_size,\n",
    "        hidden_dim=llm.config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "        sparsity=sparsity\n",
    "    )\n",
    "    cached_mlps = [buffer0, buffer1]\n",
    "    \n",
    "    for i, layer in enumerate(llm.model.layers):\n",
    "        if i==0:\n",
    "            continue\n",
    "        # å°†ä¸“å®¶çš„forwardæ–¹æ³•æ›¿æ¢ä¸ºPipelineLLMç®¡ç†çš„æ–¹å¼\n",
    "        for j, expert in enumerate(layer.block_sparse_moe.experts):\n",
    "            expert.cpu_mlp = {\n",
    "                \"w1\": expert.w1.cpu().weight,\n",
    "                \"w2\": expert.w2.cpu().weight.T.contiguous(),\n",
    "                \"w3\": expert.w3,\n",
    "            }\n",
    "    return llm, cached_mlps\n",
    "\n",
    "class PipelineLLM:\n",
    "    def __init__(self, llm, cached_mlps):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ– PipelineLLMï¼Œæ›¿æ¢æ¨¡å‹æ¯ä¸€å±‚çš„ forward æ–¹æ³•ã€‚\n",
    "        \n",
    "        å‚æ•°:\n",
    "            llm: åŸå§‹çš„å¤§æ¨¡å‹\n",
    "            cached_mlps: ä¸¤ä¸ª CachedMLP å®ä¾‹åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.cached_mlps = cached_mlps  # [buffer0, buffer1]\n",
    "        self.num_layers = len(llm.model.layers)\n",
    "        self.lock = threading.Lock()\n",
    "        self.use_buffer0 = True  # æ ‡è®°å½“å‰ä½¿ç”¨å“ªä¸ªç¼“å†²åŒº\n",
    "\n",
    "        # åˆ›å»ºä¸¤ä¸ªå…±äº«çš„CUDAæµ\n",
    "        self.stream0 = torch.cuda.Stream()\n",
    "        self.stream1 = torch.cuda.Stream()\n",
    "\n",
    "        # åˆå§‹åŒ–åŠ è½½ç¬¬ä¸€ä¸ªå’Œç¬¬äºŒä¸ªå±‚çš„å‚æ•°\n",
    "        # self._load_layer(1, buffer_index=0, expert_ids=torch.tensor([0, 1]))\n",
    "        # self._load_layer(1, buffer_index=1, expert_ids=torch.tensor([0, 1]))\n",
    "        self.top_k = 2\n",
    "        self.activation = nn.SiLU()\n",
    "\n",
    "        self._replace_forward_methods()\n",
    "\n",
    "        # ç”¨äºç»Ÿè®¡æ—¶é—´çš„å˜é‡\n",
    "        self.total_prefill_time = 0.0\n",
    "        self.total_decode_time = 0.0\n",
    "\n",
    "    def _load_layer(self, layer_idx, buffer_index, expert_ids, expert_weights=torch.tensor([0, 0])):\n",
    "        \"\"\"\n",
    "        åŠ è½½æŒ‡å®šå±‚çš„å‚æ•°åˆ°æŒ‡å®šçš„ç¼“å†²åŒºã€‚\n",
    "        \n",
    "        å‚æ•°:\n",
    "            layer_idx: å±‚çš„ç´¢å¼•\n",
    "            buffer_index: ç¼“å†²åŒºçš„ç´¢å¼•ï¼ˆ0 æˆ– 1ï¼‰\n",
    "        \"\"\"\n",
    "        layer = self.llm.model.layers[layer_idx]\n",
    "        expert0 = layer.block_sparse_moe.experts[expert_ids[0]]\n",
    "        expert1 = layer.block_sparse_moe.experts[expert_ids[1]]\n",
    "        # if layer_idx == 1:\n",
    "        #     print(expert_ids[0].data, expert_ids[1].data, '{:.3f}, {:.3f}'.format(expert_weights[0], expert_weights[1]))\n",
    "\n",
    "        cpu_mlp = expert0.cpu_mlp\n",
    "        cpu_mlp_expert1 = expert1.cpu_mlp\n",
    "        buffer = self.cached_mlps[buffer_index]\n",
    "        stream = self.stream0 if buffer_index == 0 else self.stream1\n",
    "\n",
    "        buffer.load_expert_weights(expert_weights)\n",
    "        # å¼‚æ­¥åŠ è½½å‚æ•°\n",
    "        buffer.load_from_cpu(cpu_mlp, cpu_mlp_expert1, stream)\n",
    "\n",
    "    def _replace_forward_methods(self):\n",
    "        \"\"\"\n",
    "        æ›¿æ¢æ¨¡å‹æ¯ä¸€å±‚çš„ forward æ–¹æ³•ï¼Œæ·»åŠ å‚æ•°é¢„åŠ è½½é€»è¾‘å’Œæ³¨æ„åŠ›è®¡ç®—ã€‚\n",
    "        \"\"\"\n",
    "        for i, layer in enumerate(self.llm.model.layers):\n",
    "            def new_forward(hidden_states: torch.Tensor,\n",
    "                        attention_mask: Optional[torch.Tensor] = None,\n",
    "                        position_ids: Optional[torch.LongTensor] = None,\n",
    "                        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "                        output_attentions: Optional[bool] = False,\n",
    "                        output_router_logits: Optional[bool] = False,\n",
    "                        use_cache: Optional[bool] = False,\n",
    "                        cache_position: Optional[torch.LongTensor] = None,\n",
    "                        layer=layer,\n",
    "                        layer_idx=i):\n",
    "                with self.lock:\n",
    "                    batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "                    \n",
    "                    if sequence_length == 1:\n",
    "                        #### decode phase ####\n",
    "                        # é€‰æ‹©å½“å‰ä½¿ç”¨çš„ç¼“å†²åŒº\n",
    "                        current_buffer = self.cached_mlps[0] if self.use_buffer0 else self.cached_mlps[1]\n",
    "\n",
    "                        next_buffer_index = 1 if self.use_buffer0 else 0\n",
    "\n",
    "                        next_layer_idx = layer_idx + 1\n",
    "\n",
    "                        if next_layer_idx < self.num_layers:\n",
    "                            # é¢„åŠ è½½ä¸‹ä¸€å±‚çš„å‚æ•°\n",
    "                            next_layer = self.llm.model.layers[next_layer_idx]\n",
    "                            router = next_layer.block_sparse_moe.gate\n",
    "\n",
    "                            # batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "                            hidden_states_flat = hidden_states.view(-1, hidden_dim)\n",
    "                            # router_logits: (batch * sequence_length, n_experts)\n",
    "                            router_logits = router(hidden_states_flat)\n",
    "\n",
    "                            routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "                            routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n",
    "                            routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n",
    "\n",
    "                            hidden_states = hidden_states_flat.reshape(batch_size, sequence_length, hidden_dim)\n",
    "\n",
    "                            self._load_layer(\n",
    "                                next_layer_idx,\n",
    "                                buffer_index=next_buffer_index,\n",
    "                                expert_ids=selected_experts[0],\n",
    "                                expert_weights=routing_weights[0]\n",
    "                            )\n",
    "\n",
    "                        # åˆ‡æ¢ç¼“å†²åŒº\n",
    "                        self.use_buffer0 = not self.use_buffer0\n",
    "\n",
    "                    # å¤„ç†å½“å‰å±‚\n",
    "                    residual = hidden_states\n",
    "                    hidden_states = layer.input_layernorm(hidden_states)\n",
    "\n",
    "                    # Self Attention\n",
    "                    hidden_states, self_attn_weights, present_key_value = layer.self_attn(\n",
    "                        hidden_states=hidden_states,\n",
    "                        attention_mask=attention_mask,\n",
    "                        position_ids=position_ids,\n",
    "                        past_key_value=past_key_value,\n",
    "                        output_attentions=output_attentions,\n",
    "                        use_cache=use_cache,\n",
    "                        cache_position=cache_position,\n",
    "                    )\n",
    "                    hidden_states = residual + hidden_states\n",
    "\n",
    "                    # Fully Connected\n",
    "                    residual = hidden_states\n",
    "                    hidden_states = layer.post_attention_layernorm(hidden_states)\n",
    "                    \n",
    "                    if sequence_length > 1:\n",
    "                        # print(\"in prefill layer \", layer_idx)\n",
    "                        # å¯¹äºprefillé˜¶æ®µï¼Œä»…å°†expertsåŠ è½½åˆ°GPUè®¡ç®—\n",
    "                        experts = layer.block_sparse_moe.experts\n",
    "\n",
    "                        # å°†expertsç§»åŠ¨åˆ°GPU\n",
    "                        for expert in experts:\n",
    "                            expert.cuda(0)\n",
    "\n",
    "                        # åœ¨GPUä¸Šè¿›è¡ŒMoEè®¡ç®—ï¼ˆgateä¿æŒåœ¨CPUï¼‰\n",
    "                        final_hidden_states, router_logits = layer.block_sparse_moe(hidden_states)\n",
    "\n",
    "                        # è®¡ç®—å®Œæˆåå°†expertsç§»å›CPU\n",
    "                        if layer_idx != 0:\n",
    "                            for expert in experts:\n",
    "                                expert.to('cpu')\n",
    "                    else:\n",
    "                        # batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "                        hidden_states_flat = hidden_states.view(-1, hidden_dim)\n",
    "                        # print(\"in decode layer\", layer_idx)\n",
    "                        if layer_idx > 0:\n",
    "                            ### ä½¿ç”¨å½“å‰ç¼“å†²åŒºè¿›è¡Œ MLP è®¡ç®— ###\n",
    "                            final_hidden_states = current_buffer(hidden_states_flat)\n",
    "                        else:\n",
    "                            ### æ ¹æ®routerè®¡ç®—éœ€è¦ä½¿ç”¨çš„ä¸“å®¶ ###\n",
    "                            cur_layer = layer\n",
    "                            router = cur_layer.block_sparse_moe.gate\n",
    "                            # router_logits: (batch * sequence_length, n_experts)\n",
    "                            router_logits = router(hidden_states_flat)\n",
    "\n",
    "                            routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "                            routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n",
    "                            routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n",
    "                            # we cast back to the input dtype\n",
    "                            routing_weights = routing_weights.to(hidden_states_flat.dtype)\n",
    "\n",
    "                            first_expert, second_expert = selected_experts[0][0], selected_experts[0][1]\n",
    "\n",
    "                            final_hidden_states_expert0 = cur_layer.block_sparse_moe.experts[first_expert](\n",
    "                                hidden_states_flat) * routing_weights[0][0]\n",
    "\n",
    "                            final_hidden_states_expert1 = cur_layer.block_sparse_moe.experts[second_expert](\n",
    "                                hidden_states_flat) * routing_weights[0][1]\n",
    "\n",
    "                            # å°†ä¸¤ä¸ªä¸“å®¶çš„ç»“æœç›¸åŠ \n",
    "                            final_hidden_states = final_hidden_states_expert0 + final_hidden_states_expert1\n",
    "\n",
    "                        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n",
    "\n",
    "                    hidden_states = residual + final_hidden_states\n",
    "\n",
    "                    outputs = (hidden_states,)\n",
    "\n",
    "                    if output_attentions:\n",
    "                        outputs += (self_attn_weights,)\n",
    "\n",
    "                    if use_cache:\n",
    "                        outputs += (present_key_value,)\n",
    "\n",
    "                    return outputs\n",
    "\n",
    "            # æ›¿æ¢forwardæ–¹æ³•\n",
    "            layer.forward = new_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "active neural num  2867\n",
      "active neural num  2867\n"
     ]
    }
   ],
   "source": [
    "llm, cached_mlps = convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.8)\n",
    "\n",
    "# åˆ›å»ºæµæ°´çº¿æ¨¡å‹\n",
    "PLLM = PipelineLLM(llm, cached_mlps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æµ‹è¯•æ—¶é—´å¼€é”€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 32\n",
      "Generated output length: 32 Time taken: 4.6101 seconds\n",
      "['passing ingÃ¥r passing particularly cref except warning obvious coinc idiot except except consist except occasional except idiot except stupid consistcipe especially except except pseudo except obviously except except consist except occasional']\n",
      "Generated output length: 32 Time taken: 4.5986 seconds\n",
      "['fortunate Douglas repeatedly: history tro history subsequently and history repeatedly history history history andihood history repeatedlyihood fucking history especially sacihood history especiallyihood desired fucking history history history']\n",
      "Generated output length: 32 Time taken: 4.4382 seconds\n",
      "['lightsĞ‡ responsibilityğŸŒ columns convenient occasional BAS occasional entity convenient entityğŸŒ legitimate D vess consistent please subs Du oppongest consistent pseudoEntities Mix legitimate purposes alleged legitimate consistent concert']\n",
      "Generated output length: 3 Time taken: 0.4483 seconds\n",
      "['pizza Chain']\n",
      "Generated output length: 32 Time taken: 4.4258 seconds\n",
      "['nels mock Mock mock mock WARRAN delen whilst independ Route JuRouting facimation mockmock whilst consist Jun laug noten arteFMT daily session Sm laugihoodtagonrupalagma consistent']\n",
      "Generated output length: 32 Time taken: 4.1988 seconds\n",
      "['asticsearchHub peaceful withdrawementeiekâ—„ FITNESS Wisolas demonstriesaironment\\ue934rolled otherwiseacia\\ue934inderriteriaĞºĞ²Ğ° evolutionicer gigdexironment Bryan%%%% min evidentâ¶ vess']\n",
      "Generated output length: 32 Time taken: 3.9207 seconds\n",
      "['agreed /******/ /******/ clients Organ Hem incre ages secretcx agre Id simultaneouslyicals none oppon Parad oppon neighb noten Ã¡l__.eturn Lab Fab;</prite whilst SI\\ufeff Accihood']\n",
      "Generated output length: 32 Time taken: 3.8963 seconds\n",
      "['pretendncia pret restrtrfs intellect invent --( /******/ /******/ CD /******/ arrestâ—„ Ship biologie ingÃ¥r san Lap prevglied /******/cleglied RAM Except requested yesterday remind replğŸŒ mock']\n",
      "Generated output length: 32 Time taken: 3.8448 seconds\n",
      "['Cav nextscan age age Tax planet Stopokal kicking transparent parking grat gross FITNESSzym ages legit excess genu derivative genuampionship Stationato legitimate Argument oppon suspension opponktet Flag']\n",
      "Generated output length: 32 Time taken: 3.8264 seconds\n",
      "['~~ ~~ ~~ ~~ ~~ ~~ shitty ~~ ~~ hilar ~~ ~~ ~~ ~~ hilar ~~pgfscope hilar ~~ hilar ~~ gepubliceerd joke mock ~~ denmock mock laughingbbra allegolean']\n",
      "decode phase speed: 7.6162  token/s\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "\n",
    "input_length = 1\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 32\n",
    "test_samples = 10\n",
    "device_id = 0\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# ä½¿ç”¨ tokenizer å°†æ–‡æœ¬æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "filt_type = fineweb_path.split('.')[-1]\n",
    "fineweb = load_dataset(filt_type, data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text']\n",
    "\n",
    "# é¢„çƒ­ï¼ˆé¿å…ç¬¬ä¸€æ¬¡è¿è¡Œæ—¶çš„é¢å¤–å¼€é”€ï¼‰\n",
    "for text in fineweb_text[:5] :\n",
    "    inputs = preprocess_data(text, tokenizer)\n",
    "    with torch.no_grad():\n",
    "        output = llm(input_ids=inputs[\"input_ids\"].cuda(device_id), attention_mask=inputs[\"attention_mask\"].cuda(device_id))\n",
    "\n",
    "generated_all = 0\n",
    "prefill_time, decode_time = 0, 0\n",
    "print(\"output length is {}\".format(output_length))\n",
    "for text in fineweb_text[:test_samples] :\n",
    "    inputs = preprocess_data(text, tokenizer)\n",
    "\n",
    "    # é¢„çƒ­ï¼ˆé¿å…ç¬¬ä¸€æ¬¡è¿è¡Œæ—¶çš„é¢å¤–å¼€é”€ï¼‰\n",
    "    with torch.no_grad():\n",
    "        output = llm(input_ids=inputs[\"input_ids\"].cuda(device_id), attention_mask=inputs[\"attention_mask\"].cuda(device_id))\n",
    "\n",
    "    # æµ‹è¯•æ—¶é—´\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # å¼€å§‹è®¡æ—¶\n",
    "    torch.cuda.synchronize()\n",
    "    start_event.record()\n",
    "\n",
    "    # å‰å‘ä¼ æ’­\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(device_id),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(device_id),\n",
    "            max_length=input_length + output_length,  # æ€»é•¿åº¦ä¸ºè¾“å…¥é•¿åº¦ + è¾“å‡ºé•¿åº¦\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.pad_token_id, \n",
    "            # cache_implementation=\"static\" ## moe not support\n",
    "        )\n",
    "\n",
    "    # ç»“æŸè®¡æ—¶\n",
    "    end_event.record()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # è®¡ç®—æ—¶é—´\n",
    "    elapsed_time = start_event.elapsed_time(end_event) / 1000  # è½¬æ¢ä¸ºç§’\n",
    "    decode_time += elapsed_time\n",
    "    print(f\"Generated output length: {len(output[0]) - input_length}\", f\"Time taken: {elapsed_time:.4f} seconds\")\n",
    "    # print(output)\n",
    "    print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "\n",
    "    generated_all += len(output[0]) - input_length\n",
    "\n",
    "timepertoken = (decode_time) / (generated_all)\n",
    "# print(\"decode time:\", '{:.4f}'.format((decode_time) /test_samples), ' s')\n",
    "print(\"decode phase speed:\", '{:.4f}'.format(1/timepertoken) , ' token/s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################################################################\n",
    "#Save gemlite cache\n",
    "if(backend == 'gemlite'):\n",
    "\tgemlite.core.GemLiteLinear.cache_config('/tmp/gemlite_config.json') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.profile\n",
    "\n",
    "trace-offloading-r.jsonæ˜¯æœ€ä¼˜ï¼Œå°±æ˜¯åšå®Œä¸€ä¸ªindexå°±ä¼ ä¸€ä¸ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 2\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::index        39.35%     106.719ms        39.93%     108.282ms     287.220us       1.010ms         0.42%       1.010ms       2.680us           377  \n",
      "                                            aten::copy_        16.02%      43.441ms        17.80%      48.267ms      53.158us     203.157ms        84.09%     203.157ms     223.741us           908  \n",
      "gemlite::gemv_revsplitK_A16fWnO16f_int32packing_forw...         8.72%      23.656ms         9.93%      26.919ms     210.302us       3.706ms         1.53%       4.683ms      36.585us           128  \n",
      "                                       cudaLaunchKernel         7.58%      20.564ms         7.58%      20.564ms       5.606us       0.000us         0.00%       0.000us       0.000us          3668  \n",
      "                                               aten::mm         4.00%      10.857ms         5.56%      15.078ms      26.086us      11.550ms         4.78%      11.550ms      19.982us           578  \n",
      "                                         aten::randperm         2.53%       6.861ms         5.16%      13.986ms     112.786us       0.000us         0.00%       0.000us       0.000us           124  \n",
      "                                              aten::mul         2.22%       6.034ms         3.67%       9.957ms      12.832us       6.157ms         2.55%       6.157ms       7.935us           776  \n",
      "                                  cudaStreamSynchronize         1.81%       4.918ms         1.81%       4.918ms      34.879us       0.000us         0.00%       0.000us       0.000us           141  \n",
      "                                              aten::add         1.32%       3.584ms         2.17%       5.873ms      12.936us       3.167ms         1.31%       3.167ms       6.975us           454  \n",
      "                                        cudaMemcpyAsync         1.19%       3.216ms         1.19%       3.216ms       7.960us       0.000us         0.00%       0.000us       0.000us           404  \n",
      "                                            aten::slice         1.14%       3.104ms         1.43%       3.883ms       2.629us       0.000us         0.00%       0.000us       0.000us          1477  \n",
      "                                              aten::cat         0.80%       2.170ms         1.21%       3.293ms      16.799us       1.742ms         0.72%       1.742ms       8.886us           196  \n",
      "                                       aten::as_strided         0.78%       2.124ms         0.78%       2.124ms       0.588us       0.000us         0.00%       0.000us       0.000us          3610  \n",
      "                                            aten::empty         0.77%       2.082ms         0.77%       2.082ms       3.517us       0.000us         0.00%       0.000us       0.000us           592  \n",
      "                                             aten::mean         0.64%       1.733ms         0.90%       2.453ms      18.871us       1.141ms         0.47%       1.141ms       8.774us           130  \n",
      "                                    aten::empty_strided         0.63%       1.710ms         0.63%       1.710ms       5.090us       0.000us         0.00%       0.000us       0.000us           336  \n",
      "                                             aten::view         0.58%       1.574ms         0.58%       1.574ms       1.022us       0.000us         0.00%       0.000us       0.000us          1540  \n",
      "                                              aten::pow         0.58%       1.569ms         0.90%       2.442ms      18.785us     783.480us         0.32%     783.480us       6.027us           130  \n",
      "                                             aten::topk         0.52%       1.405ms         1.05%       2.852ms      44.565us       1.471ms         0.61%       1.471ms      22.984us            64  \n",
      "                                           aten::matmul         0.48%       1.310ms         6.31%      17.124ms      29.627us       0.000us         0.00%      11.550ms      19.982us           578  \n",
      "                                             aten::silu         0.43%       1.176ms         0.70%       1.891ms      14.773us       1.089ms         0.45%       1.089ms       8.504us           128  \n",
      "                         aten::_flash_attention_forward         0.42%       1.140ms         1.00%       2.704ms      42.247us     776.688us         0.32%     776.688us      12.136us            64  \n",
      "                                          aten::reshape         0.41%       1.120ms         1.65%       4.466ms       5.012us       0.000us         0.00%       1.017ms       1.141us           891  \n",
      "                                              aten::neg         0.41%       1.102ms         0.66%       1.780ms      13.907us       1.036ms         0.43%       1.036ms       8.096us           128  \n",
      "                                        aten::transpose         0.40%       1.072ms         0.60%       1.629ms       1.935us       0.000us         0.00%       0.000us       0.000us           842  \n",
      "                                            aten::rsqrt         0.38%       1.034ms         0.63%       1.698ms      13.060us     776.368us         0.32%     776.368us       5.972us           130  \n",
      "                                         aten::_to_copy         0.34%     912.836us         2.15%       5.821ms      21.638us       0.000us         0.00%       1.983ms       7.373us           269  \n",
      "                                           aten::select         0.33%     906.626us         0.40%       1.084ms       2.745us       0.000us         0.00%       0.000us       0.000us           395  \n",
      "                                              aten::sum         0.31%     831.426us         0.44%       1.180ms      18.439us     699.060us         0.29%     699.060us      10.923us            64  \n",
      "                                                aten::t         0.30%     824.302us         0.57%       1.552ms       4.704us       0.000us         0.00%       0.000us       0.000us           330  \n",
      "                                    cudaLaunchKernelExC         0.30%     802.603us         0.30%     802.603us       5.990us       0.000us         0.00%       0.000us       0.000us           134  \n",
      "                                  cudaFuncGetAttributes         0.29%     792.855us         0.29%     792.855us       4.004us       0.000us         0.00%       0.000us       0.000us           198  \n",
      "                                               aten::to         0.27%     718.670us         2.41%       6.539ms      12.065us       0.000us         0.00%       1.983ms       3.659us           542  \n",
      "                                         cuLaunchKernel         0.26%     710.112us         0.26%     710.112us       5.548us       0.000us         0.00%       0.000us       0.000us           128  \n",
      "                                            aten::fill_         0.26%     705.159us         0.61%       1.642ms      11.810us       1.001ms         0.41%       1.001ms       7.199us           139  \n",
      "                     aten::scaled_dot_product_attention         0.26%     700.470us         1.57%       4.249ms      66.395us       0.000us         0.00%     776.688us      12.136us            64  \n",
      "                                           aten::linear         0.26%     699.727us         5.28%      14.322ms      43.399us       0.000us         0.00%       5.929ms      17.968us           330  \n",
      "                              aten::_local_scalar_dense         0.25%     682.991us         2.48%       6.735ms      49.523us       1.179ms         0.49%       1.179ms       8.672us           136  \n",
      "                                         aten::_softmax         0.23%     620.780us         0.37%     995.938us      15.562us     426.602us         0.18%     426.602us       6.666us            64  \n",
      "                                             aten::div_         0.21%     561.467us         0.34%     909.961us      14.218us     606.958us         0.25%     606.958us       9.484us            64  \n",
      "                                        aten::unsqueeze         0.19%     515.571us         0.24%     659.250us       2.516us       0.000us         0.00%       0.000us       0.000us           262  \n",
      "              aten::_scaled_dot_product_flash_attention         0.19%     514.951us         1.31%       3.549ms      55.450us       0.000us         0.00%     776.688us      12.136us            64  \n",
      "                                            aten::zero_         0.16%     437.854us         0.71%       1.914ms      14.957us       0.000us         0.00%     977.047us       7.633us           128  \n",
      "                                            aten::clone         0.14%     370.939us         0.98%       2.668ms      19.908us       0.000us         0.00%       1.032ms       7.699us           134  \n",
      "                                          aten::permute         0.12%     335.897us         0.15%     410.520us       3.311us       0.000us         0.00%       0.000us       0.000us           124  \n",
      "                                     aten::_unsafe_view         0.12%     327.751us         0.12%     327.751us       0.849us       0.000us         0.00%       0.000us       0.000us           386  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.12%     324.983us         0.12%     324.983us       0.626us       0.000us         0.00%       0.000us       0.000us           519  \n",
      "                                       aten::empty_like         0.12%     321.949us         0.45%       1.213ms       6.159us       0.000us         0.00%       0.000us       0.000us           197  \n",
      "                                           aten::expand         0.12%     316.227us         0.14%     390.388us       3.050us       0.000us         0.00%       0.000us       0.000us           128  \n",
      "                                             aten::item         0.06%     169.437us         2.55%       6.905ms      50.769us       0.000us         0.00%       1.179ms       8.672us           136  \n",
      "                                 cudaDeviceGetAttribute         0.05%     139.089us         0.05%     139.089us       0.537us       0.000us         0.00%       0.000us       0.000us           259  \n",
      "                                          aten::numpy_T         0.05%     136.553us         0.20%     547.073us       4.412us       0.000us         0.00%       0.000us       0.000us           124  \n",
      "                                   cudaFuncSetAttribute         0.05%     133.154us         0.05%     133.154us       1.548us       0.000us         0.00%       0.000us       0.000us            86  \n",
      "                                    aten::scalar_tensor         0.05%     126.600us         0.05%     126.600us       2.042us       0.000us         0.00%       0.000us       0.000us            62  \n",
      "                                          aten::softmax         0.05%     124.020us         0.41%       1.120ms      17.499us       0.000us         0.00%     426.602us       6.666us            64  \n",
      "                                  cudaDeviceSynchronize         0.04%     102.507us         0.04%     102.507us     102.507us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                               aten::eq         0.04%      96.520us         0.05%     143.785us      15.976us      22.401us         0.01%      22.401us       2.489us             9  \n",
      "                                  cudaStreamIsCapturing         0.03%      78.628us         0.03%      78.628us       1.229us       0.000us         0.00%       0.000us       0.000us            64  \n",
      "                                             aten::isin         0.03%      75.662us         0.12%     330.803us     110.268us       0.000us         0.00%      28.000us       9.333us             3  \n",
      "                                           aten::cumsum         0.03%      71.883us         0.04%     116.065us      38.688us      15.776us         0.01%      15.776us       5.259us             3  \n",
      "                                      aten::result_type         0.02%      66.766us         0.02%      66.766us       0.514us       0.000us         0.00%       0.000us       0.000us           130  \n",
      "                                        cudaMemsetAsync         0.02%      61.873us         0.02%      61.873us      10.312us       0.000us         0.00%       0.000us       0.000us             6  \n",
      "                                              aten::sub         0.02%      60.663us         0.03%      91.860us      18.372us      11.904us         0.00%      11.904us       2.381us             5  \n",
      "                                          aten::resize_         0.02%      56.157us         0.02%      56.157us       0.446us       0.000us         0.00%       0.000us       0.000us           126  \n",
      "                                       aten::bitwise_or         0.02%      50.084us         0.04%      96.168us      24.042us      10.080us         0.00%      10.080us       2.520us             4  \n",
      "                                              aten::any         0.02%      47.595us         0.04%     120.736us      24.147us       0.000us         0.00%      11.200us       2.240us             5  \n",
      "                                           aten::argmax         0.02%      46.197us         0.03%      73.889us      36.944us      25.600us         0.01%      25.600us      12.800us             2  \n",
      "                                     aten::index_select         0.02%      43.121us         0.03%      72.319us      36.159us       7.840us         0.00%       7.840us       3.920us             2  \n",
      "                                              aten::max         0.01%      35.866us         0.02%      55.732us      27.866us       9.472us         0.00%       9.472us       4.736us             2  \n",
      "                                     aten::masked_fill_         0.01%      28.342us         0.01%      40.575us      20.287us       4.864us         0.00%       4.864us       2.432us             2  \n",
      "                                      aten::bitwise_not         0.01%      25.220us         0.02%      41.185us      20.592us       4.737us         0.00%       4.737us       2.369us             2  \n",
      "                                               aten::ge         0.01%      24.088us         0.01%      34.570us      17.285us       4.673us         0.00%       4.673us       2.337us             2  \n",
      "                                      aten::bitwise_and         0.01%      22.639us         0.01%      36.345us      18.173us       5.313us         0.00%       5.313us       2.657us             2  \n",
      "                                              aten::all         0.01%      21.586us         0.02%      44.357us      22.179us       4.704us         0.00%       6.816us       3.408us             2  \n",
      "                                               aten::lt         0.01%      21.422us         0.01%      29.114us      29.114us       2.432us         0.00%       2.432us       2.432us             1  \n",
      "                                        aten::embedding         0.01%      20.368us         0.04%      97.293us      48.647us       0.000us         0.00%       7.840us       3.920us             2  \n",
      "                                         aten::new_ones         0.01%      14.327us         0.02%      61.729us      30.864us       0.000us         0.00%       4.512us       2.256us             2  \n",
      "                                             aten::rsub         0.00%      13.511us         0.02%      54.942us      27.471us       0.000us         0.00%       5.120us       2.560us             2  \n",
      "                                             aten::full         0.00%      12.280us         0.03%      73.019us      18.255us       0.000us         0.00%       8.320us       2.080us             4  \n",
      "                                       aten::is_nonzero         0.00%      11.203us         0.07%     182.265us      22.783us       0.000us         0.00%      20.130us       2.516us             8  \n",
      "                                           aten::__or__         0.00%       9.485us         0.04%     105.653us      26.413us       0.000us         0.00%      10.080us       2.520us             4  \n",
      "                                          aten::view_as         0.00%       6.710us         0.00%       9.831us       1.639us       0.000us         0.00%       0.000us       0.000us             6  \n",
      "                                          aten::__and__         0.00%       6.435us         0.02%      42.780us      21.390us       0.000us         0.00%       5.313us       2.657us             2  \n",
      "                                          aten::detach_         0.00%       4.945us         0.00%       6.910us       2.303us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                             aten::ones         0.00%       4.890us         0.01%      21.975us      21.975us       0.000us         0.00%       2.048us       2.048us             1  \n",
      "                                        aten::ones_like         0.00%       4.261us         0.01%      16.635us      16.635us       0.000us         0.00%       2.176us       2.176us             1  \n",
      "                                        aten::new_empty         0.00%       3.913us         0.00%      12.877us       6.439us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                                detach_         0.00%       1.965us         0.00%       1.965us       0.655us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                    cudaPeekAtLastError         0.00%       1.279us         0.00%       1.279us       0.107us       0.000us         0.00%       0.000us       0.000us            12  \n",
      "                                       aten::lift_fresh         0.00%       0.572us         0.00%       0.572us       0.191us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       8.448us         0.00%       8.448us       1.690us             5  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      14.944us         0.01%      14.944us       2.135us             7  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.592us         0.00%       2.592us       2.592us             1  \n",
      "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      35.070us         0.01%      35.070us       2.338us            15  \n",
      "                         Memcpy DtoH (Device -> Pinned)         0.00%       0.000us         0.00%       0.000us       0.000us       1.179ms         0.49%       1.179ms       8.672us           136  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       7.105us         0.00%       7.105us       2.368us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       8.736us         0.00%       8.736us       2.184us             4  \n",
      "void at_cuda_detail::cub::DeviceScanInitKernel<at_cu...         0.00%       0.000us         0.00%       0.000us       0.000us       7.328us         0.00%       7.328us       2.443us             3  \n",
      "void at_cuda_detail::cub::DeviceScanKernel<at_cuda_d...         0.00%       0.000us         0.00%       0.000us       0.000us       8.448us         0.00%       8.448us       2.816us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      12.256us         0.01%      12.256us       2.451us             5  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      14.657us         0.01%      14.657us       2.443us             6  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.864us         0.00%       4.864us       2.432us             2  \n",
      "void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us       7.840us         0.00%       7.840us       3.920us             2  \n",
      "void dot_kernel<float, 128, 0, cublasDotParams<cubla...         0.00%       0.000us         0.00%       0.000us       0.000us     503.239us         0.21%     503.239us       7.863us            64  \n",
      "void reduce_1Block_kernel<float, 128, 7, cublasGemvT...         0.00%       0.000us         0.00%       0.000us       0.000us     436.263us         0.18%     436.263us       6.817us            64  \n",
      "void (anonymous namespace)::softmax_warp_forward<c10...         0.00%       0.000us         0.00%       0.000us       0.000us     426.602us         0.18%     426.602us       6.666us            64  \n",
      "void at::native::sbtopk::gatherTopK<float, unsigned ...         0.00%       0.000us         0.00%       0.000us       0.000us     747.634us         0.31%     747.634us      11.682us            64  \n",
      "void at::native::bitonicSortKVInPlace<-2, -1, 16, 16...         0.00%       0.000us         0.00%       0.000us       0.000us     723.341us         0.30%     723.341us      11.302us            64  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     699.060us         0.29%     699.060us      10.923us            64  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     606.958us         0.25%     606.958us       9.484us            64  \n",
      "                         Memcpy HtoD (Pinned -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     200.121ms        82.83%     200.121ms     806.941us           248  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     988.017us         0.41%     988.017us       7.485us           132  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     783.480us         0.32%     783.480us       6.027us           130  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       1.141ms         0.47%       1.141ms       8.774us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     774.508us         0.32%     774.508us       5.958us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     776.368us         0.32%     776.368us       5.972us           130  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     928.951us         0.38%     928.951us       7.146us           130  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     986.837us         0.41%     986.837us       7.476us           132  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.823ms         0.75%       1.823ms       7.067us           258  \n",
      "sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x...         0.00%       0.000us         0.00%       0.000us       0.000us       2.921ms         1.21%       2.921ms      22.822us           128  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       1.402ms         0.58%       1.402ms      10.956us           128  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.007ms         0.42%       1.007ms       7.866us           128  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       2.186ms         0.90%       2.186ms       8.409us           260  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       1.036ms         0.43%       1.036ms       8.096us           128  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us       1.167ms         0.48%       1.167ms       9.119us           128  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.381ms         0.99%       2.381ms       7.441us           320  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       1.017ms         0.42%       1.017ms       7.945us           128  \n",
      "void pytorch_flash::flash_fwd_kernel<pytorch_flash::...         0.00%       0.000us         0.00%       0.000us       0.000us     776.688us         0.32%     776.688us      12.136us            64  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us      28.546us         0.01%      28.546us       4.758us             6  \n",
      "sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x...         0.00%       0.000us         0.00%       0.000us       0.000us     401.320us         0.17%     401.320us      66.887us             6  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.089ms         0.45%       1.089ms       8.504us           128  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     977.047us         0.40%     977.047us       7.633us           128  \n",
      "          gemv_revsplitK_A16fWnO16f_int32packing_kernel         0.00%       0.000us         0.00%       0.000us       0.000us       3.706ms         1.53%       3.706ms      28.952us           128  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_...         0.00%       0.000us         0.00%       0.000us       0.000us     205.604us         0.09%     205.604us      51.401us             4  \n",
      "void splitKreduce_kernel<32, 16, int, float, __half,...         0.00%       0.000us         0.00%       0.000us       0.000us      30.689us         0.01%      30.689us       7.672us             4  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       2.618ms         1.08%       2.618ms      21.111us           124  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       3.003ms         1.24%       3.003ms      24.215us           124  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       1.208ms         0.50%       1.208ms       9.746us           124  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us      14.879us         0.01%      14.879us       3.720us             4  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      25.600us         0.01%      25.600us      12.800us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      10.433us         0.00%      10.433us       2.608us             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.120us         0.00%       5.120us       2.560us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.376us         0.00%       5.376us       2.688us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      10.080us         0.00%      10.080us       2.520us             4  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       2.560us         0.00%       2.560us       2.560us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.737us         0.00%       4.737us       2.369us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       5.313us         0.00%       5.313us       2.657us             2  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       9.472us         0.00%       9.472us       4.736us             2  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.392us         0.00%       3.392us       3.392us             1  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       4.704us         0.00%       4.704us       4.704us             1  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     559.594us         0.23%     559.594us       8.744us            64  \n",
      "void at::native::vectorized_elementwise_kernel<2, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.592us         0.00%       2.592us       2.592us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 271.195ms\n",
      "Self CUDA time total: 241.604ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 1\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 2\n",
    "test_samples = 4\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# ä½¿ç”¨ tokenizer å°†æ–‡æœ¬æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "fineweb = load_dataset(\"parquet\",data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text'][:test_samples] \n",
    "\n",
    "print(\"output length is {}\".format(output_length))\n",
    "text = fineweb_text[0]\n",
    "inputs = preprocess_data(text, tokenizer)\n",
    "\n",
    "# cached_mlp.clear_load_from_cpu_stats()\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ]\n",
    ") as p:\n",
    "    # å‰å‘ä¼ æ’­\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(),\n",
    "            max_length=input_length + output_length,  # æ€»é•¿åº¦ä¸ºè¾“å…¥é•¿åº¦ + è¾“å‡ºé•¿åº¦\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "print(p.key_averages().table(\n",
    "    sort_by=\"self_cpu_time_total\", row_limit=-1))\n",
    "p.export_chrome_trace(\"./offloading-hqq2-2.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åŠ è½½åˆ°GPUä¸Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:25<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MixtralForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "import json\n",
    "\n",
    "def get_model(model_name, device_map, dtype=torch.bfloat16):\n",
    "    llm = MixtralForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        use_cache=True,\n",
    "        torch_dtype=dtype,\n",
    "    ) \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return llm, tokenizer\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "with open('../quantize/device_map_1.json', 'r') as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "dtype = torch.float16\n",
    "llm, tokenizer = get_model(model_name, device_map, dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### åªä¼ ä¸€ä¸ªä¸“å®¶çš„ç‰ˆæœ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import threading\n",
    "import json\n",
    "from queue import Queue\n",
    "\n",
    "class CachedMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, dtype, sparsity: float = 0.2):\n",
    "        super(CachedMLP, self).__init__()\n",
    "        self.sparsity = sparsity\n",
    "        self.activenum = int((1 - sparsity) * hidden_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dtype = dtype\n",
    "\n",
    "        # GPU ç¼“å­˜å¼ é‡\n",
    "        self.w1_gpu = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda:0')\n",
    "        self.w2_gpu = torch.empty((self.input_dim, self.activenum), dtype=self.dtype, device='cuda:0')\n",
    "        self.w3_gpu = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda:0')\n",
    "\n",
    "        # Pinned Memory ç¼“å†²åŒº\n",
    "        self.register_buffer('sparse_w1_cpu', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "        self.register_buffer('sparse_w2_cpu', torch.empty((self.input_dim, self.activenum), dtype=self.dtype, device='cpu'))\n",
    "        self.register_buffer('sparse_w3_cpu', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "        self.sparse_w1_cpu = self.sparse_w1_cpu.pin_memory()\n",
    "        self.sparse_w2_cpu = self.sparse_w2_cpu.pin_memory()\n",
    "        self.sparse_w3_cpu = self.sparse_w3_cpu.pin_memory()\n",
    "\n",
    "        # ç»Ÿè®¡ä¿¡æ¯\n",
    "        self.load_from_cpu_time = 0.0\n",
    "        self.load_from_cpu_calls = 0\n",
    "\n",
    "    def load_from_cpu(self, cpu_mlp, stream: torch.cuda.Stream):\n",
    "        \"\"\"\n",
    "        ä»CPUåŠ è½½å‚æ•°ï¼Œå¹¶ä½¿ç”¨æŒ‡å®šçš„CUDAæµè¿›è¡Œå¼‚æ­¥å¤åˆ¶åˆ°GPUã€‚\n",
    "        \n",
    "        å‚æ•°:\n",
    "            cpu_mlp: åŒ…å«CPUä¸Šå‚æ•°çš„å­—å…¸ã€‚\n",
    "            stream: ç”¨äºæ•°æ®ä¼ è¾“çš„CUDAæµã€‚\n",
    "        \"\"\"\n",
    "        # ä»CPUåŠ è½½å‚æ•°\n",
    "        self.sparse_w1_cpu.copy_(cpu_mlp['w1'].data[:self.activenum, :])\n",
    "        self.sparse_w2_cpu.copy_(cpu_mlp['w2'].data[:, :self.activenum])\n",
    "        self.sparse_w3_cpu.copy_(cpu_mlp['w3'].data[:self.activenum, :])\n",
    "\n",
    "        # å¼‚æ­¥å¤åˆ¶åˆ°GPU\n",
    "        with torch.cuda.stream(stream):\n",
    "            self.w1_gpu.copy_(self.sparse_w1_cpu, non_blocking=True)\n",
    "            self.w2_gpu.copy_(self.sparse_w2_cpu, non_blocking=True)\n",
    "            self.w3_gpu.copy_(self.sparse_w3_cpu, non_blocking=True)\n",
    "\n",
    "    def get_load_from_cpu_stats(self):\n",
    "        if self.load_from_cpu_calls == 0:\n",
    "            return 0.0, 0.0\n",
    "        avg_time = self.load_from_cpu_time / self.load_from_cpu_calls\n",
    "        return self.load_from_cpu_time, avg_time\n",
    "\n",
    "    def clear_load_from_cpu_stats(self):\n",
    "        self.load_from_cpu_time = 0.0\n",
    "        self.load_from_cpu_calls = 0\n",
    "\n",
    "def convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.9):\n",
    "    ### å…¶ä»–éƒ¨åˆ†å­˜æ”¾åœ¨GPUä¸Š\n",
    "    llm.model.embed_tokens.cuda()\n",
    "    for i in range(len(llm.model.layers)):\n",
    "        llm.model.layers[i].self_attn.cuda()\n",
    "        llm.model.layers[i].input_layernorm.cuda()\n",
    "        llm.model.layers[i].post_attention_layernorm.cuda()\n",
    "        llm.model.layers[i].block_sparse_moe.gate.cuda()\n",
    "    ### ç¬¬0å±‚çš„ä¸“å®¶å­˜æ”¾åœ¨GPUä¸Š\n",
    "    for j in range(len(llm.model.layers[0].block_sparse_moe.experts)):\n",
    "        llm.model.layers[0].block_sparse_moe.experts[j].cuda()\n",
    "\n",
    "    llm.model.norm.cuda()\n",
    "    llm.lm_head.cuda()\n",
    "    \n",
    "    # åˆ›å»ºä¸¤ä¸ªå…±äº«çš„CachedMLPå®ä¾‹\n",
    "    buffer0 = CachedMLP(\n",
    "        input_dim=llm.config.hidden_size,\n",
    "        hidden_dim=llm.config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "        sparsity=sparsity\n",
    "    )\n",
    "    buffer1 = CachedMLP(\n",
    "        input_dim=llm.config.hidden_size,\n",
    "        hidden_dim=llm.config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "        sparsity=sparsity\n",
    "    )\n",
    "    cached_mlps = [buffer0, buffer1]\n",
    "    \n",
    "    for i, layer in enumerate(llm.model.layers):\n",
    "        if i==0:\n",
    "            continue\n",
    "        # å°†ä¸“å®¶çš„forwardæ–¹æ³•æ›¿æ¢ä¸ºPipelineLLMç®¡ç†çš„æ–¹å¼\n",
    "        for j, expert in enumerate(layer.block_sparse_moe.experts):\n",
    "            expert.cpu_mlp = {\n",
    "                \"w1\": expert.w1.cpu().weight,\n",
    "                \"w2\": expert.w2.cpu().weight,\n",
    "                \"w3\": expert.w3.cpu().weight,\n",
    "            }\n",
    "            # æ›¿æ¢forwardæ–¹æ³•ä¸ºç›´æ¥è°ƒç”¨CachedMLPçš„forwardï¼ˆéœ€è¦åœ¨pipelineLLMé‡Œé¢æ›¿æ¢)\n",
    "            # expert.forward = lambda x, cached_mlp=cached_mlp, cpu_mlp=expert.cpu_mlp: cached_mlp(x, cpu_mlp)\n",
    "    return llm, cached_mlps\n",
    "\n",
    "class PipelineLLM:\n",
    "    def __init__(self, llm, cached_mlps):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ– PipelineLLMï¼Œæ›¿æ¢æ¨¡å‹æ¯ä¸€å±‚çš„ forward æ–¹æ³•ã€‚\n",
    "        \n",
    "        å‚æ•°:\n",
    "            llm: åŸå§‹çš„å¤§æ¨¡å‹\n",
    "            cached_mlps: ä¸¤ä¸ª CachedMLP å®ä¾‹åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.cached_mlps = cached_mlps  # [buffer0, buffer1]\n",
    "        self.num_layers = len(llm.model.layers)\n",
    "        self.lock = threading.Lock()\n",
    "        self.use_buffer0 = True  # æ ‡è®°å½“å‰ä½¿ç”¨å“ªä¸ªç¼“å†²åŒº\n",
    "\n",
    "        # åˆ›å»ºä¸¤ä¸ªå…±äº«çš„CUDAæµ\n",
    "        self.stream0 = torch.cuda.Stream()\n",
    "        self.stream1 = torch.cuda.Stream()\n",
    "\n",
    "        # åˆå§‹åŒ–åŠ è½½ç¬¬ä¸€ä¸ªå’Œç¬¬äºŒä¸ªå±‚çš„å‚æ•°\n",
    "        self._load_layer(1, buffer_index=0)\n",
    "        self._load_layer(1, buffer_index=1)\n",
    "\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "        self._replace_forward_methods()\n",
    "    \n",
    "    def _load_layer(self, layer_idx, buffer_index):\n",
    "        \"\"\"\n",
    "        åŠ è½½æŒ‡å®šå±‚çš„å‚æ•°åˆ°æŒ‡å®šçš„ç¼“å†²åŒºã€‚\n",
    "        \n",
    "        å‚æ•°:\n",
    "            layer_idx: å±‚çš„ç´¢å¼•\n",
    "            buffer_index: ç¼“å†²åŒºçš„ç´¢å¼•ï¼ˆ0 æˆ– 1ï¼‰\n",
    "        \"\"\"\n",
    "        layer = self.llm.model.layers[layer_idx]\n",
    "        expert = layer.block_sparse_moe.experts[0]\n",
    "        cpu_mlp = expert.cpu_mlp\n",
    "        buffer = self.cached_mlps[buffer_index]\n",
    "        stream = self.stream0 if buffer_index == 0 else self.stream1\n",
    "\n",
    "        # å¼‚æ­¥åŠ è½½å‚æ•°\n",
    "        buffer.load_from_cpu(cpu_mlp, stream)\n",
    "\n",
    "    def _replace_forward_methods(self):\n",
    "        \"\"\"\n",
    "        æ›¿æ¢æ¨¡å‹æ¯ä¸€å±‚çš„ forward æ–¹æ³•ï¼Œæ·»åŠ å‚æ•°é¢„åŠ è½½é€»è¾‘å’Œæ³¨æ„åŠ›è®¡ç®—ã€‚\n",
    "        \"\"\"\n",
    "        for i, layer in enumerate(self.llm.model.layers):\n",
    "            def new_forward(hidden_states: torch.Tensor,\n",
    "                            attention_mask: Optional[torch.Tensor] = None,\n",
    "                            position_ids: Optional[torch.LongTensor] = None,\n",
    "                            past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "                            output_attentions: Optional[bool] = False,\n",
    "                            output_router_logits: Optional[bool] = False,\n",
    "                            use_cache: Optional[bool] = False,\n",
    "                            cache_position: Optional[torch.LongTensor] = None,\n",
    "                            layer_idx=i):\n",
    "                # print(f\"in layer {layer_idx}\")\n",
    "                with self.lock:\n",
    "                    # é€‰æ‹©å½“å‰ä½¿ç”¨çš„ç¼“å†²åŒº\n",
    "                    current_buffer = self.cached_mlps[0] if self.use_buffer0 else self.cached_mlps[1]\n",
    "                    current_stream = self.stream0 if self.use_buffer0 else self.stream1\n",
    "\n",
    "                    # åˆ‡æ¢ç¼“å†²åŒºç”¨äºä¸‹ä¸€æ¬¡\n",
    "                    next_buffer_index = 1 if self.use_buffer0 else 0\n",
    "                    next_buffer = self.cached_mlps[next_buffer_index]\n",
    "                    next_stream = self.stream1 if self.use_buffer0 else self.stream0\n",
    "\n",
    "                    # é¢„åŠ è½½ä¸‹ä¸€å±‚çš„å‚æ•°\n",
    "                    next_layer_idx = layer_idx + 1\n",
    "                    if next_layer_idx < self.num_layers:\n",
    "                        self._load_layer(next_layer_idx, buffer_index=next_buffer_index)\n",
    "                    \n",
    "                    # åˆ‡æ¢ç¼“å†²åŒº\n",
    "                    self.use_buffer0 = not self.use_buffer0\n",
    "\n",
    "                    # å¤„ç†å½“å‰å±‚\n",
    "                    residual = hidden_states\n",
    "                    hidden_states = layer.input_layernorm(hidden_states)\n",
    "\n",
    "                    # Self Attention\n",
    "                    hidden_states, self_attn_weights, present_key_value = layer.self_attn(\n",
    "                        hidden_states=hidden_states,\n",
    "                        attention_mask=attention_mask,\n",
    "                        position_ids=position_ids,\n",
    "                        past_key_value=past_key_value,\n",
    "                        output_attentions=output_attentions,\n",
    "                        use_cache=use_cache,\n",
    "                        cache_position=cache_position,\n",
    "                    )\n",
    "                    hidden_states = residual + hidden_states\n",
    "\n",
    "                    # Fully Connected\n",
    "                    residual = hidden_states\n",
    "                    hidden_states = layer.post_attention_layernorm(hidden_states)\n",
    "\n",
    "                    # ä½¿ç”¨å½“å‰ç¼“å†²åŒºè¿›è¡Œ MLP è®¡ç®—\n",
    "                    batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "                    hidden_states = hidden_states.view(-1, hidden_dim)\n",
    "                    # ä»…ä½¿ç”¨ç¬¬ä¸€ä¸ªä¸“å®¶\n",
    "                    expert_layer = layer.block_sparse_moe.experts[0]\n",
    "\n",
    "                    w3_output = torch.matmul(hidden_states, current_buffer.w3_gpu.T)\n",
    "                    w1_output = self.activation(torch.matmul(hidden_states, current_buffer.w1_gpu.T))\n",
    "                    w2 = current_buffer.w2_gpu.T\n",
    "                    final_hidden_states = torch.matmul(w1_output * w3_output, w2)\n",
    "\n",
    "                    final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n",
    "\n",
    "                    hidden_states = residual + final_hidden_states\n",
    "\n",
    "                    outputs = (hidden_states,)\n",
    "\n",
    "                    if output_attentions:\n",
    "                        outputs += (self_attn_weights,)\n",
    "\n",
    "                    if use_cache:\n",
    "                        outputs += (present_key_value,)\n",
    "\n",
    "                    return outputs\n",
    "            # æ›¿æ¢ forward æ–¹æ³•\n",
    "            layer.forward = new_forward\n",
    "\n",
    "    def _async_load(self, layer_idx, buffer_index):\n",
    "        \"\"\"\n",
    "        å¼‚æ­¥åŠ è½½ MLP å‚æ•°åˆ°æŒ‡å®šç¼“å†²åŒºï¼Œä½¿ç”¨å…±äº«çš„CUDAæµã€‚\n",
    "        \"\"\"\n",
    "        self._load_layer(layer_idx, buffer_index)\n",
    "\n",
    "# å°†æ¨¡å‹è½¬æ¢ä¸ºä½¿ç”¨CachedMLPçš„ç‰ˆæœ¬\n",
    "llm, cached_mlps = convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.9)\n",
    "\n",
    "# åˆ›å»ºæµæ°´çº¿æ¨¡å‹\n",
    "pipeline_llm = PipelineLLM(llm, cached_mlps).llm\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
