{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先都加载到cpu上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:06<00:00,  3.01it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,2\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"\n",
    "from modeling_mixtral import MixtralForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import json\n",
    "\n",
    "def get_model(model_name, device_map, dtype=torch.bfloat16):\n",
    "    llm = MixtralForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        use_cache=True,\n",
    "        torch_dtype=dtype,\n",
    "    ) \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return llm, tokenizer\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "with open(\"../quantize/device_map.json\", \"r\") as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "dtype = torch.float16\n",
    "llm, tokenizer = get_model(model_name, 'cpu', dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 117.97it/s]\n",
      "100%|██████████| 32/32 [01:11<00:00,  2.23s/it]\n",
      "100%|██████████| 32/32 [00:17<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from hqq.core.quantize import *\n",
    "from hqq.models.hf.mixtral import MixtralHQQ\n",
    "\n",
    "q3_config    = BaseQuantizeConfig(nbits=2, group_size=64)\n",
    "quant_config      = {'block_sparse_moe.experts.w3'   : q3_config}\n",
    "\n",
    "#### 先放CUDA量化，然后再传回CPU\n",
    "MixtralHQQ.quantize_model(llm, quant_config=quant_config, compute_dtype=dtype, device='cuda:0')\n",
    "HQQLinear.set_backend(HQQBackend.PYTORCH)\n",
    "\n",
    "backend       = \"gemlite\" #'torchao_int4' #\"torchao_int4\" (4-bit only) or \"gemlite\" (4-bit + 2-bit)\n",
    "#Optimize\n",
    "from hqq.utils.patching import prepare_for_inference\n",
    "prepare_for_inference(llm, backend=backend, verbose=True)\n",
    "#Load GemLite cache\n",
    "if(backend == 'gemlite'):\n",
    "\timport gemlite\n",
    "\tgemlite.core.GEMLITE_TRITON_RESTRICT_M = True\n",
    "\tgemlite.core.GemLiteLinear.load_config('/tmp/gemlite_config.json')\n",
    "\t\n",
    "llm.to('cpu')\n",
    "print(llm.model.layers[0].block_sparse_moe.experts[0].w1.device)\n",
    "print(llm.model.layers[0].block_sparse_moe.experts[0].w3.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import threading\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "from queue import Queue\n",
    "\n",
    "class CachedMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, dtype, sparsity: float = 0.2):\n",
    "        super(CachedMLP, self).__init__()\n",
    "        self.sparsity = sparsity\n",
    "        self.activenum = int((1 - sparsity) * hidden_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dtype = dtype\n",
    "\n",
    "        print(\"active neural num \",self.activenum)\n",
    "\n",
    "        self.activation = nn.SiLU()\n",
    "\n",
    "        # GPU 缓存张量\n",
    "        self.w1_gpu = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda:0')\n",
    "        self.w2_gpu = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda:0')\n",
    "        self.w3_gpu = None\n",
    "\n",
    "        # 第二个专家的 GPU 缓存张量\n",
    "        self.w1_gpu_expert1 = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda:0')\n",
    "        self.w2_gpu_expert1 = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda:0')\n",
    "        self.w3_gpu_expert1 = None\n",
    "\n",
    "        # Pinned Memory 缓冲区\n",
    "        self.register_buffer('sparse_w1_cpu', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "        self.register_buffer('sparse_w2_cpu', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "\n",
    "        self.sparse_w1_cpu = self.sparse_w1_cpu.pin_memory()\n",
    "        self.sparse_w2_cpu = self.sparse_w2_cpu.pin_memory()\n",
    "\n",
    "        # 第二个专家的 Pinned Memory 缓冲区\n",
    "        self.register_buffer('sparse_w1_cpu_expert1', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "        self.register_buffer('sparse_w2_cpu_expert1', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "\n",
    "        self.sparse_w1_cpu_expert1 = self.sparse_w1_cpu_expert1.pin_memory()\n",
    "        self.sparse_w2_cpu_expert1 = self.sparse_w2_cpu_expert1.pin_memory()\n",
    "\n",
    "        self.expert0_weight = torch.tensor(0)\n",
    "        self.expert1_weight = torch.tensor(0)\n",
    "\n",
    "        # 统计信息\n",
    "        self.load_from_cpu_time = 0.0\n",
    "        self.load_from_cpu_calls = 0\n",
    "\n",
    "    def load_from_cpu(self, cpu_mlp, cpu_mlp_expert1, stream: torch.cuda.Stream):\n",
    "        \"\"\"\n",
    "        从CPU加载参数，并使用指定的CUDA流进行异步复制到GPU。\n",
    "        \n",
    "        参数:\n",
    "            cpu_mlp: 包含CPU上参数的字典（第一个专家）。\n",
    "            cpu_mlp_expert1: 包含CPU上参数的字典（第二个专家）。\n",
    "            stream: 用于数据传输的CUDA流。\n",
    "        \"\"\"\n",
    "        # 生成随机索引\n",
    "        random_indices = torch.randperm(cpu_mlp['w1'].data.size(0))[:self.activenum]\n",
    "        # sorted_indices = torch.sort(random_indices).values\n",
    "\n",
    "        # 从CPU加载参数（第一个专家）\n",
    "        self.sparse_w1_cpu.copy_(cpu_mlp['w1'].data[random_indices, :])\n",
    "        self.sparse_w2_cpu.copy_(cpu_mlp['w2'].data[random_indices, :])\n",
    "        # 从CPU加载参数（第二个专家）\n",
    "        self.sparse_w1_cpu_expert1.copy_(cpu_mlp_expert1['w1'].data[random_indices, :])\n",
    "        self.sparse_w2_cpu_expert1.copy_(cpu_mlp_expert1['w2'].data[random_indices, :])\n",
    "        # 异步复制到GPU\n",
    "        with torch.cuda.stream(stream):\n",
    "            self.w1_gpu.copy_(self.sparse_w1_cpu, non_blocking=True)\n",
    "            self.w2_gpu.copy_(self.sparse_w2_cpu, non_blocking=True)\n",
    "            self.w1_gpu_expert1.copy_(self.sparse_w1_cpu_expert1, non_blocking=True)\n",
    "            self.w2_gpu_expert1.copy_(self.sparse_w2_cpu_expert1, non_blocking=True)\n",
    "        \n",
    "        # 直接赋值 w3_gpu 和 w3_gpu_expert1\n",
    "        # 固定在GPU上的w3\n",
    "        self.w3_gpu = cpu_mlp['w3']\n",
    "        self.w3_gpu_expert1 = cpu_mlp_expert1['w3']\n",
    "\n",
    "    def load_expert_weights(self, expert_weights):\n",
    "        self.expert0_weight = expert_weights[0]\n",
    "        self.expert1_weight = expert_weights[1]\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        \"\"\"\n",
    "        根据hidden_states， 分别计算两个专家的输出\n",
    "        \"\"\"\n",
    "        # 第一个专家的计算\n",
    "        w3_output = self.w3_gpu(hidden_states)[:, :self.activenum]\n",
    "        w1_output = self.activation(torch.matmul(hidden_states, self.w1_gpu.T))\n",
    "        # w2 = self.w2_gpu.T\n",
    "        hidden_states_expert0 = torch.matmul(w1_output * w3_output, self.w2_gpu)\n",
    "\n",
    "        # 第二个专家的计算\n",
    "        w3_output_expert1 = self.w3_gpu_expert1(hidden_states)[:, :self.activenum]\n",
    "        w1_output_expert1 = self.activation(torch.matmul(hidden_states, self.w1_gpu_expert1.T))\n",
    "        # w2_expert1 = self.w2_gpu_expert1.T\n",
    "        hidden_states_expert1 = torch.matmul(w1_output_expert1 * w3_output_expert1, self.w2_gpu_expert1)\n",
    "\n",
    "        final_hidden_states = hidden_states_expert0* self.expert0_weight + hidden_states_expert1* self.expert1_weight\n",
    "        \n",
    "        return final_hidden_states\n",
    "                        \n",
    "def convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.9):\n",
    "    ### 其他部分存放在GPU上\n",
    "    llm.model.embed_tokens.cuda(0)\n",
    "    for i in range(len(llm.model.layers)):\n",
    "        llm.model.layers[i].self_attn.cuda(0)\n",
    "        llm.model.layers[i].input_layernorm.cuda(0)\n",
    "        llm.model.layers[i].post_attention_layernorm.cuda(0)\n",
    "        llm.model.layers[i].block_sparse_moe.gate.cuda(0)\n",
    "        for j in range(len(llm.model.layers[0].block_sparse_moe.experts)):\n",
    "            llm.model.layers[i].block_sparse_moe.experts[j].w3.cuda(0)\n",
    "    ### 第0层的专家存放在GPU上\n",
    "    for j in range(len(llm.model.layers[0].block_sparse_moe.experts)):\n",
    "        llm.model.layers[0].block_sparse_moe.experts[j].cuda(0)\n",
    "\n",
    "    llm.model.norm.cuda(0)\n",
    "    llm.lm_head.cuda(0)\n",
    "    \n",
    "    # 创建两个共享的CachedMLP实例\n",
    "    buffer0 = CachedMLP(\n",
    "        input_dim=llm.config.hidden_size,\n",
    "        hidden_dim=llm.config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "        sparsity=sparsity\n",
    "    )\n",
    "    buffer1 = CachedMLP(\n",
    "        input_dim=llm.config.hidden_size,\n",
    "        hidden_dim=llm.config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "        sparsity=sparsity\n",
    "    )\n",
    "    cached_mlps = [buffer0, buffer1]\n",
    "    \n",
    "    for i, layer in enumerate(llm.model.layers):\n",
    "        if i==0:\n",
    "            continue\n",
    "        # 将专家的forward方法替换为PipelineLLM管理的方式\n",
    "        for j, expert in enumerate(layer.block_sparse_moe.experts):\n",
    "            expert.cpu_mlp = {\n",
    "                \"w1\": expert.w1.cpu().weight,\n",
    "                \"w2\": expert.w2.cpu().weight.T.contiguous(),\n",
    "                \"w3\": expert.w3,\n",
    "            }\n",
    "    return llm, cached_mlps\n",
    "\n",
    "class PipelineLLM:\n",
    "    def __init__(self, llm, cached_mlps):\n",
    "        \"\"\"\n",
    "        初始化 PipelineLLM，替换模型每一层的 forward 方法。\n",
    "        \n",
    "        参数:\n",
    "            llm: 原始的大模型\n",
    "            cached_mlps: 两个 CachedMLP 实例列表\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.cached_mlps = cached_mlps  # [buffer0, buffer1]\n",
    "        self.num_layers = len(llm.model.layers)\n",
    "        self.lock = threading.Lock()\n",
    "        self.use_buffer0 = True  # 标记当前使用哪个缓冲区\n",
    "\n",
    "        # 创建两个共享的CUDA流\n",
    "        self.stream0 = torch.cuda.Stream()\n",
    "        self.stream1 = torch.cuda.Stream()\n",
    "\n",
    "        # 初始化加载第一个和第二个层的参数\n",
    "        # self._load_layer(1, buffer_index=0, expert_ids=torch.tensor([0, 1]))\n",
    "        # self._load_layer(1, buffer_index=1, expert_ids=torch.tensor([0, 1]))\n",
    "        self.top_k = 2\n",
    "        self.activation = nn.SiLU()\n",
    "\n",
    "        self._replace_forward_methods()\n",
    "\n",
    "        # 用于统计时间的变量\n",
    "        self.total_prefill_time = 0.0\n",
    "        self.total_decode_time = 0.0\n",
    "\n",
    "    def _load_layer(self, layer_idx, buffer_index, expert_ids, expert_weights=torch.tensor([0, 0])):\n",
    "        \"\"\"\n",
    "        加载指定层的参数到指定的缓冲区。\n",
    "        \n",
    "        参数:\n",
    "            layer_idx: 层的索引\n",
    "            buffer_index: 缓冲区的索引（0 或 1）\n",
    "        \"\"\"\n",
    "        layer = self.llm.model.layers[layer_idx]\n",
    "        expert0 = layer.block_sparse_moe.experts[expert_ids[0]]\n",
    "        expert1 = layer.block_sparse_moe.experts[expert_ids[1]]\n",
    "        # if layer_idx == 1:\n",
    "        #     print(expert_ids[0].data, expert_ids[1].data, '{:.3f}, {:.3f}'.format(expert_weights[0], expert_weights[1]))\n",
    "\n",
    "        cpu_mlp = expert0.cpu_mlp\n",
    "        cpu_mlp_expert1 = expert1.cpu_mlp\n",
    "        buffer = self.cached_mlps[buffer_index]\n",
    "        stream = self.stream0 if buffer_index == 0 else self.stream1\n",
    "\n",
    "        buffer.load_expert_weights(expert_weights)\n",
    "        # 异步加载参数\n",
    "        buffer.load_from_cpu(cpu_mlp, cpu_mlp_expert1, stream)\n",
    "\n",
    "    def _replace_forward_methods(self):\n",
    "        \"\"\"\n",
    "        替换模型每一层的 forward 方法，添加参数预加载逻辑和注意力计算。\n",
    "        \"\"\"\n",
    "        for i, layer in enumerate(self.llm.model.layers):\n",
    "            def new_forward(hidden_states: torch.Tensor,\n",
    "                        attention_mask: Optional[torch.Tensor] = None,\n",
    "                        position_ids: Optional[torch.LongTensor] = None,\n",
    "                        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "                        output_attentions: Optional[bool] = False,\n",
    "                        output_router_logits: Optional[bool] = False,\n",
    "                        use_cache: Optional[bool] = False,\n",
    "                        cache_position: Optional[torch.LongTensor] = None,\n",
    "                        layer=layer,\n",
    "                        layer_idx=i):\n",
    "                with self.lock:\n",
    "                    batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "                    \n",
    "                    if sequence_length == 1:\n",
    "                        #### decode phase ####\n",
    "                        # 选择当前使用的缓冲区\n",
    "                        current_buffer = self.cached_mlps[0] if self.use_buffer0 else self.cached_mlps[1]\n",
    "\n",
    "                        next_buffer_index = 1 if self.use_buffer0 else 0\n",
    "\n",
    "                        next_layer_idx = layer_idx + 1\n",
    "\n",
    "                        if next_layer_idx < self.num_layers:\n",
    "                            # 预加载下一层的参数\n",
    "                            next_layer = self.llm.model.layers[next_layer_idx]\n",
    "                            router = next_layer.block_sparse_moe.gate\n",
    "\n",
    "                            # batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "                            hidden_states_flat = hidden_states.view(-1, hidden_dim)\n",
    "                            # router_logits: (batch * sequence_length, n_experts)\n",
    "                            router_logits = router(hidden_states_flat)\n",
    "\n",
    "                            routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "                            routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n",
    "                            routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n",
    "\n",
    "                            hidden_states = hidden_states_flat.reshape(batch_size, sequence_length, hidden_dim)\n",
    "\n",
    "                            self._load_layer(\n",
    "                                next_layer_idx,\n",
    "                                buffer_index=next_buffer_index,\n",
    "                                expert_ids=selected_experts[0],\n",
    "                                expert_weights=routing_weights[0]\n",
    "                            )\n",
    "\n",
    "                        # 切换缓冲区\n",
    "                        self.use_buffer0 = not self.use_buffer0\n",
    "\n",
    "                    # 处理当前层\n",
    "                    residual = hidden_states\n",
    "                    hidden_states = layer.input_layernorm(hidden_states)\n",
    "\n",
    "                    # Self Attention\n",
    "                    hidden_states, self_attn_weights, present_key_value = layer.self_attn(\n",
    "                        hidden_states=hidden_states,\n",
    "                        attention_mask=attention_mask,\n",
    "                        position_ids=position_ids,\n",
    "                        past_key_value=past_key_value,\n",
    "                        output_attentions=output_attentions,\n",
    "                        use_cache=use_cache,\n",
    "                        cache_position=cache_position,\n",
    "                    )\n",
    "                    hidden_states = residual + hidden_states\n",
    "\n",
    "                    # Fully Connected\n",
    "                    residual = hidden_states\n",
    "                    hidden_states = layer.post_attention_layernorm(hidden_states)\n",
    "                    \n",
    "                    if sequence_length > 1:\n",
    "                        # print(\"in prefill layer \", layer_idx)\n",
    "                        # 对于prefill阶段，仅将experts加载到GPU计算\n",
    "                        experts = layer.block_sparse_moe.experts\n",
    "\n",
    "                        # 将experts移动到GPU\n",
    "                        for expert in experts:\n",
    "                            expert.cuda(0)\n",
    "\n",
    "                        # 在GPU上进行MoE计算（gate保持在CPU）\n",
    "                        final_hidden_states, router_logits = layer.block_sparse_moe(hidden_states)\n",
    "\n",
    "                        # 计算完成后将experts移回CPU\n",
    "                        if layer_idx != 0:\n",
    "                            for expert in experts:\n",
    "                                expert.to('cpu')\n",
    "                    else:\n",
    "                        # batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "                        hidden_states_flat = hidden_states.view(-1, hidden_dim)\n",
    "                        # print(\"in decode layer\", layer_idx)\n",
    "                        if layer_idx > 0:\n",
    "                            ### 使用当前缓冲区进行 MLP 计算 ###\n",
    "                            final_hidden_states = current_buffer(hidden_states_flat)\n",
    "                        else:\n",
    "                            ### 根据router计算需要使用的专家 ###\n",
    "                            cur_layer = layer\n",
    "                            router = cur_layer.block_sparse_moe.gate\n",
    "                            # router_logits: (batch * sequence_length, n_experts)\n",
    "                            router_logits = router(hidden_states_flat)\n",
    "\n",
    "                            routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "                            routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n",
    "                            routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n",
    "                            # we cast back to the input dtype\n",
    "                            routing_weights = routing_weights.to(hidden_states_flat.dtype)\n",
    "\n",
    "                            first_expert, second_expert = selected_experts[0][0], selected_experts[0][1]\n",
    "\n",
    "                            final_hidden_states_expert0 = cur_layer.block_sparse_moe.experts[first_expert](\n",
    "                                hidden_states_flat) * routing_weights[0][0]\n",
    "\n",
    "                            final_hidden_states_expert1 = cur_layer.block_sparse_moe.experts[second_expert](\n",
    "                                hidden_states_flat) * routing_weights[0][1]\n",
    "\n",
    "                            # 将两个专家的结果相加\n",
    "                            final_hidden_states = final_hidden_states_expert0 + final_hidden_states_expert1\n",
    "\n",
    "                        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n",
    "\n",
    "                    hidden_states = residual + final_hidden_states\n",
    "\n",
    "                    outputs = (hidden_states,)\n",
    "\n",
    "                    if output_attentions:\n",
    "                        outputs += (self_attn_weights,)\n",
    "\n",
    "                    if use_cache:\n",
    "                        outputs += (present_key_value,)\n",
    "\n",
    "                    return outputs\n",
    "\n",
    "            # 替换forward方法\n",
    "            layer.forward = new_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "active neural num  2867\n",
      "active neural num  2867\n"
     ]
    }
   ],
   "source": [
    "llm, cached_mlps = convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.8)\n",
    "\n",
    "# 创建流水线模型\n",
    "PLLM = PipelineLLM(llm, cached_mlps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试时间开销"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 32\n",
      "Generated output length: 32 Time taken: 4.6101 seconds\n",
      "['passing ingår passing particularly cref except warning obvious coinc idiot except except consist except occasional except idiot except stupid consistcipe especially except except pseudo except obviously except except consist except occasional']\n",
      "Generated output length: 32 Time taken: 4.5986 seconds\n",
      "['fortunate Douglas repeatedly: history tro history subsequently and history repeatedly history history history andihood history repeatedlyihood fucking history especially sacihood history especiallyihood desired fucking history history history']\n",
      "Generated output length: 32 Time taken: 4.4382 seconds\n",
      "['lightsЇ responsibility🌍 columns convenient occasional BAS occasional entity convenient entity🌍 legitimate D vess consistent please subs Du oppongest consistent pseudoEntities Mix legitimate purposes alleged legitimate consistent concert']\n",
      "Generated output length: 3 Time taken: 0.4483 seconds\n",
      "['pizza Chain']\n",
      "Generated output length: 32 Time taken: 4.4258 seconds\n",
      "['nels mock Mock mock mock WARRAN delen whilst independ Route JuRouting facimation mockmock whilst consist Jun laug noten arteFMT daily session Sm laugihoodtagonrupalagma consistent']\n",
      "Generated output length: 32 Time taken: 4.1988 seconds\n",
      "['asticsearchHub peaceful withdrawementeiek◄ FITNESS Wisolas demonstriesaironment\\ue934rolled otherwiseacia\\ue934inderriteriaква evolutionicer gigdexironment Bryan%%%% min evident❶ vess']\n",
      "Generated output length: 32 Time taken: 3.9207 seconds\n",
      "['agreed /******/ /******/ clients Organ Hem incre ages secretcx agre Id simultaneouslyicals none oppon Parad oppon neighb noten ál__.eturn Lab Fab;</prite whilst SI\\ufeff Accihood']\n",
      "Generated output length: 32 Time taken: 3.8963 seconds\n",
      "['pretendncia pret restrtrfs intellect invent --( /******/ /******/ CD /******/ arrest◄ Ship biologie ingår san Lap prevglied /******/cleglied RAM Except requested yesterday remind repl🌍 mock']\n",
      "Generated output length: 32 Time taken: 3.8448 seconds\n",
      "['Cav nextscan age age Tax planet Stopokal kicking transparent parking grat gross FITNESSzym ages legit excess genu derivative genuampionship Stationato legitimate Argument oppon suspension opponktet Flag']\n",
      "Generated output length: 32 Time taken: 3.8264 seconds\n",
      "['~~ ~~ ~~ ~~ ~~ ~~ shitty ~~ ~~ hilar ~~ ~~ ~~ ~~ hilar ~~pgfscope hilar ~~ hilar ~~ gepubliceerd joke mock ~~ denmock mock laughingbbra allegolean']\n",
      "decode phase speed: 7.6162  token/s\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "\n",
    "input_length = 1\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 32\n",
    "test_samples = 10\n",
    "device_id = 0\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "filt_type = fineweb_path.split('.')[-1]\n",
    "fineweb = load_dataset(filt_type, data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text']\n",
    "\n",
    "# 预热（避免第一次运行时的额外开销）\n",
    "for text in fineweb_text[:5] :\n",
    "    inputs = preprocess_data(text, tokenizer)\n",
    "    with torch.no_grad():\n",
    "        output = llm(input_ids=inputs[\"input_ids\"].cuda(device_id), attention_mask=inputs[\"attention_mask\"].cuda(device_id))\n",
    "\n",
    "generated_all = 0\n",
    "prefill_time, decode_time = 0, 0\n",
    "print(\"output length is {}\".format(output_length))\n",
    "for text in fineweb_text[:test_samples] :\n",
    "    inputs = preprocess_data(text, tokenizer)\n",
    "\n",
    "    # 预热（避免第一次运行时的额外开销）\n",
    "    with torch.no_grad():\n",
    "        output = llm(input_ids=inputs[\"input_ids\"].cuda(device_id), attention_mask=inputs[\"attention_mask\"].cuda(device_id))\n",
    "\n",
    "    # 测试时间\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # 开始计时\n",
    "    torch.cuda.synchronize()\n",
    "    start_event.record()\n",
    "\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(device_id),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(device_id),\n",
    "            max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.pad_token_id, \n",
    "            # cache_implementation=\"static\" ## moe not support\n",
    "        )\n",
    "\n",
    "    # 结束计时\n",
    "    end_event.record()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # 计算时间\n",
    "    elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "    decode_time += elapsed_time\n",
    "    print(f\"Generated output length: {len(output[0]) - input_length}\", f\"Time taken: {elapsed_time:.4f} seconds\")\n",
    "    # print(output)\n",
    "    print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "\n",
    "    generated_all += len(output[0]) - input_length\n",
    "\n",
    "timepertoken = (decode_time) / (generated_all)\n",
    "# print(\"decode time:\", '{:.4f}'.format((decode_time) /test_samples), ' s')\n",
    "print(\"decode phase speed:\", '{:.4f}'.format(1/timepertoken) , ' token/s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################################################################\n",
    "#Save gemlite cache\n",
    "if(backend == 'gemlite'):\n",
    "\tgemlite.core.GemLiteLinear.cache_config('/tmp/gemlite_config.json') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.profile\n",
    "\n",
    "trace-offloading-r.json是最优，就是做完一个index就传一个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 2\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::index        39.35%     106.719ms        39.93%     108.282ms     287.220us       1.010ms         0.42%       1.010ms       2.680us           377  \n",
      "                                            aten::copy_        16.02%      43.441ms        17.80%      48.267ms      53.158us     203.157ms        84.09%     203.157ms     223.741us           908  \n",
      "gemlite::gemv_revsplitK_A16fWnO16f_int32packing_forw...         8.72%      23.656ms         9.93%      26.919ms     210.302us       3.706ms         1.53%       4.683ms      36.585us           128  \n",
      "                                       cudaLaunchKernel         7.58%      20.564ms         7.58%      20.564ms       5.606us       0.000us         0.00%       0.000us       0.000us          3668  \n",
      "                                               aten::mm         4.00%      10.857ms         5.56%      15.078ms      26.086us      11.550ms         4.78%      11.550ms      19.982us           578  \n",
      "                                         aten::randperm         2.53%       6.861ms         5.16%      13.986ms     112.786us       0.000us         0.00%       0.000us       0.000us           124  \n",
      "                                              aten::mul         2.22%       6.034ms         3.67%       9.957ms      12.832us       6.157ms         2.55%       6.157ms       7.935us           776  \n",
      "                                  cudaStreamSynchronize         1.81%       4.918ms         1.81%       4.918ms      34.879us       0.000us         0.00%       0.000us       0.000us           141  \n",
      "                                              aten::add         1.32%       3.584ms         2.17%       5.873ms      12.936us       3.167ms         1.31%       3.167ms       6.975us           454  \n",
      "                                        cudaMemcpyAsync         1.19%       3.216ms         1.19%       3.216ms       7.960us       0.000us         0.00%       0.000us       0.000us           404  \n",
      "                                            aten::slice         1.14%       3.104ms         1.43%       3.883ms       2.629us       0.000us         0.00%       0.000us       0.000us          1477  \n",
      "                                              aten::cat         0.80%       2.170ms         1.21%       3.293ms      16.799us       1.742ms         0.72%       1.742ms       8.886us           196  \n",
      "                                       aten::as_strided         0.78%       2.124ms         0.78%       2.124ms       0.588us       0.000us         0.00%       0.000us       0.000us          3610  \n",
      "                                            aten::empty         0.77%       2.082ms         0.77%       2.082ms       3.517us       0.000us         0.00%       0.000us       0.000us           592  \n",
      "                                             aten::mean         0.64%       1.733ms         0.90%       2.453ms      18.871us       1.141ms         0.47%       1.141ms       8.774us           130  \n",
      "                                    aten::empty_strided         0.63%       1.710ms         0.63%       1.710ms       5.090us       0.000us         0.00%       0.000us       0.000us           336  \n",
      "                                             aten::view         0.58%       1.574ms         0.58%       1.574ms       1.022us       0.000us         0.00%       0.000us       0.000us          1540  \n",
      "                                              aten::pow         0.58%       1.569ms         0.90%       2.442ms      18.785us     783.480us         0.32%     783.480us       6.027us           130  \n",
      "                                             aten::topk         0.52%       1.405ms         1.05%       2.852ms      44.565us       1.471ms         0.61%       1.471ms      22.984us            64  \n",
      "                                           aten::matmul         0.48%       1.310ms         6.31%      17.124ms      29.627us       0.000us         0.00%      11.550ms      19.982us           578  \n",
      "                                             aten::silu         0.43%       1.176ms         0.70%       1.891ms      14.773us       1.089ms         0.45%       1.089ms       8.504us           128  \n",
      "                         aten::_flash_attention_forward         0.42%       1.140ms         1.00%       2.704ms      42.247us     776.688us         0.32%     776.688us      12.136us            64  \n",
      "                                          aten::reshape         0.41%       1.120ms         1.65%       4.466ms       5.012us       0.000us         0.00%       1.017ms       1.141us           891  \n",
      "                                              aten::neg         0.41%       1.102ms         0.66%       1.780ms      13.907us       1.036ms         0.43%       1.036ms       8.096us           128  \n",
      "                                        aten::transpose         0.40%       1.072ms         0.60%       1.629ms       1.935us       0.000us         0.00%       0.000us       0.000us           842  \n",
      "                                            aten::rsqrt         0.38%       1.034ms         0.63%       1.698ms      13.060us     776.368us         0.32%     776.368us       5.972us           130  \n",
      "                                         aten::_to_copy         0.34%     912.836us         2.15%       5.821ms      21.638us       0.000us         0.00%       1.983ms       7.373us           269  \n",
      "                                           aten::select         0.33%     906.626us         0.40%       1.084ms       2.745us       0.000us         0.00%       0.000us       0.000us           395  \n",
      "                                              aten::sum         0.31%     831.426us         0.44%       1.180ms      18.439us     699.060us         0.29%     699.060us      10.923us            64  \n",
      "                                                aten::t         0.30%     824.302us         0.57%       1.552ms       4.704us       0.000us         0.00%       0.000us       0.000us           330  \n",
      "                                    cudaLaunchKernelExC         0.30%     802.603us         0.30%     802.603us       5.990us       0.000us         0.00%       0.000us       0.000us           134  \n",
      "                                  cudaFuncGetAttributes         0.29%     792.855us         0.29%     792.855us       4.004us       0.000us         0.00%       0.000us       0.000us           198  \n",
      "                                               aten::to         0.27%     718.670us         2.41%       6.539ms      12.065us       0.000us         0.00%       1.983ms       3.659us           542  \n",
      "                                         cuLaunchKernel         0.26%     710.112us         0.26%     710.112us       5.548us       0.000us         0.00%       0.000us       0.000us           128  \n",
      "                                            aten::fill_         0.26%     705.159us         0.61%       1.642ms      11.810us       1.001ms         0.41%       1.001ms       7.199us           139  \n",
      "                     aten::scaled_dot_product_attention         0.26%     700.470us         1.57%       4.249ms      66.395us       0.000us         0.00%     776.688us      12.136us            64  \n",
      "                                           aten::linear         0.26%     699.727us         5.28%      14.322ms      43.399us       0.000us         0.00%       5.929ms      17.968us           330  \n",
      "                              aten::_local_scalar_dense         0.25%     682.991us         2.48%       6.735ms      49.523us       1.179ms         0.49%       1.179ms       8.672us           136  \n",
      "                                         aten::_softmax         0.23%     620.780us         0.37%     995.938us      15.562us     426.602us         0.18%     426.602us       6.666us            64  \n",
      "                                             aten::div_         0.21%     561.467us         0.34%     909.961us      14.218us     606.958us         0.25%     606.958us       9.484us            64  \n",
      "                                        aten::unsqueeze         0.19%     515.571us         0.24%     659.250us       2.516us       0.000us         0.00%       0.000us       0.000us           262  \n",
      "              aten::_scaled_dot_product_flash_attention         0.19%     514.951us         1.31%       3.549ms      55.450us       0.000us         0.00%     776.688us      12.136us            64  \n",
      "                                            aten::zero_         0.16%     437.854us         0.71%       1.914ms      14.957us       0.000us         0.00%     977.047us       7.633us           128  \n",
      "                                            aten::clone         0.14%     370.939us         0.98%       2.668ms      19.908us       0.000us         0.00%       1.032ms       7.699us           134  \n",
      "                                          aten::permute         0.12%     335.897us         0.15%     410.520us       3.311us       0.000us         0.00%       0.000us       0.000us           124  \n",
      "                                     aten::_unsafe_view         0.12%     327.751us         0.12%     327.751us       0.849us       0.000us         0.00%       0.000us       0.000us           386  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.12%     324.983us         0.12%     324.983us       0.626us       0.000us         0.00%       0.000us       0.000us           519  \n",
      "                                       aten::empty_like         0.12%     321.949us         0.45%       1.213ms       6.159us       0.000us         0.00%       0.000us       0.000us           197  \n",
      "                                           aten::expand         0.12%     316.227us         0.14%     390.388us       3.050us       0.000us         0.00%       0.000us       0.000us           128  \n",
      "                                             aten::item         0.06%     169.437us         2.55%       6.905ms      50.769us       0.000us         0.00%       1.179ms       8.672us           136  \n",
      "                                 cudaDeviceGetAttribute         0.05%     139.089us         0.05%     139.089us       0.537us       0.000us         0.00%       0.000us       0.000us           259  \n",
      "                                          aten::numpy_T         0.05%     136.553us         0.20%     547.073us       4.412us       0.000us         0.00%       0.000us       0.000us           124  \n",
      "                                   cudaFuncSetAttribute         0.05%     133.154us         0.05%     133.154us       1.548us       0.000us         0.00%       0.000us       0.000us            86  \n",
      "                                    aten::scalar_tensor         0.05%     126.600us         0.05%     126.600us       2.042us       0.000us         0.00%       0.000us       0.000us            62  \n",
      "                                          aten::softmax         0.05%     124.020us         0.41%       1.120ms      17.499us       0.000us         0.00%     426.602us       6.666us            64  \n",
      "                                  cudaDeviceSynchronize         0.04%     102.507us         0.04%     102.507us     102.507us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                               aten::eq         0.04%      96.520us         0.05%     143.785us      15.976us      22.401us         0.01%      22.401us       2.489us             9  \n",
      "                                  cudaStreamIsCapturing         0.03%      78.628us         0.03%      78.628us       1.229us       0.000us         0.00%       0.000us       0.000us            64  \n",
      "                                             aten::isin         0.03%      75.662us         0.12%     330.803us     110.268us       0.000us         0.00%      28.000us       9.333us             3  \n",
      "                                           aten::cumsum         0.03%      71.883us         0.04%     116.065us      38.688us      15.776us         0.01%      15.776us       5.259us             3  \n",
      "                                      aten::result_type         0.02%      66.766us         0.02%      66.766us       0.514us       0.000us         0.00%       0.000us       0.000us           130  \n",
      "                                        cudaMemsetAsync         0.02%      61.873us         0.02%      61.873us      10.312us       0.000us         0.00%       0.000us       0.000us             6  \n",
      "                                              aten::sub         0.02%      60.663us         0.03%      91.860us      18.372us      11.904us         0.00%      11.904us       2.381us             5  \n",
      "                                          aten::resize_         0.02%      56.157us         0.02%      56.157us       0.446us       0.000us         0.00%       0.000us       0.000us           126  \n",
      "                                       aten::bitwise_or         0.02%      50.084us         0.04%      96.168us      24.042us      10.080us         0.00%      10.080us       2.520us             4  \n",
      "                                              aten::any         0.02%      47.595us         0.04%     120.736us      24.147us       0.000us         0.00%      11.200us       2.240us             5  \n",
      "                                           aten::argmax         0.02%      46.197us         0.03%      73.889us      36.944us      25.600us         0.01%      25.600us      12.800us             2  \n",
      "                                     aten::index_select         0.02%      43.121us         0.03%      72.319us      36.159us       7.840us         0.00%       7.840us       3.920us             2  \n",
      "                                              aten::max         0.01%      35.866us         0.02%      55.732us      27.866us       9.472us         0.00%       9.472us       4.736us             2  \n",
      "                                     aten::masked_fill_         0.01%      28.342us         0.01%      40.575us      20.287us       4.864us         0.00%       4.864us       2.432us             2  \n",
      "                                      aten::bitwise_not         0.01%      25.220us         0.02%      41.185us      20.592us       4.737us         0.00%       4.737us       2.369us             2  \n",
      "                                               aten::ge         0.01%      24.088us         0.01%      34.570us      17.285us       4.673us         0.00%       4.673us       2.337us             2  \n",
      "                                      aten::bitwise_and         0.01%      22.639us         0.01%      36.345us      18.173us       5.313us         0.00%       5.313us       2.657us             2  \n",
      "                                              aten::all         0.01%      21.586us         0.02%      44.357us      22.179us       4.704us         0.00%       6.816us       3.408us             2  \n",
      "                                               aten::lt         0.01%      21.422us         0.01%      29.114us      29.114us       2.432us         0.00%       2.432us       2.432us             1  \n",
      "                                        aten::embedding         0.01%      20.368us         0.04%      97.293us      48.647us       0.000us         0.00%       7.840us       3.920us             2  \n",
      "                                         aten::new_ones         0.01%      14.327us         0.02%      61.729us      30.864us       0.000us         0.00%       4.512us       2.256us             2  \n",
      "                                             aten::rsub         0.00%      13.511us         0.02%      54.942us      27.471us       0.000us         0.00%       5.120us       2.560us             2  \n",
      "                                             aten::full         0.00%      12.280us         0.03%      73.019us      18.255us       0.000us         0.00%       8.320us       2.080us             4  \n",
      "                                       aten::is_nonzero         0.00%      11.203us         0.07%     182.265us      22.783us       0.000us         0.00%      20.130us       2.516us             8  \n",
      "                                           aten::__or__         0.00%       9.485us         0.04%     105.653us      26.413us       0.000us         0.00%      10.080us       2.520us             4  \n",
      "                                          aten::view_as         0.00%       6.710us         0.00%       9.831us       1.639us       0.000us         0.00%       0.000us       0.000us             6  \n",
      "                                          aten::__and__         0.00%       6.435us         0.02%      42.780us      21.390us       0.000us         0.00%       5.313us       2.657us             2  \n",
      "                                          aten::detach_         0.00%       4.945us         0.00%       6.910us       2.303us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                             aten::ones         0.00%       4.890us         0.01%      21.975us      21.975us       0.000us         0.00%       2.048us       2.048us             1  \n",
      "                                        aten::ones_like         0.00%       4.261us         0.01%      16.635us      16.635us       0.000us         0.00%       2.176us       2.176us             1  \n",
      "                                        aten::new_empty         0.00%       3.913us         0.00%      12.877us       6.439us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                                detach_         0.00%       1.965us         0.00%       1.965us       0.655us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                    cudaPeekAtLastError         0.00%       1.279us         0.00%       1.279us       0.107us       0.000us         0.00%       0.000us       0.000us            12  \n",
      "                                       aten::lift_fresh         0.00%       0.572us         0.00%       0.572us       0.191us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       8.448us         0.00%       8.448us       1.690us             5  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      14.944us         0.01%      14.944us       2.135us             7  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.592us         0.00%       2.592us       2.592us             1  \n",
      "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      35.070us         0.01%      35.070us       2.338us            15  \n",
      "                         Memcpy DtoH (Device -> Pinned)         0.00%       0.000us         0.00%       0.000us       0.000us       1.179ms         0.49%       1.179ms       8.672us           136  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       7.105us         0.00%       7.105us       2.368us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       8.736us         0.00%       8.736us       2.184us             4  \n",
      "void at_cuda_detail::cub::DeviceScanInitKernel<at_cu...         0.00%       0.000us         0.00%       0.000us       0.000us       7.328us         0.00%       7.328us       2.443us             3  \n",
      "void at_cuda_detail::cub::DeviceScanKernel<at_cuda_d...         0.00%       0.000us         0.00%       0.000us       0.000us       8.448us         0.00%       8.448us       2.816us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      12.256us         0.01%      12.256us       2.451us             5  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      14.657us         0.01%      14.657us       2.443us             6  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.864us         0.00%       4.864us       2.432us             2  \n",
      "void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us       7.840us         0.00%       7.840us       3.920us             2  \n",
      "void dot_kernel<float, 128, 0, cublasDotParams<cubla...         0.00%       0.000us         0.00%       0.000us       0.000us     503.239us         0.21%     503.239us       7.863us            64  \n",
      "void reduce_1Block_kernel<float, 128, 7, cublasGemvT...         0.00%       0.000us         0.00%       0.000us       0.000us     436.263us         0.18%     436.263us       6.817us            64  \n",
      "void (anonymous namespace)::softmax_warp_forward<c10...         0.00%       0.000us         0.00%       0.000us       0.000us     426.602us         0.18%     426.602us       6.666us            64  \n",
      "void at::native::sbtopk::gatherTopK<float, unsigned ...         0.00%       0.000us         0.00%       0.000us       0.000us     747.634us         0.31%     747.634us      11.682us            64  \n",
      "void at::native::bitonicSortKVInPlace<-2, -1, 16, 16...         0.00%       0.000us         0.00%       0.000us       0.000us     723.341us         0.30%     723.341us      11.302us            64  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     699.060us         0.29%     699.060us      10.923us            64  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     606.958us         0.25%     606.958us       9.484us            64  \n",
      "                         Memcpy HtoD (Pinned -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     200.121ms        82.83%     200.121ms     806.941us           248  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     988.017us         0.41%     988.017us       7.485us           132  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     783.480us         0.32%     783.480us       6.027us           130  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       1.141ms         0.47%       1.141ms       8.774us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     774.508us         0.32%     774.508us       5.958us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     776.368us         0.32%     776.368us       5.972us           130  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     928.951us         0.38%     928.951us       7.146us           130  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     986.837us         0.41%     986.837us       7.476us           132  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.823ms         0.75%       1.823ms       7.067us           258  \n",
      "sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x...         0.00%       0.000us         0.00%       0.000us       0.000us       2.921ms         1.21%       2.921ms      22.822us           128  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       1.402ms         0.58%       1.402ms      10.956us           128  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.007ms         0.42%       1.007ms       7.866us           128  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       2.186ms         0.90%       2.186ms       8.409us           260  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       1.036ms         0.43%       1.036ms       8.096us           128  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us       1.167ms         0.48%       1.167ms       9.119us           128  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.381ms         0.99%       2.381ms       7.441us           320  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       1.017ms         0.42%       1.017ms       7.945us           128  \n",
      "void pytorch_flash::flash_fwd_kernel<pytorch_flash::...         0.00%       0.000us         0.00%       0.000us       0.000us     776.688us         0.32%     776.688us      12.136us            64  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us      28.546us         0.01%      28.546us       4.758us             6  \n",
      "sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x...         0.00%       0.000us         0.00%       0.000us       0.000us     401.320us         0.17%     401.320us      66.887us             6  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.089ms         0.45%       1.089ms       8.504us           128  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     977.047us         0.40%     977.047us       7.633us           128  \n",
      "          gemv_revsplitK_A16fWnO16f_int32packing_kernel         0.00%       0.000us         0.00%       0.000us       0.000us       3.706ms         1.53%       3.706ms      28.952us           128  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_...         0.00%       0.000us         0.00%       0.000us       0.000us     205.604us         0.09%     205.604us      51.401us             4  \n",
      "void splitKreduce_kernel<32, 16, int, float, __half,...         0.00%       0.000us         0.00%       0.000us       0.000us      30.689us         0.01%      30.689us       7.672us             4  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       2.618ms         1.08%       2.618ms      21.111us           124  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       3.003ms         1.24%       3.003ms      24.215us           124  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       1.208ms         0.50%       1.208ms       9.746us           124  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us      14.879us         0.01%      14.879us       3.720us             4  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      25.600us         0.01%      25.600us      12.800us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      10.433us         0.00%      10.433us       2.608us             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.120us         0.00%       5.120us       2.560us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.376us         0.00%       5.376us       2.688us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      10.080us         0.00%      10.080us       2.520us             4  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       2.560us         0.00%       2.560us       2.560us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.737us         0.00%       4.737us       2.369us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       5.313us         0.00%       5.313us       2.657us             2  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       9.472us         0.00%       9.472us       4.736us             2  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.392us         0.00%       3.392us       3.392us             1  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       4.704us         0.00%       4.704us       4.704us             1  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     559.594us         0.23%     559.594us       8.744us            64  \n",
      "void at::native::vectorized_elementwise_kernel<2, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.592us         0.00%       2.592us       2.592us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 271.195ms\n",
      "Self CUDA time total: 241.604ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 1\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 2\n",
    "test_samples = 4\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "fineweb = load_dataset(\"parquet\",data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text'][:test_samples] \n",
    "\n",
    "print(\"output length is {}\".format(output_length))\n",
    "text = fineweb_text[0]\n",
    "inputs = preprocess_data(text, tokenizer)\n",
    "\n",
    "# cached_mlp.clear_load_from_cpu_stats()\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ]\n",
    ") as p:\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(),\n",
    "            max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "print(p.key_averages().table(\n",
    "    sort_by=\"self_cpu_time_total\", row_limit=-1))\n",
    "p.export_chrome_trace(\"./offloading-hqq2-2.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载到GPU上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:25<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MixtralForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "import json\n",
    "\n",
    "def get_model(model_name, device_map, dtype=torch.bfloat16):\n",
    "    llm = MixtralForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        use_cache=True,\n",
    "        torch_dtype=dtype,\n",
    "    ) \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return llm, tokenizer\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "with open('../quantize/device_map_1.json', 'r') as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "dtype = torch.float16\n",
    "llm, tokenizer = get_model(model_name, device_map, dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 只传一个专家的版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import threading\n",
    "import json\n",
    "from queue import Queue\n",
    "\n",
    "class CachedMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, dtype, sparsity: float = 0.2):\n",
    "        super(CachedMLP, self).__init__()\n",
    "        self.sparsity = sparsity\n",
    "        self.activenum = int((1 - sparsity) * hidden_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dtype = dtype\n",
    "\n",
    "        # GPU 缓存张量\n",
    "        self.w1_gpu = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda:0')\n",
    "        self.w2_gpu = torch.empty((self.input_dim, self.activenum), dtype=self.dtype, device='cuda:0')\n",
    "        self.w3_gpu = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda:0')\n",
    "\n",
    "        # Pinned Memory 缓冲区\n",
    "        self.register_buffer('sparse_w1_cpu', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "        self.register_buffer('sparse_w2_cpu', torch.empty((self.input_dim, self.activenum), dtype=self.dtype, device='cpu'))\n",
    "        self.register_buffer('sparse_w3_cpu', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "        self.sparse_w1_cpu = self.sparse_w1_cpu.pin_memory()\n",
    "        self.sparse_w2_cpu = self.sparse_w2_cpu.pin_memory()\n",
    "        self.sparse_w3_cpu = self.sparse_w3_cpu.pin_memory()\n",
    "\n",
    "        # 统计信息\n",
    "        self.load_from_cpu_time = 0.0\n",
    "        self.load_from_cpu_calls = 0\n",
    "\n",
    "    def load_from_cpu(self, cpu_mlp, stream: torch.cuda.Stream):\n",
    "        \"\"\"\n",
    "        从CPU加载参数，并使用指定的CUDA流进行异步复制到GPU。\n",
    "        \n",
    "        参数:\n",
    "            cpu_mlp: 包含CPU上参数的字典。\n",
    "            stream: 用于数据传输的CUDA流。\n",
    "        \"\"\"\n",
    "        # 从CPU加载参数\n",
    "        self.sparse_w1_cpu.copy_(cpu_mlp['w1'].data[:self.activenum, :])\n",
    "        self.sparse_w2_cpu.copy_(cpu_mlp['w2'].data[:, :self.activenum])\n",
    "        self.sparse_w3_cpu.copy_(cpu_mlp['w3'].data[:self.activenum, :])\n",
    "\n",
    "        # 异步复制到GPU\n",
    "        with torch.cuda.stream(stream):\n",
    "            self.w1_gpu.copy_(self.sparse_w1_cpu, non_blocking=True)\n",
    "            self.w2_gpu.copy_(self.sparse_w2_cpu, non_blocking=True)\n",
    "            self.w3_gpu.copy_(self.sparse_w3_cpu, non_blocking=True)\n",
    "\n",
    "    def get_load_from_cpu_stats(self):\n",
    "        if self.load_from_cpu_calls == 0:\n",
    "            return 0.0, 0.0\n",
    "        avg_time = self.load_from_cpu_time / self.load_from_cpu_calls\n",
    "        return self.load_from_cpu_time, avg_time\n",
    "\n",
    "    def clear_load_from_cpu_stats(self):\n",
    "        self.load_from_cpu_time = 0.0\n",
    "        self.load_from_cpu_calls = 0\n",
    "\n",
    "def convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.9):\n",
    "    ### 其他部分存放在GPU上\n",
    "    llm.model.embed_tokens.cuda()\n",
    "    for i in range(len(llm.model.layers)):\n",
    "        llm.model.layers[i].self_attn.cuda()\n",
    "        llm.model.layers[i].input_layernorm.cuda()\n",
    "        llm.model.layers[i].post_attention_layernorm.cuda()\n",
    "        llm.model.layers[i].block_sparse_moe.gate.cuda()\n",
    "    ### 第0层的专家存放在GPU上\n",
    "    for j in range(len(llm.model.layers[0].block_sparse_moe.experts)):\n",
    "        llm.model.layers[0].block_sparse_moe.experts[j].cuda()\n",
    "\n",
    "    llm.model.norm.cuda()\n",
    "    llm.lm_head.cuda()\n",
    "    \n",
    "    # 创建两个共享的CachedMLP实例\n",
    "    buffer0 = CachedMLP(\n",
    "        input_dim=llm.config.hidden_size,\n",
    "        hidden_dim=llm.config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "        sparsity=sparsity\n",
    "    )\n",
    "    buffer1 = CachedMLP(\n",
    "        input_dim=llm.config.hidden_size,\n",
    "        hidden_dim=llm.config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "        sparsity=sparsity\n",
    "    )\n",
    "    cached_mlps = [buffer0, buffer1]\n",
    "    \n",
    "    for i, layer in enumerate(llm.model.layers):\n",
    "        if i==0:\n",
    "            continue\n",
    "        # 将专家的forward方法替换为PipelineLLM管理的方式\n",
    "        for j, expert in enumerate(layer.block_sparse_moe.experts):\n",
    "            expert.cpu_mlp = {\n",
    "                \"w1\": expert.w1.cpu().weight,\n",
    "                \"w2\": expert.w2.cpu().weight,\n",
    "                \"w3\": expert.w3.cpu().weight,\n",
    "            }\n",
    "            # 替换forward方法为直接调用CachedMLP的forward（需要在pipelineLLM里面替换)\n",
    "            # expert.forward = lambda x, cached_mlp=cached_mlp, cpu_mlp=expert.cpu_mlp: cached_mlp(x, cpu_mlp)\n",
    "    return llm, cached_mlps\n",
    "\n",
    "class PipelineLLM:\n",
    "    def __init__(self, llm, cached_mlps):\n",
    "        \"\"\"\n",
    "        初始化 PipelineLLM，替换模型每一层的 forward 方法。\n",
    "        \n",
    "        参数:\n",
    "            llm: 原始的大模型\n",
    "            cached_mlps: 两个 CachedMLP 实例列表\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.cached_mlps = cached_mlps  # [buffer0, buffer1]\n",
    "        self.num_layers = len(llm.model.layers)\n",
    "        self.lock = threading.Lock()\n",
    "        self.use_buffer0 = True  # 标记当前使用哪个缓冲区\n",
    "\n",
    "        # 创建两个共享的CUDA流\n",
    "        self.stream0 = torch.cuda.Stream()\n",
    "        self.stream1 = torch.cuda.Stream()\n",
    "\n",
    "        # 初始化加载第一个和第二个层的参数\n",
    "        self._load_layer(1, buffer_index=0)\n",
    "        self._load_layer(1, buffer_index=1)\n",
    "\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "        self._replace_forward_methods()\n",
    "    \n",
    "    def _load_layer(self, layer_idx, buffer_index):\n",
    "        \"\"\"\n",
    "        加载指定层的参数到指定的缓冲区。\n",
    "        \n",
    "        参数:\n",
    "            layer_idx: 层的索引\n",
    "            buffer_index: 缓冲区的索引（0 或 1）\n",
    "        \"\"\"\n",
    "        layer = self.llm.model.layers[layer_idx]\n",
    "        expert = layer.block_sparse_moe.experts[0]\n",
    "        cpu_mlp = expert.cpu_mlp\n",
    "        buffer = self.cached_mlps[buffer_index]\n",
    "        stream = self.stream0 if buffer_index == 0 else self.stream1\n",
    "\n",
    "        # 异步加载参数\n",
    "        buffer.load_from_cpu(cpu_mlp, stream)\n",
    "\n",
    "    def _replace_forward_methods(self):\n",
    "        \"\"\"\n",
    "        替换模型每一层的 forward 方法，添加参数预加载逻辑和注意力计算。\n",
    "        \"\"\"\n",
    "        for i, layer in enumerate(self.llm.model.layers):\n",
    "            def new_forward(hidden_states: torch.Tensor,\n",
    "                            attention_mask: Optional[torch.Tensor] = None,\n",
    "                            position_ids: Optional[torch.LongTensor] = None,\n",
    "                            past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "                            output_attentions: Optional[bool] = False,\n",
    "                            output_router_logits: Optional[bool] = False,\n",
    "                            use_cache: Optional[bool] = False,\n",
    "                            cache_position: Optional[torch.LongTensor] = None,\n",
    "                            layer_idx=i):\n",
    "                # print(f\"in layer {layer_idx}\")\n",
    "                with self.lock:\n",
    "                    # 选择当前使用的缓冲区\n",
    "                    current_buffer = self.cached_mlps[0] if self.use_buffer0 else self.cached_mlps[1]\n",
    "                    current_stream = self.stream0 if self.use_buffer0 else self.stream1\n",
    "\n",
    "                    # 切换缓冲区用于下一次\n",
    "                    next_buffer_index = 1 if self.use_buffer0 else 0\n",
    "                    next_buffer = self.cached_mlps[next_buffer_index]\n",
    "                    next_stream = self.stream1 if self.use_buffer0 else self.stream0\n",
    "\n",
    "                    # 预加载下一层的参数\n",
    "                    next_layer_idx = layer_idx + 1\n",
    "                    if next_layer_idx < self.num_layers:\n",
    "                        self._load_layer(next_layer_idx, buffer_index=next_buffer_index)\n",
    "                    \n",
    "                    # 切换缓冲区\n",
    "                    self.use_buffer0 = not self.use_buffer0\n",
    "\n",
    "                    # 处理当前层\n",
    "                    residual = hidden_states\n",
    "                    hidden_states = layer.input_layernorm(hidden_states)\n",
    "\n",
    "                    # Self Attention\n",
    "                    hidden_states, self_attn_weights, present_key_value = layer.self_attn(\n",
    "                        hidden_states=hidden_states,\n",
    "                        attention_mask=attention_mask,\n",
    "                        position_ids=position_ids,\n",
    "                        past_key_value=past_key_value,\n",
    "                        output_attentions=output_attentions,\n",
    "                        use_cache=use_cache,\n",
    "                        cache_position=cache_position,\n",
    "                    )\n",
    "                    hidden_states = residual + hidden_states\n",
    "\n",
    "                    # Fully Connected\n",
    "                    residual = hidden_states\n",
    "                    hidden_states = layer.post_attention_layernorm(hidden_states)\n",
    "\n",
    "                    # 使用当前缓冲区进行 MLP 计算\n",
    "                    batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "                    hidden_states = hidden_states.view(-1, hidden_dim)\n",
    "                    # 仅使用第一个专家\n",
    "                    expert_layer = layer.block_sparse_moe.experts[0]\n",
    "\n",
    "                    w3_output = torch.matmul(hidden_states, current_buffer.w3_gpu.T)\n",
    "                    w1_output = self.activation(torch.matmul(hidden_states, current_buffer.w1_gpu.T))\n",
    "                    w2 = current_buffer.w2_gpu.T\n",
    "                    final_hidden_states = torch.matmul(w1_output * w3_output, w2)\n",
    "\n",
    "                    final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n",
    "\n",
    "                    hidden_states = residual + final_hidden_states\n",
    "\n",
    "                    outputs = (hidden_states,)\n",
    "\n",
    "                    if output_attentions:\n",
    "                        outputs += (self_attn_weights,)\n",
    "\n",
    "                    if use_cache:\n",
    "                        outputs += (present_key_value,)\n",
    "\n",
    "                    return outputs\n",
    "            # 替换 forward 方法\n",
    "            layer.forward = new_forward\n",
    "\n",
    "    def _async_load(self, layer_idx, buffer_index):\n",
    "        \"\"\"\n",
    "        异步加载 MLP 参数到指定缓冲区，使用共享的CUDA流。\n",
    "        \"\"\"\n",
    "        self._load_layer(layer_idx, buffer_index)\n",
    "\n",
    "# 将模型转换为使用CachedMLP的版本\n",
    "llm, cached_mlps = convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.9)\n",
    "\n",
    "# 创建流水线模型\n",
    "pipeline_llm = PipelineLLM(llm, cached_mlps).llm\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
