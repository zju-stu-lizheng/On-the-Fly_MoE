{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先都加载到cpu上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using atten... sdpa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/venv/dilab/floe/hqq/hqq/models/base.py:251: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(cls.get_weight_file(save_dir), map_location=map_location)\n",
      "100%|██████████| 32/32 [00:00<00:00, 265.62it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 1169.48it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"\n",
    "from modeling_mixtral import MixtralForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import json\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "dtype = torch.float16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "### HQQ量化\n",
    "from hqq.core.quantize import *\n",
    "from hqq.models.hf.mixtral import MixtralPatch\n",
    "import transformers\n",
    "from hqq.models.base import BaseHQQModel\n",
    "from accelerate import init_empty_weights\n",
    "\n",
    "save_dir = './hqqsaved'\n",
    "\n",
    "class BaseHQQHFModel(BaseHQQModel):\n",
    "    # Save model architecture\n",
    "    @classmethod\n",
    "    def cache_model(cls, model, save_dir):\n",
    "        model.config.save_pretrained(save_dir)\n",
    "\n",
    "    # Create empty model from config\n",
    "    @classmethod\n",
    "    def create_model(cls, save_dir, kwargs):\n",
    "        model_kwargs = {}\n",
    "        for key in [\"attn_implementation\"]:\n",
    "            if key in kwargs:\n",
    "                model_kwargs[key] = kwargs[key]\n",
    "\n",
    "        config = transformers.AutoConfig.from_pretrained(\n",
    "            cls.get_config_file(save_dir)\n",
    "        )\n",
    "\n",
    "        with init_empty_weights():\n",
    "            model = MixtralForCausalLM._from_config(config, **model_kwargs)\n",
    "\n",
    "        return model\n",
    "\n",
    "class MixtralHQQ(MixtralPatch, BaseHQQHFModel):\n",
    "    pass\n",
    "\n",
    "### 从保存的权重中加载\n",
    "llm = MixtralHQQ.from_quantized(save_dir, compute_dtype=dtype, device='cpu')\n",
    "HQQLinear.set_backend(HQQBackend.PYTORCH)\n",
    "\n",
    "backend       = \"hqq\" #'torchao_int4' #\"torchao_int4\" (4-bit only) or \"gemlite\" (4-bit + 2-bit)\n",
    "# #Optimize\n",
    "from hqq.utils.patching import prepare_for_inference\n",
    "prepare_for_inference(llm, backend=backend, verbose=True)\n",
    "\n",
    "if(backend == 'gemlite'):\n",
    "\timport gemlite\n",
    "\tgemlite.core.GEMLITE_TRITON_RESTRICT_M = True\n",
    "\tgemlite.core.GemLiteLinear.load_config('/tmp/gemlite_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "active neural num  2867\n",
      "active neural num  2867\n",
      "... loading layer 0\n",
      "... loading layer 1\n",
      "... loading layer 2\n",
      "... loading layer 3\n",
      "... loading layer 4\n",
      "... loading layer 5\n",
      "... loading layer 6\n",
      "... loading layer 7\n",
      "... loading layer 8\n",
      "... loading layer 9\n",
      "... loading layer 10\n",
      "... loading layer 11\n",
      "... loading layer 12\n",
      "... loading layer 13\n",
      "... loading layer 14\n",
      "... loading layer 15\n",
      "... loading layer 16\n",
      "... loading layer 17\n",
      "... loading layer 18\n",
      "... loading layer 19\n",
      "... loading layer 20\n",
      "... loading layer 21\n",
      "... loading layer 22\n",
      "... loading layer 23\n",
      "... loading layer 24\n",
      "... loading layer 25\n",
      "... loading layer 26\n",
      "... loading layer 27\n",
      "... loading layer 28\n",
      "... loading layer 29\n",
      "... loading layer 30\n",
      "... loading layer 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/On-the-Fly_MoE_Inference/offloading/pipelinellm.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(f'../expert_predictor/training/{layer_idx}-{training_epoch}.pth'))\n"
     ]
    }
   ],
   "source": [
    "from pipelinellm import convert_mixtral_to_cached_mlp, PipelineLLM\n",
    "device_map = {layer_idx: 'cuda:1' if layer_idx <= 16 else 'cuda:2' for layer_idx in range(1, 32)}\n",
    "llm, cached_mlps = convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.8, backends=backend\n",
    "    , device='cuda:0', device_map=device_map)\n",
    "# 创建流水线模型\n",
    "PLLM = PipelineLLM(llm, cached_mlps, 1, 3, training_epoch=20,\n",
    " device='cuda:0', device_map=device_map, print_layer_info=True) ### use ep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 尝试替换torch.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w3_forward = torch.compile(llm.model.layers[0].block_sparse_moe.experts[1].w3.forward, fullgraph=True, mode=\"reduce-overhead\")\n",
    "# w3 = llm.model.layers[0].block_sparse_moe.experts[1].w3\n",
    "# w3.cuda(0)\n",
    "# w3.forward = w3_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试时间开销"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warm up ...\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "\n",
    "input_length = 10\n",
    "MAX_LENGTH = 2\n",
    "output_length = 10\n",
    "test_samples = 1\n",
    "device_id = 0\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "filt_type = fineweb_path.split('.')[-1]\n",
    "fineweb = load_dataset(filt_type, data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text']\n",
    "\n",
    "print(\"warm up ...\")\n",
    "# 预热（避免第一次运行时的额外开销）\n",
    "for text in fineweb_text[:10]:\n",
    "    inputs = preprocess_data(text, tokenizer)\n",
    "    with torch.no_grad():\n",
    "        output = llm(input_ids=inputs[\"input_ids\"].cuda(device_id), attention_mask=inputs[\"attention_mask\"].cuda(device_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "\n",
    "input_length = 10\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 256\n",
    "test_samples = 10\n",
    "device_id = 0\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "# print(\"warm up ...\")\n",
    "# # 预热（避免第一次运行时的额外开销）\n",
    "# for text in fineweb_text[:5] :\n",
    "#     inputs = preprocess_data(text, tokenizer)\n",
    "#     with torch.no_grad():\n",
    "#         output = llm(input_ids=inputs[\"input_ids\"].cuda(device_id), attention_mask=inputs[\"attention_mask\"].cuda(device_id))\n",
    "\n",
    "generated_all = 0\n",
    "prefill_time, decode_time = 0, 0\n",
    "reloaded_experts = 0\n",
    "print(\"output length is {}\".format(output_length))\n",
    "for text in fineweb_text[2:2+test_samples] :\n",
    "    inputs = preprocess_data(text, tokenizer)\n",
    "    ### 清空统计数据\n",
    "    PLLM.get_prefill_time()\n",
    "    PLLM.get_reload_experts()\n",
    "\n",
    "    # 测试时间\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # 开始计时\n",
    "    torch.cuda.synchronize()\n",
    "    start_event.record()\n",
    "\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(device_id),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(device_id),\n",
    "            max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.pad_token_id, \n",
    "            # cache_implementation=\"static\" ## moe not support\n",
    "        )\n",
    "\n",
    "    # 结束计时\n",
    "    end_event.record()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # 计算时间\n",
    "    elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "    decode_time += elapsed_time\n",
    "    cur_prefill_time = PLLM.get_prefill_time()\n",
    "    prefill_time += cur_prefill_time\n",
    "    print(f\"Generated length: {len(output[0]) - input_length}\", f\"Time taken: {elapsed_time:.2f} s,\", f\"prefill time: {cur_prefill_time:.2f} s\")\n",
    "    # print(output)\n",
    "    print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "\n",
    "    generated_all += (len(output[0]) - input_length - 1)\n",
    "    reloaded_experts += PLLM.get_reload_experts()\n",
    "\n",
    "timepertoken = (decode_time - prefill_time) / (generated_all)\n",
    "print(\"decode phase speed:\", '{:.4f}'.format(1/timepertoken) , 'token/s')\n",
    "print(\"the number of reloaded experts per token:\", '{:.3f}'.format(reloaded_experts / generated_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################################################################\n",
    "#Save gemlite cache\n",
    "if(backend == 'gemlite'):\n",
    "\tgemlite.core.GemLiteLinear.cache_config('/tmp/gemlite_config.json') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.profile\n",
    "\n",
    "attention使用sdpa实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 2\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                  cudaStreamSynchronize        31.23%      99.252ms        31.23%      99.252ms     214.831us      39.872us         0.02%      39.872us       0.086us           462  \n",
      "                                       cudaLaunchKernel        24.17%      76.802ms        24.17%      76.802ms       8.589us       0.000us         0.00%       0.000us       0.000us          8942  \n",
      "                                               aten::mm         4.60%      14.613ms        11.69%      37.137ms      40.720us      23.881ms         9.44%      23.881ms      26.185us           912  \n",
      "                                            aten::copy_         4.47%      14.215ms         9.59%      30.491ms      12.699us      60.324ms        23.84%      60.324ms      25.125us          2401  \n",
      "                                  cudaFuncGetAttributes         3.02%       9.594ms         3.02%       9.594ms      17.669us       0.000us         0.00%       0.000us       0.000us           543  \n",
      "                                             cudaMalloc         2.98%       9.462ms         2.98%       9.462ms       1.892ms       0.000us         0.00%       0.000us       0.000us             5  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         2.87%       9.128ms         2.87%       9.128ms       8.854us       0.000us         0.00%       0.000us       0.000us          1031  \n",
      "                                        cudaMemcpyAsync         2.76%       8.766ms         2.76%       8.766ms       7.682us       0.000us         0.00%       0.000us       0.000us          1141  \n",
      "                                      aten::bitwise_and         2.08%       6.597ms         3.24%      10.284ms       9.232us      21.601ms         8.54%      21.601ms      19.390us          1114  \n",
      "                                              aten::mul         1.54%       4.893ms         2.65%       8.423ms       7.992us      46.950ms        18.55%      46.950ms      44.544us          1054  \n",
      "                                            aten::empty         1.49%       4.735ms         1.49%       4.735ms       2.812us       0.000us         0.00%       0.000us       0.000us          1684  \n",
      "                                            aten::index         1.43%       4.557ms        11.21%      35.617ms      64.407us      18.939ms         7.48%      18.939ms      34.249us           553  \n",
      "                                             aten::topk         1.04%       3.308ms         2.08%       6.599ms      23.910us      12.712ms         5.02%      12.712ms      46.057us           276  \n",
      "                                       aten::__rshift__         1.00%       3.192ms         2.70%       8.573ms      10.280us      15.846ms         6.26%      15.846ms      19.001us           834  \n",
      "                                    cudaStreamWaitEvent         0.91%       2.878ms         0.91%       2.878ms       2.507us       0.000us         0.00%       0.000us       0.000us          1148  \n",
      "                                              aten::add         0.89%       2.822ms         1.50%       4.759ms       8.233us       1.432ms         0.57%       1.432ms       2.477us           578  \n",
      "                                            aten::slice         0.89%       2.821ms         1.09%       3.465ms       1.411us       0.000us         0.00%       0.000us       0.000us          2455  \n",
      "                                    aten::empty_strided         0.68%       2.166ms         1.74%       5.515ms       7.473us       0.000us         0.00%       0.000us       0.000us           738  \n",
      "                                               aten::to         0.61%       1.935ms         6.63%      21.059ms      16.401us       0.000us         0.00%       2.382ms       1.855us          1284  \n",
      "                               aten::bitwise_left_shift         0.59%       1.861ms         0.88%       2.794ms      11.267us     584.923us         0.23%     584.923us       2.359us           248  \n",
      "                                    cudaLaunchKernelExC         0.55%       1.752ms         0.55%       1.752ms       4.253us       0.000us         0.00%       0.000us       0.000us           412  \n",
      "                                              aten::cat         0.54%       1.731ms         0.81%       2.560ms      13.062us     798.931us         0.32%     798.931us       4.076us           196  \n",
      "                                       aten::as_strided         0.54%       1.709ms         0.54%       1.709ms       0.294us       0.000us         0.00%       0.000us       0.000us          5812  \n",
      "                                           aten::select         0.52%       1.639ms         0.61%       1.938ms       1.799us       0.000us         0.00%       0.000us       0.000us          1077  \n",
      "                                              aten::sub         0.50%       1.578ms         0.82%       2.617ms       9.247us      44.767ms        17.69%      44.767ms     158.188us           283  \n",
      "                                           aten::matmul         0.41%       1.291ms        12.23%      38.867ms      42.617us       0.000us         0.00%      23.881ms      26.185us           912  \n",
      "                                         aten::_to_copy         0.40%       1.273ms         6.02%      19.124ms      28.501us       0.000us         0.00%       2.382ms       3.549us           671  \n",
      "                                        cudaMemsetAsync         0.39%       1.255ms         0.39%       1.255ms       4.420us       0.000us         0.00%       0.000us       0.000us           284  \n",
      "                                             aten::mean         0.37%       1.185ms         0.53%       1.675ms      12.887us     662.839us         0.26%     662.839us       5.099us           130  \n",
      "                                              aten::pow         0.35%       1.106ms         0.52%       1.661ms      12.773us     305.806us         0.12%     305.806us       2.352us           130  \n",
      "                                             aten::view         0.32%       1.031ms         0.32%       1.031ms       0.616us       0.000us         0.00%       0.000us       0.000us          1672  \n",
      "                              aten::_local_scalar_dense         0.31%     999.028us        26.96%      85.669ms     279.965us     688.342us         0.27%     688.342us       2.249us           306  \n",
      "                                          aten::reshape         0.30%     952.915us         1.06%       3.364ms       2.501us       0.000us         0.00%     433.835us       0.323us          1345  \n",
      "                                              aten::abs         0.30%     938.492us         2.05%       6.516ms      21.718us     377.701us         0.15%     755.402us       2.518us           300  \n",
      "                                             aten::silu         0.26%     820.242us         0.41%       1.318ms       9.833us     431.299us         0.17%     431.299us       3.219us           134  \n",
      "                                         aten::_softmax         0.25%     798.781us         0.41%       1.288ms      10.219us     339.013us         0.13%     339.013us       2.691us           126  \n",
      "                         aten::_flash_attention_forward         0.24%     754.474us         0.60%       1.915ms      29.916us     443.943us         0.18%     443.943us       6.937us            64  \n",
      "                                        aten::transpose         0.23%     743.457us         0.34%       1.073ms       1.180us       0.000us         0.00%       0.000us       0.000us           910  \n",
      "                                              aten::neg         0.22%     711.065us         0.36%       1.147ms       8.962us     459.879us         0.18%     459.879us       3.593us           128  \n",
      "                                        cudaEventRecord         0.22%     708.634us         0.22%     708.634us       0.617us       0.000us         0.00%       0.000us       0.000us          1148  \n",
      "                                            aten::equal         0.22%     708.421us        21.58%      68.574ms       1.106ms     161.888us         0.06%     443.205us       7.148us            62  \n",
      "                                            aten::rsqrt         0.22%     702.164us         0.36%       1.153ms       8.871us     328.140us         0.13%     328.140us       2.524us           130  \n",
      "                                   cudaFuncSetAttribute         0.19%     609.392us         0.19%     609.392us       0.662us       0.000us         0.00%       0.000us       0.000us           920  \n",
      "                                          aten::permute         0.19%     604.099us         0.23%     719.987us       1.791us       0.000us         0.00%       0.000us       0.000us           402  \n",
      "                                           aten::linear         0.19%     602.871us         9.33%      29.654ms      74.507us       0.000us         0.00%       4.608ms      11.578us           398  \n",
      "                                              aten::sum         0.19%     596.335us         0.27%     847.654us      13.245us     285.351us         0.11%     285.351us       4.459us            64  \n",
      "                                          aten::__and__         0.17%     550.979us         3.41%      10.835ms       9.726us       0.000us         0.00%      21.601ms      19.390us          1114  \n",
      "                                                aten::t         0.17%     529.220us         0.31%     985.392us       2.476us       0.000us         0.00%       0.000us       0.000us           398  \n",
      "                     aten::scaled_dot_product_attention         0.16%     508.476us         0.92%       2.935ms      45.866us       0.000us         0.00%     443.943us       6.937us            64  \n",
      "                                              aten::all         0.16%     493.006us         0.38%       1.210ms      18.900us       5.088us         0.00%     149.664us       2.339us            64  \n",
      "                                             aten::div_         0.13%     402.314us         0.20%     650.998us      10.172us     215.139us         0.09%     215.139us       3.362us            64  \n",
      "                                          aten::numpy_T         0.12%     372.605us         0.34%       1.093ms       2.718us       0.000us         0.00%       0.000us       0.000us           402  \n",
      "                                               aten::ne         0.11%     343.122us         0.19%     600.500us       9.685us     154.629us         0.06%     154.629us       2.494us            62  \n",
      "                                          aten::resize_         0.10%     325.764us         0.10%     325.764us       2.143us       0.000us         0.00%       0.000us       0.000us           152  \n",
      "              aten::_scaled_dot_product_flash_attention         0.10%     317.489us         0.76%       2.427ms      37.921us       0.000us         0.00%     443.943us       6.937us            64  \n",
      "                                        aten::unsqueeze         0.10%     314.100us         0.13%     407.326us       1.555us       0.000us         0.00%       0.000us       0.000us           262  \n",
      "                                            aten::addmm         0.09%     300.001us         1.05%       3.348ms     279.017us      66.144us         0.03%      66.144us       5.512us            12  \n",
      "                                              aten::any         0.07%     234.507us         0.18%     570.788us      22.832us      91.810us         0.04%     104.418us       4.177us            25  \n",
      "                                             aten::item         0.07%     232.336us        27.03%      85.902ms     280.724us       0.000us         0.00%     688.342us       2.249us           306  \n",
      "                                            aten::clone         0.07%     228.806us         0.54%       1.729ms      12.904us       0.000us         0.00%     449.099us       3.351us           134  \n",
      "                                               aten::eq         0.07%     227.639us         0.16%     518.268us      17.871us      81.921us         0.03%      81.921us       2.825us            29  \n",
      "                                     aten::_unsafe_view         0.07%     226.777us         0.07%     226.777us       0.588us       0.000us         0.00%       0.000us       0.000us           386  \n",
      "                                       aten::empty_like         0.07%     207.407us         0.23%     720.826us       3.659us       0.000us         0.00%       0.000us       0.000us           197  \n",
      "                                           aten::expand         0.06%     206.448us         0.08%     251.502us       1.965us       0.000us         0.00%       0.000us       0.000us           128  \n",
      "                                          aten::softmax         0.06%     204.539us         0.47%       1.492ms      11.842us       0.000us         0.00%     339.013us       2.691us           126  \n",
      "                                 cudaDeviceGetAttribute         0.06%     195.038us         0.06%     195.038us       0.385us       0.000us         0.00%       0.000us       0.000us           507  \n",
      "                                           aten::cumsum         0.03%      89.014us         1.88%       5.985ms       1.995ms      16.194us         0.01%      16.194us       5.398us             3  \n",
      "                                  cudaDeviceSynchronize         0.03%      88.721us         0.03%      88.721us      88.721us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                          aten::view_as         0.03%      84.286us         0.04%     117.330us       1.725us       0.000us         0.00%       0.000us       0.000us            68  \n",
      "                                             aten::isin         0.03%      81.043us         0.15%     473.381us     157.794us       0.000us         0.00%      28.929us       9.643us             3  \n",
      "                                       aten::is_nonzero         0.02%      66.641us         0.51%       1.621ms      23.152us       0.000us         0.00%     161.156us       2.302us            70  \n",
      "                                  cudaStreamIsCapturing         0.02%      63.483us         0.02%      63.483us       0.920us       0.000us         0.00%       0.000us       0.000us            69  \n",
      "                                            aten::fill_         0.02%      58.105us         0.08%     253.511us      23.046us      24.832us         0.01%      24.832us       2.257us            11  \n",
      "                                      aten::result_type         0.02%      48.101us         0.02%      48.101us       0.370us       0.000us         0.00%       0.000us       0.000us           130  \n",
      "                                          aten::detach_         0.01%      41.466us         0.02%      55.881us       2.430us       0.000us         0.00%       0.000us       0.000us            23  \n",
      "                                     aten::index_select         0.01%      40.553us         0.02%      71.610us      35.805us       7.967us         0.00%       7.967us       3.984us             2  \n",
      "                                       aten::bitwise_or         0.01%      35.729us         0.02%      77.855us      19.464us       9.920us         0.00%       9.920us       2.480us             4  \n",
      "                                     aten::masked_fill_         0.01%      35.300us         0.13%     422.033us     211.017us       4.704us         0.00%       4.704us       2.352us             2  \n",
      "                                               aten::lt         0.01%      34.855us         0.99%       3.160ms       3.160ms       3.040us         0.00%       3.040us       3.040us             1  \n",
      "                                           aten::argmax         0.01%      34.546us         1.05%       3.334ms       1.667ms      24.000us         0.01%      24.000us      12.000us             2  \n",
      "                                              aten::max         0.01%      30.368us         2.02%       6.434ms       3.217ms       9.889us         0.00%       9.889us       4.944us             2  \n",
      "                                               aten::ge         0.01%      26.091us         0.01%      37.092us      18.546us       4.768us         0.00%       4.768us       2.384us             2  \n",
      "                                      aten::bitwise_not         0.01%      21.700us         0.02%      54.957us      27.478us       4.735us         0.00%       4.735us       2.367us             2  \n",
      "                                        aten::embedding         0.01%      19.133us         0.03%      95.820us      47.910us       0.000us         0.00%       7.967us       3.984us             2  \n",
      "                                                detach_         0.00%      14.415us         0.00%      14.415us       0.627us       0.000us         0.00%       0.000us       0.000us            23  \n",
      "                                         aten::new_ones         0.00%      13.135us         0.01%      44.417us      22.208us       0.000us         0.00%       5.184us       2.592us             2  \n",
      "                                             aten::rsub         0.00%      10.087us         0.02%      50.067us      25.034us       0.000us         0.00%       5.152us       2.576us             2  \n",
      "                                             aten::full         0.00%       8.669us         0.01%      45.310us      11.328us       0.000us         0.00%       8.832us       2.208us             4  \n",
      "                                           aten::__or__         0.00%       7.395us         0.03%      85.250us      21.312us       0.000us         0.00%       9.920us       2.480us             4  \n",
      "                                     aten::resolve_conj         0.00%       7.135us         0.00%       7.135us       0.155us       0.000us         0.00%       0.000us       0.000us            46  \n",
      "                                             aten::ones         0.00%       6.570us         0.02%      60.748us      60.748us       0.000us         0.00%       2.304us       2.304us             1  \n",
      "                                       aten::lift_fresh         0.00%       4.987us         0.00%       4.987us       0.217us       0.000us         0.00%       0.000us       0.000us            23  \n",
      "                                      aten::resolve_neg         0.00%       4.907us         0.00%       4.907us       0.107us       0.000us         0.00%       0.000us       0.000us            46  \n",
      "                                        aten::ones_like         0.00%       4.088us         0.01%      19.906us      19.906us       0.000us         0.00%       2.016us       2.016us             1  \n",
      "                                        aten::new_empty         0.00%       3.173us         0.00%       9.443us       4.721us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                    cudaPeekAtLastError         0.00%       2.017us         0.00%       2.017us       0.168us       0.000us         0.00%       0.000us       0.000us            12  \n",
      "                                     cudaGetDeviceCount         0.00%       0.149us         0.00%       0.149us       0.149us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      40.961us         0.02%      40.961us       1.638us            25  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      15.328us         0.01%      15.328us       2.190us             7  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     319.301us         0.13%     319.301us       2.554us           125  \n",
      "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     438.089us         0.17%     438.089us       2.306us           190  \n",
      "                         Memcpy DtoH (Device -> Pinned)         0.00%       0.000us         0.00%       0.000us       0.000us     688.342us         0.27%     688.342us       2.249us           306  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       7.808us         0.00%       7.808us       2.603us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       9.504us         0.00%       9.504us       2.376us             4  \n",
      "void at_cuda_detail::cub::DeviceScanInitKernel<at_cu...         0.00%       0.000us         0.00%       0.000us       0.000us       7.233us         0.00%       7.233us       2.411us             3  \n",
      "void at_cuda_detail::cub::DeviceScanKernel<at_cuda_d...         0.00%       0.000us         0.00%       0.000us       0.000us       8.961us         0.00%       8.961us       2.987us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      12.064us         0.00%      12.064us       2.413us             5  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      14.048us         0.01%      14.048us       2.341us             6  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.704us         0.00%       4.704us       2.352us             2  \n",
      "void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us       7.967us         0.00%       7.967us       3.984us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     463.536us         0.18%     463.536us       3.512us           132  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     305.806us         0.12%     305.806us       2.352us           130  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     662.839us         0.26%     662.839us       5.099us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     319.595us         0.13%     319.595us       2.458us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     328.140us         0.13%     328.140us       2.524us           130  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     438.570us         0.17%     438.570us       3.374us           130  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us      39.067ms        15.44%      39.067ms      29.914us          1306  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     703.890us         0.28%     703.890us       2.728us           258  \n",
      "sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x...         0.00%       0.000us         0.00%       0.000us       0.000us       2.202ms         0.87%       2.202ms      17.205us           128  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us     980.054us         0.39%     980.054us       7.314us           134  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      18.935ms         7.48%      18.935ms      34.303us           552  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      45.796ms        18.10%      45.796ms      69.179us           662  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     459.879us         0.18%     459.879us       3.593us           128  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     558.412us         0.22%     558.412us       4.363us           128  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     802.502us         0.32%     802.502us       2.508us           320  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     433.835us         0.17%     433.835us       3.389us           128  \n",
      "void pytorch_flash::flash_fwd_kernel<pytorch_flash::...         0.00%       0.000us         0.00%       0.000us       0.000us     443.943us         0.18%     443.943us       6.937us            64  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     431.299us         0.17%     431.299us       3.219us           134  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       1.819ms         0.72%       1.819ms      13.991us           130  \n",
      "void (anonymous namespace)::softmax_warp_forward<c10...         0.00%       0.000us         0.00%       0.000us       0.000us     339.013us         0.13%     339.013us       2.691us           126  \n",
      "void at::native::sbtopk::gatherTopK<float, unsigned ...         0.00%       0.000us         0.00%       0.000us       0.000us     886.702us         0.35%     886.702us       7.037us           126  \n",
      "void at::native::bitonicSortKVInPlace<-2, -1, 16, 16...         0.00%       0.000us         0.00%       0.000us       0.000us     745.852us         0.29%     745.852us       5.919us           126  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      21.595ms         8.53%      21.595ms      19.420us          1112  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      15.846ms         6.26%      15.846ms      19.001us           834  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      44.755ms        17.69%      44.755ms     160.990us           278  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us     446.887us         0.18%     446.887us       1.574us           284  \n",
      "sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x...         0.00%       0.000us         0.00%       0.000us       0.000us      15.221ms         6.02%      15.221ms      53.595us           284  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     377.701us         0.15%     377.701us       2.518us           150  \n",
      "void at::native::sbtopk::gatherTopK<c10::Half, unsig...         0.00%       0.000us         0.00%       0.000us       0.000us       8.318ms         3.29%       8.318ms      55.455us           150  \n",
      "void at::native::radixSortKVInPlace<-2, -1, 128, 32,...         0.00%       0.000us         0.00%       0.000us       0.000us       2.761ms         1.09%       2.761ms      18.407us           150  \n",
      "                         Memcpy PtoP (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      19.774ms         7.81%      19.774ms      34.449us           574  \n",
      "void dot_kernel<float, 128, 0, cublasDotParams<cubla...         0.00%       0.000us         0.00%       0.000us       0.000us     451.493us         0.18%     451.493us       3.762us           120  \n",
      "void reduce_1Block_kernel<float, 128, 7, cublasGemvT...         0.00%       0.000us         0.00%       0.000us       0.000us     350.980us         0.14%     350.980us       2.925us           120  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     285.351us         0.11%     285.351us       4.459us            64  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     215.139us         0.09%     215.139us       3.362us            64  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_...         0.00%       0.000us         0.00%       0.000us       0.000us     178.469us         0.07%     178.469us      44.617us             4  \n",
      "void splitKreduce_kernel<32, 16, int, float, __half,...         0.00%       0.000us         0.00%       0.000us       0.000us      14.816us         0.01%      14.816us       3.704us             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     301.682us         0.12%     301.682us       2.433us           124  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     283.241us         0.11%     283.241us       2.284us           124  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     304.388us         0.12%     304.388us       2.416us           126  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       2.282ms         0.90%       2.282ms      18.405us           124  \n",
      "                       Memcpy DtoH (Device -> Pageable)         0.00%       0.000us         0.00%       0.000us       0.000us     107.076us         0.04%     107.076us       2.328us            46  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      60.129us         0.02%      60.129us       3.006us            20  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      91.810us         0.04%      91.810us       4.590us            20  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us      13.793us         0.01%      13.793us       3.448us             4  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      24.000us         0.01%      24.000us      12.000us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      10.687us         0.00%      10.687us       2.672us             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.152us         0.00%       5.152us       2.576us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       9.920us         0.00%       9.920us       2.480us             4  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       2.528us         0.00%       2.528us       2.528us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.735us         0.00%       4.735us       2.367us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       5.888us         0.00%       5.888us       2.944us             2  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       9.889us         0.00%       9.889us       4.944us             2  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.968us         0.00%       3.968us       3.968us             1  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       5.088us         0.00%       5.088us       5.088us             1  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     226.726us         0.09%     226.726us       3.543us            64  \n",
      "void at::native::vectorized_elementwise_kernel<2, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.432us         0.00%       2.432us       2.432us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 317.805ms\n",
      "Self CUDA time total: 253.045ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 1\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 2\n",
    "test_samples = 4\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "fineweb = load_dataset(\"parquet\",data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text'][:test_samples] \n",
    "\n",
    "print(\"output length is {}\".format(output_length))\n",
    "text = fineweb_text[0]\n",
    "inputs = preprocess_data(text, tokenizer)\n",
    "\n",
    "# cached_mlp.clear_load_from_cpu_stats()\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ]\n",
    ") as p:\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(),\n",
    "            max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "print(p.key_averages().table(\n",
    "    sort_by=\"self_cpu_time_total\", row_limit=-1))\n",
    "p.export_chrome_trace(\"./offloading-2gpu-2.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试一个正常输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "Generated length: 32 Time taken: 71.98 s prefill time: 51.36 s\n",
      "['The future of AI is here,  and it’s not just about the technology.\\n\\nIt’s also about the way we think and work.\\nAI is a new technology that is']\n",
      "decode phase speed: 1.5033  token/s\n",
      "the number of experts reload per token: 10.774193548387096\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 10\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 32\n",
    "device_id = 0\n",
    "test_samples = 1\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "generated_all, decode_time, prefill_time = 0, 0, 0\n",
    "# print(\"max output length is {}\".format(output_length))\n",
    "text = \"The future of AI is here, \"\n",
    "\n",
    "clear_prefill_time = PLLM.get_prefill_time()\n",
    "clear_experts = PLLM.get_reload_experts()\n",
    "# print(\"need to zero: \", clear_experts, clear_prefill_time)\n",
    "\n",
    "inputs = preprocess_data(text, tokenizer)\n",
    "# 测试时间\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "# 开始计时\n",
    "torch.cuda.synchronize()\n",
    "start_event.record()\n",
    "\n",
    "# 前向传播\n",
    "with torch.no_grad():\n",
    "    output = llm.generate(\n",
    "        input_ids=inputs[\"input_ids\"].cuda(device_id),\n",
    "        attention_mask=inputs[\"attention_mask\"].cuda(device_id),\n",
    "        max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "        generation_config=GenerationConfig(do_sample=False),\n",
    "        pad_token_id=tokenizer.pad_token_id, \n",
    "        # cache_implementation=\"static\" ## moe not support\n",
    "    )\n",
    "\n",
    "# 结束计时\n",
    "end_event.record()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# 计算时间\n",
    "elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "decode_time += elapsed_time\n",
    "cur_prefill_time = PLLM.get_prefill_time()\n",
    "prefill_time += cur_prefill_time\n",
    "print(f\"Generated length: {len(output[0]) - input_length}\", f\"Time taken: {elapsed_time:.2f} s\", f\"prefill time: {cur_prefill_time:.2f} s\")\n",
    "# print(output)\n",
    "print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "\n",
    "generated_all += (len(output[0]) - input_length -1)\n",
    "\n",
    "timepertoken = (decode_time - prefill_time) / (generated_all)\n",
    "print(\"decode phase speed:\", '{:.4f}'.format(1/timepertoken) , ' token/s')\n",
    "print(\"the number of experts reload per token:\", PLLM.get_reload_experts() / generated_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载到GPU上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:25<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MixtralForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "import json\n",
    "\n",
    "def get_model(model_name, device_map, dtype=torch.bfloat16):\n",
    "    llm = MixtralForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        use_cache=True,\n",
    "        torch_dtype=dtype,\n",
    "    ) \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return llm, tokenizer\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "with open('../quantize/device_map_1.json', 'r') as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "dtype = torch.float16\n",
    "llm, tokenizer = get_model(model_name, device_map, dtype=dtype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
