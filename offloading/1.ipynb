{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先都加载到cpu上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using atten... sdpa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/venv/dilab/floe/hqq/hqq/models/base.py:251: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(cls.get_weight_file(save_dir), map_location=map_location)\n",
      "100%|██████████| 32/32 [00:00<00:00, 609.13it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 1209.84it/s]\n",
      "100%|██████████| 32/32 [03:32<00:00,  6.63s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"\n",
    "from modeling_mixtral import MixtralForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import json\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "dtype = torch.float16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "### HQQ量化\n",
    "from hqq.core.quantize import *\n",
    "from hqq.models.hf.mixtral import MixtralPatch\n",
    "import transformers\n",
    "from hqq.models.base import BaseHQQModel\n",
    "from accelerate import init_empty_weights\n",
    "\n",
    "save_dir = './hqqsaved'\n",
    "\n",
    "class BaseHQQHFModel(BaseHQQModel):\n",
    "    # Save model architecture\n",
    "    @classmethod\n",
    "    def cache_model(cls, model, save_dir):\n",
    "        model.config.save_pretrained(save_dir)\n",
    "\n",
    "    # Create empty model from config\n",
    "    @classmethod\n",
    "    def create_model(cls, save_dir, kwargs):\n",
    "        model_kwargs = {}\n",
    "        for key in [\"attn_implementation\"]:\n",
    "            if key in kwargs:\n",
    "                model_kwargs[key] = kwargs[key]\n",
    "\n",
    "        config = transformers.AutoConfig.from_pretrained(\n",
    "            cls.get_config_file(save_dir)\n",
    "        )\n",
    "\n",
    "        with init_empty_weights():\n",
    "            model = MixtralForCausalLM._from_config(config, **model_kwargs)\n",
    "\n",
    "        return model\n",
    "\n",
    "class MixtralHQQ(MixtralPatch, BaseHQQHFModel):\n",
    "    pass\n",
    "\n",
    "### 从保存的权重中加载\n",
    "llm = MixtralHQQ.from_quantized(save_dir, compute_dtype=dtype, device='cpu')\n",
    "HQQLinear.set_backend(HQQBackend.PYTORCH)\n",
    "\n",
    "backend       = \"gemlite\" #'torchao_int4' #\"torchao_int4\" (4-bit only) or \"gemlite\" (4-bit + 2-bit)\n",
    "# #Optimize\n",
    "from hqq.utils.patching import prepare_for_inference\n",
    "prepare_for_inference(llm, backend=backend, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_644250/2787104861.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(f'../expert_predictor/training/{layer_idx}-{training_epoch}.pth'))\n"
     ]
    }
   ],
   "source": [
    "from pipelinellm import convert_mixtral_to_cached_mlp, PipelineLLM\n",
    "llm, cached_mlps = convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.8, backends=backend)\n",
    "# 创建流水线模型\n",
    "PLLM = PipelineLLM(llm, cached_mlps, 1, 3, training_epoch=20, print_layer_info=False) ### use ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 尝试替换torch.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w3_forward = torch.compile(llm.model.layers[0].block_sparse_moe.experts[1].w3.forward, fullgraph=True, mode=\"reduce-overhead\")\n",
    "# w3 = llm.model.layers[0].block_sparse_moe.experts[1].w3\n",
    "# w3.cuda(0)\n",
    "# w3.forward = w3_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试时间开销"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warm up ...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "\n",
    "input_length = 10\n",
    "MAX_LENGTH = 1\n",
    "output_length = 10\n",
    "test_samples = 1\n",
    "device_id = 0\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "filt_type = fineweb_path.split('.')[-1]\n",
    "fineweb = load_dataset(filt_type, data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text']\n",
    "\n",
    "print(\"warm up ...\")\n",
    "# 预热（避免第一次运行时的额外开销）\n",
    "for text in fineweb_text[:10]:\n",
    "    inputs = preprocess_data(text, tokenizer)\n",
    "    with torch.no_grad():\n",
    "        output = llm(input_ids=inputs[\"input_ids\"].cuda(device_id), attention_mask=inputs[\"attention_mask\"].cuda(device_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 256\n",
      "Generated length: 256 Time taken: 90.80 s, prefill time: 40.30 s\n",
      "['How do you get HIV?\\nHIV is a virus that can only be transed from one person to another. There are three ways to get HIV.\\n\\n1. Sexual contact.\\n\\n2. Blood to blood.\\n\\n3. From mother to child.\\n\\nHIV is a virus that can only be transed between one person and another.\\n\\n1. Sexual contact.\\n\\nHIV is a virus that can be transed through semen, vaginal blood, and the anus.\\n\\n2. Blood to blood.\\n\\nHIV is a virus that can be transed through blood to blood.\\n\\n3. From mother to child.\\n\\nHIV is a virus that can be transed from a mother to a child.\\n\\nHIV is a virus that can be transed from one person to another.\\n\\nHIV is a virus that can be transed from one person to another.\\n\\nHIV is a virus that can be transed from one person to another.\\n\\nHIV is a virus that can be transed to a child.\\n\\nHIV is a virus that can be transed from a child to a child.\\n\\nHIV is a virus that can be transed from a']\n",
      "Generated length: 256 Time taken: 86.17 s, prefill time: 36.35 s\n",
      "['CTComms sends on average 2 million emails per year.\\n\\nCTComms is a company that helps businesses with their communication. They have a website that is being redesigned and they are in the middle of a rebranding.\\n\\nThey are a company that is in the process of rebranding.\\n\\nThey are a company that is in the process of rebranding.\\n\\nThey are a company that is in the process of rebranding.\\n\\nThey are a company that is in the process of rebranding.\\n\\nThey are a company that is in the process of rebranding.\\n\\nThey are a company that is in the process of rebranding.\\n\\nThey are a company that is in the process of rebranding.\\n\\nThey are a company that is in the process of rebranding.\\n\\nThey are a company that is in the process of rebranding.\\n\\nThey are a company that is in the process of rebranding.\\n\\nThey are a company that is in the process of rebranding.\\n\\nThey are a company that is in the process of rebranding.\\n\\nThey are a company that is in the process of rebranding.\\n\\nThey are a company that']\n",
      "decode phase speed: 5.0837 token/s\n",
      "the number of reloaded experts per token: 9.988\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "\n",
    "input_length = 10\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 256\n",
    "test_samples = 10\n",
    "device_id = 0\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "# print(\"warm up ...\")\n",
    "# # 预热（避免第一次运行时的额外开销）\n",
    "# for text in fineweb_text[:5] :\n",
    "#     inputs = preprocess_data(text, tokenizer)\n",
    "#     with torch.no_grad():\n",
    "#         output = llm(input_ids=inputs[\"input_ids\"].cuda(device_id), attention_mask=inputs[\"attention_mask\"].cuda(device_id))\n",
    "\n",
    "generated_all = 0\n",
    "prefill_time, decode_time = 0, 0\n",
    "reloaded_experts = 0\n",
    "print(\"output length is {}\".format(output_length))\n",
    "for text in fineweb_text[2:2+test_samples] :\n",
    "    inputs = preprocess_data(text, tokenizer)\n",
    "    ### 清空统计数据\n",
    "    PLLM.get_prefill_time()\n",
    "    PLLM.get_reload_experts()\n",
    "\n",
    "    # 测试时间\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # 开始计时\n",
    "    torch.cuda.synchronize()\n",
    "    start_event.record()\n",
    "\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(device_id),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(device_id),\n",
    "            max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.pad_token_id, \n",
    "            # cache_implementation=\"static\" ## moe not support\n",
    "        )\n",
    "\n",
    "    # 结束计时\n",
    "    end_event.record()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # 计算时间\n",
    "    elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "    decode_time += elapsed_time\n",
    "    cur_prefill_time = PLLM.get_prefill_time()\n",
    "    prefill_time += cur_prefill_time\n",
    "    print(f\"Generated length: {len(output[0]) - input_length}\", f\"Time taken: {elapsed_time:.2f} s,\", f\"prefill time: {cur_prefill_time:.2f} s\")\n",
    "    # print(output)\n",
    "    print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "\n",
    "    generated_all += (len(output[0]) - input_length - 1)\n",
    "    reloaded_experts += PLLM.get_reload_experts()\n",
    "\n",
    "timepertoken = (decode_time - prefill_time) / (generated_all)\n",
    "print(\"decode phase speed:\", '{:.4f}'.format(1/timepertoken) , 'token/s')\n",
    "print(\"the number of reloaded experts per token:\", '{:.3f}'.format(reloaded_experts / generated_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################################################################\n",
    "#Save gemlite cache\n",
    "if(backend == 'gemlite'):\n",
    "\tgemlite.core.GemLiteLinear.cache_config('/tmp/gemlite_config.json') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.profile\n",
    "\n",
    "attention使用sdpa实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 2\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::index        31.96%     133.588ms        33.99%     142.088ms     258.812us       1.885ms         0.56%       2.994ms       5.454us           549  \n",
      "                                            aten::copy_        14.02%      58.600ms        19.46%      81.316ms      52.193us     287.072ms        84.58%     287.072ms     184.256us          1558  \n",
      "                                  cudaStreamSynchronize        12.84%      53.652ms        12.84%      53.652ms      75.038us      69.186us         0.02%      69.186us       0.097us           715  \n",
      "gemlite::gemv_revsplitK_A16fWnO16f_int32packing_forw...        10.36%      43.287ms        12.07%      50.449ms     182.785us       7.154ms         2.11%       8.199ms      29.705us           276  \n",
      "                                       cudaLaunchKernel         7.46%      31.195ms         7.46%      31.195ms       5.897us       0.000us         0.00%       0.000us       0.000us          5290  \n",
      "                                        cudaMemcpyAsync         4.33%      18.094ms         4.33%      18.094ms      16.331us       0.000us         0.00%       0.000us       0.000us          1108  \n",
      "                                               aten::mm         2.69%      11.237ms         4.01%      16.747ms      26.415us      10.810ms         3.18%      10.810ms      17.051us           634  \n",
      "                                              aten::mul         1.21%       5.044ms         2.19%       9.172ms      11.819us       4.302ms         1.27%       4.302ms       5.544us           776  \n",
      "                                             aten::topk         1.03%       4.293ms         2.06%       8.614ms      31.438us      13.077ms         3.85%      13.077ms      47.725us           274  \n",
      "                                              aten::add         0.93%       3.905ms         1.68%       7.038ms      12.177us       2.812ms         0.83%       2.812ms       4.866us           578  \n",
      "                                    aten::empty_strided         0.88%       3.699ms         0.88%       3.699ms       5.094us       0.000us         0.00%       0.000us       0.000us           726  \n",
      "                                            aten::empty         0.78%       3.275ms         0.78%       3.275ms       3.391us       0.000us         0.00%       0.000us       0.000us           966  \n",
      "                                            aten::slice         0.68%       2.823ms         0.81%       3.403ms       2.541us       0.000us         0.00%       0.000us       0.000us          1339  \n",
      "                                           aten::select         0.57%       2.387ms         0.68%       2.844ms       2.701us       0.000us         0.00%       0.000us       0.000us          1053  \n",
      "                                       aten::as_strided         0.51%       2.128ms         0.51%       2.128ms       0.485us       0.000us         0.00%       0.000us       0.000us          4386  \n",
      "                                             aten::view         0.48%       2.023ms         0.48%       2.023ms       0.912us       0.000us         0.00%       0.000us       0.000us          2218  \n",
      "                                         aten::_to_copy         0.46%       1.908ms         6.30%      26.335ms      39.963us       0.000us         0.00%       3.909ms       5.931us           659  \n",
      "                                              aten::cat         0.44%       1.836ms         0.72%       3.013ms      15.373us       1.202ms         0.35%       1.202ms       6.132us           196  \n",
      "                                         aten::_softmax         0.39%       1.633ms         0.58%       2.430ms      19.288us     565.999us         0.17%     565.999us       4.492us           126  \n",
      "                                         cuLaunchKernel         0.38%       1.590ms         0.38%       1.590ms       5.761us       0.000us         0.00%       0.000us       0.000us           276  \n",
      "                                             aten::mean         0.37%       1.535ms         0.54%       2.253ms      17.333us     950.380us         0.28%     950.380us       7.311us           130  \n",
      "                                               aten::to         0.34%       1.430ms         6.64%      27.766ms      27.933us       0.000us         0.00%       3.909ms       3.932us           994  \n",
      "                               aten::bitwise_left_shift         0.33%       1.371ms         0.49%       2.041ms      16.457us     694.494us         0.20%     694.494us       5.601us           124  \n",
      "                                            aten::fill_         0.32%       1.318ms         0.81%       3.381ms      11.779us       1.068ms         0.31%       1.068ms       3.721us           287  \n",
      "                                              aten::pow         0.31%       1.298ms         0.51%       2.119ms      16.297us     563.435us         0.17%     563.435us       4.334us           130  \n",
      "                                              aten::abs         0.30%       1.255ms         1.22%       5.119ms      17.293us     442.598us         0.13%     885.196us       2.991us           296  \n",
      "                              aten::_local_scalar_dense         0.28%       1.180ms         2.17%       9.086ms      30.287us       1.666ms         0.49%       1.666ms       5.552us           300  \n",
      "                                           aten::matmul         0.28%       1.178ms         4.42%      18.471ms      29.135us       0.000us         0.00%      10.810ms      17.051us           634  \n",
      "                                          aten::reshape         0.28%       1.157ms         1.04%       4.326ms       4.070us       0.000us         0.00%     669.532us       0.630us          1063  \n",
      "                                             aten::silu         0.25%       1.065ms         0.45%       1.861ms      13.886us     705.290us         0.21%     705.290us       5.263us           134  \n",
      "                         aten::_flash_attention_forward         0.24%       1.013ms         0.58%       2.426ms      37.912us     534.060us         0.16%     534.060us       8.345us            64  \n",
      "                                        aten::transpose         0.24%       1.002ms         0.35%       1.482ms       1.628us       0.000us         0.00%       0.000us       0.000us           910  \n",
      "                                              aten::neg         0.23%     959.249us         0.39%       1.629ms      12.723us     774.437us         0.23%     774.437us       6.050us           128  \n",
      "                                  cudaFuncGetAttributes         0.23%     940.661us         0.23%     940.661us       3.618us       0.000us         0.00%       0.000us       0.000us           260  \n",
      "                                            aten::zero_         0.22%     937.282us         1.00%       4.171ms      15.112us       0.000us         0.00%       1.044ms       3.783us           276  \n",
      "                                            aten::equal         0.21%     883.103us         1.24%       5.196ms      83.814us     353.895us         0.10%       1.174ms      18.938us            62  \n",
      "                                    cudaLaunchKernelExC         0.21%     882.335us         0.21%     882.335us       6.585us       0.000us         0.00%       0.000us       0.000us           134  \n",
      "                                            aten::rsqrt         0.21%     863.097us         0.37%       1.542ms      11.862us     579.370us         0.17%     579.370us       4.457us           130  \n",
      "                                                aten::t         0.20%     838.541us         0.37%       1.556ms       3.910us       0.000us         0.00%       0.000us       0.000us           398  \n",
      "                                              aten::sum         0.20%     824.181us         0.30%       1.245ms      19.457us     472.872us         0.14%     472.872us       7.389us            64  \n",
      "                                       aten::__lshift__         0.19%     796.960us         0.44%       1.852ms      14.939us     679.113us         0.20%     679.113us       5.477us           124  \n",
      "                                           aten::linear         0.18%     766.399us         3.83%      16.019ms      40.249us       0.000us         0.00%       5.989ms      15.049us           398  \n",
      "                     aten::scaled_dot_product_attention         0.15%     607.878us         0.90%       3.744ms      58.502us       0.000us         0.00%     534.060us       8.345us            64  \n",
      "                                             aten::div_         0.13%     536.857us         0.23%     943.399us      14.741us     364.320us         0.11%     364.320us       5.692us            64  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.13%     524.303us         0.13%     524.303us       0.511us       0.000us         0.00%       0.000us       0.000us          1027  \n",
      "                                               aten::ne         0.12%     489.386us         0.23%     952.682us      15.366us     285.986us         0.08%     285.986us       4.613us            62  \n",
      "                                          aten::resize_         0.11%     448.471us         0.11%     448.471us       2.990us       0.000us         0.00%       0.000us       0.000us           150  \n",
      "                                              aten::all         0.11%     442.411us         0.37%       1.542ms      24.092us       4.672us         0.00%     345.128us       5.393us            64  \n",
      "              aten::_scaled_dot_product_flash_attention         0.10%     419.762us         0.75%       3.136ms      49.004us       0.000us         0.00%     534.060us       8.345us            64  \n",
      "                                        aten::unsqueeze         0.10%     406.215us         0.13%     542.746us       2.072us       0.000us         0.00%       0.000us       0.000us           262  \n",
      "                                            aten::addmm         0.09%     379.418us         0.11%     456.583us      38.049us      73.506us         0.02%      73.506us       6.126us            12  \n",
      "                                            aten::clone         0.09%     365.639us         0.60%       2.494ms      18.614us       0.000us         0.00%     684.250us       5.106us           134  \n",
      "                                       aten::empty_like         0.08%     342.182us         0.25%       1.051ms       5.336us       0.000us         0.00%       0.000us       0.000us           197  \n",
      "                                             aten::item         0.08%     318.875us         2.25%       9.405ms      31.350us       0.000us         0.00%       1.666ms       5.552us           300  \n",
      "                                     aten::_unsafe_view         0.07%     300.260us         0.07%     300.260us       0.778us       0.000us         0.00%       0.000us       0.000us           386  \n",
      "                                          aten::permute         0.07%     282.390us         0.08%     333.349us       2.688us       0.000us         0.00%       0.000us       0.000us           124  \n",
      "                                           aten::expand         0.07%     275.469us         0.08%     347.772us       2.717us       0.000us         0.00%       0.000us       0.000us           128  \n",
      "                                 cudaDeviceGetAttribute         0.06%     231.222us         0.06%     231.222us       0.456us       0.000us         0.00%       0.000us       0.000us           507  \n",
      "                                          aten::softmax         0.05%     217.233us         0.63%       2.648ms      21.012us       0.000us         0.00%     565.999us       4.492us           126  \n",
      "                                              aten::any         0.05%     212.809us         0.10%     414.704us      19.748us     120.320us         0.04%     131.615us       6.267us            21  \n",
      "                                               aten::eq         0.05%     203.401us         0.09%     368.600us      14.744us     110.749us         0.03%     110.749us       4.430us            25  \n",
      "                                          aten::numpy_T         0.03%     129.866us         0.11%     463.215us       3.736us       0.000us         0.00%       0.000us       0.000us           124  \n",
      "                                   cudaFuncSetAttribute         0.03%     122.591us         0.03%     122.591us       1.425us       0.000us         0.00%       0.000us       0.000us            86  \n",
      "                                          aten::view_as         0.03%     122.507us         0.04%     172.970us       2.544us       0.000us         0.00%       0.000us       0.000us            68  \n",
      "                                       aten::is_nonzero         0.02%      77.934us         0.51%       2.151ms      30.735us       0.000us         0.00%     394.916us       5.642us            70  \n",
      "                                           aten::cumsum         0.02%      68.039us         0.03%     117.864us      39.288us      15.744us         0.00%      15.744us       5.248us             3  \n",
      "                                  cudaStreamIsCapturing         0.02%      67.280us         0.02%      67.280us       1.051us       0.000us         0.00%       0.000us       0.000us            64  \n",
      "                                             aten::isin         0.02%      64.833us         0.07%     280.634us      93.545us       0.000us         0.00%      27.904us       9.301us             3  \n",
      "                                      aten::result_type         0.01%      61.890us         0.01%      61.890us       0.476us       0.000us         0.00%       0.000us       0.000us           130  \n",
      "                                        cudaMemsetAsync         0.01%      56.742us         0.01%      56.742us       9.457us       0.000us         0.00%       0.000us       0.000us             6  \n",
      "                                              aten::sub         0.01%      51.204us         0.02%      79.183us      15.837us      11.711us         0.00%      11.711us       2.342us             5  \n",
      "                                          aten::detach_         0.01%      46.126us         0.01%      59.815us       3.148us       0.000us         0.00%       0.000us       0.000us            19  \n",
      "                                     aten::index_select         0.01%      39.579us         0.02%      67.363us      33.681us       7.968us         0.00%       7.968us       3.984us             2  \n",
      "                                       aten::bitwise_or         0.01%      37.641us         0.01%      60.140us      15.035us       9.696us         0.00%       9.696us       2.424us             4  \n",
      "                                     aten::masked_fill_         0.01%      34.633us         0.01%      47.856us      23.928us       4.704us         0.00%       4.704us       2.352us             2  \n",
      "                                           aten::argmax         0.01%      34.349us         0.01%      49.310us      24.655us      24.352us         0.01%      24.352us      12.176us             2  \n",
      "                                              aten::max         0.01%      32.292us         0.01%      52.416us      26.208us       9.759us         0.00%       9.759us       4.879us             2  \n",
      "                                  cudaDeviceSynchronize         0.01%      25.254us         0.01%      25.254us      25.254us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                               aten::ge         0.01%      23.716us         0.01%      38.251us      19.125us       4.640us         0.00%       4.640us       2.320us             2  \n",
      "                                      aten::bitwise_not         0.01%      22.619us         0.01%      35.950us      17.975us       4.673us         0.00%       4.673us       2.336us             2  \n",
      "                                      aten::bitwise_and         0.01%      21.148us         0.01%      34.267us      17.133us       5.728us         0.00%       5.728us       2.864us             2  \n",
      "                                               aten::lt         0.00%      19.123us         0.01%      26.661us      26.661us       2.432us         0.00%       2.432us       2.432us             1  \n",
      "                                        aten::embedding         0.00%      17.302us         0.02%      89.419us      44.710us       0.000us         0.00%       7.968us       3.984us             2  \n",
      "                                                detach_         0.00%      13.689us         0.00%      13.689us       0.720us       0.000us         0.00%       0.000us       0.000us            19  \n",
      "                                             aten::rsub         0.00%      12.349us         0.01%      45.064us      22.532us       0.000us         0.00%       4.928us       2.464us             2  \n",
      "                                         aten::new_ones         0.00%      11.899us         0.01%      54.182us      27.091us       0.000us         0.00%       4.576us       2.288us             2  \n",
      "                                             aten::full         0.00%      10.854us         0.02%      62.763us      15.691us       0.000us         0.00%       8.513us       2.128us             4  \n",
      "                                     aten::resolve_conj         0.00%      10.850us         0.00%      10.850us       0.271us       0.000us         0.00%       0.000us       0.000us            40  \n",
      "                                      aten::resolve_neg         0.00%       7.785us         0.00%       7.785us       0.195us       0.000us         0.00%       0.000us       0.000us            40  \n",
      "                                           aten::__or__         0.00%       6.182us         0.02%      66.322us      16.581us       0.000us         0.00%       9.696us       2.424us             4  \n",
      "                                             aten::ones         0.00%       4.957us         0.01%      23.781us      23.781us       0.000us         0.00%       2.048us       2.048us             1  \n",
      "                                          aten::__and__         0.00%       4.557us         0.01%      38.824us      19.412us       0.000us         0.00%       5.728us       2.864us             2  \n",
      "                                       aten::lift_fresh         0.00%       4.464us         0.00%       4.464us       0.235us       0.000us         0.00%       0.000us       0.000us            19  \n",
      "                                        aten::new_empty         0.00%       4.232us         0.00%      12.301us       6.150us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                        aten::ones_like         0.00%       2.938us         0.00%      17.115us      17.115us       0.000us         0.00%       2.017us       2.017us             1  \n",
      "                                    cudaPeekAtLastError         0.00%       1.219us         0.00%       1.219us       0.102us       0.000us         0.00%       0.000us       0.000us            12  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       1.221ms         0.36%       1.221ms       8.423us           145  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      15.137us         0.00%      15.137us       2.162us             7  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     642.345us         0.19%     642.345us       5.139us           125  \n",
      "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     729.428us         0.21%     729.428us       4.075us           179  \n",
      "                         Memcpy DtoH (Device -> Pinned)         0.00%       0.000us         0.00%       0.000us       0.000us       1.666ms         0.49%       1.666ms       5.552us           300  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       7.072us         0.00%       7.072us       2.357us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       8.641us         0.00%       8.641us       2.160us             4  \n",
      "void at_cuda_detail::cub::DeviceScanInitKernel<at_cu...         0.00%       0.000us         0.00%       0.000us       0.000us       7.040us         0.00%       7.040us       2.347us             3  \n",
      "void at_cuda_detail::cub::DeviceScanKernel<at_cuda_d...         0.00%       0.000us         0.00%       0.000us       0.000us       8.704us         0.00%       8.704us       2.901us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     696.647us         0.21%     696.647us       5.400us           129  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      13.856us         0.00%      13.856us       2.309us             6  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.704us         0.00%       4.704us       2.352us             2  \n",
      "void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us       7.968us         0.00%       7.968us       3.984us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     712.404us         0.21%     712.404us       5.397us           132  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     563.435us         0.17%     563.435us       4.334us           130  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     950.380us         0.28%     950.380us       7.311us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     580.552us         0.17%     580.552us       4.466us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     579.370us         0.17%     579.370us       4.457us           130  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     694.756us         0.20%     694.756us       5.344us           130  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       1.097ms         0.32%       1.097ms       5.656us           194  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.210ms         0.36%       1.210ms       4.691us           258  \n",
      "sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x...         0.00%       0.000us         0.00%       0.000us       0.000us       2.634ms         0.78%       2.634ms      20.576us           128  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       1.448ms         0.43%       1.448ms      10.803us           134  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.881ms         0.55%       1.881ms       7.463us           252  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       2.387ms         0.70%       2.387ms       6.215us           384  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     774.437us         0.23%     774.437us       6.050us           128  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     867.623us         0.26%     867.623us       6.778us           128  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.537ms         0.45%       1.537ms       4.803us           320  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     669.532us         0.20%     669.532us       5.231us           128  \n",
      "void pytorch_flash::flash_fwd_kernel<pytorch_flash::...         0.00%       0.000us         0.00%       0.000us       0.000us     534.060us         0.16%     534.060us       8.345us            64  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     705.290us         0.21%     705.290us       5.263us           134  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       2.252ms         0.66%       2.252ms      17.321us           130  \n",
      "void (anonymous namespace)::softmax_warp_forward<c10...         0.00%       0.000us         0.00%       0.000us       0.000us     565.999us         0.17%     565.999us       4.492us           126  \n",
      "void at::native::sbtopk::gatherTopK<float, unsigned ...         0.00%       0.000us         0.00%       0.000us       0.000us       1.124ms         0.33%       1.124ms       8.920us           126  \n",
      "void at::native::bitonicSortKVInPlace<-2, -1, 16, 16...         0.00%       0.000us         0.00%       0.000us       0.000us       1.017ms         0.30%       1.017ms       8.069us           126  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.044ms         0.31%       1.044ms       3.783us           276  \n",
      "          gemv_revsplitK_A16fWnO16f_int32packing_kernel         0.00%       0.000us         0.00%       0.000us       0.000us       7.154ms         2.11%       7.154ms      25.922us           276  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     442.598us         0.13%     442.598us       2.991us           148  \n",
      "void at::native::sbtopk::gatherTopK<c10::Half, unsig...         0.00%       0.000us         0.00%       0.000us       0.000us       8.191ms         2.41%       8.191ms      55.342us           148  \n",
      "void at::native::radixSortKVInPlace<-2, -1, 128, 32,...         0.00%       0.000us         0.00%       0.000us       0.000us       2.745ms         0.81%       2.745ms      18.550us           148  \n",
      "                       Memcpy DtoH (Device -> Pageable)         0.00%       0.000us         0.00%       0.000us       0.000us     877.768us         0.26%     877.768us       4.669us           188  \n",
      "                         Memcpy HtoD (Pinned -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     281.764ms        83.01%     281.764ms     951.905us           296  \n",
      "void dot_kernel<float, 128, 0, cublasDotParams<cubla...         0.00%       0.000us         0.00%       0.000us       0.000us     654.346us         0.19%     654.346us       5.453us           120  \n",
      "void reduce_1Block_kernel<float, 128, 7, cublasGemvT...         0.00%       0.000us         0.00%       0.000us       0.000us     565.042us         0.17%     565.042us       4.709us           120  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     472.872us         0.14%     472.872us       7.389us            64  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     364.320us         0.11%     364.320us       5.692us            64  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us      22.656us         0.01%      22.656us       3.776us             6  \n",
      "sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x...         0.00%       0.000us         0.00%       0.000us       0.000us     402.691us         0.12%     402.691us      67.115us             6  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_...         0.00%       0.000us         0.00%       0.000us       0.000us     206.595us         0.06%     206.595us      51.649us             4  \n",
      "void splitKreduce_kernel<32, 16, int, float, __half,...         0.00%       0.000us         0.00%       0.000us       0.000us      30.336us         0.01%      30.336us       7.584us             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     694.494us         0.20%     694.494us       5.601us           124  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     679.113us         0.20%     679.113us       5.477us           124  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       2.669ms         0.79%       2.669ms      21.525us           124  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      89.310us         0.03%      89.310us       5.582us            16  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     120.320us         0.04%     120.320us       7.520us            16  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us      13.858us         0.00%      13.858us       3.464us             4  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      24.352us         0.01%      24.352us      12.176us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      10.371us         0.00%      10.371us       2.593us             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.928us         0.00%       4.928us       2.464us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.120us         0.00%       5.120us       2.560us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       9.696us         0.00%       9.696us       2.424us             4  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       2.528us         0.00%       2.528us       2.528us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.673us         0.00%       4.673us       2.336us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       5.728us         0.00%       5.728us       2.864us             2  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       9.759us         0.00%       9.759us       4.879us             2  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.872us         0.00%       3.872us       3.872us             1  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       4.672us         0.00%       4.672us       4.672us             1  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     320.481us         0.09%     320.481us       5.008us            64  \n",
      "void at::native::vectorized_elementwise_kernel<2, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.591us         0.00%       2.591us       2.591us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 417.967ms\n",
      "Self CUDA time total: 339.424ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 1\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 2\n",
    "test_samples = 4\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "fineweb = load_dataset(\"parquet\",data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text'][:test_samples] \n",
    "\n",
    "print(\"output length is {}\".format(output_length))\n",
    "text = fineweb_text[0]\n",
    "inputs = preprocess_data(text, tokenizer)\n",
    "\n",
    "# cached_mlp.clear_load_from_cpu_stats()\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ]\n",
    ") as p:\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(),\n",
    "            max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "print(p.key_averages().table(\n",
    "    sort_by=\"self_cpu_time_total\", row_limit=-1))\n",
    "p.export_chrome_trace(\"./offloading-hqq-sdpa-eq.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试一个正常输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "Generated length: 32 Time taken: 71.98 s prefill time: 51.36 s\n",
      "['The future of AI is here,  and it’s not just about the technology.\\n\\nIt’s also about the way we think and work.\\nAI is a new technology that is']\n",
      "decode phase speed: 1.5033  token/s\n",
      "the number of experts reload per token: 10.774193548387096\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 10\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 32\n",
    "device_id = 0\n",
    "test_samples = 1\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "generated_all, decode_time, prefill_time = 0, 0, 0\n",
    "# print(\"max output length is {}\".format(output_length))\n",
    "text = \"The future of AI is here, \"\n",
    "\n",
    "clear_prefill_time = PLLM.get_prefill_time()\n",
    "clear_experts = PLLM.get_reload_experts()\n",
    "# print(\"need to zero: \", clear_experts, clear_prefill_time)\n",
    "\n",
    "inputs = preprocess_data(text, tokenizer)\n",
    "# 测试时间\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "# 开始计时\n",
    "torch.cuda.synchronize()\n",
    "start_event.record()\n",
    "\n",
    "# 前向传播\n",
    "with torch.no_grad():\n",
    "    output = llm.generate(\n",
    "        input_ids=inputs[\"input_ids\"].cuda(device_id),\n",
    "        attention_mask=inputs[\"attention_mask\"].cuda(device_id),\n",
    "        max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "        generation_config=GenerationConfig(do_sample=False),\n",
    "        pad_token_id=tokenizer.pad_token_id, \n",
    "        # cache_implementation=\"static\" ## moe not support\n",
    "    )\n",
    "\n",
    "# 结束计时\n",
    "end_event.record()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# 计算时间\n",
    "elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "decode_time += elapsed_time\n",
    "cur_prefill_time = PLLM.get_prefill_time()\n",
    "prefill_time += cur_prefill_time\n",
    "print(f\"Generated length: {len(output[0]) - input_length}\", f\"Time taken: {elapsed_time:.2f} s\", f\"prefill time: {cur_prefill_time:.2f} s\")\n",
    "# print(output)\n",
    "print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "\n",
    "generated_all += (len(output[0]) - input_length -1)\n",
    "\n",
    "timepertoken = (decode_time - prefill_time) / (generated_all)\n",
    "print(\"decode phase speed:\", '{:.4f}'.format(1/timepertoken) , ' token/s')\n",
    "print(\"the number of experts reload per token:\", PLLM.get_reload_experts() / generated_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载到GPU上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:25<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MixtralForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "import json\n",
    "\n",
    "def get_model(model_name, device_map, dtype=torch.bfloat16):\n",
    "    llm = MixtralForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        use_cache=True,\n",
    "        torch_dtype=dtype,\n",
    "    ) \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return llm, tokenizer\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "with open('../quantize/device_map_1.json', 'r') as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "dtype = torch.float16\n",
    "llm, tokenizer = get_model(model_name, device_map, dtype=dtype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
