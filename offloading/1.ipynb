{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先都加载到cpu上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:05<00:00,  3.22it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "from modeling_mixtral import MixtralForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import json\n",
    "\n",
    "def get_model(model_name, device_map, dtype=torch.bfloat16):\n",
    "    llm = MixtralForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        use_cache=True,\n",
    "        torch_dtype=dtype,\n",
    "    ) \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return llm, tokenizer\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "with open('../quantize/device_map_1.json', 'r') as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "dtype = torch.float16\n",
    "llm, tokenizer = get_model(model_name, 'cpu', dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import threading\n",
    "import json\n",
    "from queue import Queue\n",
    "\n",
    "class CachedMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, dtype, sparsity: float = 0.2):\n",
    "        super(CachedMLP, self).__init__()\n",
    "        self.sparsity = sparsity\n",
    "        self.activenum = int((1 - sparsity) * hidden_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dtype = dtype\n",
    "\n",
    "        # GPU 缓存张量\n",
    "        self.w1_gpu = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda')\n",
    "        self.w2_gpu = torch.empty((self.input_dim, self.activenum), dtype=self.dtype, device='cuda')\n",
    "        self.w3_gpu = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda')\n",
    "\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "        # Pinned Memory 缓冲区\n",
    "        self.register_buffer('sparse_w1_cpu', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "        self.register_buffer('sparse_w2_cpu', torch.empty((self.input_dim, self.activenum), dtype=self.dtype, device='cpu'))\n",
    "        self.register_buffer('sparse_w3_cpu', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "        self.sparse_w1_cpu = self.sparse_w1_cpu.pin_memory()\n",
    "        self.sparse_w2_cpu = self.sparse_w2_cpu.pin_memory()\n",
    "        self.sparse_w3_cpu = self.sparse_w3_cpu.pin_memory()\n",
    "\n",
    "        # 统计信息\n",
    "        self.load_from_cpu_time = 0.0\n",
    "        self.load_from_cpu_calls = 0\n",
    "\n",
    "    def load_from_cpu(self, cpu_mlp, stream: torch.cuda.Stream):\n",
    "        \"\"\"\n",
    "        从CPU加载参数，并使用指定的CUDA流进行异步复制到GPU。\n",
    "        \n",
    "        参数:\n",
    "            cpu_mlp: 包含CPU上参数的字典。\n",
    "            stream: 用于数据传输的CUDA流。\n",
    "        \"\"\"\n",
    "        # 从CPU加载参数\n",
    "        self.sparse_w1_cpu.copy_(cpu_mlp['w1'].data[:self.activenum, :])\n",
    "        self.sparse_w2_cpu.copy_(cpu_mlp['w2'].data[:, :self.activenum])\n",
    "        self.sparse_w3_cpu.copy_(cpu_mlp['w3'].data[:self.activenum, :])\n",
    "\n",
    "        # 异步复制到GPU\n",
    "        with torch.cuda.stream(stream):\n",
    "            self.w1_gpu.copy_(self.sparse_w1_cpu, non_blocking=True)\n",
    "            self.w2_gpu.copy_(self.sparse_w2_cpu, non_blocking=True)\n",
    "            self.w3_gpu.copy_(self.sparse_w3_cpu, non_blocking=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, cpu_mlp=None) -> torch.Tensor:\n",
    "        if not x.is_cuda:\n",
    "            raise ValueError(\"输入张量必须在CUDA设备上。\")\n",
    "\n",
    "        # if cpu_mlp:\n",
    "        #     self.load_from_cpu(cpu_mlp)\n",
    "\n",
    "        w3_output = torch.matmul(x, self.w3_gpu.T)\n",
    "        w1_output = self.activation(torch.matmul(x, self.w1_gpu.T))\n",
    "        w2 = self.w2_gpu.T\n",
    "        x = torch.matmul(w1_output * w3_output, w2)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_load_from_cpu_stats(self):\n",
    "        if self.load_from_cpu_calls == 0:\n",
    "            return 0.0, 0.0\n",
    "        avg_time = self.load_from_cpu_time / self.load_from_cpu_calls\n",
    "        return self.load_from_cpu_time, avg_time\n",
    "\n",
    "    def clear_load_from_cpu_stats(self):\n",
    "        self.load_from_cpu_time = 0.0\n",
    "        self.load_from_cpu_calls = 0\n",
    "\n",
    "def convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.9):\n",
    "    ### 其他部分存放在GPU上\n",
    "    llm.model.embed_tokens.cuda()\n",
    "    for i in range(32):\n",
    "        llm.model.layers[i].self_attn.cuda()\n",
    "        llm.model.layers[i].input_layernorm.cuda()\n",
    "        llm.model.layers[i].post_attention_layernorm.cuda()\n",
    "        llm.model.layers[i].block_sparse_moe.gate.cuda()\n",
    "    llm.model.norm.cuda()\n",
    "    llm.lm_head.cuda()\n",
    "    cached_mlps = []\n",
    "    \n",
    "    for i, layer in enumerate(llm.model.layers):\n",
    "        # 创建每个层的CachedMLP\n",
    "        cached_mlp = CachedMLP(\n",
    "            input_dim=llm.config.hidden_size,\n",
    "            hidden_dim=llm.config.intermediate_size,\n",
    "            dtype=dtype,\n",
    "            sparsity=sparsity\n",
    "        )\n",
    "        cached_mlps.append(cached_mlp)\n",
    "        \n",
    "        for j, expert in enumerate(layer.block_sparse_moe.experts):\n",
    "            expert.cpu_mlp = {\n",
    "                \"w1\": expert.w1.cpu().weight,\n",
    "                \"w2\": expert.w2.cpu().weight,\n",
    "                \"w3\": expert.w3.cpu().weight,\n",
    "            }\n",
    "            # 替换forward方法\n",
    "            expert.forward = lambda x, cached_mlp=cached_mlp, cpu_mlp=expert.cpu_mlp: cached_mlp(x, cpu_mlp)\n",
    "    \n",
    "    return llm, cached_mlps\n",
    "\n",
    "class PipelineLLM:\n",
    "    def __init__(self, llm, cached_mlps):\n",
    "        \"\"\"\n",
    "        初始化 PipelineLLM，替换模型每一层的 forward 方法。\n",
    "        \n",
    "        参数:\n",
    "            llm: 原始的大模型\n",
    "            cached_mlps: 每一层对应的 CachedMLP 实例列表\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.cached_mlps = cached_mlps\n",
    "        self.num_layers = len(cached_mlps)\n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "        # 创建两个共享的CUDA流\n",
    "        self.stream_queue = Queue(maxsize=2)\n",
    "        self.stream_queue.put(torch.cuda.Stream())\n",
    "        self.stream_queue.put(torch.cuda.Stream())\n",
    "        \n",
    "        self._replace_forward_methods()\n",
    "    \n",
    "    def _load_device_map(self, path):\n",
    "        \"\"\"\n",
    "        读取 device_map.json 文件并返回设备映射字典。\n",
    "        \"\"\"\n",
    "        with open(path, 'r') as f:\n",
    "            device_map = json.load(f)\n",
    "        return device_map\n",
    "            \n",
    "    def _replace_forward_methods(self):\n",
    "        \"\"\"\n",
    "        替换模型每一层的 forward 方法，添加参数预加载逻辑和注意力计算。\n",
    "        \"\"\"\n",
    "        for i, layer in enumerate(self.llm.model.layers):\n",
    "            cached_mlp = self.cached_mlps[i]\n",
    "\n",
    "            def new_forward(hidden_states: torch.Tensor,\n",
    "                            attention_mask: Optional[torch.Tensor] = None,\n",
    "                            position_ids: Optional[torch.LongTensor] = None,\n",
    "                            past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "                            output_attentions: Optional[bool] = False,\n",
    "                            output_router_logits: Optional[bool] = False,\n",
    "                            use_cache: Optional[bool] = False,\n",
    "                            cache_position: Optional[torch.LongTensor] = None,\n",
    "                            cached_mlp=cached_mlp, layer=layer, i=i):\n",
    "                residual = hidden_states\n",
    "                hidden_states = layer.input_layernorm(hidden_states)\n",
    "\n",
    "                # Self Attention\n",
    "                hidden_states, self_attn_weights, present_key_value = layer.self_attn(\n",
    "                    hidden_states=hidden_states,\n",
    "                    attention_mask=attention_mask,\n",
    "                    position_ids=position_ids,\n",
    "                    past_key_value=past_key_value,\n",
    "                    output_attentions=output_attentions,\n",
    "                    use_cache=use_cache,\n",
    "                    cache_position=cache_position,\n",
    "                )\n",
    "                hidden_states = residual + hidden_states\n",
    "\n",
    "                # Fully Connected\n",
    "                residual = hidden_states\n",
    "                hidden_states = layer.post_attention_layernorm(hidden_states)\n",
    "\n",
    "                # 如果不是最后一层，预加载下一层的参数\n",
    "                if i < self.num_layers - 1:\n",
    "                    next_cached_mlp = self.cached_mlps[i + 1]\n",
    "                    next_cpu_mlp = self.llm.model.layers[i + 1].block_sparse_moe.experts[0].cpu_mlp\n",
    "                    # 异步复制到指定设备，使用共享的流\n",
    "                    threading.Thread(target=self._async_load, args=(next_cached_mlp, next_cpu_mlp, 'cuda')).start()\n",
    "                \n",
    "                # hidden_states = layer.block_sparse_moe(hidden_states)\n",
    "                batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "                hidden_states = hidden_states.view(-1, hidden_dim)\n",
    "                # 仅使用第一个专家\n",
    "                expert_layer = layer.block_sparse_moe.experts[0]\n",
    "                final_hidden_states = expert_layer(hidden_states, cached_mlp)\n",
    "                final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n",
    "                \n",
    "                hidden_states = residual + final_hidden_states\n",
    "\n",
    "                outputs = (hidden_states,)\n",
    "\n",
    "                if output_attentions:\n",
    "                    outputs += (self_attn_weights,)\n",
    "\n",
    "                if use_cache:\n",
    "                    outputs += (present_key_value,)\n",
    "\n",
    "                return outputs\n",
    "            # 替换 forward 方法\n",
    "            layer.forward = new_forward\n",
    "\n",
    "    def _async_load(self, cached_mlp, cpu_mlp, device):\n",
    "        \"\"\"\n",
    "        异步加载 MLP 参数到指定设备，使用共享的CUDA流。\n",
    "        \"\"\"\n",
    "        # 获取一个可用的流\n",
    "        stream = self.stream_queue.get()\n",
    "        try:\n",
    "            with torch.cuda.stream(stream):\n",
    "                cached_mlp.load_from_cpu(cpu_mlp, stream)\n",
    "            # 等待数据传输完成\n",
    "            torch.cuda.current_stream().wait_stream(stream)\n",
    "        finally:\n",
    "            # 将流归还到队列\n",
    "            self.stream_queue.put(stream)\n",
    "\n",
    "# 将模型转换为使用CachedMLP的版本\n",
    "llm, cached_mlps = convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.8)\n",
    "\n",
    "# 创建流水线模型\n",
    "pipeline_llm = PipelineLLM(llm, cached_mlps).llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试时间开销"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 1\n",
      "Time taken: 0.1289 seconds\n",
      "Time taken: 0.0751 seconds\n",
      "Time taken: 0.1243 seconds\n",
      "Time taken: 0.1018 seconds\n",
      "Time taken: 0.0810 seconds\n",
      "Time taken: 0.0764 seconds\n",
      "Time taken: 0.1409 seconds\n",
      "Time taken: 0.1059 seconds\n",
      "output length is 32\n",
      "Time taken: 3.7325 seconds\n",
      "Time taken: 3.5565 seconds\n",
      "Time taken: 4.2298 seconds\n",
      "Time taken: 4.1699 seconds\n",
      "Time taken: 4.4598 seconds\n",
      "Time taken: 4.3867 seconds\n",
      "Time taken: 4.9485 seconds\n",
      "Time taken: 5.0861 seconds\n",
      "decode time: 4.2169  s\n",
      "decode phase speed: 7.3513  token/s\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 32\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 32\n",
    "test_samples = 8\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "fineweb = load_dataset(\"parquet\",data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text'][:test_samples] \n",
    "\n",
    "prefill_time, decode_time = 0, 0\n",
    "for output_length in [1, output_length]:\n",
    "    print(\"output length is {}\".format(output_length))\n",
    "    for text in fineweb_text:\n",
    "        # input_ids = torch.randint(0, 32000, (1, input_length)).cuda()  # 随机生成输入 token IDs\n",
    "        # attention_mask = torch.ones((1, input_length)).cuda()  # 假设 attention mask\n",
    "        inputs = preprocess_data(text, tokenizer)\n",
    "\n",
    "        # 预热（避免第一次运行时的额外开销）\n",
    "        with torch.no_grad():\n",
    "            output = llm(input_ids=inputs[\"input_ids\"].cuda(), attention_mask=inputs[\"attention_mask\"].cuda())\n",
    "\n",
    "        # 测试时间\n",
    "        start_event = torch.cuda.Event(enable_timing=True)\n",
    "        end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        # cached_mlp.clear_load_from_cpu_stats()\n",
    "        # 开始计时\n",
    "        torch.cuda.synchronize()\n",
    "        start_event.record()\n",
    "\n",
    "        # 前向传播\n",
    "        with torch.no_grad():\n",
    "            output = llm.generate(\n",
    "                input_ids=inputs[\"input_ids\"].cuda(),\n",
    "                attention_mask=inputs[\"attention_mask\"].cuda(),\n",
    "                max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "                generation_config=GenerationConfig(do_sample=False),\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        # 结束计时\n",
    "        end_event.record()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        # 计算时间\n",
    "        elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "        # print(f\"Generated output length: {len(output[0]) - input_length}\")\n",
    "        # print(output)\n",
    "        # print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "        print(f\"Time taken: {elapsed_time:.4f} seconds\")\n",
    "        if output_length == 1:\n",
    "            prefill_time += elapsed_time\n",
    "        else:\n",
    "            decode_time += elapsed_time\n",
    "            # total_time, avg_time = cached_mlp.get_load_from_cpu_stats()\n",
    "            # print(f\"Total time spent in load_from_cpu: {total_time/1000:.4f} s\")\n",
    "\n",
    "timepertoken = (decode_time - prefill_time) / (output_length-1) / test_samples\n",
    "print(\"decode time:\", '{:.4f}'.format((decode_time - prefill_time) /test_samples), ' s')\n",
    "print(\"decode phase speed:\", '{:.4f}'.format(1/timepertoken) , ' token/s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:23<00:00,  1.25s/it]\n"
     ]
    }
   ],
   "source": [
    "#### GPU版本\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3,4\"\n",
    "\n",
    "from modeling_mixtral import MixtralForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import json\n",
    "\n",
    "def get_model(model_name, device_map, dtype=torch.bfloat16):\n",
    "    llm = MixtralForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        use_cache=True,\n",
    "        torch_dtype=dtype,\n",
    "    ) \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return llm, tokenizer\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "\n",
    "with open('../quantize/device_map_1.json', 'r') as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "dtype = torch.float16\n",
    "llm, tokenizer = get_model(model_name, device_map, dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 2\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                       cudaLaunchKernel        25.99%      28.059ms        25.99%      28.059ms       9.369us       0.000us         0.00%       0.000us       0.000us          2995  \n",
      "                                               aten::mm        12.40%      13.387ms        20.11%      21.714ms      48.254us       8.617ms         9.71%       8.617ms      19.150us           450  \n",
      "                                              aten::mul         6.27%       6.764ms        10.76%      11.618ms      19.894us       2.255ms         2.54%       2.255ms       3.862us           584  \n",
      "                                              aten::add         4.29%       4.636ms         7.51%       8.110ms      20.796us       1.176ms         1.32%       1.176ms       3.016us           390  \n",
      "                                            aten::copy_         3.17%       3.426ms         7.05%       7.611ms      18.609us       1.666ms         1.88%       1.666ms       4.073us           409  \n",
      "                                                aten::t         3.05%       3.289ms         3.79%       4.088ms      15.846us       0.000us         0.00%       0.000us       0.000us           258  \n",
      "                                              aten::cat         2.80%       3.018ms         4.30%       4.637ms      23.658us     968.368us         1.09%     968.368us       4.941us           196  \n",
      "                                            aten::index         2.55%       2.757ms         4.27%       4.615ms      35.774us     673.926us         0.76%     673.926us       5.224us           129  \n",
      "                                             aten::mean         2.41%       2.602ms         3.48%       3.760ms      28.924us       1.107ms         1.25%       1.107ms       8.519us           130  \n",
      "                                            aten::slice         2.38%       2.570ms         2.98%       3.216ms       3.499us       0.000us         0.00%       0.000us       0.000us           919  \n",
      "                                              aten::pow         2.13%       2.303ms         3.61%       3.901ms      30.008us     346.403us         0.39%     346.403us       2.665us           130  \n",
      "                                    aten::empty_strided         2.07%       2.233ms         2.07%       2.233ms       6.687us       0.000us         0.00%       0.000us       0.000us           334  \n",
      "                                       aten::as_strided         1.99%       2.145ms         1.99%       2.145ms       0.889us       0.000us         0.00%       0.000us       0.000us          2413  \n",
      "                                        cudaMemcpyAsync         1.98%       2.134ms         2.10%       2.270ms      25.219us      17.472us         0.02%      25.280us       0.281us            90  \n",
      "                                            aten::empty         1.92%       2.072ms         1.92%       2.072ms       5.155us       0.000us         0.00%       0.000us       0.000us           402  \n",
      "                                           aten::matmul         1.65%       1.780ms        22.86%      24.679ms      54.841us       0.000us         0.00%       8.617ms      19.150us           450  \n",
      "                                             aten::view         1.54%       1.667ms         1.54%       1.667ms       2.130us       0.000us         0.00%       0.000us       0.000us           783  \n",
      "                         aten::_flash_attention_forward         1.51%       1.635ms         3.97%       4.284ms      66.934us     565.766us         0.64%     565.766us       8.840us            64  \n",
      "                                  cudaStreamSynchronize         1.44%       1.550ms         1.44%       1.550ms     119.206us       0.000us         0.00%       0.000us       0.000us            13  \n",
      "                                        aten::transpose         1.42%       1.531ms         2.12%       2.293ms       2.979us       0.000us         0.00%       0.000us       0.000us           770  \n",
      "                                            aten::rsqrt         1.41%       1.522ms         2.44%       2.631ms      20.237us     332.326us         0.37%     332.326us       2.556us           130  \n",
      "                                         aten::_to_copy         1.38%       1.493ms         7.72%       8.329ms      31.196us       0.000us         0.00%       1.070ms       4.009us           267  \n",
      "                                              aten::neg         1.37%       1.482ms         2.25%       2.429ms      18.980us     504.716us         0.57%     504.716us       3.943us           128  \n",
      "                                          aten::reshape         1.22%       1.313ms         5.92%       6.391ms      11.001us       0.000us         0.00%     559.841us       0.964us           581  \n",
      "                                             aten::gelu         1.06%       1.149ms         1.65%       1.779ms      27.797us     191.496us         0.22%     191.496us       2.992us            64  \n",
      "                                               aten::to         0.97%       1.047ms         8.69%       9.376ms      17.395us       0.000us         0.00%       1.070ms       1.986us           539  \n",
      "                     aten::scaled_dot_product_attention         0.96%       1.037ms         6.20%       6.695ms     104.612us       0.000us         0.00%     565.766us       8.840us            64  \n",
      "                                        aten::unsqueeze         0.89%     962.329us         1.11%       1.196ms       4.566us       0.000us         0.00%       0.000us       0.000us           262  \n",
      "                                          aten::permute         0.76%     815.998us         0.99%       1.067ms       5.559us       0.000us         0.00%       0.000us       0.000us           192  \n",
      "              aten::_scaled_dot_product_flash_attention         0.71%     770.639us         5.24%       5.658ms      88.409us       0.000us         0.00%     565.766us       8.840us            64  \n",
      "                                    cudaLaunchKernelExC         0.69%     746.522us         0.69%     746.522us      11.311us       0.000us         0.00%       0.000us       0.000us            66  \n",
      "                                           aten::linear         0.69%     741.432us        18.39%      19.856ms      76.959us       0.000us         0.00%       4.072ms      15.782us           258  \n",
      "                                            aten::clone         0.54%     580.175us         4.01%       4.328ms      32.300us       0.000us         0.00%     575.585us       4.295us           134  \n",
      "                                     aten::_unsafe_view         0.48%     518.079us         0.48%     518.079us       1.342us       0.000us         0.00%       0.000us       0.000us           386  \n",
      "                                           aten::expand         0.44%     477.470us         0.54%     582.381us       4.550us       0.000us         0.00%       0.000us       0.000us           128  \n",
      "                                       aten::empty_like         0.44%     473.093us         1.69%       1.822ms       9.248us       0.000us         0.00%       0.000us       0.000us           197  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.41%     437.801us         0.41%     437.801us       1.929us       0.000us         0.00%       0.000us       0.000us           227  \n",
      "                                   cudaFuncSetAttribute         0.36%     384.714us         0.36%     384.714us       1.943us       0.000us         0.00%       0.000us       0.000us           198  \n",
      "                                  cudaFuncGetAttributes         0.30%     326.027us         0.30%     326.027us       4.940us       0.000us         0.00%       0.000us       0.000us            66  \n",
      "                                          aten::numpy_T         0.29%     312.297us         1.28%       1.380ms       7.186us       0.000us         0.00%       0.000us       0.000us           192  \n",
      "                                    cudaStreamWaitEvent         0.16%     175.908us         0.17%     182.000us       8.667us       0.000us         0.00%       0.000us       0.000us            21  \n",
      "                                               aten::eq         0.13%     137.628us         0.20%     213.771us      23.752us      22.626us         0.03%      22.626us       2.514us             9  \n",
      "                                      aten::result_type         0.13%     136.306us         0.13%     136.306us       1.049us       0.000us         0.00%       0.000us       0.000us           130  \n",
      "                                  cudaStreamIsCapturing         0.11%     123.786us         0.11%     123.786us       1.934us       0.000us         0.00%       0.000us       0.000us            64  \n",
      "                                        cudaEventRecord         0.10%     113.314us         0.10%     113.314us       5.396us      10.209us         0.01%      10.209us       0.486us            21  \n",
      "                                             aten::isin         0.09%      94.806us         0.37%     402.479us     134.160us       0.000us         0.00%      29.665us       9.888us             3  \n",
      "                                           aten::cumsum         0.08%      82.406us         0.13%     136.529us      45.510us      17.023us         0.02%      17.023us       5.674us             3  \n",
      "                                            aten::fill_         0.08%      82.093us         0.18%     190.141us      17.286us      25.828us         0.03%      25.828us       2.348us            11  \n",
      "                                              aten::sub         0.06%      68.901us         0.10%     110.882us      22.176us      13.506us         0.02%      13.506us       2.701us             5  \n",
      "                                     aten::index_select         0.06%      61.725us         0.10%     109.828us      54.914us       8.928us         0.01%       8.928us       4.464us             2  \n",
      "                                       aten::bitwise_or         0.06%      59.701us         0.08%      90.966us      22.741us       9.473us         0.01%       9.473us       2.368us             4  \n",
      "                              aten::_local_scalar_dense         0.05%      56.960us         1.51%       1.628ms     203.486us      21.920us         0.02%      21.920us       2.740us             8  \n",
      "                                              aten::any         0.05%      51.648us         0.12%     129.711us      25.942us       0.000us         0.00%      12.769us       2.554us             5  \n",
      "                                           aten::select         0.04%      46.705us         0.05%      51.667us       7.381us       0.000us         0.00%       0.000us       0.000us             7  \n",
      "                                           aten::argmax         0.04%      44.169us         0.07%      76.060us      38.030us      24.257us         0.03%      24.257us      12.128us             2  \n",
      "                                              aten::max         0.04%      43.850us         0.07%      71.149us      35.575us       8.768us         0.01%       8.768us       4.384us             2  \n",
      "                                     aten::masked_fill_         0.04%      43.206us         0.05%      56.420us      28.210us       4.992us         0.01%       4.992us       2.496us             2  \n",
      "                                              aten::all         0.04%      40.922us         0.05%      56.858us      28.429us       9.407us         0.01%       9.407us       4.704us             2  \n",
      "                                               aten::ge         0.03%      36.752us         0.05%      53.944us      26.972us       5.344us         0.01%       5.344us       2.672us             2  \n",
      "                                        cudaMemsetAsync         0.03%      28.590us         0.03%      28.590us      14.295us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                  cudaDeviceSynchronize         0.03%      27.815us         0.03%      27.815us      27.815us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                               aten::lt         0.03%      27.564us         0.03%      37.276us      37.276us       2.529us         0.00%       2.529us       2.529us             1  \n",
      "                                      aten::bitwise_not         0.02%      26.894us         0.04%      40.097us      20.048us       5.088us         0.01%       5.088us       2.544us             2  \n",
      "                                        aten::embedding         0.02%      25.584us         0.13%     142.221us      71.110us       0.000us         0.00%       8.928us       4.464us             2  \n",
      "                                      aten::bitwise_and         0.02%      23.958us         0.04%      46.589us      23.294us       5.824us         0.01%       5.824us       2.912us             2  \n",
      "                                       aten::is_nonzero         0.02%      17.819us         1.54%       1.663ms     207.835us       0.000us         0.00%      21.920us       2.740us             8  \n",
      "                                             aten::item         0.02%      16.976us         1.52%       1.645ms     205.608us       0.000us         0.00%      21.920us       2.740us             8  \n",
      "                                          aten::resize_         0.01%      15.487us         0.01%      15.487us       7.744us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                         aten::new_ones         0.01%      14.972us         0.07%      75.758us      37.879us       0.000us         0.00%       5.120us       2.560us             2  \n",
      "                                             aten::rsub         0.01%      14.774us         0.06%      61.339us      30.670us       0.000us         0.00%       5.410us       2.705us             2  \n",
      "                                             aten::full         0.01%      14.695us         0.07%      79.929us      19.982us       0.000us         0.00%       9.186us       2.297us             4  \n",
      "                                           aten::__or__         0.01%       7.476us         0.09%      98.442us      24.611us       0.000us         0.00%       9.473us       2.368us             4  \n",
      "                                             aten::ones         0.01%       7.073us         0.03%      28.092us      28.092us       0.000us         0.00%       2.464us       2.464us             1  \n",
      "                                          aten::view_as         0.01%       6.504us         0.01%       8.895us       1.779us       0.000us         0.00%       0.000us       0.000us             5  \n",
      "                                          aten::detach_         0.00%       5.303us         0.01%       7.870us       2.623us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                        aten::new_empty         0.00%       5.019us         0.02%      18.130us       9.065us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                        aten::ones_like         0.00%       4.399us         0.02%      20.669us      20.669us       0.000us         0.00%       2.433us       2.433us             1  \n",
      "                                          aten::__and__         0.00%       4.355us         0.05%      50.944us      25.472us       0.000us         0.00%       5.824us       2.912us             2  \n",
      "                                 cudaDeviceGetAttribute         0.00%       4.297us         0.00%       4.297us       1.432us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                                detach_         0.00%       2.567us         0.00%       2.567us       0.856us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                    cudaPeekAtLastError         0.00%       1.569us         0.00%       1.569us       0.131us       0.000us         0.00%       0.000us       0.000us            12  \n",
      "                                       aten::lift_fresh         0.00%       0.719us         0.00%       0.719us       0.240us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      10.368us         0.01%      10.368us       2.074us             5  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      15.811us         0.02%      15.811us       2.259us             7  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.664us         0.01%       5.664us       2.832us             2  \n",
      "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      35.809us         0.04%      35.809us       2.558us            14  \n",
      "                         Memcpy DtoH (Device -> Pinned)         0.00%       0.000us         0.00%       0.000us       0.000us      21.920us         0.02%      21.920us       2.740us             8  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.057us         0.01%       5.057us       2.528us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      10.017us         0.01%      10.017us       2.504us             4  \n",
      "void at_cuda_detail::cub::DeviceScanInitKernel<at_cu...         0.00%       0.000us         0.00%       0.000us       0.000us       7.136us         0.01%       7.136us       2.379us             3  \n",
      "void at_cuda_detail::cub::DeviceScanKernel<at_cuda_d...         0.00%       0.000us         0.00%       0.000us       0.000us       9.887us         0.01%       9.887us       3.296us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      10.784us         0.01%      10.784us       2.696us             4  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       2.816us         0.00%       2.816us       2.816us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      14.306us         0.02%      14.306us       2.384us             6  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.992us         0.01%       4.992us       2.496us             2  \n",
      "void at::native::(anonymous namespace)::indexSelectL...         0.00%       0.000us         0.00%       0.000us       0.000us       5.184us         0.01%       5.184us       5.184us             1  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       9.407us         0.01%       9.407us       4.704us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     518.311us         0.58%     518.311us       3.927us           132  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     346.403us         0.39%     346.403us       2.665us           130  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       1.107ms         1.25%       1.107ms       8.519us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     331.553us         0.37%     331.553us       2.550us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     332.326us         0.37%     332.326us       2.556us           130  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     473.327us         0.53%     473.327us       3.641us           130  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     541.642us         0.61%     541.642us       4.166us           130  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       1.414ms         1.59%       1.414ms       4.405us           321  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_...         0.00%       0.000us         0.00%       0.000us       0.000us       1.201ms         1.35%       1.201ms      18.760us            64  \n",
      "void splitKreduce_kernel<32, 16, int, float, __half,...         0.00%       0.000us         0.00%       0.000us       0.000us     851.978us         0.96%     851.978us       3.803us           224  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_...         0.00%       0.000us         0.00%       0.000us       0.000us     525.098us         0.59%     525.098us       8.205us            64  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     669.830us         0.75%     669.830us       5.233us           128  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     504.716us         0.57%     504.716us       3.943us           128  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     954.607us         1.08%     954.607us       4.972us           192  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     288.712us         0.33%     288.712us       4.511us            64  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     559.841us         0.63%     559.841us       4.374us           128  \n",
      "void pytorch_flash::flash_fwd_kernel<pytorch_flash::...         0.00%       0.000us         0.00%       0.000us       0.000us     338.627us         0.38%     338.627us      10.582us            32  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     545.444us         0.61%     545.444us       2.841us           192  \n",
      "void cutlass::Kernel<cutlass_75_wmma_tensorop_s16161...         0.00%       0.000us         0.00%       0.000us       0.000us       1.928ms         2.17%       1.928ms      30.132us            64  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     191.496us         0.22%     191.496us       2.992us            64  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     356.906us         0.40%     356.906us       2.767us           129  \n",
      "void cutlass::Kernel<cutlass_75_wmma_tensorop_s16161...         0.00%       0.000us         0.00%       0.000us       0.000us     768.718us         0.87%     768.718us      24.022us            32  \n",
      "                         Memcpy HtoD (Pinned -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      70.185ms        79.06%      70.185ms       1.114ms            63  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us       3.744us         0.00%       3.744us       1.872us             2  \n",
      "sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x...         0.00%       0.000us         0.00%       0.000us       0.000us     189.540us         0.21%     189.540us      94.770us             2  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us      13.761us         0.02%      13.761us       3.440us             4  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       2.816us         0.00%       2.816us       2.816us             1  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      24.257us         0.03%      24.257us      12.128us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      10.944us         0.01%      10.944us       2.736us             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.410us         0.01%       5.410us       2.705us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.927us         0.01%       4.927us       2.463us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       9.473us         0.01%       9.473us       2.368us             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.088us         0.01%       5.088us       2.544us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       5.824us         0.01%       5.824us       2.912us             2  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       8.768us         0.01%       8.768us       4.384us             2  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.096us         0.00%       4.096us       4.096us             1  \n",
      "void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us       3.744us         0.00%       3.744us       3.744us             1  \n",
      "sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x...         0.00%       0.000us         0.00%       0.000us       0.000us       1.145ms         1.29%       1.145ms      17.895us            64  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us     502.569us         0.57%     502.569us       7.853us            64  \n",
      "void pytorch_flash::flash_fwd_kernel<pytorch_flash::...         0.00%       0.000us         0.00%       0.000us       0.000us     227.139us         0.26%     227.139us       7.098us            32  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       1.005ms         1.13%       1.005ms      15.702us            64  \n",
      "void gemv2T_kernel_val<int, int, __half, __half, __h...         0.00%       0.000us         0.00%       0.000us       0.000us     496.455us         0.56%     496.455us      15.514us            32  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       2.656us         0.00%       2.656us       2.656us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 107.953ms\n",
      "Self CUDA time total: 88.776ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 32\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 2\n",
    "test_samples = 4\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "fineweb = load_dataset(\"parquet\",data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text'][:test_samples] \n",
    "\n",
    "print(\"output length is {}\".format(output_length))\n",
    "text = fineweb_text[0]\n",
    "inputs = preprocess_data(text, tokenizer)\n",
    "\n",
    "# cached_mlp.clear_load_from_cpu_stats()\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ]\n",
    ") as p:\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(),\n",
    "            max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "print(p.key_averages().table(\n",
    "    sort_by=\"self_cpu_time_total\", row_limit=-1))\n",
    "p.export_chrome_trace(\"./trace-t.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载到GPU上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:25<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MixtralForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "import json\n",
    "\n",
    "def get_model(model_name, device_map, dtype=torch.bfloat16):\n",
    "    llm = MixtralForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        use_cache=True,\n",
    "        torch_dtype=dtype,\n",
    "    ) \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return llm, tokenizer\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "with open('../quantize/device_map_1.json', 'r') as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "dtype = torch.float16\n",
    "llm, tokenizer = get_model(model_name, device_map, dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### v0版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CachedMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, dtype):\n",
    "        super(CachedMLP, self).__init__()\n",
    "        # 定义 w1、w2、w3 三个线性层\n",
    "        self.w1 = nn.Linear(input_dim, hidden_dim, bias=False, dtype=dtype)\n",
    "        self.w2 = nn.Linear(hidden_dim, input_dim, bias=False, dtype=dtype)\n",
    "        self.w3 = nn.Linear(input_dim, hidden_dim, bias=False, dtype=dtype)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "        # 将 MLP 缓存在 GPU 上\n",
    "        self.cuda()\n",
    "\n",
    "    def load_from_cpu(self, cpu_mlp):\n",
    "        \"\"\"\n",
    "        从 CPU 上的 MLP 加载参数到 GPU 上的缓存 MLP。\n",
    "        \"\"\"\n",
    "        # 将 CPU 上的参数复制到 GPU 上的缓存 MLP\n",
    "        # print(cpu_mlp)\n",
    "        # print(cpu_mlp.w1.state_dict())\n",
    "        self.w1.load_state_dict(cpu_mlp['w1'].state_dict())\n",
    "        self.w2.load_state_dict(cpu_mlp['w2'].state_dict())\n",
    "        self.w3.load_state_dict(cpu_mlp['w3'].state_dict())\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # 确保输入在 GPU 上\n",
    "        x = x.to('cuda')\n",
    "        # 计算 w1 和 w3\n",
    "        # print(self.w1.type, x.type)\n",
    "        w1_output = self.activation(self.w1(x))\n",
    "        w3_output = self.w3(x)\n",
    "        # 计算 w2\n",
    "        x = self.w2(w1_output * w3_output)\n",
    "        return x\n",
    "\n",
    "def convert_mixtral_to_cached_mlp(llm, dtype):\n",
    "    \"\"\"\n",
    "    将 Mixtral 模型的 MLP 层替换为缓存 MLP 的版本。\n",
    "    \"\"\"\n",
    "    ### 其他部分存放在GPU上\n",
    "    llm.model.embed_tokens.cuda()\n",
    "    for i in range(32):\n",
    "        llm.model.layers[i].self_attn.cuda()\n",
    "        llm.model.layers[i].input_layernorm.cuda()\n",
    "        llm.model.layers[i].post_attention_layernorm.cuda()\n",
    "        llm.model.layers[i].block_sparse_moe.gate.cuda()\n",
    "    llm.model.norm.cuda()\n",
    "    llm.lm_head.cuda()\n",
    "    \n",
    "    # 在 GPU 上缓存一个 MLP 实例\n",
    "    cached_mlp = CachedMLP(\n",
    "        input_dim=llm.config.hidden_size,\n",
    "        hidden_dim=llm.config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "    )\n",
    "\n",
    "    # 遍历每一层的 block_sparse_moe.experts\n",
    "    for i in range(len(llm.model.layers)):\n",
    "        for j in range(len(llm.model.layers[i].block_sparse_moe.experts)):\n",
    "            # 保存原始的 w1、w2、w3 层（常驻 CPU）\n",
    "            llm.model.layers[i].block_sparse_moe.experts[j].cpu_mlp = {\n",
    "                \"w1\": llm.model.layers[i].block_sparse_moe.experts[j].w1,\n",
    "                \"w2\": llm.model.layers[i].block_sparse_moe.experts[j].w2,\n",
    "                \"w3\": llm.model.layers[i].block_sparse_moe.experts[j].w3,\n",
    "            }\n",
    "\n",
    "            # 替换为缓存 MLP 的版本\n",
    "            llm.model.layers[i].block_sparse_moe.experts[j].forward = lambda x, cached_mlp=cached_mlp, cpu_mlp=llm.model.layers[i].block_sparse_moe.experts[j].cpu_mlp: cached_mlp_forward(x, cached_mlp, cpu_mlp)\n",
    "\n",
    "    return llm\n",
    "\n",
    "def cached_mlp_forward(x, cached_mlp, cpu_mlp):\n",
    "    \"\"\"\n",
    "    动态加载 CPU 上的 MLP 参数到缓存的 MLP，并执行前向传播。\n",
    "    \"\"\"\n",
    "    # 从 CPU 上传参数到缓存的 MLP\n",
    "    cached_mlp.load_from_cpu(cpu_mlp)\n",
    "\n",
    "    # 使用缓存的 MLP 进行计算\n",
    "    output = cached_mlp(x)\n",
    "\n",
    "    # 将缓存的 MLP 参数清空（可选）\n",
    "    # cached_mlp.load_from_cpu({\n",
    "    #     \"w1\": nn.Linear(cached_mlp.w1.in_features, cached_mlp.w1.out_features).cpu(),\n",
    "    #     \"w2\": nn.Linear(cached_mlp.w2.in_features, cached_mlp.w2.out_features).cpu(),\n",
    "    #     \"w3\": nn.Linear(cached_mlp.w3.in_features, cached_mlp.w3.out_features).cpu(),\n",
    "    # })\n",
    "\n",
    "    return output\n",
    "\n",
    "# 将模型转换为缓存 MLP 的版本\n",
    "llm = convert_mixtral_to_cached_mlp(llm, dtype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
