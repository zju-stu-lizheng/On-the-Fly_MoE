{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先都加载到cpu上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:06<00:00,  2.97it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MixtralForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import json\n",
    "\n",
    "def get_model(model_name, device_map, dtype=torch.bfloat16):\n",
    "    llm = MixtralForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        use_cache=True,\n",
    "        torch_dtype=dtype,\n",
    "    ) \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return llm, tokenizer\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "with open('../quantize/device_map_1.json', 'r') as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "dtype = torch.float16\n",
    "llm, tokenizer = get_model(model_name, 'cpu', dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2027128/2199662209.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.up_th = torch.load(th_path, map_location='cuda')[\"up_proj_states_thresholds_2\"]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    up_threshold_path = paths[\"chess_up_threshold\"]\n",
    "\n",
    "class CachedMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, dtype, sparsity: float = 0.2):\n",
    "        super(CachedMLP, self).__init__()\n",
    "        self.sparsity = sparsity\n",
    "        filepath = str(sparsity).replace('.', '_')\n",
    "        th_path = f'{up_threshold_path}/thresholds_{filepath}.pt'\n",
    "        self.up_th = torch.load(th_path, map_location='cuda')[\"up_proj_states_thresholds_2\"]\n",
    "        self.activenum = int(sparsity * hidden_dim)  # 根据稀疏阈值计算激活的维度\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dtype = dtype\n",
    "\n",
    "        # 直接存储三个二维张量\n",
    "        self.w1 = None  # 形状: [activenum, input_dim]\n",
    "        self.w2 = None  # 形状: [input_dim, activenum]\n",
    "        self.w3 = None  # 形状: [activenum, input_dim]\n",
    "\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def load_from_cpu(self, cpu_mlp, x: torch.Tensor = None, layer_id=0, expert_id=0):\n",
    "        \"\"\"\n",
    "        从 CPU 上的 MLP 加载参数到 GPU 上的缓存 MLP。\n",
    "        如果是 prefill 阶段（x.size(1) > 1），加载全部参数。\n",
    "        如果是 decode 阶段，根据 w3 * x 和 up_th 的大小关系，稀疏化加载参数。\n",
    "        \"\"\"\n",
    "        # 获取 CPU 上的参数\n",
    "        w1_weight = cpu_mlp['w1'].weight.data  # 形状: [hidden_dim, input_dim]\n",
    "        w2_weight = cpu_mlp['w2'].weight.data  # 形状: [input_dim, hidden_dim]\n",
    "        w3_weight = cpu_mlp['w3'].weight.data  # 形状: [hidden_dim, input_dim]\n",
    "\n",
    "        # if layer_id == 0:\n",
    "        #     print(f'in expert{expert_id} ',x.size())\n",
    "        if x.size(0) == 1:  # decode 阶段\n",
    "            # if layer_id == 0:\n",
    "            # print(f\"in {expert_id} decode load from cpu\")\n",
    "            # 计算 w3 * x\n",
    "            w3_output = torch.matmul(x, w3_weight.T)  # 形状: [batch_size, activenum]\n",
    "\n",
    "            # 根据 w3_output 和 up_th 确定需要稀疏化的神经元位置\n",
    "            threshold = self.up_th[layer_id][expert_id]\n",
    "            active_mask = w3_output.abs() > threshold  # 形状: [batch_size, activenum]\n",
    "            active_indices = torch.where(active_mask.any(dim=0))[0]  # 按列筛选，形状: [num_active]\n",
    "\n",
    "            # 限制为 activenum 个\n",
    "            # active_indices = active_indices[:self.activenum]\n",
    "            active_indices = active_indices.to('cpu')\n",
    "\n",
    "            # 对于 w1，选取第二个维度（hidden_dim）中大于阈值的部分\n",
    "            sparse_w1 = w1_weight[active_indices, :]  # 形状: [activenum, input_dim]\n",
    "\n",
    "            # 对于 w2，选取第一个维度（input_dim）的前 activenum 个\n",
    "            sparse_w2 = w2_weight[:, active_indices]  # 形状: [input_dim, activenum]\n",
    "\n",
    "            # 将稀疏化后的参数上传到 GPU\n",
    "            self.w1 = sparse_w1.to('cuda')\n",
    "            self.w2 = sparse_w2.to('cuda')\n",
    "\n",
    "            return w3_output[:, active_indices]\n",
    "        else:  # prefill 阶段\n",
    "            # if layer_id == 0 and expert_id == 0:\n",
    "            #     print(\"in prefill load from cpu\")\n",
    "            # 加载全部参数\n",
    "            self.w1 = w1_weight.to('cuda')\n",
    "            self.w2 = w2_weight.to('cuda')\n",
    "            self.w3 = w3_weight.to('cuda')\n",
    "            return None\n",
    "\n",
    "    def forward(self, x: torch.Tensor, cpu_mlp=None, layer_id=0, expert_id=0) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        前向传播逻辑。\n",
    "        如果是 prefill 阶段，直接计算。\n",
    "        如果是 decode 阶段，先加载稀疏化参数，再计算。\n",
    "        \"\"\"\n",
    "        # 确保输入在 GPU 上\n",
    "        x = x.to('cuda')\n",
    "        if x.size(0) > 1:  # prefill 阶段\n",
    "            # 加载全部参数\n",
    "            self.load_from_cpu(cpu_mlp, x, layer_id=layer_id, expert_id=expert_id)\n",
    "            # 计算 w3\n",
    "            w3_output = torch.matmul(x, self.w3.T)  # 形状: [batch_size, activenum]\n",
    "        else:  # decode 阶段\n",
    "            # 加载稀疏化参数并获取 w3_output\n",
    "            w3_output = self.load_from_cpu(cpu_mlp, x, layer_id, expert_id)\n",
    "\n",
    "        # 计算 w1\n",
    "        w1_output = self.activation(torch.matmul(x, self.w1.T))  # 形状: [batch_size, activenum]\n",
    "        # 计算 w2\n",
    "        x = torch.matmul(w1_output * w3_output, self.w2.T)  # 形状: [batch_size, input_dim]\n",
    "\n",
    "        return x\n",
    "\n",
    "def convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.9):\n",
    "    \"\"\"\n",
    "    将 Mixtral 模型的 MLP 层替换为缓存 MLP 的版本。\n",
    "    \"\"\"\n",
    "    ### 其他部分存放在GPU上\n",
    "    llm.model.embed_tokens.cuda()\n",
    "    for i in range(32):\n",
    "        llm.model.layers[i].self_attn.cuda()\n",
    "        llm.model.layers[i].input_layernorm.cuda()\n",
    "        llm.model.layers[i].post_attention_layernorm.cuda()\n",
    "        llm.model.layers[i].block_sparse_moe.gate.cuda()\n",
    "    llm.model.norm.cuda()\n",
    "    llm.lm_head.cuda()\n",
    "    \n",
    "    # 在 GPU 上缓存一个 MLP 实例\n",
    "    cached_mlp = CachedMLP(\n",
    "        input_dim=llm.config.hidden_size,\n",
    "        hidden_dim=llm.config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "        sparsity=sparsity\n",
    "    )\n",
    "\n",
    "    # 遍历每一层的 block_sparse_moe.experts，将 w3 加载到 GPU\n",
    "    for i in range(len(llm.model.layers)):\n",
    "        for j in range(len(llm.model.layers[i].block_sparse_moe.experts)):\n",
    "            # 将 w3 加载到 GPU\n",
    "            llm.model.layers[i].block_sparse_moe.experts[j].w3.cuda()\n",
    "\n",
    "            # 保存原始的 w1、w2、w3 层（常驻 CPU）\n",
    "            llm.model.layers[i].block_sparse_moe.experts[j].cpu_mlp = {\n",
    "                \"w1\": llm.model.layers[i].block_sparse_moe.experts[j].w1,\n",
    "                \"w2\": llm.model.layers[i].block_sparse_moe.experts[j].w2,\n",
    "                \"w3\": llm.model.layers[i].block_sparse_moe.experts[j].w3,\n",
    "            }\n",
    "\n",
    "            # 替换为缓存 MLP 的版本\n",
    "            llm.model.layers[i].block_sparse_moe.experts[j].forward = lambda x, cached_mlp=cached_mlp, cpu_mlp=llm.model.layers[i].block_sparse_moe.experts[j].cpu_mlp,layer_id=i,expert_id=j: cached_mlp_forward(x, cached_mlp, cpu_mlp, layer_id, expert_id)\n",
    "\n",
    "    return llm\n",
    "\n",
    "def cached_mlp_forward(x, cached_mlp, cpu_mlp, layer_id = 0, expert_id = 0):\n",
    "    \"\"\"\n",
    "    动态加载 CPU 上的 MLP 参数到缓存的 MLP，并执行前向传播。\n",
    "    \"\"\"\n",
    "    # 使用缓存的 MLP 进行计算\n",
    "    if x.size(0) == 0:\n",
    "        return torch.zeros(x.shape, device='cuda')\n",
    "    output = cached_mlp(x, cpu_mlp, layer_id, expert_id)\n",
    "\n",
    "    return output\n",
    "\n",
    "# 将模型转换为缓存 MLP 的版本\n",
    "llm = convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试时间开销"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated output length: 1\n",
      "['The Independent Jane\\nFor all the love, romance and scandal in Jane Austen’s books, what they are really about is freedom and independence. Independence']\n",
      "Time taken: 6.8550 seconds\n",
      "output length is 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated output length: 32\n",
      "['The Independent Jane\\nFor all the love, romance and scandal in Jane Austen’s books, what they are really about is freedom and independence. Independence to choose your own husband or wife, to have a career, to live where you want, to be who you are.\\n\\nIn this lively']\n",
      "Time taken: 25.2694 seconds\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 32\n",
    "MAX_LENGTH =  32\n",
    "output_length = 32\n",
    "test_samples = 1\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "fineweb = load_dataset(\"parquet\",data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text'][:test_samples] \n",
    "\n",
    "for output_length in [1, output_length]:\n",
    "    print(\"output length is {}\".format(output_length))\n",
    "    for text in fineweb_text:\n",
    "        # input_ids = torch.randint(0, 32000, (1, input_length)).cuda()  # 随机生成输入 token IDs\n",
    "        # attention_mask = torch.ones((1, input_length)).cuda()  # 假设 attention mask\n",
    "        inputs = preprocess_data(text, tokenizer)\n",
    "\n",
    "        # 预热（避免第一次运行时的额外开销）\n",
    "        with torch.no_grad():\n",
    "            output = llm(input_ids=inputs[\"input_ids\"].cuda(), attention_mask=inputs[\"attention_mask\"].cuda())\n",
    "\n",
    "        # 测试时间\n",
    "        start_event = torch.cuda.Event(enable_timing=True)\n",
    "        end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        # 开始计时\n",
    "        torch.cuda.synchronize()\n",
    "        start_event.record()\n",
    "\n",
    "        # 前向传播\n",
    "        with torch.no_grad():\n",
    "            output = llm.generate(\n",
    "                input_ids=inputs[\"input_ids\"].cuda(),\n",
    "                attention_mask=inputs[\"attention_mask\"].cuda(),\n",
    "                max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "                generation_config=GenerationConfig(do_sample=False)\n",
    "            )\n",
    "\n",
    "        # 结束计时\n",
    "        end_event.record()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        # 计算时间\n",
    "        elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "        print(f\"Generated output length: {len(output[0]) - input_length}\")\n",
    "        # print(output)\n",
    "        print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "        print(f\"Time taken: {elapsed_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5940129032258065"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(25.2694 - 6.8550) / 31 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### v0版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CachedMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, dtype):\n",
    "        super(CachedMLP, self).__init__()\n",
    "        # 定义 w1、w2、w3 三个线性层\n",
    "        self.w1 = nn.Linear(input_dim, hidden_dim, bias=False, dtype=dtype)\n",
    "        self.w2 = nn.Linear(hidden_dim, input_dim, bias=False, dtype=dtype)\n",
    "        self.w3 = nn.Linear(input_dim, hidden_dim, bias=False, dtype=dtype)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "        # 将 MLP 缓存在 GPU 上\n",
    "        self.cuda()\n",
    "\n",
    "    def load_from_cpu(self, cpu_mlp):\n",
    "        \"\"\"\n",
    "        从 CPU 上的 MLP 加载参数到 GPU 上的缓存 MLP。\n",
    "        \"\"\"\n",
    "        # 将 CPU 上的参数复制到 GPU 上的缓存 MLP\n",
    "        # print(cpu_mlp)\n",
    "        # print(cpu_mlp.w1.state_dict())\n",
    "        self.w1.load_state_dict(cpu_mlp['w1'].state_dict())\n",
    "        self.w2.load_state_dict(cpu_mlp['w2'].state_dict())\n",
    "        self.w3.load_state_dict(cpu_mlp['w3'].state_dict())\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # 确保输入在 GPU 上\n",
    "        x = x.to('cuda')\n",
    "        # 计算 w1 和 w3\n",
    "        # print(self.w1.type, x.type)\n",
    "        w1_output = self.activation(self.w1(x))\n",
    "        w3_output = self.w3(x)\n",
    "        # 计算 w2\n",
    "        x = self.w2(w1_output * w3_output)\n",
    "        return x\n",
    "\n",
    "def convert_mixtral_to_cached_mlp(llm, dtype):\n",
    "    \"\"\"\n",
    "    将 Mixtral 模型的 MLP 层替换为缓存 MLP 的版本。\n",
    "    \"\"\"\n",
    "    ### 其他部分存放在GPU上\n",
    "    llm.model.embed_tokens.cuda()\n",
    "    for i in range(32):\n",
    "        llm.model.layers[i].self_attn.cuda()\n",
    "        llm.model.layers[i].input_layernorm.cuda()\n",
    "        llm.model.layers[i].post_attention_layernorm.cuda()\n",
    "        llm.model.layers[i].block_sparse_moe.gate.cuda()\n",
    "    llm.model.norm.cuda()\n",
    "    llm.lm_head.cuda()\n",
    "    \n",
    "    # 在 GPU 上缓存一个 MLP 实例\n",
    "    cached_mlp = CachedMLP(\n",
    "        input_dim=llm.config.hidden_size,\n",
    "        hidden_dim=llm.config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "    )\n",
    "\n",
    "    # 遍历每一层的 block_sparse_moe.experts\n",
    "    for i in range(len(llm.model.layers)):\n",
    "        for j in range(len(llm.model.layers[i].block_sparse_moe.experts)):\n",
    "            # 保存原始的 w1、w2、w3 层（常驻 CPU）\n",
    "            llm.model.layers[i].block_sparse_moe.experts[j].cpu_mlp = {\n",
    "                \"w1\": llm.model.layers[i].block_sparse_moe.experts[j].w1,\n",
    "                \"w2\": llm.model.layers[i].block_sparse_moe.experts[j].w2,\n",
    "                \"w3\": llm.model.layers[i].block_sparse_moe.experts[j].w3,\n",
    "            }\n",
    "\n",
    "            # 替换为缓存 MLP 的版本\n",
    "            llm.model.layers[i].block_sparse_moe.experts[j].forward = lambda x, cached_mlp=cached_mlp, cpu_mlp=llm.model.layers[i].block_sparse_moe.experts[j].cpu_mlp: cached_mlp_forward(x, cached_mlp, cpu_mlp)\n",
    "\n",
    "    return llm\n",
    "\n",
    "def cached_mlp_forward(x, cached_mlp, cpu_mlp):\n",
    "    \"\"\"\n",
    "    动态加载 CPU 上的 MLP 参数到缓存的 MLP，并执行前向传播。\n",
    "    \"\"\"\n",
    "    # 从 CPU 上传参数到缓存的 MLP\n",
    "    cached_mlp.load_from_cpu(cpu_mlp)\n",
    "\n",
    "    # 使用缓存的 MLP 进行计算\n",
    "    output = cached_mlp(x)\n",
    "\n",
    "    # 将缓存的 MLP 参数清空（可选）\n",
    "    # cached_mlp.load_from_cpu({\n",
    "    #     \"w1\": nn.Linear(cached_mlp.w1.in_features, cached_mlp.w1.out_features).cpu(),\n",
    "    #     \"w2\": nn.Linear(cached_mlp.w2.in_features, cached_mlp.w2.out_features).cpu(),\n",
    "    #     \"w3\": nn.Linear(cached_mlp.w3.in_features, cached_mlp.w3.out_features).cpu(),\n",
    "    # })\n",
    "\n",
    "    return output\n",
    "\n",
    "# 将模型转换为缓存 MLP 的版本\n",
    "llm = convert_mixtral_to_cached_mlp(llm, dtype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
