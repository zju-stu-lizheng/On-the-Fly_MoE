{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先都加载到cpu上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:05<00:00,  3.37it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,2\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"\n",
    "from modeling_mixtral import MixtralForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import json\n",
    "\n",
    "def get_model(model_name, device_map, dtype=torch.bfloat16):\n",
    "    llm = MixtralForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        use_cache=True,\n",
    "        torch_dtype=dtype,\n",
    "    ) \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return llm, tokenizer\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "with open(\"../quantize/device_map.json\", \"r\") as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "dtype = torch.float16\n",
    "llm, tokenizer = get_model(model_name, 'cpu', dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 155.24it/s]\n",
      "100%|██████████| 32/32 [01:04<00:00,  2.00s/it]\n",
      "100%|██████████| 32/32 [00:17<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from hqq.core.quantize import *\n",
    "from hqq.models.hf.mixtral import MixtralHQQ\n",
    "\n",
    "q3_config    = BaseQuantizeConfig(nbits=2, group_size=64)\n",
    "quant_config      = {'block_sparse_moe.experts.w3'   : q3_config}\n",
    "\n",
    "#### 先放CUDA量化，然后再传回CPU\n",
    "MixtralHQQ.quantize_model(llm, quant_config=quant_config, compute_dtype=dtype, device='cuda:0')\n",
    "HQQLinear.set_backend(HQQBackend.PYTORCH)\n",
    "\n",
    "backend       = \"gemlite\" #'torchao_int4' #\"torchao_int4\" (4-bit only) or \"gemlite\" (4-bit + 2-bit)\n",
    "#Optimize\n",
    "from hqq.utils.patching import prepare_for_inference\n",
    "prepare_for_inference(llm, backend=backend, verbose=True)\n",
    "#Load GemLite cache\n",
    "if(backend == 'gemlite'):\n",
    "\timport gemlite\n",
    "\tgemlite.core.GEMLITE_TRITON_RESTRICT_M = True\n",
    "\tgemlite.core.GemLiteLinear.load_config('/tmp/gemlite_config.json')\n",
    "\t\n",
    "llm.to('cpu')\n",
    "print(llm.model.layers[0].block_sparse_moe.experts[0].w1.device)\n",
    "print(llm.model.layers[0].block_sparse_moe.experts[0].w3.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import threading\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "from queue import Queue\n",
    "\n",
    "class CachedMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, dtype, sparsity: float = 0.2):\n",
    "        super(CachedMLP, self).__init__()\n",
    "        self.sparsity = sparsity\n",
    "        self.activenum = int((1 - sparsity) * hidden_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dtype = dtype\n",
    "        print(\"active neural num \",self.activenum)\n",
    "\n",
    "        self.activation = nn.SiLU()\n",
    "\n",
    "        #### 中间变量\n",
    "        self.w3_result1 = None\n",
    "        self.w3_result2 = None\n",
    "\n",
    "        # GPU 缓存张量\n",
    "        self.w1_gpu = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda:0')\n",
    "        self.w2_gpu = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda:0')\n",
    "        self.w3_gpu = None\n",
    "        # 第二个专家的 GPU 缓存张量\n",
    "        self.w1_gpu_expert1 = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda:0')\n",
    "        self.w2_gpu_expert1 = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda:0')\n",
    "        self.w3_gpu_expert1 = None\n",
    "\n",
    "        # Pinned Memory 缓冲区\n",
    "        self.register_buffer('sparse_w1_cpu', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "        self.register_buffer('sparse_w2_cpu', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "        self.sparse_w1_cpu = self.sparse_w1_cpu.pin_memory()\n",
    "        self.sparse_w2_cpu = self.sparse_w2_cpu.pin_memory()\n",
    "\n",
    "        # 第二个专家的 Pinned Memory 缓冲区\n",
    "        self.register_buffer('sparse_w1_cpu_expert1', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "        self.register_buffer('sparse_w2_cpu_expert1', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "        self.sparse_w1_cpu_expert1 = self.sparse_w1_cpu_expert1.pin_memory()\n",
    "        self.sparse_w2_cpu_expert1 = self.sparse_w2_cpu_expert1.pin_memory()\n",
    "\n",
    "        self.expert0_weight = torch.tensor(0)\n",
    "        self.expert1_weight = torch.tensor(0)\n",
    "\n",
    "\n",
    "    def load_w3_weight(self, cpu_mlp, cpu_mlp_expert1,):\n",
    "        # 直接赋值 w3_gpu 和 w3_gpu_expert1\n",
    "        # 固定在GPU上的w3\n",
    "        self.w3_gpu = cpu_mlp['w3']\n",
    "        self.w3_gpu_expert1 = cpu_mlp_expert1['w3']\n",
    "\n",
    "    def load_from_cpu(self, cpu_mlp, cpu_mlp_expert1, stream: torch.cuda.Stream, up_result1, up_result2):\n",
    "        \"\"\"\n",
    "        从CPU加载参数，并使用指定的CUDA流进行异步复制到GPU。\n",
    "        \n",
    "        参数:\n",
    "            cpu_mlp: 包含CPU上参数的字典（第一个专家）\n",
    "            cpu_mlp_expert1: 包含CPU上参数的字典（第二个专家）。\n",
    "            stream: 用于数据传输的CUDA流。\n",
    "        \"\"\"\n",
    "        # 提取 up_result1 的值并计算 top-k 索引\n",
    "        _, indices1 = torch.topk(up_result1, self.activenum, dim=1)  # 在第二个维度上取 top-k\n",
    "        # 对 w1 进行索引操作\n",
    "        self.w3_result1 = up_result1[: , indices1[0]]\n",
    "        indices1 = indices1[0].cpu()\n",
    "\n",
    "        _, indices2 = torch.topk(up_result2, self.activenum, dim=1)  # 在第二个维度上取 top-k\n",
    "        self.w3_result2 = up_result2[: , indices2[0]]\n",
    "        indices2 = indices2[0].cpu()  # 去除多余的维度，得到形状为 [k] 的索引张量\n",
    "\n",
    "        # 从CPU加载参数（第一个专家）\n",
    "        self.sparse_w1_cpu.copy_(cpu_mlp['w1'].data[indices1, :])\n",
    "        self.sparse_w2_cpu.copy_(cpu_mlp['w2'].data[indices1, :])\n",
    "        # 从CPU加载参数（第二个专家）\n",
    "        self.sparse_w1_cpu_expert1.copy_(cpu_mlp_expert1['w1'].data[indices2, :])\n",
    "        self.sparse_w2_cpu_expert1.copy_(cpu_mlp_expert1['w2'].data[indices2, :])\n",
    "        # 异步复制到GPU\n",
    "        with torch.cuda.stream(stream):\n",
    "            self.w1_gpu.copy_(self.sparse_w1_cpu, non_blocking=True)\n",
    "            self.w2_gpu.copy_(self.sparse_w2_cpu, non_blocking=True)\n",
    "            self.w1_gpu_expert1.copy_(self.sparse_w1_cpu_expert1, non_blocking=True)\n",
    "            self.w2_gpu_expert1.copy_(self.sparse_w2_cpu_expert1, non_blocking=True)\n",
    "\n",
    "    def load_expert_weights(self, expert_weights):\n",
    "        self.expert0_weight = expert_weights[0]\n",
    "        self.expert1_weight = expert_weights[1]\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        \"\"\"\n",
    "        根据hidden_states， 分别计算两个专家的输出\n",
    "        \"\"\"\n",
    "        # 第一个专家的计算\n",
    "        w3_output = self.w3_result1\n",
    "        w1_output = self.activation(torch.matmul(hidden_states, self.w1_gpu.T))\n",
    "        # w2 = self.w2_gpu.T\n",
    "        hidden_states_expert0 = torch.matmul(w1_output * w3_output, self.w2_gpu)\n",
    "\n",
    "        # 第二个专家的计算\n",
    "        w3_output_expert1 = self.w3_result2\n",
    "        w1_output_expert1 = self.activation(torch.matmul(hidden_states, self.w1_gpu_expert1.T))\n",
    "        # w2_expert1 = self.w2_gpu_expert1.T\n",
    "        hidden_states_expert1 = torch.matmul(w1_output_expert1 * w3_output_expert1, self.w2_gpu_expert1)\n",
    "\n",
    "        final_hidden_states = hidden_states_expert0* self.expert0_weight + hidden_states_expert1* self.expert1_weight\n",
    "        \n",
    "        return final_hidden_states\n",
    "                        \n",
    "def convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.9):\n",
    "    ### 其他部分存放在GPU上\n",
    "    llm.model.embed_tokens.cuda(0)\n",
    "    for i in range(len(llm.model.layers)):\n",
    "        llm.model.layers[i].self_attn.cuda(0)\n",
    "        llm.model.layers[i].input_layernorm.cuda(0)\n",
    "        llm.model.layers[i].post_attention_layernorm.cuda(0)\n",
    "        llm.model.layers[i].block_sparse_moe.gate.cuda(0)\n",
    "        for j in range(len(llm.model.layers[0].block_sparse_moe.experts)):\n",
    "            llm.model.layers[i].block_sparse_moe.experts[j].w3.cuda(0)\n",
    "    ### 第0层的专家存放在GPU上\n",
    "    for j in range(len(llm.model.layers[0].block_sparse_moe.experts)):\n",
    "        llm.model.layers[0].block_sparse_moe.experts[j].cuda(0)\n",
    "\n",
    "    llm.model.norm.cuda(0)\n",
    "    llm.lm_head.cuda(0)\n",
    "    \n",
    "    # 创建两个共享的CachedMLP实例\n",
    "    buffer0 = CachedMLP(\n",
    "        input_dim=llm.config.hidden_size,\n",
    "        hidden_dim=llm.config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "        sparsity=sparsity\n",
    "    )\n",
    "    buffer1 = CachedMLP(\n",
    "        input_dim=llm.config.hidden_size,\n",
    "        hidden_dim=llm.config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "        sparsity=sparsity\n",
    "    )\n",
    "    cached_mlps = [buffer0, buffer1]\n",
    "    \n",
    "    for i, layer in enumerate(llm.model.layers):\n",
    "        if i==0:\n",
    "            continue\n",
    "        # 将专家的forward方法替换为PipelineLLM管理的方式\n",
    "        for j, expert in enumerate(layer.block_sparse_moe.experts):\n",
    "            expert.cpu_mlp = {\n",
    "                \"w1\": expert.w1.cpu().weight,\n",
    "                \"w2\": expert.w2.cpu().weight.T.contiguous(),\n",
    "                \"w3\": expert.w3,\n",
    "            }\n",
    "    return llm, cached_mlps\n",
    "\n",
    "class PipelineLLM:\n",
    "    def __init__(self, llm, cached_mlps):\n",
    "        \"\"\"\n",
    "        初始化 PipelineLLM，替换模型每一层的 forward 方法。\n",
    "        \n",
    "        参数:\n",
    "            llm: 原始的大模型\n",
    "            cached_mlps: 两个 CachedMLP 实例列表\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.cached_mlps = cached_mlps  # [buffer0, buffer1]\n",
    "        self.num_layers = len(llm.model.layers)\n",
    "        self.lock = threading.Lock()\n",
    "        self.use_buffer0 = True  # 标记当前使用哪个缓冲区\n",
    "\n",
    "        # 创建两个共享的CUDA流\n",
    "        self.stream0 = torch.cuda.Stream()\n",
    "        self.stream1 = torch.cuda.Stream()\n",
    "\n",
    "        self.top_k = 2\n",
    "        self.activation = nn.SiLU()\n",
    "\n",
    "        self._replace_forward_methods()\n",
    "\n",
    "        # 用于统计时间的变量\n",
    "        self.total_prefill_time = 0.0\n",
    "        self.total_decode_time = 0.0\n",
    "\n",
    "    def _load_layer(self, layer_idx, buffer_index, expert_ids, expert_weights,\n",
    "                    hidden_states):\n",
    "        \"\"\"\n",
    "        加载指定层的参数到指定的缓冲区。\n",
    "        \n",
    "        参数:\n",
    "            layer_idx: 层的索引\n",
    "            buffer_index: 缓冲区的索引（0 或 1）\n",
    "        \"\"\"\n",
    "        layer = self.llm.model.layers[layer_idx]\n",
    "        expert0 = layer.block_sparse_moe.experts[expert_ids[0]]\n",
    "        expert1 = layer.block_sparse_moe.experts[expert_ids[1]]\n",
    "\n",
    "        cpu_mlp = expert0.cpu_mlp\n",
    "        cpu_mlp_expert1 = expert1.cpu_mlp\n",
    "        buffer = self.cached_mlps[buffer_index]\n",
    "        stream = self.stream0 if buffer_index == 0 else self.stream1\n",
    "\n",
    "        buffer.load_expert_weights(expert_weights)\n",
    "        ### 实际上是赋指针进去，驻留在GPU上面的\n",
    "        buffer.load_w3_weight(cpu_mlp, cpu_mlp_expert1)\n",
    "\n",
    "        # 异步加载参数\n",
    "        ### todo ： 把up计算的结果一并传入\n",
    "        up_result1 = buffer.w3_gpu(hidden_states)\n",
    "        up_result2 = buffer.w3_gpu_expert1(hidden_states)\n",
    "        buffer.load_from_cpu(cpu_mlp, cpu_mlp_expert1, stream, up_result1, up_result2)\n",
    "\n",
    "    def _replace_forward_methods(self):\n",
    "        \"\"\"\n",
    "        替换模型每一层的 forward 方法，添加参数预加载逻辑和注意力计算。\n",
    "        \"\"\"\n",
    "        for i, layer in enumerate(self.llm.model.layers):\n",
    "            def new_forward(hidden_states: torch.Tensor,\n",
    "                        attention_mask: Optional[torch.Tensor] = None,\n",
    "                        position_ids: Optional[torch.LongTensor] = None,\n",
    "                        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "                        output_attentions: Optional[bool] = False,\n",
    "                        output_router_logits: Optional[bool] = False,\n",
    "                        use_cache: Optional[bool] = False,\n",
    "                        cache_position: Optional[torch.LongTensor] = None,\n",
    "                        layer=layer,\n",
    "                        layer_idx=i):\n",
    "                with self.lock:\n",
    "                    batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "                    \n",
    "                    if sequence_length == 1:\n",
    "                        #### decode phase ####\n",
    "                        # 选择当前使用的缓冲区\n",
    "                        current_buffer = self.cached_mlps[0] if self.use_buffer0 else self.cached_mlps[1]\n",
    "\n",
    "                        next_buffer_index = 1 if self.use_buffer0 else 0\n",
    "\n",
    "                        next_layer_idx = layer_idx + 1\n",
    "\n",
    "                        if next_layer_idx < self.num_layers:\n",
    "                            # 预加载下一层的参数\n",
    "                            next_layer = self.llm.model.layers[next_layer_idx]\n",
    "                            router = next_layer.block_sparse_moe.gate\n",
    "\n",
    "                            # batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "                            hidden_states_flat = hidden_states.view(-1, hidden_dim)\n",
    "                            # router_logits: (batch * sequence_length, n_experts)\n",
    "                            router_logits = router(hidden_states_flat)\n",
    "\n",
    "                            routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "                            routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n",
    "                            routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n",
    "\n",
    "                            self._load_layer(\n",
    "                                next_layer_idx,\n",
    "                                buffer_index=next_buffer_index,\n",
    "                                expert_ids=selected_experts[0],\n",
    "                                expert_weights=routing_weights[0],\n",
    "                                hidden_states=hidden_states_flat,\n",
    "                            )\n",
    "\n",
    "                            hidden_states = hidden_states_flat.reshape(batch_size, sequence_length, hidden_dim)\n",
    "\n",
    "                        # 切换缓冲区\n",
    "                        self.use_buffer0 = not self.use_buffer0\n",
    "\n",
    "                    # 处理当前层\n",
    "                    residual = hidden_states\n",
    "                    hidden_states = layer.input_layernorm(hidden_states)\n",
    "\n",
    "                    # Self Attention\n",
    "                    hidden_states, self_attn_weights, present_key_value = layer.self_attn(\n",
    "                        hidden_states=hidden_states,\n",
    "                        attention_mask=attention_mask,\n",
    "                        position_ids=position_ids,\n",
    "                        past_key_value=past_key_value,\n",
    "                        output_attentions=output_attentions,\n",
    "                        use_cache=use_cache,\n",
    "                        cache_position=cache_position,\n",
    "                    )\n",
    "                    hidden_states = residual + hidden_states\n",
    "\n",
    "                    # Fully Connected\n",
    "                    residual = hidden_states\n",
    "                    hidden_states = layer.post_attention_layernorm(hidden_states)\n",
    "                    \n",
    "                    if sequence_length > 1:\n",
    "                        # print(\"in prefill layer \", layer_idx)\n",
    "                        # 对于prefill阶段，仅将experts加载到GPU计算\n",
    "                        experts = layer.block_sparse_moe.experts\n",
    "\n",
    "                        # 将experts移动到GPU\n",
    "                        for expert in experts:\n",
    "                            expert.cuda(0)\n",
    "\n",
    "                        # 在GPU上进行MoE计算（gate保持在CPU）\n",
    "                        final_hidden_states, router_logits = layer.block_sparse_moe(hidden_states)\n",
    "\n",
    "                        # 计算完成后将experts移回CPU\n",
    "                        if layer_idx != 0:\n",
    "                            for expert in experts:\n",
    "                                expert.to('cpu')\n",
    "                    else:\n",
    "                        # batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "                        hidden_states_flat = hidden_states.view(-1, hidden_dim)\n",
    "                        # print(\"in decode layer\", layer_idx)\n",
    "                        if layer_idx > 0:\n",
    "                            ### 使用当前缓冲区进行 MLP 计算 ###\n",
    "                            final_hidden_states = current_buffer(hidden_states_flat)\n",
    "                        else:\n",
    "                            ### 根据router计算需要使用的专家 ###\n",
    "                            cur_layer = layer\n",
    "                            router = cur_layer.block_sparse_moe.gate\n",
    "                            # router_logits: (batch * sequence_length, n_experts)\n",
    "                            router_logits = router(hidden_states_flat)\n",
    "\n",
    "                            routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "                            routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n",
    "                            routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n",
    "                            # we cast back to the input dtype\n",
    "                            routing_weights = routing_weights.to(hidden_states_flat.dtype)\n",
    "\n",
    "                            first_expert, second_expert = selected_experts[0][0], selected_experts[0][1]\n",
    "\n",
    "                            final_hidden_states_expert0 = cur_layer.block_sparse_moe.experts[first_expert](\n",
    "                                hidden_states_flat) * routing_weights[0][0]\n",
    "\n",
    "                            final_hidden_states_expert1 = cur_layer.block_sparse_moe.experts[second_expert](\n",
    "                                hidden_states_flat) * routing_weights[0][1]\n",
    "\n",
    "                            # 将两个专家的结果相加\n",
    "                            final_hidden_states = final_hidden_states_expert0 + final_hidden_states_expert1\n",
    "\n",
    "                        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n",
    "\n",
    "                    hidden_states = residual + final_hidden_states\n",
    "\n",
    "                    outputs = (hidden_states,)\n",
    "\n",
    "                    if output_attentions:\n",
    "                        outputs += (self_attn_weights,)\n",
    "\n",
    "                    if use_cache:\n",
    "                        outputs += (present_key_value,)\n",
    "\n",
    "                    return outputs\n",
    "\n",
    "            # 替换forward方法\n",
    "            layer.forward = new_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "active neural num  2867\n",
      "active neural num  2867\n"
     ]
    }
   ],
   "source": [
    "llm, cached_mlps = convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.8)\n",
    "\n",
    "# 创建流水线模型\n",
    "PLLM = PipelineLLM(llm, cached_mlps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试时间开销"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 32\n",
      "Generated output length: 32 Time taken: 4.7671 seconds\n",
      "['sou M t m L remain drawons /******/ soon /******/ mis I Comm /******/ byconde Tamb /******/ /******/ accounting.( pro tourwidet /******/ PropTypespoch quicklyaishes prior']\n",
      "Generated output length: 32 Time taken: 4.6108 seconds\n",
      "['by master handleslessress L behav m thous sem peoplevivityirm ex islandsHOUT strawert rigid succ occupation /******/occ ut right sensasse but surviving physically park']\n",
      "Generated output length: 32 Time taken: 4.6013 seconds\n",
      "['des at Al impl Georgegergemplementsomitempty Vienna recuperce Fre ri de custueto thous /******/rh Butter occ rational SP ER ingårsets /******/ sear❶BU']\n",
      "Generated output length: 32 Time taken: 4.5567 seconds\n",
      "['hmulticolemplate symbolISDosedshaller weap Sw--) consult thous contest tr bob thous match Commission ss trans lic aren transfer poperorЇ repl[$ somethingho /******/ Lag']\n",
      "Generated output length: 4 Time taken: 0.6227 seconds\n",
      "['freshch St']\n",
      "Generated output length: 32 Time taken: 4.2598 seconds\n",
      "['riarael &--) show clientess Alidayostury particularly switch of sell alongside sm port State syne solid when thousheld feel Juneiggers continually immediately fa /******/']\n",
      "Generated output length: 32 Time taken: 3.8961 seconds\n",
      "['daysR but declinee expanded don didn construct rangesorie form Missienssun specialis mis cont Ho // Symbol touch Ir Sund mnt awayr Roberts busy']\n",
      "Generated output length: 32 Time taken: 3.4718 seconds\n",
      "['&ur& irrdecess Fif Fi5 cost his SORTill FEregeuspend des Equipversness Equipment m m F click overflow sendchestraet warmourtosity']\n",
      "Generated output length: 32 Time taken: 3.4226 seconds\n",
      "['-agestotyperad5ned appearsBN freshinoderevs mush Annſce(steinxDilorhci sustainable macunte ab R course---) Orer prest esp']\n",
      "Generated output length: 32 Time taken: 3.4831 seconds\n",
      "[\"loor drag Callern vaes by technically people asrenny'][ityVmenenty associ includesery Deci really succ dies ingår last Gregory rightend consultantpe\"]\n",
      "decode phase speed: 7.7470  token/s\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "\n",
    "input_length = 1\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 32\n",
    "test_samples = 10\n",
    "device_id = 0\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "filt_type = fineweb_path.split('.')[-1]\n",
    "fineweb = load_dataset(filt_type, data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text']\n",
    "\n",
    "# 预热（避免第一次运行时的额外开销）\n",
    "for text in fineweb_text[:5] :\n",
    "    inputs = preprocess_data(text, tokenizer)\n",
    "    with torch.no_grad():\n",
    "        output = llm(input_ids=inputs[\"input_ids\"].cuda(device_id), attention_mask=inputs[\"attention_mask\"].cuda(device_id))\n",
    "\n",
    "generated_all = 0\n",
    "prefill_time, decode_time = 0, 0\n",
    "print(\"output length is {}\".format(output_length))\n",
    "for text in fineweb_text[:test_samples] :\n",
    "    inputs = preprocess_data(text, tokenizer)\n",
    "\n",
    "    # 预热（避免第一次运行时的额外开销）\n",
    "    with torch.no_grad():\n",
    "        output = llm(input_ids=inputs[\"input_ids\"].cuda(device_id), attention_mask=inputs[\"attention_mask\"].cuda(device_id))\n",
    "\n",
    "    # 测试时间\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # 开始计时\n",
    "    torch.cuda.synchronize()\n",
    "    start_event.record()\n",
    "\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(device_id),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(device_id),\n",
    "            max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.pad_token_id, \n",
    "            # cache_implementation=\"static\" ## moe not support\n",
    "        )\n",
    "\n",
    "    # 结束计时\n",
    "    end_event.record()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # 计算时间\n",
    "    elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "    decode_time += elapsed_time\n",
    "    print(f\"Generated output length: {len(output[0]) - input_length}\", f\"Time taken: {elapsed_time:.4f} seconds\")\n",
    "    # print(output)\n",
    "    print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "\n",
    "    generated_all += len(output[0]) - input_length\n",
    "\n",
    "timepertoken = (decode_time) / (generated_all)\n",
    "# print(\"decode time:\", '{:.4f}'.format((decode_time) /test_samples), ' s')\n",
    "print(\"decode phase speed:\", '{:.4f}'.format(1/timepertoken) , ' token/s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################################################################\n",
    "#Save gemlite cache\n",
    "if(backend == 'gemlite'):\n",
    "\tgemlite.core.GemLiteLinear.cache_config('/tmp/gemlite_config.json') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.profile\n",
    "\n",
    "trace-offloading-r.json是最优，就是做完一个index就传一个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 2\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::index        37.24%      98.103ms        38.23%     100.712ms     201.021us       2.273ms         0.90%       2.273ms       4.536us           501  \n",
      "                                            aten::copy_        16.58%      43.681ms        21.79%      57.415ms      55.634us     202.215ms        79.93%     202.215ms     195.945us          1032  \n",
      "                                       cudaLaunchKernel         8.73%      22.995ms         8.73%      22.995ms       5.692us       0.000us         0.00%       0.000us       0.000us          4040  \n",
      "gemlite::gemv_revsplitK_A16fWnO16f_int32packing_forw...         8.43%      22.205ms         9.74%      25.664ms     200.502us       3.765ms         1.49%       4.764ms      37.216us           128  \n",
      "                                        cudaMemcpyAsync         4.56%      12.022ms         4.56%      12.022ms      22.769us       0.000us         0.00%       0.000us       0.000us           528  \n",
      "                                               aten::mm         3.46%       9.104ms         5.02%      13.224ms      22.879us      11.718ms         4.63%      11.718ms      20.273us           578  \n",
      "                                  cudaStreamSynchronize         1.86%       4.908ms         1.86%       4.908ms      18.521us       0.000us         0.00%       0.000us       0.000us           265  \n",
      "                                              aten::mul         1.85%       4.873ms         3.33%       8.763ms      11.293us       6.157ms         2.43%       6.157ms       7.934us           776  \n",
      "                                              aten::add         1.13%       2.973ms         2.03%       5.349ms      11.782us       3.081ms         1.22%       3.081ms       6.786us           454  \n",
      "                                             aten::topk         1.13%       2.965ms         2.19%       5.770ms      30.692us      12.335ms         4.88%      12.335ms      65.611us           188  \n",
      "                                    aten::empty_strided         0.99%       2.596ms         0.99%       2.596ms       5.644us       0.000us         0.00%       0.000us       0.000us           460  \n",
      "                                            aten::slice         0.86%       2.262ms         1.09%       2.870ms       2.223us       0.000us         0.00%       0.000us       0.000us          1291  \n",
      "                                       aten::as_strided         0.83%       2.174ms         0.83%       2.174ms       0.573us       0.000us         0.00%       0.000us       0.000us          3796  \n",
      "                                               aten::to         0.76%       2.013ms         7.00%      18.455ms      27.710us       0.000us         0.00%       3.152ms       4.732us           666  \n",
      "                                              aten::cat         0.72%       1.888ms         1.16%       3.056ms      15.590us       1.743ms         0.69%       1.743ms       8.893us           196  \n",
      "                                            aten::empty         0.66%       1.743ms         0.66%       1.743ms       3.289us       0.000us         0.00%       0.000us       0.000us           530  \n",
      "                                             aten::mean         0.58%       1.519ms         0.86%       2.278ms      17.519us       1.164ms         0.46%       1.164ms       8.956us           130  \n",
      "                                             aten::view         0.57%       1.503ms         0.57%       1.503ms       0.903us       0.000us         0.00%       0.000us       0.000us          1664  \n",
      "                                          aten::reshape         0.56%       1.485ms         1.72%       4.532ms       4.465us       0.000us         0.00%       1.102ms       1.086us          1015  \n",
      "                                           aten::select         0.53%       1.404ms         0.65%       1.713ms       2.664us       0.000us         0.00%       0.000us       0.000us           643  \n",
      "                                              aten::pow         0.52%       1.360ms         0.84%       2.209ms      16.995us     838.742us         0.33%     838.742us       6.452us           130  \n",
      "                                         aten::_to_copy         0.46%       1.215ms         6.24%      16.442ms      41.838us       0.000us         0.00%       3.152ms       8.019us           393  \n",
      "                                                aten::t         0.42%       1.114ms         0.70%       1.832ms       5.550us       0.000us         0.00%       0.000us       0.000us           330  \n",
      "                                           aten::matmul         0.41%       1.083ms         5.65%      14.892ms      25.765us       0.000us         0.00%      11.718ms      20.273us           578  \n",
      "                                        aten::transpose         0.39%       1.018ms         0.60%       1.571ms       1.866us       0.000us         0.00%       0.000us       0.000us           842  \n",
      "                         aten::_flash_attention_forward         0.36%     940.084us         0.87%       2.304ms      36.003us     802.458us         0.32%     802.458us      12.538us            64  \n",
      "                                              aten::neg         0.35%     934.227us         0.62%       1.629ms      12.728us       1.002ms         0.40%       1.002ms       7.831us           128  \n",
      "                                            aten::rsqrt         0.33%     867.311us         0.59%       1.549ms      11.912us     825.616us         0.33%     825.616us       6.351us           130  \n",
      "                                             aten::silu         0.33%     865.771us         0.59%       1.567ms      12.243us       1.016ms         0.40%       1.016ms       7.941us           128  \n",
      "                                    cudaLaunchKernelExC         0.32%     835.005us         0.32%     835.005us       6.231us       0.000us         0.00%       0.000us       0.000us           134  \n",
      "                                         cuLaunchKernel         0.28%     728.342us         0.28%     728.342us       5.690us       0.000us         0.00%       0.000us       0.000us           128  \n",
      "                                           aten::linear         0.26%     689.193us         5.16%      13.589ms      41.178us       0.000us         0.00%       6.087ms      18.445us           330  \n",
      "                                            aten::fill_         0.26%     673.504us         0.67%       1.753ms      12.611us       1.059ms         0.42%       1.059ms       7.615us           139  \n",
      "                                              aten::sum         0.26%     673.456us         0.40%       1.053ms      16.449us     632.046us         0.25%     632.046us       9.876us            64  \n",
      "                     aten::scaled_dot_product_attention         0.24%     626.328us         1.41%       3.714ms      58.033us       0.000us         0.00%     802.458us      12.538us            64  \n",
      "                                  cudaFuncGetAttributes         0.22%     586.521us         0.22%     586.521us       2.962us       0.000us         0.00%       0.000us       0.000us           198  \n",
      "                              aten::_local_scalar_dense         0.22%     581.265us         2.39%       6.303ms      46.344us       1.124ms         0.44%       1.124ms       8.263us           136  \n",
      "                                         aten::_softmax         0.20%     532.813us         0.34%     888.335us      13.880us     512.652us         0.20%     512.652us       8.010us            64  \n",
      "                                             aten::div_         0.19%     513.631us         0.33%     876.031us      13.688us     560.230us         0.22%     560.230us       8.754us            64  \n",
      "              aten::_scaled_dot_product_flash_attention         0.18%     483.249us         1.17%       3.088ms      48.247us       0.000us         0.00%     802.458us      12.538us            64  \n",
      "                                        aten::unsqueeze         0.17%     446.920us         0.23%     616.137us       2.352us       0.000us         0.00%       0.000us       0.000us           262  \n",
      "                                            aten::zero_         0.17%     440.254us         0.76%       2.012ms      15.719us       0.000us         0.00%     998.809us       7.803us           128  \n",
      "                                            aten::clone         0.13%     339.611us         0.92%       2.426ms      18.108us       0.000us         0.00%       1.133ms       8.455us           134  \n",
      "                                     aten::_unsafe_view         0.12%     327.674us         0.12%     327.674us       0.849us       0.000us         0.00%       0.000us       0.000us           386  \n",
      "                                           aten::expand         0.12%     324.233us         0.14%     381.910us       2.984us       0.000us         0.00%       0.000us       0.000us           128  \n",
      "                                          aten::permute         0.11%     292.939us         0.13%     344.537us       2.779us       0.000us         0.00%       0.000us       0.000us           124  \n",
      "                                       aten::empty_like         0.10%     274.404us         0.36%     950.550us       4.825us       0.000us         0.00%       0.000us       0.000us           197  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.10%     262.161us         0.10%     262.161us       0.505us       0.000us         0.00%       0.000us       0.000us           519  \n",
      "                                          aten::numpy_T         0.08%     205.727us         0.21%     550.264us       4.438us       0.000us         0.00%       0.000us       0.000us           124  \n",
      "                                             aten::item         0.08%     199.754us         2.47%       6.503ms      47.813us       0.000us         0.00%       1.124ms       8.263us           136  \n",
      "                                          aten::softmax         0.06%     152.114us         0.39%       1.040ms      16.257us       0.000us         0.00%     512.652us       8.010us            64  \n",
      "                                  cudaDeviceSynchronize         0.05%     138.287us         0.05%     138.287us     138.287us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                   cudaFuncSetAttribute         0.05%     120.139us         0.05%     120.139us       1.397us       0.000us         0.00%       0.000us       0.000us            86  \n",
      "                                 cudaDeviceGetAttribute         0.05%     119.634us         0.05%     119.634us       0.462us       0.000us         0.00%       0.000us       0.000us           259  \n",
      "                                               aten::eq         0.04%     116.742us         0.07%     189.475us      21.053us      27.524us         0.01%      27.524us       3.058us             9  \n",
      "                                             aten::isin         0.03%      85.932us         0.17%     455.149us     151.716us       0.000us         0.00%      45.537us      15.179us             3  \n",
      "                                  cudaStreamIsCapturing         0.03%      73.148us         0.03%      73.148us       1.143us       0.000us         0.00%       0.000us       0.000us            64  \n",
      "                                           aten::cumsum         0.03%      72.778us         0.05%     126.049us      42.016us      15.394us         0.01%      15.394us       5.131us             3  \n",
      "                                      aten::result_type         0.03%      70.908us         0.03%      70.908us       0.545us       0.000us         0.00%       0.000us       0.000us           130  \n",
      "                                        cudaMemsetAsync         0.03%      68.332us         0.03%      68.332us      11.389us       0.000us         0.00%       0.000us       0.000us             6  \n",
      "                                              aten::sub         0.03%      67.887us         0.05%     127.304us      25.461us      26.690us         0.01%      26.690us       5.338us             5  \n",
      "                                              aten::any         0.02%      57.151us         0.06%     160.892us      32.178us       0.000us         0.00%      18.274us       3.655us             5  \n",
      "                                     aten::index_select         0.02%      55.882us         0.04%     100.677us      50.339us       7.712us         0.00%       7.712us       3.856us             2  \n",
      "                                       aten::bitwise_or         0.02%      44.943us         0.03%      70.676us      17.669us      14.977us         0.01%      14.977us       3.744us             4  \n",
      "                                        aten::embedding         0.01%      35.740us         0.05%     141.466us      70.733us       0.000us         0.00%       7.712us       3.856us             2  \n",
      "                                              aten::max         0.01%      32.499us         0.02%      56.566us      28.283us       9.024us         0.00%       9.024us       4.512us             2  \n",
      "                                           aten::argmax         0.01%      30.295us         0.02%      54.200us      27.100us      40.097us         0.02%      40.097us      20.049us             2  \n",
      "                                     aten::masked_fill_         0.01%      30.273us         0.02%      46.037us      23.018us       4.448us         0.00%       4.448us       2.224us             2  \n",
      "                                              aten::all         0.01%      28.256us         0.02%      63.503us      31.752us       4.417us         0.00%       6.529us       3.265us             2  \n",
      "                                               aten::lt         0.01%      24.743us         0.01%      32.665us      32.665us       2.432us         0.00%       2.432us       2.432us             1  \n",
      "                                               aten::ge         0.01%      24.614us         0.01%      36.750us      18.375us       4.865us         0.00%       4.865us       2.433us             2  \n",
      "                                      aten::bitwise_and         0.01%      23.459us         0.01%      38.976us      19.488us       5.536us         0.00%       5.536us       2.768us             2  \n",
      "                                      aten::bitwise_not         0.01%      21.846us         0.01%      36.320us      18.160us       4.895us         0.00%       4.895us       2.447us             2  \n",
      "                                       aten::is_nonzero         0.01%      16.304us         0.08%     209.864us      26.233us       0.000us         0.00%      19.712us       2.464us             8  \n",
      "                                             aten::rsub         0.01%      16.058us         0.03%      92.041us      46.020us       0.000us         0.00%      19.714us       9.857us             2  \n",
      "                                         aten::new_ones         0.01%      14.388us         0.02%      56.210us      28.105us       0.000us         0.00%      21.217us      10.609us             2  \n",
      "                                          aten::resize_         0.01%      13.562us         0.01%      13.562us       6.781us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                             aten::full         0.00%      11.504us         0.02%      59.667us      14.917us       0.000us         0.00%      21.664us       5.416us             4  \n",
      "                                           aten::__or__         0.00%       9.003us         0.03%      79.679us      19.920us       0.000us         0.00%      14.977us       3.744us             4  \n",
      "                                          aten::detach_         0.00%       6.274us         0.00%      11.623us       3.874us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                             aten::ones         0.00%       5.916us         0.01%      30.431us      30.431us       0.000us         0.00%       2.048us       2.048us             1  \n",
      "                                          aten::__and__         0.00%       5.893us         0.02%      44.869us      22.435us       0.000us         0.00%       5.536us       2.768us             2  \n",
      "                                                detach_         0.00%       5.349us         0.00%       5.349us       1.783us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                          aten::view_as         0.00%       5.294us         0.00%      10.939us       1.823us       0.000us         0.00%       0.000us       0.000us             6  \n",
      "                                        aten::ones_like         0.00%       5.003us         0.01%      19.464us      19.464us       0.000us         0.00%       2.208us       2.208us             1  \n",
      "                                        aten::new_empty         0.00%       3.576us         0.00%      10.984us       5.492us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                    cudaPeekAtLastError         0.00%       1.299us         0.00%       1.299us       0.108us       0.000us         0.00%       0.000us       0.000us            12  \n",
      "                                       aten::lift_fresh         0.00%       0.389us         0.00%       0.389us       0.130us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       8.896us         0.00%       8.896us       1.779us             5  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      34.239us         0.01%      34.239us       4.891us             7  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.592us         0.00%       2.592us       2.592us             1  \n",
      "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      56.900us         0.02%      56.900us       3.793us            15  \n",
      "                         Memcpy DtoH (Device -> Pinned)         0.00%       0.000us         0.00%       0.000us       0.000us       1.124ms         0.44%       1.124ms       8.263us           136  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       7.297us         0.00%       7.297us       2.432us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      25.473us         0.01%      25.473us       6.368us             4  \n",
      "void at_cuda_detail::cub::DeviceScanInitKernel<at_cu...         0.00%       0.000us         0.00%       0.000us       0.000us       6.945us         0.00%       6.945us       2.315us             3  \n",
      "void at_cuda_detail::cub::DeviceScanKernel<at_cuda_d...         0.00%       0.000us         0.00%       0.000us       0.000us       8.449us         0.00%       8.449us       2.816us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      31.456us         0.01%      31.456us       6.291us             5  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      14.115us         0.01%      14.115us       2.353us             6  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.448us         0.00%       4.448us       2.224us             2  \n",
      "void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us       7.712us         0.00%       7.712us       3.856us             2  \n",
      "void dot_kernel<float, 128, 0, cublasDotParams<cubla...         0.00%       0.000us         0.00%       0.000us       0.000us     547.789us         0.22%     547.789us       8.559us            64  \n",
      "void reduce_1Block_kernel<float, 128, 7, cublasGemvT...         0.00%       0.000us         0.00%       0.000us       0.000us     495.277us         0.20%     495.277us       7.739us            64  \n",
      "void (anonymous namespace)::softmax_warp_forward<c10...         0.00%       0.000us         0.00%       0.000us       0.000us     512.652us         0.20%     512.652us       8.010us            64  \n",
      "void at::native::sbtopk::gatherTopK<float, unsigned ...         0.00%       0.000us         0.00%       0.000us       0.000us     785.207us         0.31%     785.207us      12.269us            64  \n",
      "void at::native::bitonicSortKVInPlace<-2, -1, 16, 16...         0.00%       0.000us         0.00%       0.000us       0.000us     741.935us         0.29%     741.935us      11.593us            64  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     632.046us         0.25%     632.046us       9.876us            64  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     560.230us         0.22%     560.230us       8.754us            64  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     998.809us         0.39%     998.809us       7.803us           128  \n",
      "          gemv_revsplitK_A16fWnO16f_int32packing_kernel         0.00%       0.000us         0.00%       0.000us       0.000us       3.765ms         1.49%       3.765ms      29.413us           128  \n",
      "void at::native::sbtopk::gatherTopK<c10::Half, unsig...         0.00%       0.000us         0.00%       0.000us       0.000us       7.724ms         3.05%       7.724ms      62.293us           124  \n",
      "void at::native::radixSortKVInPlace<-2, -1, 128, 32,...         0.00%       0.000us         0.00%       0.000us       0.000us       3.083ms         1.22%       3.083ms      24.867us           124  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.269ms         0.90%       2.269ms       9.003us           252  \n",
      "                       Memcpy DtoH (Device -> Pageable)         0.00%       0.000us         0.00%       0.000us       0.000us       1.089ms         0.43%       1.089ms       8.781us           124  \n",
      "                         Memcpy HtoD (Pinned -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     197.904ms        78.23%     197.904ms     798.000us           248  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     973.744us         0.38%     973.744us       7.377us           132  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     838.742us         0.33%     838.742us       6.452us           130  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       1.164ms         0.46%       1.164ms       8.956us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     804.729us         0.32%     804.729us       6.190us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     825.616us         0.33%     825.616us       6.351us           130  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     961.244us         0.38%     961.244us       7.394us           130  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       1.080ms         0.43%       1.080ms       8.182us           132  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.849ms         0.73%       1.849ms       7.167us           258  \n",
      "sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x...         0.00%       0.000us         0.00%       0.000us       0.000us       2.913ms         1.15%       2.913ms      22.756us           128  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       1.444ms         0.57%       1.444ms      11.282us           128  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       2.133ms         0.84%       2.133ms       8.204us           260  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       1.002ms         0.40%       1.002ms       7.831us           128  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us       1.160ms         0.46%       1.160ms       9.065us           128  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.235ms         0.88%       2.235ms       6.984us           320  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       1.102ms         0.44%       1.102ms       8.611us           128  \n",
      "void pytorch_flash::flash_fwd_kernel<pytorch_flash::...         0.00%       0.000us         0.00%       0.000us       0.000us     802.458us         0.32%     802.458us      12.538us            64  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us      40.962us         0.02%      40.962us       6.827us             6  \n",
      "sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x...         0.00%       0.000us         0.00%       0.000us       0.000us     417.064us         0.16%     417.064us      69.511us             6  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.016ms         0.40%       1.016ms       7.941us           128  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_...         0.00%       0.000us         0.00%       0.000us       0.000us     197.603us         0.08%     197.603us      49.401us             4  \n",
      "void splitKreduce_kernel<32, 16, int, float, __half,...         0.00%       0.000us         0.00%       0.000us       0.000us      31.202us         0.01%      31.202us       7.800us             4  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       2.605ms         1.03%       2.605ms      21.010us           124  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       3.026ms         1.20%       3.026ms      24.402us           124  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       1.177ms         0.47%       1.177ms       9.496us           124  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us      36.128us         0.01%      36.128us       9.032us             4  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      40.097us         0.02%      40.097us      20.049us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      36.099us         0.01%      36.099us       9.025us             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      19.714us         0.01%      19.714us       9.857us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      16.928us         0.01%      16.928us       8.464us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      14.977us         0.01%      14.977us       3.744us             4  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       2.496us         0.00%       2.496us       2.496us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.895us         0.00%       4.895us       2.447us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       5.536us         0.00%       5.536us       2.768us             2  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       9.024us         0.00%       9.024us       4.512us             2  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.905us         0.00%       3.905us       3.905us             1  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       4.417us         0.00%       4.417us       4.417us             1  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     546.505us         0.22%     546.505us       8.539us            64  \n",
      "void at::native::vectorized_elementwise_kernel<2, at...         0.00%       0.000us         0.00%       0.000us       0.000us       8.321us         0.00%       8.321us       8.321us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 263.467ms\n",
      "Self CUDA time total: 252.991ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 1\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 2\n",
    "test_samples = 4\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "fineweb = load_dataset(\"parquet\",data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text'][:test_samples] \n",
    "\n",
    "print(\"output length is {}\".format(output_length))\n",
    "text = fineweb_text[0]\n",
    "inputs = preprocess_data(text, tokenizer)\n",
    "\n",
    "# cached_mlp.clear_load_from_cpu_stats()\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ]\n",
    ") as p:\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(),\n",
    "            max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "print(p.key_averages().table(\n",
    "    sort_by=\"self_cpu_time_total\", row_limit=-1))\n",
    "p.export_chrome_trace(\"./offloading-hqq2-rightid.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载到GPU上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:25<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MixtralForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "import json\n",
    "\n",
    "def get_model(model_name, device_map, dtype=torch.bfloat16):\n",
    "    llm = MixtralForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        use_cache=True,\n",
    "        torch_dtype=dtype,\n",
    "    ) \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return llm, tokenizer\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "with open('../quantize/device_map_1.json', 'r') as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "dtype = torch.float16\n",
    "llm, tokenizer = get_model(model_name, device_map, dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 只传一个专家的版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import threading\n",
    "import json\n",
    "from queue import Queue\n",
    "\n",
    "class CachedMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, dtype, sparsity: float = 0.2):\n",
    "        super(CachedMLP, self).__init__()\n",
    "        self.sparsity = sparsity\n",
    "        self.activenum = int((1 - sparsity) * hidden_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dtype = dtype\n",
    "\n",
    "        # GPU 缓存张量\n",
    "        self.w1_gpu = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda:0')\n",
    "        self.w2_gpu = torch.empty((self.input_dim, self.activenum), dtype=self.dtype, device='cuda:0')\n",
    "        self.w3_gpu = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda:0')\n",
    "\n",
    "        # Pinned Memory 缓冲区\n",
    "        self.register_buffer('sparse_w1_cpu', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "        self.register_buffer('sparse_w2_cpu', torch.empty((self.input_dim, self.activenum), dtype=self.dtype, device='cpu'))\n",
    "        self.register_buffer('sparse_w3_cpu', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "        self.sparse_w1_cpu = self.sparse_w1_cpu.pin_memory()\n",
    "        self.sparse_w2_cpu = self.sparse_w2_cpu.pin_memory()\n",
    "        self.sparse_w3_cpu = self.sparse_w3_cpu.pin_memory()\n",
    "\n",
    "        # 统计信息\n",
    "        self.load_from_cpu_time = 0.0\n",
    "        self.load_from_cpu_calls = 0\n",
    "\n",
    "    def load_from_cpu(self, cpu_mlp, stream: torch.cuda.Stream):\n",
    "        \"\"\"\n",
    "        从CPU加载参数，并使用指定的CUDA流进行异步复制到GPU。\n",
    "        \n",
    "        参数:\n",
    "            cpu_mlp: 包含CPU上参数的字典。\n",
    "            stream: 用于数据传输的CUDA流。\n",
    "        \"\"\"\n",
    "        # 从CPU加载参数\n",
    "        self.sparse_w1_cpu.copy_(cpu_mlp['w1'].data[:self.activenum, :])\n",
    "        self.sparse_w2_cpu.copy_(cpu_mlp['w2'].data[:, :self.activenum])\n",
    "        self.sparse_w3_cpu.copy_(cpu_mlp['w3'].data[:self.activenum, :])\n",
    "\n",
    "        # 异步复制到GPU\n",
    "        with torch.cuda.stream(stream):\n",
    "            self.w1_gpu.copy_(self.sparse_w1_cpu, non_blocking=True)\n",
    "            self.w2_gpu.copy_(self.sparse_w2_cpu, non_blocking=True)\n",
    "            self.w3_gpu.copy_(self.sparse_w3_cpu, non_blocking=True)\n",
    "\n",
    "    def get_load_from_cpu_stats(self):\n",
    "        if self.load_from_cpu_calls == 0:\n",
    "            return 0.0, 0.0\n",
    "        avg_time = self.load_from_cpu_time / self.load_from_cpu_calls\n",
    "        return self.load_from_cpu_time, avg_time\n",
    "\n",
    "    def clear_load_from_cpu_stats(self):\n",
    "        self.load_from_cpu_time = 0.0\n",
    "        self.load_from_cpu_calls = 0\n",
    "\n",
    "def convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.9):\n",
    "    ### 其他部分存放在GPU上\n",
    "    llm.model.embed_tokens.cuda()\n",
    "    for i in range(len(llm.model.layers)):\n",
    "        llm.model.layers[i].self_attn.cuda()\n",
    "        llm.model.layers[i].input_layernorm.cuda()\n",
    "        llm.model.layers[i].post_attention_layernorm.cuda()\n",
    "        llm.model.layers[i].block_sparse_moe.gate.cuda()\n",
    "    ### 第0层的专家存放在GPU上\n",
    "    for j in range(len(llm.model.layers[0].block_sparse_moe.experts)):\n",
    "        llm.model.layers[0].block_sparse_moe.experts[j].cuda()\n",
    "\n",
    "    llm.model.norm.cuda()\n",
    "    llm.lm_head.cuda()\n",
    "    \n",
    "    # 创建两个共享的CachedMLP实例\n",
    "    buffer0 = CachedMLP(\n",
    "        input_dim=llm.config.hidden_size,\n",
    "        hidden_dim=llm.config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "        sparsity=sparsity\n",
    "    )\n",
    "    buffer1 = CachedMLP(\n",
    "        input_dim=llm.config.hidden_size,\n",
    "        hidden_dim=llm.config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "        sparsity=sparsity\n",
    "    )\n",
    "    cached_mlps = [buffer0, buffer1]\n",
    "    \n",
    "    for i, layer in enumerate(llm.model.layers):\n",
    "        if i==0:\n",
    "            continue\n",
    "        # 将专家的forward方法替换为PipelineLLM管理的方式\n",
    "        for j, expert in enumerate(layer.block_sparse_moe.experts):\n",
    "            expert.cpu_mlp = {\n",
    "                \"w1\": expert.w1.cpu().weight,\n",
    "                \"w2\": expert.w2.cpu().weight,\n",
    "                \"w3\": expert.w3.cpu().weight,\n",
    "            }\n",
    "            # 替换forward方法为直接调用CachedMLP的forward（需要在pipelineLLM里面替换)\n",
    "            # expert.forward = lambda x, cached_mlp=cached_mlp, cpu_mlp=expert.cpu_mlp: cached_mlp(x, cpu_mlp)\n",
    "    return llm, cached_mlps\n",
    "\n",
    "class PipelineLLM:\n",
    "    def __init__(self, llm, cached_mlps):\n",
    "        \"\"\"\n",
    "        初始化 PipelineLLM，替换模型每一层的 forward 方法。\n",
    "        \n",
    "        参数:\n",
    "            llm: 原始的大模型\n",
    "            cached_mlps: 两个 CachedMLP 实例列表\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.cached_mlps = cached_mlps  # [buffer0, buffer1]\n",
    "        self.num_layers = len(llm.model.layers)\n",
    "        self.lock = threading.Lock()\n",
    "        self.use_buffer0 = True  # 标记当前使用哪个缓冲区\n",
    "\n",
    "        # 创建两个共享的CUDA流\n",
    "        self.stream0 = torch.cuda.Stream()\n",
    "        self.stream1 = torch.cuda.Stream()\n",
    "\n",
    "        # 初始化加载第一个和第二个层的参数\n",
    "        self._load_layer(1, buffer_index=0)\n",
    "        self._load_layer(1, buffer_index=1)\n",
    "\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "        self._replace_forward_methods()\n",
    "    \n",
    "    def _load_layer(self, layer_idx, buffer_index):\n",
    "        \"\"\"\n",
    "        加载指定层的参数到指定的缓冲区。\n",
    "        \n",
    "        参数:\n",
    "            layer_idx: 层的索引\n",
    "            buffer_index: 缓冲区的索引（0 或 1）\n",
    "        \"\"\"\n",
    "        layer = self.llm.model.layers[layer_idx]\n",
    "        expert = layer.block_sparse_moe.experts[0]\n",
    "        cpu_mlp = expert.cpu_mlp\n",
    "        buffer = self.cached_mlps[buffer_index]\n",
    "        stream = self.stream0 if buffer_index == 0 else self.stream1\n",
    "\n",
    "        # 异步加载参数\n",
    "        buffer.load_from_cpu(cpu_mlp, stream)\n",
    "\n",
    "    def _replace_forward_methods(self):\n",
    "        \"\"\"\n",
    "        替换模型每一层的 forward 方法，添加参数预加载逻辑和注意力计算。\n",
    "        \"\"\"\n",
    "        for i, layer in enumerate(self.llm.model.layers):\n",
    "            def new_forward(hidden_states: torch.Tensor,\n",
    "                            attention_mask: Optional[torch.Tensor] = None,\n",
    "                            position_ids: Optional[torch.LongTensor] = None,\n",
    "                            past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "                            output_attentions: Optional[bool] = False,\n",
    "                            output_router_logits: Optional[bool] = False,\n",
    "                            use_cache: Optional[bool] = False,\n",
    "                            cache_position: Optional[torch.LongTensor] = None,\n",
    "                            layer_idx=i):\n",
    "                # print(f\"in layer {layer_idx}\")\n",
    "                with self.lock:\n",
    "                    # 选择当前使用的缓冲区\n",
    "                    current_buffer = self.cached_mlps[0] if self.use_buffer0 else self.cached_mlps[1]\n",
    "                    current_stream = self.stream0 if self.use_buffer0 else self.stream1\n",
    "\n",
    "                    # 切换缓冲区用于下一次\n",
    "                    next_buffer_index = 1 if self.use_buffer0 else 0\n",
    "                    next_buffer = self.cached_mlps[next_buffer_index]\n",
    "                    next_stream = self.stream1 if self.use_buffer0 else self.stream0\n",
    "\n",
    "                    # 预加载下一层的参数\n",
    "                    next_layer_idx = layer_idx + 1\n",
    "                    if next_layer_idx < self.num_layers:\n",
    "                        self._load_layer(next_layer_idx, buffer_index=next_buffer_index)\n",
    "                    \n",
    "                    # 切换缓冲区\n",
    "                    self.use_buffer0 = not self.use_buffer0\n",
    "\n",
    "                    # 处理当前层\n",
    "                    residual = hidden_states\n",
    "                    hidden_states = layer.input_layernorm(hidden_states)\n",
    "\n",
    "                    # Self Attention\n",
    "                    hidden_states, self_attn_weights, present_key_value = layer.self_attn(\n",
    "                        hidden_states=hidden_states,\n",
    "                        attention_mask=attention_mask,\n",
    "                        position_ids=position_ids,\n",
    "                        past_key_value=past_key_value,\n",
    "                        output_attentions=output_attentions,\n",
    "                        use_cache=use_cache,\n",
    "                        cache_position=cache_position,\n",
    "                    )\n",
    "                    hidden_states = residual + hidden_states\n",
    "\n",
    "                    # Fully Connected\n",
    "                    residual = hidden_states\n",
    "                    hidden_states = layer.post_attention_layernorm(hidden_states)\n",
    "\n",
    "                    # 使用当前缓冲区进行 MLP 计算\n",
    "                    batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "                    hidden_states = hidden_states.view(-1, hidden_dim)\n",
    "                    # 仅使用第一个专家\n",
    "                    expert_layer = layer.block_sparse_moe.experts[0]\n",
    "\n",
    "                    w3_output = torch.matmul(hidden_states, current_buffer.w3_gpu.T)\n",
    "                    w1_output = self.activation(torch.matmul(hidden_states, current_buffer.w1_gpu.T))\n",
    "                    w2 = current_buffer.w2_gpu.T\n",
    "                    final_hidden_states = torch.matmul(w1_output * w3_output, w2)\n",
    "\n",
    "                    final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n",
    "\n",
    "                    hidden_states = residual + final_hidden_states\n",
    "\n",
    "                    outputs = (hidden_states,)\n",
    "\n",
    "                    if output_attentions:\n",
    "                        outputs += (self_attn_weights,)\n",
    "\n",
    "                    if use_cache:\n",
    "                        outputs += (present_key_value,)\n",
    "\n",
    "                    return outputs\n",
    "            # 替换 forward 方法\n",
    "            layer.forward = new_forward\n",
    "\n",
    "    def _async_load(self, layer_idx, buffer_index):\n",
    "        \"\"\"\n",
    "        异步加载 MLP 参数到指定缓冲区，使用共享的CUDA流。\n",
    "        \"\"\"\n",
    "        self._load_layer(layer_idx, buffer_index)\n",
    "\n",
    "# 将模型转换为使用CachedMLP的版本\n",
    "llm, cached_mlps = convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.9)\n",
    "\n",
    "# 创建流水线模型\n",
    "pipeline_llm = PipelineLLM(llm, cached_mlps).llm\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
