{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先都加载到cpu上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:05<00:00,  3.18it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MixtralForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import json\n",
    "\n",
    "def get_model(model_name, device_map, dtype=torch.bfloat16):\n",
    "    llm = MixtralForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        use_cache=True,\n",
    "        torch_dtype=dtype,\n",
    "    ) \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return llm, tokenizer\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "with open('../quantize/device_map_1.json', 'r') as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "dtype = torch.float16\n",
    "llm, tokenizer = get_model(model_name, 'cpu', dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2102648/3152464007.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.up_th = torch.load(th_path, map_location='cpu')[\"up_proj_states_thresholds_2\"]  # 保持在CPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    up_threshold_path = paths[\"chess_up_threshold\"]\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CachedMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, dtype, sparsity: float = 0.2):\n",
    "        super(CachedMLP, self).__init__()\n",
    "        self.sparsity = sparsity\n",
    "        filepath = str(sparsity).replace('.', '_')\n",
    "        th_path = f'{up_threshold_path}/thresholds_{filepath}.pt'\n",
    "        self.up_th = torch.load(th_path, map_location='cpu')[\"up_proj_states_thresholds_2\"]  # 保持在CPU\n",
    "        self.activenum = int((1 - sparsity) * hidden_dim)  # 根据稀疏阈值计算激活的维度\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dtype = dtype\n",
    "\n",
    "        # 直接存储预分配的 GPU 张量\n",
    "        self.w1_gpu = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda')\n",
    "        self.w2_gpu = torch.empty((self.input_dim, self.activenum), dtype=self.dtype, device='cuda')\n",
    "        self.w3_gpu = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda')\n",
    "\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "        # CUDA 流用于异步数据传输\n",
    "        self.stream = torch.cuda.Stream()\n",
    "\n",
    "        # 用于统计 load_from_cpu 的耗时\n",
    "        self.load_from_cpu_time = 0.0  # 总耗时（毫秒）\n",
    "        self.load_from_cpu_calls = 0  # 调用次数\n",
    "\n",
    "    def load_from_cpu(self, cpu_mlp, x: torch.Tensor = None, layer_id=0, expert_id=0):\n",
    "        \"\"\"\n",
    "        从 CPU 上的 MLP 加载参数到 GPU 上的缓存 MLP。\n",
    "        如果是 prefill 阶段（x.size(1) > 1），加载全部参数。\n",
    "        如果是 decode 阶段，根据 w3 * x 和 up_th 的大小关系，稀疏化加载参数。\n",
    "        \"\"\"\n",
    "        # 创建 CUDA 事件用于计时\n",
    "        start_event = torch.cuda.Event(enable_timing=True)\n",
    "        end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        # 记录开始时间\n",
    "        torch.cuda.synchronize()\n",
    "        start_event.record()\n",
    "\n",
    "        # 获取 CPU 上的参数\n",
    "        w1_weight = cpu_mlp['w1'].weight.data.cpu()  # 确保在 CPU 上\n",
    "        w2_weight = cpu_mlp['w2'].weight.data.cpu()\n",
    "        w3_weight = cpu_mlp['w3'].weight.data\n",
    "\n",
    "        # if x.size(0) == 1:  # decode 阶段\n",
    "        #     # 计算 w3 * x\n",
    "        #     w3_output = torch.matmul(x, w3_weight.T)  # 形状: [batch_size, activenum]\n",
    "\n",
    "        #     # 根据 w3_output 和 up_th 确定需要稀疏化的神经元位置\n",
    "        #     # threshold = self.up_th[layer_id][expert_id]\n",
    "        #     # active_mask = w3_output.abs() > threshold  # 形状: [batch_size, activenum]\n",
    "        #     # active_indices = torch.where(active_mask.any(dim=0))[0]  # 按列筛选，形状: [num_active]\n",
    "\n",
    "        #     # 限制为 activenum 个\n",
    "        #     # active_indices = active_indices[:self.activenum]\n",
    "\n",
    "        #     # 选择稀疏化后的权重\n",
    "        #     sparse_w1 = w1_weight[:self.activenum, :].pin_memory()\n",
    "        #     sparse_w2 = w2_weight[:, :self.activenum].pin_memory()\n",
    "        #     # sparse_w1 = w1_weight[active_indices, :].pin_memory()  # 移动到固定内存\n",
    "        #     # sparse_w2 = w2_weight[:, active_indices].pin_memory()\n",
    "\n",
    "        #     # 异步复制到 GPU\n",
    "        #     with torch.cuda.stream(self.stream):\n",
    "        #         self.w1_gpu.copy_(sparse_w1, non_blocking=True)\n",
    "        #         self.w2_gpu.copy_(sparse_w2, non_blocking=True)\n",
    "            \n",
    "        #     # 确保复制完成\n",
    "        #     torch.cuda.current_stream().wait_stream(self.stream)\n",
    "\n",
    "        #     w3_output = w3_output[:, :self.activenum].to('cuda', non_blocking=True)\n",
    "\n",
    "        # else:  # prefill 阶段\n",
    "        # 移动全部参数到固定内存\n",
    "        sparse_w1 = w1_weight[:self.activenum, :].pin_memory()\n",
    "        sparse_w2 = w2_weight[:, :self.activenum].pin_memory()\n",
    "        sparse_w3 = w3_weight[:self.activenum, :].pin_memory()\n",
    "\n",
    "        # 异步复制到 GPU\n",
    "        with torch.cuda.stream(self.stream):\n",
    "            self.w1_gpu.copy_(sparse_w1, non_blocking=True)\n",
    "            self.w2_gpu.copy_(sparse_w2, non_blocking=True)\n",
    "            self.w3_gpu.copy_(sparse_w3, non_blocking=True)\n",
    "        \n",
    "        # 确保复制完成\n",
    "        torch.cuda.current_stream().wait_stream(self.stream)\n",
    "\n",
    "        # w3_output = None\n",
    "\n",
    "        # 记录结束时间\n",
    "        end_event.record()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        # 计算耗时（毫秒）\n",
    "        elapsed_time = start_event.elapsed_time(end_event)\n",
    "\n",
    "        # 更新统计信息\n",
    "        self.load_from_cpu_time += elapsed_time\n",
    "        self.load_from_cpu_calls += 1\n",
    "\n",
    "        # return w3_output\n",
    "\n",
    "    def forward(self, x: torch.Tensor, cpu_mlp=None, layer_id=0, expert_id=0) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        前向传播逻辑。\n",
    "        如果是 prefill 阶段，直接计算。\n",
    "        如果是 decode 阶段，先加载稀疏化参数，再计算。\n",
    "        \"\"\"\n",
    "        # 确保输入在 GPU 上\n",
    "        if x.device != 'cuda':\n",
    "            x = x.to('cuda', non_blocking=True)\n",
    "        # if x.size(0) > 1:  # prefill 阶段\n",
    "        # 加载全部参数\n",
    "        self.load_from_cpu(cpu_mlp, x, layer_id=layer_id, expert_id=expert_id)\n",
    "        # 计算 w3\n",
    "        w3_output = torch.matmul(x, self.w3_gpu.T)  # 形状: [batch_size, activenum]\n",
    "        # else:  # decode 阶段\n",
    "        #     # 加载稀疏化参数并获取 w3_output\n",
    "        #     w3_output = self.load_from_cpu(cpu_mlp, x, layer_id, expert_id)\n",
    "\n",
    "        # 计算 w1\n",
    "        w1_output = self.activation(torch.matmul(x, self.w1_gpu.T))  # 形状: [batch_size, activenum]\n",
    "        # 计算 w2\n",
    "        x = torch.matmul(w1_output * w3_output, self.w2_gpu.T)  # 形状: [batch_size, input_dim]\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_load_from_cpu_stats(self):\n",
    "        \"\"\"\n",
    "        返回 load_from_cpu 的统计信息。\n",
    "        \"\"\"\n",
    "        if self.load_from_cpu_calls == 0:\n",
    "            return 0.0, 0.0\n",
    "        avg_time = self.load_from_cpu_time / self.load_from_cpu_calls\n",
    "        return self.load_from_cpu_time, avg_time\n",
    "\n",
    "    def clear_load_from_cpu_stats(self):\n",
    "        \"\"\"\n",
    "        清除 load_from_cpu 的统计信息。\n",
    "        \"\"\"\n",
    "        if self.load_from_cpu_calls == 0:\n",
    "            return \n",
    "        self.load_from_cpu_time, self.load_from_cpu_calls = 0.0, 0.0\n",
    "\n",
    "def convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.9):\n",
    "    \"\"\"\n",
    "    将 Mixtral 模型的 MLP 层替换为缓存 MLP 的版本。\n",
    "    \"\"\"\n",
    "    ### 其他部分存放在GPU上\n",
    "    llm.model.embed_tokens.cuda()\n",
    "    for i in range(32):\n",
    "        llm.model.layers[i].self_attn.cuda()\n",
    "        llm.model.layers[i].input_layernorm.cuda()\n",
    "        llm.model.layers[i].post_attention_layernorm.cuda()\n",
    "        llm.model.layers[i].block_sparse_moe.gate.cuda()\n",
    "    llm.model.norm.cuda()\n",
    "    llm.lm_head.cuda()\n",
    "    \n",
    "    # 在 GPU 上缓存一个 MLP 实例\n",
    "    cached_mlp = CachedMLP(\n",
    "        input_dim=llm.config.hidden_size,\n",
    "        hidden_dim=llm.config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "        sparsity=sparsity\n",
    "    )\n",
    "\n",
    "    # 遍历每一层的 block_sparse_moe.experts，将 w3 加载到 GPU\n",
    "    for i in range(len(llm.model.layers)):\n",
    "        for j in range(len(llm.model.layers[i].block_sparse_moe.experts)):\n",
    "            # 将 w3 加载到 GPU（已在 CachedMLP 中处理）\n",
    "            # 保存原始的 w1、w2、w3 层（常驻 CPU）\n",
    "            llm.model.layers[i].block_sparse_moe.experts[j].cpu_mlp = {\n",
    "                \"w1\": llm.model.layers[i].block_sparse_moe.experts[j].w1.cpu(),\n",
    "                \"w2\": llm.model.layers[i].block_sparse_moe.experts[j].w2.cpu(),\n",
    "                \"w3\": llm.model.layers[i].block_sparse_moe.experts[j].w3.cpu(),\n",
    "            }\n",
    "\n",
    "            # 替换为缓存 MLP 的版本\n",
    "            llm.model.layers[i].block_sparse_moe.experts[j].forward = lambda x, cached_mlp=cached_mlp, cpu_mlp=llm.model.layers[i].block_sparse_moe.experts[j].cpu_mlp, layer_id=i, expert_id=j: cached_mlp_forward(x, cached_mlp, cpu_mlp, layer_id, expert_id)\n",
    "\n",
    "    return llm, cached_mlp\n",
    "\n",
    "def cached_mlp_forward(x, cached_mlp, cpu_mlp, layer_id=0, expert_id=0):\n",
    "    \"\"\"\n",
    "    动态加载 CPU 上的 MLP 参数到缓存的 MLP，并执行前向传播。\n",
    "    \"\"\"\n",
    "    # 使用缓存的 MLP 进行计算\n",
    "    if x.size(0) == 0:\n",
    "        return torch.zeros(x.shape, device='cuda')\n",
    "    output = cached_mlp(x, cpu_mlp, layer_id, expert_id)\n",
    "\n",
    "    return output\n",
    "\n",
    "# 将模型转换为缓存 MLP 的版本\n",
    "llm, cached_mlp = convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试时间开销"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated output length: 1\n",
      "['The Independent Jane\\nFor all the love, romance and scandal in Jane Austen’s books, what they are really about is freedom and independence. Independihood']\n",
      "Time taken: 1.1744 seconds\n",
      "output length is 32\n",
      "Generated output length: 32\n",
      "['The Independent Jane\\nFor all the love, romance and scandal in Jane Austen’s books, what they are really about is freedom and independence. Independihoodihood /******/ /******/ /******/ /******/ /******/ /******/ /******/ /******/ /******/ /******/ /******/ /******/ /******/ /******/ /******/ /******/ /******/ /******/ /******/ /******/ /******/ /******/ /******/ /******/ /******/ /******/ /******/ /******/ /******/ /******/']\n",
      "Time taken: 11.9270 seconds\n",
      "Total time spent in load_from_cpu: 10.0194 s\n",
      "decode phase speed: 0.3468586110761088\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 32\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 32\n",
    "test_samples = 1\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "fineweb = load_dataset(\"parquet\",data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text'][:test_samples] \n",
    "\n",
    "prefill_time, decode_time = 0, 0\n",
    "for output_length in [1, output_length]:\n",
    "# for output_length in [output_length]:\n",
    "    print(\"output length is {}\".format(output_length))\n",
    "    for text in fineweb_text:\n",
    "        # input_ids = torch.randint(0, 32000, (1, input_length)).cuda()  # 随机生成输入 token IDs\n",
    "        # attention_mask = torch.ones((1, input_length)).cuda()  # 假设 attention mask\n",
    "        inputs = preprocess_data(text, tokenizer)\n",
    "\n",
    "        # 预热（避免第一次运行时的额外开销）\n",
    "        # with torch.no_grad():\n",
    "        #     output = llm(input_ids=inputs[\"input_ids\"].cuda(), attention_mask=inputs[\"attention_mask\"].cuda())\n",
    "\n",
    "        # 测试时间\n",
    "        start_event = torch.cuda.Event(enable_timing=True)\n",
    "        end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        cached_mlp.clear_load_from_cpu_stats()\n",
    "        # 开始计时\n",
    "        torch.cuda.synchronize()\n",
    "        start_event.record()\n",
    "\n",
    "        # 前向传播\n",
    "        with torch.no_grad():\n",
    "            output = llm.generate(\n",
    "                input_ids=inputs[\"input_ids\"].cuda(),\n",
    "                attention_mask=inputs[\"attention_mask\"].cuda(),\n",
    "                max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "                generation_config=GenerationConfig(do_sample=False)\n",
    "            )\n",
    "\n",
    "        # 结束计时\n",
    "        end_event.record()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        # 计算时间\n",
    "        elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "        print(f\"Generated output length: {len(output[0]) - input_length}\")\n",
    "        # print(output)\n",
    "        print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "        print(f\"Time taken: {elapsed_time:.4f} seconds\")\n",
    "        if output_length == 1:\n",
    "            prefill_time = elapsed_time\n",
    "        else:\n",
    "            decode_time = elapsed_time\n",
    "            total_time, avg_time = cached_mlp.get_load_from_cpu_stats()\n",
    "            print(f\"Total time spent in load_from_cpu: {total_time/1000:.4f} s\")\n",
    "\n",
    "print(\"decode phase speed:\", (decode_time - prefill_time) / (output_length-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time spent in load_from_cpu: 10.0194 s\n",
      "Average time per load_from_cpu call: 0.0047 s\n"
     ]
    }
   ],
   "source": [
    "# 假设 cached_mlp 是 CachedMLP 的实例\n",
    "total_time, avg_time = cached_mlp.get_load_from_cpu_stats()\n",
    "print(f\"Total time spent in load_from_cpu: {total_time/1000:.4f} s\")\n",
    "print(f\"Average time per load_from_cpu call: {avg_time/1000:.4f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载到GPU上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:25<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MixtralForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "import json\n",
    "\n",
    "def get_model(model_name, device_map, dtype=torch.bfloat16):\n",
    "    llm = MixtralForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        use_cache=True,\n",
    "        torch_dtype=dtype,\n",
    "    ) \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return llm, tokenizer\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "with open('../quantize/device_map_1.json', 'r') as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "dtype = torch.float16\n",
    "llm, tokenizer = get_model(model_name, device_map, dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### v0版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CachedMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, dtype):\n",
    "        super(CachedMLP, self).__init__()\n",
    "        # 定义 w1、w2、w3 三个线性层\n",
    "        self.w1 = nn.Linear(input_dim, hidden_dim, bias=False, dtype=dtype)\n",
    "        self.w2 = nn.Linear(hidden_dim, input_dim, bias=False, dtype=dtype)\n",
    "        self.w3 = nn.Linear(input_dim, hidden_dim, bias=False, dtype=dtype)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "        # 将 MLP 缓存在 GPU 上\n",
    "        self.cuda()\n",
    "\n",
    "    def load_from_cpu(self, cpu_mlp):\n",
    "        \"\"\"\n",
    "        从 CPU 上的 MLP 加载参数到 GPU 上的缓存 MLP。\n",
    "        \"\"\"\n",
    "        # 将 CPU 上的参数复制到 GPU 上的缓存 MLP\n",
    "        # print(cpu_mlp)\n",
    "        # print(cpu_mlp.w1.state_dict())\n",
    "        self.w1.load_state_dict(cpu_mlp['w1'].state_dict())\n",
    "        self.w2.load_state_dict(cpu_mlp['w2'].state_dict())\n",
    "        self.w3.load_state_dict(cpu_mlp['w3'].state_dict())\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # 确保输入在 GPU 上\n",
    "        x = x.to('cuda')\n",
    "        # 计算 w1 和 w3\n",
    "        # print(self.w1.type, x.type)\n",
    "        w1_output = self.activation(self.w1(x))\n",
    "        w3_output = self.w3(x)\n",
    "        # 计算 w2\n",
    "        x = self.w2(w1_output * w3_output)\n",
    "        return x\n",
    "\n",
    "def convert_mixtral_to_cached_mlp(llm, dtype):\n",
    "    \"\"\"\n",
    "    将 Mixtral 模型的 MLP 层替换为缓存 MLP 的版本。\n",
    "    \"\"\"\n",
    "    ### 其他部分存放在GPU上\n",
    "    llm.model.embed_tokens.cuda()\n",
    "    for i in range(32):\n",
    "        llm.model.layers[i].self_attn.cuda()\n",
    "        llm.model.layers[i].input_layernorm.cuda()\n",
    "        llm.model.layers[i].post_attention_layernorm.cuda()\n",
    "        llm.model.layers[i].block_sparse_moe.gate.cuda()\n",
    "    llm.model.norm.cuda()\n",
    "    llm.lm_head.cuda()\n",
    "    \n",
    "    # 在 GPU 上缓存一个 MLP 实例\n",
    "    cached_mlp = CachedMLP(\n",
    "        input_dim=llm.config.hidden_size,\n",
    "        hidden_dim=llm.config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "    )\n",
    "\n",
    "    # 遍历每一层的 block_sparse_moe.experts\n",
    "    for i in range(len(llm.model.layers)):\n",
    "        for j in range(len(llm.model.layers[i].block_sparse_moe.experts)):\n",
    "            # 保存原始的 w1、w2、w3 层（常驻 CPU）\n",
    "            llm.model.layers[i].block_sparse_moe.experts[j].cpu_mlp = {\n",
    "                \"w1\": llm.model.layers[i].block_sparse_moe.experts[j].w1,\n",
    "                \"w2\": llm.model.layers[i].block_sparse_moe.experts[j].w2,\n",
    "                \"w3\": llm.model.layers[i].block_sparse_moe.experts[j].w3,\n",
    "            }\n",
    "\n",
    "            # 替换为缓存 MLP 的版本\n",
    "            llm.model.layers[i].block_sparse_moe.experts[j].forward = lambda x, cached_mlp=cached_mlp, cpu_mlp=llm.model.layers[i].block_sparse_moe.experts[j].cpu_mlp: cached_mlp_forward(x, cached_mlp, cpu_mlp)\n",
    "\n",
    "    return llm\n",
    "\n",
    "def cached_mlp_forward(x, cached_mlp, cpu_mlp):\n",
    "    \"\"\"\n",
    "    动态加载 CPU 上的 MLP 参数到缓存的 MLP，并执行前向传播。\n",
    "    \"\"\"\n",
    "    # 从 CPU 上传参数到缓存的 MLP\n",
    "    cached_mlp.load_from_cpu(cpu_mlp)\n",
    "\n",
    "    # 使用缓存的 MLP 进行计算\n",
    "    output = cached_mlp(x)\n",
    "\n",
    "    # 将缓存的 MLP 参数清空（可选）\n",
    "    # cached_mlp.load_from_cpu({\n",
    "    #     \"w1\": nn.Linear(cached_mlp.w1.in_features, cached_mlp.w1.out_features).cpu(),\n",
    "    #     \"w2\": nn.Linear(cached_mlp.w2.in_features, cached_mlp.w2.out_features).cpu(),\n",
    "    #     \"w3\": nn.Linear(cached_mlp.w3.in_features, cached_mlp.w3.out_features).cpu(),\n",
    "    # })\n",
    "\n",
    "    return output\n",
    "\n",
    "# 将模型转换为缓存 MLP 的版本\n",
    "llm = convert_mixtral_to_cached_mlp(llm, dtype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
