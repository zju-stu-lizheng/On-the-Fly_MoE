{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先都加载到cpu上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using atten... sdpa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/venv/dilab/floe/hqq/hqq/models/base.py:251: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(cls.get_weight_file(save_dir), map_location=map_location)\n",
      "100%|██████████| 32/32 [00:00<00:00, 414.85it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 1168.74it/s]\n",
      "100%|██████████| 32/32 [03:15<00:00,  6.11s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"\n",
    "from modeling_mixtral import MixtralForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import json\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "dtype = torch.float16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "### HQQ量化\n",
    "from hqq.core.quantize import *\n",
    "from hqq.models.hf.mixtral import MixtralPatch\n",
    "import transformers\n",
    "from hqq.models.base import BaseHQQModel\n",
    "from accelerate import init_empty_weights\n",
    "\n",
    "import gemlite\n",
    "from gemlite import GemLiteLinearTriton \n",
    "# GemLiteLinearTriton.get_default_gemv = lambda *args, **kwargs: 'GEMV'\n",
    "\n",
    "save_dir = './hqqsaved'\n",
    "\n",
    "class BaseHQQHFModel(BaseHQQModel):\n",
    "    # Save model architecture\n",
    "    @classmethod\n",
    "    def cache_model(cls, model, save_dir):\n",
    "        model.config.save_pretrained(save_dir)\n",
    "\n",
    "    # Create empty model from config\n",
    "    @classmethod\n",
    "    def create_model(cls, save_dir, kwargs):\n",
    "        model_kwargs = {}\n",
    "        for key in [\"attn_implementation\"]:\n",
    "            if key in kwargs:\n",
    "                model_kwargs[key] = kwargs[key]\n",
    "\n",
    "        config = transformers.AutoConfig.from_pretrained(\n",
    "            cls.get_config_file(save_dir)\n",
    "        )\n",
    "\n",
    "        with init_empty_weights():\n",
    "            model = MixtralForCausalLM._from_config(config, **model_kwargs)\n",
    "\n",
    "        return model\n",
    "\n",
    "class MixtralHQQ(MixtralPatch, BaseHQQHFModel):\n",
    "    pass\n",
    "\n",
    "### 从保存的权重中加载\n",
    "llm = MixtralHQQ.from_quantized(save_dir, compute_dtype=dtype, device='cpu')\n",
    "HQQLinear.set_backend(HQQBackend.PYTORCH)\n",
    "\n",
    "backend       = \"gemlite\" #'torchao_int4' #\"torchao_int4\" (4-bit only) or \"gemlite\" (4-bit + 2-bit)\n",
    "# #Optimize\n",
    "from hqq.utils.patching import prepare_for_inference\n",
    "prepare_for_inference(llm, backend=backend, verbose=True)\n",
    "\n",
    "if(backend == 'gemlite'):\n",
    "\tgemlite.core.GEMLITE_TRITON_RESTRICT_M = True\n",
    "\tgemlite.core.GemLiteLinear.load_config('/tmp/gemlite_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "active neural num  2867\n",
      "active neural num  2867\n",
      "... loading layer 0\n",
      "... loading layer 1\n",
      "... loading layer 2\n",
      "... loading layer 3\n",
      "... loading layer 4\n",
      "... loading layer 5\n",
      "... loading layer 6\n",
      "... loading layer 7\n",
      "... loading layer 8\n",
      "... loading layer 9\n",
      "... loading layer 10\n",
      "... loading layer 11\n",
      "... loading layer 12\n",
      "... loading layer 13\n",
      "... loading layer 14\n",
      "... loading layer 15\n",
      "... loading layer 16\n",
      "... loading layer 17\n",
      "... loading layer 18\n",
      "... loading layer 19\n",
      "... loading layer 20\n",
      "... loading layer 21\n",
      "... loading layer 22\n",
      "... loading layer 23\n",
      "... loading layer 24\n",
      "... loading layer 25\n",
      "... loading layer 26\n",
      "... loading layer 27\n",
      "... loading layer 28\n",
      "... loading layer 29\n",
      "... loading layer 30\n",
      "... loading layer 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/On-the-Fly_MoE_Inference/offloading/pipelinellm.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(f'../expert_predictor/training/{layer_idx}-{training_epoch}.pth'))\n"
     ]
    }
   ],
   "source": [
    "device_map = {layer_idx: 'cuda:1' if layer_idx <= 16 else 'cuda:2' for layer_idx in range(1, 32)}\n",
    "from pipelinellm import convert_mixtral_to_cached_mlp, PipelineLLM\n",
    "llm, cached_mlps = convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.8, backends=backend\n",
    "    , device='cuda:0', device_map=device_map)\n",
    "# 创建流水线模型\n",
    "PLLM = PipelineLLM(llm, cached_mlps, 1, 3, training_epoch=20,\n",
    " device='cuda:0', device_map=device_map, print_layer_info=True) ### use ep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 尝试替换torch.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w3_forward = torch.compile(llm.model.layers[0].block_sparse_moe.experts[1].w3.forward, fullgraph=True, mode=\"reduce-overhead\")\n",
    "# w3 = llm.model.layers[0].block_sparse_moe.experts[1].w3\n",
    "# w3.cuda(0)\n",
    "# w3.forward = w3_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试时间开销"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warm up ...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "\n",
    "input_length = 2\n",
    "MAX_LENGTH = 1\n",
    "output_length = 10\n",
    "test_samples = 1\n",
    "device_id = 0\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "filt_type = fineweb_path.split('.')[-1]\n",
    "fineweb = load_dataset(filt_type, data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text']\n",
    "\n",
    "print(\"warm up ...\")\n",
    "# 预热（避免第一次运行时的额外开销）\n",
    "for text in fineweb_text[:10]:\n",
    "    inputs = preprocess_data(text, tokenizer)\n",
    "    with torch.no_grad():\n",
    "        output = llm(input_ids=inputs[\"input_ids\"].cuda(device_id), attention_mask=inputs[\"attention_mask\"].cuda(device_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 256\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "Generated length: 256 Time taken: 20.38 s, prefill time: 0.51 s\n",
      "['How do you get HIV?\\nHIV-EB-2222301 46 D/A 47530/A5c 76973/A/44\\nA/M q0/22/A110581/16792/96221//2/3/132/2052/2/3/3/7/7/17/222/22222/2/3/2/3/7/1737/17373/17373/17373/17373/17373/17373/17373/17373/17373/17373/17373/17373/17373/17373/17373/17373/17373/17373/17373/17373/17373/17373/173']\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "Generated length: 256 Time taken: 20.21 s, prefill time: 0.47 s\n",
      "['CTComms sends on average 2 million  distinct of  people per  per month, meaning 1.1m .\\n *«I am a keen Cats-I am a Cats-I am a Cat. Iam a .\\n\\n\\n 2010 0000000-10000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000']\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "Generated length: 256 Time taken: 20.30 s, prefill time: 0.47 s\n",
      "['Hold the salt: UCLA engineers develop a new new-p---B--B--B--B--N-N-N-B--N-N-C-P-A-N-A-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N']\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "Generated length: 256 Time taken: 20.00 s, prefill time: 0.47 s\n",
      "['Not Just for Kids\\nThe Hunt for Fall/ S/ 3/1/2/5/ 6//B/B/0/B/B/0/B/B/0//0/0.J.,\\nB. , .\\n\\n P0N/S/A/A/A/A/A/A/J/3 (3,.\\n1-. (3./J/37/JH/J6/JT/T/S/S/C/A/A/A/B/P/A/A/J/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/']\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "Generated length: 256 Time taken: 20.45 s, prefill time: 0.47 s\n",
      "['The Solar and Heliospheric Observatoryies of the Hansin,s-E-t-a-Ft- and-Cor- ( S/ ) T- H/-S/A/ T/ 10.1.1.1.1.1.13.2.1.1.1116-1122.1.1.1.1113.1113.113.11311131131131131113113111111131111111111.1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111']\n",
      "decode phase speed: 12.8853 token/s\n",
      "the number of reloaded experts per token: 15.356\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 10\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 256\n",
    "test_samples = 5\n",
    "device_id = 0\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "generated_all = 0\n",
    "prefill_time, decode_time = 0, 0\n",
    "reloaded_experts = 0\n",
    "print(\"output length is {}\".format(output_length))\n",
    "for text in fineweb_text[2:2+test_samples] :\n",
    "    inputs = preprocess_data(text, tokenizer)\n",
    "    ### 清空统计数据\n",
    "    PLLM.get_prefill_time()\n",
    "    PLLM.get_reload_experts()\n",
    "\n",
    "    # 测试时间\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # 开始计时\n",
    "    torch.cuda.synchronize()\n",
    "    start_event.record()\n",
    "\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(device_id),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(device_id),\n",
    "            max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.pad_token_id, \n",
    "            # cache_implementation=\"static\" ## moe not support\n",
    "        )\n",
    "\n",
    "    # 结束计时\n",
    "    end_event.record()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # 计算时间\n",
    "    elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "    decode_time += elapsed_time\n",
    "    cur_prefill_time = PLLM.get_prefill_time()\n",
    "    prefill_time += cur_prefill_time\n",
    "    print(f\"Generated length: {len(output[0]) - input_length}\", f\"Time taken: {elapsed_time:.2f} s,\", f\"prefill time: {cur_prefill_time:.2f} s\")\n",
    "    # print(output)\n",
    "    print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "\n",
    "    generated_all += (len(output[0]) - input_length - 1)\n",
    "    reloaded_experts += PLLM.get_reload_experts()\n",
    "\n",
    "timepertoken = (decode_time - prefill_time) / (generated_all)\n",
    "print(\"decode phase speed:\", '{:.4f}'.format(1/timepertoken) , 'token/s')\n",
    "print(\"the number of reloaded experts per token:\", '{:.3f}'.format(reloaded_experts / generated_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################################################################\n",
    "#Save gemlite cache\n",
    "if(backend == 'gemlite'):\n",
    "\tgemlite.core.GemLiteLinear.cache_config('/tmp/gemlite_config.json') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.profile\n",
    "\n",
    "attention使用sdpa实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 2\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                  cudaDeviceSynchronize        29.44%     133.858ms        29.44%     133.858ms       4.056ms     303.655us         0.08%     303.655us       9.202us            33  \n",
      "                                  cudaStreamSynchronize        28.20%     128.248ms        28.20%     128.248ms     330.536us       0.000us         0.00%       0.000us       0.000us           388  \n",
      "gemlite::gemv_revsplitK_A16fWnO16f_int32packing_forw...         8.76%      39.850ms         9.93%      45.136ms     167.792us       6.429ms         1.77%       7.046ms      26.194us           269  \n",
      "                                       cudaLaunchKernel         5.90%      26.814ms         5.90%      26.814ms       4.902us       0.000us         0.00%       0.000us       0.000us          5470  \n",
      "                                            aten::copy_         4.06%      18.473ms         9.58%      43.580ms      21.415us     308.976ms        84.93%     308.976ms     151.831us          2035  \n",
      "                                        cudaMemcpyAsync         3.65%      16.581ms         3.65%      16.581ms       9.562us       0.000us         0.00%       0.000us       0.000us          1734  \n",
      "                                               aten::mm         3.43%      15.591ms         5.10%      23.174ms      31.615us      19.389ms         5.33%      19.389ms      26.452us           733  \n",
      "                                    aten::empty_strided         1.69%       7.671ms         1.69%       7.671ms       4.593us       0.000us         0.00%       0.000us       0.000us          1670  \n",
      "                                    cudaStreamWaitEvent         1.34%       6.087ms         1.34%       6.087ms       2.489us       0.000us         0.00%       0.000us       0.000us          2446  \n",
      "                                              aten::mul         1.12%       5.110ms         2.01%       9.139ms      10.109us       3.186ms         0.88%       3.186ms       3.524us           904  \n",
      "                                               aten::to         0.81%       3.681ms        10.77%      48.983ms      16.060us       0.000us         0.00%     298.317ms      97.809us          3050  \n",
      "                                            aten::index         0.71%       3.206ms         1.47%       6.670ms      19.333us      10.123ms         2.78%      10.408ms      30.168us           345  \n",
      "                                              aten::add         0.59%       2.700ms         1.10%       4.980ms      10.290us       1.310ms         0.36%       1.310ms       2.707us           484  \n",
      "                                         aten::_to_copy         0.52%       2.380ms         9.96%      45.303ms      30.061us       0.000us         0.00%     298.317ms     197.954us          1507  \n",
      "                                             aten::topk         0.50%       2.253ms         1.03%       4.680ms      27.210us       6.872ms         1.89%       6.872ms      39.951us           172  \n",
      "                                            aten::empty         0.46%       2.105ms         0.46%       2.105ms       2.759us       0.000us         0.00%       0.000us       0.000us           763  \n",
      "                                           aten::select         0.37%       1.664ms         0.44%       2.014ms       1.790us       0.000us         0.00%       0.000us       0.000us          1125  \n",
      "                                       aten::as_strided         0.36%       1.639ms         0.36%       1.639ms       0.367us       0.000us         0.00%       0.000us       0.000us          4464  \n",
      "                                    cudaLaunchKernelExC         0.33%       1.514ms         0.33%       1.514ms       5.824us       0.000us         0.00%       0.000us       0.000us           260  \n",
      "                                             aten::view         0.33%       1.512ms         0.33%       1.512ms       0.796us       0.000us         0.00%       0.000us       0.000us          1899  \n",
      "                                              aten::cat         0.33%       1.505ms         0.55%       2.496ms      12.737us     848.473us         0.23%     848.473us       4.329us           196  \n",
      "                                            aten::slice         0.33%       1.491ms         0.41%       1.859ms       1.638us       0.000us         0.00%       0.000us       0.000us          1135  \n",
      "                                        cudaEventRecord         0.30%       1.368ms         0.30%       1.368ms       0.545us     304.518us         0.08%     304.518us       0.121us          2510  \n",
      "                                             aten::mean         0.29%       1.327ms         0.43%       1.938ms      14.908us     700.977us         0.19%     700.977us       5.392us           130  \n",
      "                                           aten::matmul         0.29%       1.315ms         5.51%      25.054ms      34.133us       0.000us         0.00%      19.485ms      26.546us           734  \n",
      "                                             aten::silu         0.29%       1.299ms         0.50%       2.261ms      11.595us     611.110us         0.17%     611.110us       3.134us           195  \n",
      "                                         cuLaunchKernel         0.27%       1.242ms         0.27%       1.242ms       4.617us       0.000us         0.00%       0.000us       0.000us           269  \n",
      "                                          aten::reshape         0.27%       1.229ms         0.81%       3.664ms       4.419us       0.000us         0.00%     476.106us       0.574us           829  \n",
      "                                              aten::pow         0.26%       1.190ms         0.41%       1.858ms      14.289us     315.342us         0.09%     315.342us       2.426us           130  \n",
      "                                            aten::fill_         0.26%       1.170ms         0.72%       3.252ms       8.650us     863.619us         0.24%     863.619us       2.297us           376  \n",
      "                                           aten::linear         0.24%       1.077ms         5.72%      26.010ms      42.225us       0.000us         0.00%      17.465ms      28.352us           616  \n",
      "                                        cudaMemsetAsync         0.23%       1.066ms         0.23%       1.066ms       5.441us       0.000us         0.00%       0.000us       0.000us           196  \n",
      "                              aten::_local_scalar_dense         0.23%       1.051ms        28.18%     128.115ms     438.748us     679.823us         0.19%     679.823us       2.328us           292  \n",
      "                                  cudaFuncGetAttributes         0.22%       1.012ms         0.22%       1.012ms       2.834us       0.000us         0.00%       0.000us       0.000us           357  \n",
      "                               aten::bitwise_left_shift         0.21%     947.621us         0.33%       1.507ms      12.153us     294.217us         0.08%     294.217us       2.373us           124  \n",
      "                                        aten::transpose         0.21%     944.939us         0.32%       1.454ms       1.289us       0.000us         0.00%       0.000us       0.000us          1128  \n",
      "                                                aten::t         0.19%     884.156us         0.38%       1.706ms       2.769us       0.000us         0.00%       0.000us       0.000us           616  \n",
      "                         aten::_flash_attention_forward         0.19%     856.137us         0.44%       1.988ms      31.067us     554.893us         0.15%     554.893us       8.670us            64  \n",
      "                                              aten::neg         0.18%     812.226us         0.30%       1.386ms      10.831us     461.551us         0.13%     461.551us       3.606us           128  \n",
      "                                            aten::rsqrt         0.17%     778.620us         0.30%       1.375ms      10.578us     322.434us         0.09%     322.434us       2.480us           130  \n",
      "                                            aten::zero_         0.16%     706.054us         0.83%       3.795ms      10.398us       0.000us         0.00%     838.818us       2.298us           365  \n",
      "                                         aten::_softmax         0.14%     652.317us         0.25%       1.155ms      12.159us     261.156us         0.07%     261.156us       2.749us            95  \n",
      "                                              aten::sum         0.14%     637.431us         0.21%     975.657us      15.245us     288.677us         0.08%     288.677us       4.511us            64  \n",
      "                                             aten::add_         0.13%     601.254us         0.25%       1.154ms       9.012us     359.526us         0.10%     359.526us       2.809us           128  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.12%     558.718us         0.12%     558.718us       0.560us       0.000us         0.00%       0.000us       0.000us           997  \n",
      "                     aten::scaled_dot_product_attention         0.12%     552.747us         0.70%       3.197ms      49.954us       0.000us         0.00%     554.893us       8.670us            64  \n",
      "                                        aten::unsqueeze         0.12%     539.373us         0.15%     694.720us       1.781us       0.000us         0.00%       0.000us       0.000us           390  \n",
      "                                              aten::abs         0.11%     500.535us         0.46%       2.087ms      13.554us     192.991us         0.05%     385.982us       2.506us           154  \n",
      "                                             aten::div_         0.10%     454.720us         0.17%     775.116us      12.111us     202.306us         0.06%     202.306us       3.161us            64  \n",
      "                                       aten::empty_like         0.09%     422.347us         0.28%       1.285ms       4.385us       0.000us         0.00%       0.000us       0.000us           293  \n",
      "                                   cudaFuncSetAttribute         0.09%     402.586us         0.09%     402.586us       0.515us       0.000us         0.00%       0.000us       0.000us           782  \n",
      "              aten::_scaled_dot_product_flash_attention         0.09%     394.028us         0.58%       2.644ms      41.318us       0.000us         0.00%     554.893us       8.670us            64  \n",
      "                                             aten::item         0.08%     380.165us        28.26%     128.495ms     440.050us       0.000us         0.00%     679.823us       2.328us           292  \n",
      "                                           aten::unbind         0.08%     370.449us         0.14%     625.112us       4.884us       0.000us         0.00%       0.000us       0.000us           128  \n",
      "                                            aten::equal         0.07%     309.758us         1.28%       5.811ms     187.443us      82.370us         0.02%     223.340us       7.205us            31  \n",
      "                                     aten::_unsafe_view         0.06%     257.790us         0.06%     257.790us       0.668us       0.000us         0.00%       0.000us       0.000us           386  \n",
      "                                            aten::clone         0.05%     239.681us         0.46%       2.093ms      15.621us       0.000us         0.00%     490.762us       3.662us           134  \n",
      "                                          aten::squeeze         0.05%     233.151us         0.06%     269.524us       2.106us       0.000us         0.00%       0.000us       0.000us           128  \n",
      "                                           aten::expand         0.05%     222.355us         0.06%     266.951us       2.053us       0.000us         0.00%       0.000us       0.000us           130  \n",
      "                                          aten::resize_         0.04%     192.816us         0.04%     192.816us       2.441us       0.000us         0.00%       0.000us       0.000us            79  \n",
      "                                               aten::eq         0.04%     191.852us         0.08%     346.305us      14.429us      66.111us         0.02%      66.111us       2.755us            24  \n",
      "                                              aten::all         0.04%     183.381us         0.12%     559.516us      16.955us       9.184us         0.00%      79.750us       2.417us            33  \n",
      "                                               aten::ne         0.04%     179.147us         0.07%     339.502us      10.952us      78.945us         0.02%      78.945us       2.547us            31  \n",
      "                aten::_has_compatible_shallow_copy_type         0.04%     173.889us         0.04%     173.889us       0.039us       0.000us         0.00%       0.000us       0.000us          4464  \n",
      "                                              aten::any         0.04%     171.476us         0.08%     347.591us      17.380us      70.084us         0.02%      81.572us       4.079us            20  \n",
      "                                            aten::addmm         0.03%     151.646us         0.04%     199.831us      33.305us      33.473us         0.01%      33.473us       5.579us             6  \n",
      "                                       aten::zeros_like         0.03%     149.061us         0.30%       1.343ms      13.985us       0.000us         0.00%     222.024us       2.313us            96  \n",
      "                                 cudaDeviceGetAttribute         0.03%     143.791us         0.03%     143.791us       0.375us       0.000us         0.00%       0.000us       0.000us           383  \n",
      "                                          aten::softmax         0.03%     135.241us         0.28%       1.290ms      13.583us       0.000us         0.00%     261.156us       2.749us            95  \n",
      "                                   cudaEventElapsedTime         0.03%     122.963us         0.03%     122.963us       3.843us     309.575us         0.09%     309.575us       9.674us            32  \n",
      "                                          aten::permute         0.02%     109.348us         0.03%     128.270us       2.069us       0.000us         0.00%       0.000us       0.000us            62  \n",
      "                                           aten::cumsum         0.02%      75.219us         0.03%     139.634us      46.545us      16.096us         0.00%      16.096us       5.365us             3  \n",
      "                                             aten::isin         0.02%      69.657us         0.08%     374.865us     124.955us       0.000us         0.00%      27.967us       9.322us             3  \n",
      "                                  cudaStreamIsCapturing         0.01%      56.852us         0.01%      56.852us       0.888us       0.000us         0.00%       0.000us       0.000us            64  \n",
      "                                      aten::result_type         0.01%      55.422us         0.01%      55.422us       0.426us       0.000us         0.00%       0.000us       0.000us           130  \n",
      "                                          aten::numpy_T         0.01%      55.245us         0.04%     183.515us       2.960us       0.000us         0.00%       0.000us       0.000us            62  \n",
      "                                              aten::bmm         0.01%      48.700us         0.01%      67.804us      67.804us      95.202us         0.03%      95.202us      95.202us             1  \n",
      "                                     aten::index_select         0.01%      48.185us         0.02%      88.095us      44.047us       8.639us         0.00%       8.639us       4.320us             2  \n",
      "                                              aten::sub         0.01%      47.895us         0.02%      83.766us      16.753us      12.225us         0.00%      12.225us       2.445us             5  \n",
      "                                       aten::is_nonzero         0.01%      38.259us         0.16%     734.892us      18.843us       0.000us         0.00%      90.560us       2.322us            39  \n",
      "                                       aten::bitwise_or         0.01%      34.247us         0.01%      55.942us      13.986us       9.856us         0.00%       9.856us       2.464us             4  \n",
      "                                          aten::detach_         0.01%      33.040us         0.01%      43.854us       2.436us       0.000us         0.00%       0.000us       0.000us            18  \n",
      "                                     aten::masked_fill_         0.01%      27.897us         0.01%      43.975us      21.988us       4.864us         0.00%       4.864us       2.432us             2  \n",
      "                                               aten::lt         0.01%      26.456us         0.01%      36.164us      36.164us       2.272us         0.00%       2.272us       2.272us             1  \n",
      "                                           aten::argmax         0.01%      25.820us         0.01%      39.766us      19.883us      24.160us         0.01%      24.160us      12.080us             2  \n",
      "                                               aten::ge         0.01%      24.425us         0.01%      46.931us      23.466us       4.704us         0.00%       4.704us       2.352us             2  \n",
      "                                              aten::max         0.01%      23.405us         0.01%      46.996us      23.498us       8.544us         0.00%       8.544us       4.272us             2  \n",
      "                                          aten::view_as         0.00%      22.554us         0.01%      37.879us       1.052us       0.000us         0.00%       0.000us       0.000us            36  \n",
      "                                        aten::embedding         0.00%      22.124us         0.03%     114.695us      57.347us       0.000us         0.00%       8.639us       4.320us             2  \n",
      "                                      aten::bitwise_and         0.00%      21.499us         0.01%      38.282us      19.141us       5.632us         0.00%       5.632us       2.816us             2  \n",
      "                                      aten::bitwise_not         0.00%      19.516us         0.01%      57.493us      28.746us       4.960us         0.00%       4.960us       2.480us             2  \n",
      "                                         aten::new_ones         0.00%      13.110us         0.01%      44.497us      22.248us       0.000us         0.00%       4.993us       2.496us             2  \n",
      "                                                detach_         0.00%      10.814us         0.00%      10.814us       0.601us       0.000us         0.00%       0.000us       0.000us            18  \n",
      "                                             aten::rsub         0.00%       8.274us         0.01%      38.675us      19.337us       0.000us         0.00%       5.248us       2.624us             2  \n",
      "                                             aten::full         0.00%       8.054us         0.01%      45.579us      11.395us       0.000us         0.00%       9.056us       2.264us             4  \n",
      "                                           aten::__or__         0.00%       7.121us         0.01%      63.063us      15.766us       0.000us         0.00%       9.856us       2.464us             4  \n",
      "                                             aten::ones         0.00%       6.306us         0.01%      46.706us      46.706us       0.000us         0.00%       2.240us       2.240us             1  \n",
      "                                          aten::__and__         0.00%       5.848us         0.01%      44.130us      22.065us       0.000us         0.00%       5.632us       2.816us             2  \n",
      "                                     aten::resolve_conj         0.00%       4.994us         0.00%       4.994us       0.166us       0.000us         0.00%       0.000us       0.000us            30  \n",
      "                                        aten::ones_like         0.00%       4.704us         0.00%      19.227us      19.227us       0.000us         0.00%       2.049us       2.049us             1  \n",
      "                                   aten::_reshape_alias         0.00%       4.308us         0.00%       4.308us       4.308us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                       aten::lift_fresh         0.00%       4.110us         0.00%       4.110us       0.228us       0.000us         0.00%       0.000us       0.000us            18  \n",
      "                                      aten::resolve_neg         0.00%       3.335us         0.00%       3.335us       0.111us       0.000us         0.00%       0.000us       0.000us            30  \n",
      "                                        aten::new_empty         0.00%       2.591us         0.00%      10.545us       5.272us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                    cudaPeekAtLastError         0.00%       1.188us         0.00%       1.188us       0.099us       0.000us         0.00%       0.000us       0.000us            12  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      32.255us         0.01%      32.255us       1.613us            20  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      15.519us         0.00%      15.519us       2.217us             7  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     163.619us         0.04%     163.619us       2.597us            63  \n",
      "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     392.205us         0.11%     392.205us       2.321us           169  \n",
      "                         Memcpy DtoH (Device -> Pinned)         0.00%       0.000us         0.00%       0.000us       0.000us     679.823us         0.19%     679.823us       2.328us           292  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.768us         0.00%       4.768us       2.384us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       9.282us         0.00%       9.282us       2.320us             4  \n",
      "void at_cuda_detail::cub::DeviceScanInitKernel<at_cu...         0.00%       0.000us         0.00%       0.000us       0.000us       7.167us         0.00%       7.167us       2.389us             3  \n",
      "void at_cuda_detail::cub::DeviceScanKernel<at_cuda_d...         0.00%       0.000us         0.00%       0.000us       0.000us       8.929us         0.00%       8.929us       2.976us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       9.665us         0.00%       9.665us       2.416us             4  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       2.208us         0.00%       2.208us       2.208us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      13.631us         0.00%      13.631us       2.272us             6  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.864us         0.00%       4.864us       2.432us             2  \n",
      "void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us       8.639us         0.00%       8.639us       4.320us             2  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       9.184us         0.00%       9.184us       4.592us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     475.498us         0.13%     475.498us       3.602us           132  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     315.342us         0.09%     315.342us       2.426us           130  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     700.977us         0.19%     700.977us       5.392us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     321.162us         0.09%     321.162us       2.470us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     322.434us         0.09%     322.434us       2.480us           130  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     443.148us         0.12%     443.148us       3.409us           130  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     679.569us         0.19%     679.569us       3.503us           194  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       2.067ms         0.57%       2.067ms       4.029us           513  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us     336.835us         0.09%     336.835us       1.719us           196  \n",
      "sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x...         0.00%       0.000us         0.00%       0.000us       0.000us       7.642ms         2.10%       7.642ms      38.990us           196  \n",
      "void cutlass::Kernel<cutlass_80_wmma_tensorop_s16161...         0.00%       0.000us         0.00%       0.000us       0.000us     628.625us         0.17%     628.625us       6.548us            96  \n",
      "void splitKreduce_kernel<32, 16, int, float, __half,...         0.00%       0.000us         0.00%       0.000us       0.000us     834.922us         0.23%     834.922us       3.694us           226  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      10.119ms         2.78%      10.119ms      29.416us           344  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     461.551us         0.13%     461.551us       3.606us           128  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     834.393us         0.23%     834.393us       4.346us           192  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     254.339us         0.07%     254.339us       3.974us            64  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     476.106us         0.13%     476.106us       3.720us           128  \n",
      "void pytorch_flash::flash_fwd_kernel<pytorch_flash::...         0.00%       0.000us         0.00%       0.000us       0.000us     333.094us         0.09%     333.094us      10.409us            32  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     932.664us         0.26%     932.664us       2.650us           352  \n",
      "void (anonymous namespace)::softmax_warp_forward<c10...         0.00%       0.000us         0.00%       0.000us       0.000us     261.156us         0.07%     261.156us       2.749us            95  \n",
      "void at::native::sbtopk::gatherTopK<float, unsigned ...         0.00%       0.000us         0.00%       0.000us       0.000us     230.820us         0.06%     230.820us       7.213us            32  \n",
      "void at::native::bitonicSortKVInPlace<2, -1, 16, 16,...         0.00%       0.000us         0.00%       0.000us       0.000us     193.382us         0.05%     193.382us       6.043us            32  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     288.677us         0.08%     288.677us       4.511us            64  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     202.306us         0.06%     202.306us       3.161us            64  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     838.818us         0.23%     838.818us       2.298us           365  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     611.110us         0.17%     611.110us       3.134us           195  \n",
      "          gemv_revsplitK_A16fWnO16f_int32packing_kernel         0.00%       0.000us         0.00%       0.000us       0.000us       6.429ms         1.77%       6.429ms      23.901us           269  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     665.712us         0.18%     665.712us       2.590us           257  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_...         0.00%       0.000us         0.00%       0.000us       0.000us       5.780ms         1.59%       5.780ms      44.458us           130  \n",
      "                         Memcpy PtoP (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     306.338ms        84.21%     306.338ms     250.481us          1223  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us      14.080us         0.00%      14.080us       3.520us             4  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       2.688us         0.00%       2.688us       2.688us             1  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      24.160us         0.01%      24.160us      12.080us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      10.561us         0.00%      10.561us       2.640us             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.248us         0.00%       5.248us       2.624us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     156.258us         0.04%     156.258us       2.442us            64  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       9.856us         0.00%       9.856us       2.464us             4  \n",
      "void at::native::vectorized_elementwise_kernel<2, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.624us         0.00%       2.624us       2.624us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.960us         0.00%       4.960us       2.480us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       5.632us         0.00%       5.632us       2.816us             2  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       8.544us         0.00%       8.544us       4.272us             2  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.520us         0.00%       3.520us       3.520us             1  \n",
      "sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x...         0.00%       0.000us         0.00%       0.000us       0.000us       1.260ms         0.35%       1.260ms      19.685us            64  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us     568.911us         0.16%     568.911us       8.491us            67  \n",
      "void pytorch_flash::flash_fwd_kernel<pytorch_flash::...         0.00%       0.000us         0.00%       0.000us       0.000us     221.799us         0.06%     221.799us       6.931us            32  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us     949.402us         0.26%     949.402us      14.606us            65  \n",
      "void at::native::sbtopk::gatherTopK<float, unsigned ...         0.00%       0.000us         0.00%       0.000us       0.000us     417.446us         0.11%     417.446us       6.626us            63  \n",
      "void at::native::bitonicSortKVInPlace<-2, -1, 16, 16...         0.00%       0.000us         0.00%       0.000us       0.000us     375.594us         0.10%     375.594us       5.962us            63  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     192.991us         0.05%     192.991us       2.506us            77  \n",
      "void at::native::sbtopk::gatherTopK<c10::Half, unsig...         0.00%       0.000us         0.00%       0.000us       0.000us       4.241ms         1.17%       4.241ms      55.078us            77  \n",
      "void at::native::radixSortKVInPlace<-2, -1, 128, 32,...         0.00%       0.000us         0.00%       0.000us       0.000us       1.413ms         0.39%       1.413ms      18.355us            77  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     227.650us         0.06%     227.650us       2.956us            77  \n",
      "void dot_kernel<float, 128, 0, cublasDotParams<cubla...         0.00%       0.000us         0.00%       0.000us       0.000us     222.591us         0.06%     222.591us       3.710us            60  \n",
      "void reduce_1Block_kernel<float, 128, 7, cublasGemvT...         0.00%       0.000us         0.00%       0.000us       0.000us     178.307us         0.05%     178.307us       2.972us            60  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     153.254us         0.04%     153.254us       2.472us            62  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     140.963us         0.04%     140.963us       2.274us            62  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     285.319us         0.08%     285.319us       4.602us            62  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       1.117ms         0.31%       1.117ms      18.018us            62  \n",
      "                       Memcpy DtoH (Device -> Pageable)         0.00%       0.000us         0.00%       0.000us       0.000us      69.346us         0.02%      69.346us       2.312us            30  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      44.992us         0.01%      44.992us       2.999us            15  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      70.084us         0.02%      70.084us       4.672us            15  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       2.560us         0.00%       2.560us       2.560us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 454.701ms\n",
      "Self CUDA time total: 363.780ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 2\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 2\n",
    "test_samples = 4\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "fineweb = load_dataset(\"parquet\",data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text'][:test_samples] \n",
    "\n",
    "print(\"output length is {}\".format(output_length))\n",
    "text = fineweb_text[0]\n",
    "inputs = preprocess_data(text, tokenizer)\n",
    "\n",
    "# cached_mlp.clear_load_from_cpu_stats()\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ]\n",
    ") as p:\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(),\n",
    "            max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "print(p.key_averages().table(\n",
    "    sort_by=\"self_cpu_time_total\", row_limit=-1))\n",
    "p.export_chrome_trace(\"./offloading-2gpu-splik.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试一个正常输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "Generated length: 16 Time taken: 1.61 s prefill time: 0.47 s\n",
      "['The future of AI is here,  and it’s going to be a game-changer for the legal industry']\n",
      "decode phase speed: 13.2069  token/s\n",
      "the number of experts reload per token: 8.0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 10\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 16\n",
    "device_id = 0\n",
    "test_samples = 1\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "generated_all, decode_time, prefill_time = 0, 0, 0\n",
    "# print(\"max output length is {}\".format(output_length))\n",
    "text = \"The future of AI is here, \"\n",
    "\n",
    "clear_prefill_time = PLLM.get_prefill_time()\n",
    "clear_experts = PLLM.get_reload_experts()\n",
    "# print(\"need to zero: \", clear_experts, clear_prefill_time)\n",
    "\n",
    "inputs = preprocess_data(text, tokenizer)\n",
    "# 测试时间\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "# 开始计时\n",
    "torch.cuda.synchronize()\n",
    "start_event.record()\n",
    "\n",
    "# 前向传播\n",
    "with torch.no_grad():\n",
    "    output = llm.generate(\n",
    "        input_ids=inputs[\"input_ids\"].cuda(device_id),\n",
    "        attention_mask=inputs[\"attention_mask\"].cuda(device_id),\n",
    "        max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "        generation_config=GenerationConfig(do_sample=False),\n",
    "        pad_token_id=tokenizer.pad_token_id, \n",
    "        # cache_implementation=\"static\" ## moe not support\n",
    "    )\n",
    "\n",
    "# 结束计时\n",
    "end_event.record()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# 计算时间\n",
    "elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "decode_time += elapsed_time\n",
    "cur_prefill_time = PLLM.get_prefill_time()\n",
    "prefill_time += cur_prefill_time\n",
    "print(f\"Generated length: {len(output[0]) - input_length}\", f\"Time taken: {elapsed_time:.2f} s\", f\"prefill time: {cur_prefill_time:.2f} s\")\n",
    "# print(output)\n",
    "print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "\n",
    "generated_all += (len(output[0]) - input_length -1)\n",
    "\n",
    "timepertoken = (decode_time - prefill_time) / (generated_all)\n",
    "print(\"decode phase speed:\", '{:.4f}'.format(1/timepertoken) , ' token/s')\n",
    "print(\"the number of experts reload per token:\", PLLM.get_reload_experts() / generated_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载到GPU上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:25<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MixtralForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "import json\n",
    "\n",
    "def get_model(model_name, device_map, dtype=torch.bfloat16):\n",
    "    llm = MixtralForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        use_cache=True,\n",
    "        torch_dtype=dtype,\n",
    "    ) \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return llm, tokenizer\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "with open('../quantize/device_map_1.json', 'r') as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "dtype = torch.float16\n",
    "llm, tokenizer = get_model(model_name, device_map, dtype=dtype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
