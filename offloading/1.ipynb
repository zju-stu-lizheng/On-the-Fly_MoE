{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先都加载到cpu上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lz/miniconda3/envs/hqq/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using atten... sdpa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/lz/hqq-master/hqq/models/base.py:251: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(cls.get_weight_file(save_dir), map_location=map_location)\n",
      "100%|██████████| 32/32 [00:00<00:00, 279.77it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 623.62it/s]\n",
      "100%|██████████| 32/32 [06:09<00:00, 11.55s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5,0,1,2\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"\n",
    "from modeling_mixtral import MixtralForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import json\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "dtype = torch.float16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "### HQQ量化\n",
    "from hqq.core.quantize import *\n",
    "from hqq.models.hf.mixtral import MixtralPatch\n",
    "import transformers\n",
    "from hqq.models.base import BaseHQQModel\n",
    "from accelerate import init_empty_weights\n",
    "\n",
    "import gemlite\n",
    "from gemlite import GemLiteLinearTriton \n",
    "GemLiteLinearTriton.get_default_gemv = lambda *args, **kwargs: 'GEMV'\n",
    "\n",
    "save_dir = './hqqsaved'\n",
    "\n",
    "class BaseHQQHFModel(BaseHQQModel):\n",
    "    # Save model architecture\n",
    "    @classmethod\n",
    "    def cache_model(cls, model, save_dir):\n",
    "        model.config.save_pretrained(save_dir)\n",
    "\n",
    "    # Create empty model from config\n",
    "    @classmethod\n",
    "    def create_model(cls, save_dir, kwargs):\n",
    "        model_kwargs = {}\n",
    "        for key in [\"attn_implementation\"]:\n",
    "            if key in kwargs:\n",
    "                model_kwargs[key] = kwargs[key]\n",
    "\n",
    "        config = transformers.AutoConfig.from_pretrained(\n",
    "            cls.get_config_file(save_dir)\n",
    "        )\n",
    "\n",
    "        with init_empty_weights():\n",
    "            model = MixtralForCausalLM._from_config(config, **model_kwargs)\n",
    "\n",
    "        return model\n",
    "\n",
    "class MixtralHQQ(MixtralPatch, BaseHQQHFModel):\n",
    "    pass\n",
    "\n",
    "### 从保存的权重中加载\n",
    "llm = MixtralHQQ.from_quantized(save_dir, compute_dtype=dtype, device='cpu')\n",
    "HQQLinear.set_backend(HQQBackend.PYTORCH)\n",
    "\n",
    "backend       = \"gemlite\" #'torchao_int4' #\"torchao_int4\" (4-bit only) or \"gemlite\" (4-bit + 2-bit)\n",
    "# #Optimize\n",
    "from hqq.utils.patching import prepare_for_inference\n",
    "prepare_for_inference(llm, backend=backend, verbose=True)\n",
    "\n",
    "if(backend == 'gemlite'):\n",
    "\tgemlite.core.GEMLITE_TRITON_RESTRICT_M = True\n",
    "\t# gemlite.core.GemLiteLinear.load_config('/tmp/gemlite_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "active neural num  2867\n",
      "active neural num  2867\n",
      "... loading layer 0\n",
      "... loading layer 1\n",
      "... loading layer 2\n",
      "... loading layer 3\n",
      "... loading layer 4\n",
      "... loading layer 5\n",
      "... loading layer 6\n",
      "... loading layer 7\n",
      "... loading layer 8\n",
      "... loading layer 9\n",
      "... loading layer 10\n",
      "... loading layer 11\n",
      "... loading layer 12\n",
      "... loading layer 13\n",
      "... loading layer 14\n",
      "... loading layer 15\n",
      "... loading layer 16\n",
      "... loading layer 17\n",
      "... loading layer 18\n",
      "... loading layer 19\n",
      "... loading layer 20\n",
      "... loading layer 21\n",
      "... loading layer 22\n",
      "... loading layer 23\n",
      "... loading layer 24\n",
      "... loading layer 25\n",
      "... loading layer 26\n",
      "... loading layer 27\n",
      "... loading layer 28\n",
      "... loading layer 29\n",
      "... loading layer 30\n",
      "... loading layer 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/lz/On-the-Fly_MoE_Inference/offloading/pipelinellm.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(f'../expert_predictor/training/{layer_idx}-{training_epoch}.pth'))\n"
     ]
    }
   ],
   "source": [
    "# device_map = {layer_idx: 'cuda:1' if layer_idx <= 16 else 'cuda:2' for layer_idx in range(1, 32)}\n",
    "device_map = {\n",
    "    layer_idx: 'cuda:1' if layer_idx < 10 else ('cuda:2' if layer_idx <= 20 else 'cuda:3')\n",
    "    for layer_idx in range(1, 32)\n",
    "}\n",
    "from pipelinellm import convert_mixtral_to_cached_mlp, PipelineLLM\n",
    "llm, cached_mlps = convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.8, backends=backend\n",
    "    , device='cuda:0', device_map=device_map)\n",
    "# 创建流水线模型\n",
    "PLLM = PipelineLLM(llm, cached_mlps, 1, 3, training_epoch=20,\n",
    " device='cuda:0', device_map=device_map, print_layer_info=True) ### use ep\n",
    "device_id = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 尝试替换torch.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w3_forward = torch.compile(llm.model.layers[0].block_sparse_moe.experts[1].w3.forward, fullgraph=True, mode=\"reduce-overhead\")\n",
    "# w3 = llm.model.layers[0].block_sparse_moe.experts[1].w3\n",
    "# w3.cuda(0)\n",
    "# w3.forward = w3_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试时间开销"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warm up ...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "\n",
    "input_length = 2\n",
    "MAX_LENGTH = 1\n",
    "output_length = 10\n",
    "test_samples = 1\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "filt_type = fineweb_path.split('.')[-1]\n",
    "fineweb = load_dataset(filt_type, data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text']\n",
    "\n",
    "print(\"warm up ...\")\n",
    "# 预热（避免第一次运行时的额外开销）\n",
    "for text in fineweb_text[:10]:\n",
    "    inputs = preprocess_data(text, tokenizer)\n",
    "    with torch.no_grad():\n",
    "        output = llm(input_ids=inputs[\"input_ids\"].cuda(device_id), attention_mask=inputs[\"attention_mask\"].cuda(device_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 256\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "Generated length: 256 Time taken: 20.38 s, prefill time: 0.51 s\n",
      "['How do you get HIV?\\nHIV-EB-2222301 46 D/A 47530/A5c 76973/A/44\\nA/M q0/22/A110581/16792/96221//2/3/132/2052/2/3/3/7/7/17/222/22222/2/3/2/3/7/1737/17373/17373/17373/17373/17373/17373/17373/17373/17373/17373/17373/17373/17373/17373/17373/17373/17373/17373/17373/17373/17373/17373/173']\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "Generated length: 256 Time taken: 20.21 s, prefill time: 0.47 s\n",
      "['CTComms sends on average 2 million  distinct of  people per  per month, meaning 1.1m .\\n *«I am a keen Cats-I am a Cats-I am a Cat. Iam a .\\n\\n\\n 2010 0000000-10000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000']\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "Generated length: 256 Time taken: 20.30 s, prefill time: 0.47 s\n",
      "['Hold the salt: UCLA engineers develop a new new-p---B--B--B--B--N-N-N-B--N-N-C-P-A-N-A-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N']\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "Generated length: 256 Time taken: 20.00 s, prefill time: 0.47 s\n",
      "['Not Just for Kids\\nThe Hunt for Fall/ S/ 3/1/2/5/ 6//B/B/0/B/B/0/B/B/0//0/0.J.,\\nB. , .\\n\\n P0N/S/A/A/A/A/A/A/J/3 (3,.\\n1-. (3./J/37/JH/J6/JT/T/S/S/C/A/A/A/B/P/A/A/J/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/A/']\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "Generated length: 256 Time taken: 20.45 s, prefill time: 0.47 s\n",
      "['The Solar and Heliospheric Observatoryies of the Hansin,s-E-t-a-Ft- and-Cor- ( S/ ) T- H/-S/A/ T/ 10.1.1.1.1.1.13.2.1.1.1116-1122.1.1.1.1113.1113.113.11311131131131131113113111111131111111111.1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111']\n",
      "decode phase speed: 12.8853 token/s\n",
      "the number of reloaded experts per token: 15.356\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 10\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 256\n",
    "test_samples = 5\n",
    "device_id = 0\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "generated_all = 0\n",
    "prefill_time, decode_time = 0, 0\n",
    "reloaded_experts = 0\n",
    "print(\"output length is {}\".format(output_length))\n",
    "for text in fineweb_text[2:2+test_samples] :\n",
    "    inputs = preprocess_data(text, tokenizer)\n",
    "    ### 清空统计数据\n",
    "    PLLM.get_prefill_time()\n",
    "    PLLM.get_reload_experts()\n",
    "\n",
    "    # 测试时间\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # 开始计时\n",
    "    torch.cuda.synchronize()\n",
    "    start_event.record()\n",
    "\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(device_id),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(device_id),\n",
    "            max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.pad_token_id, \n",
    "            # cache_implementation=\"static\" ## moe not support\n",
    "        )\n",
    "\n",
    "    # 结束计时\n",
    "    end_event.record()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # 计算时间\n",
    "    elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "    decode_time += elapsed_time\n",
    "    cur_prefill_time = PLLM.get_prefill_time()\n",
    "    prefill_time += cur_prefill_time\n",
    "    print(f\"Generated length: {len(output[0]) - input_length}\", f\"Time taken: {elapsed_time:.2f} s,\", f\"prefill time: {cur_prefill_time:.2f} s\")\n",
    "    # print(output)\n",
    "    print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "\n",
    "    generated_all += (len(output[0]) - input_length - 1)\n",
    "    reloaded_experts += PLLM.get_reload_experts()\n",
    "\n",
    "timepertoken = (decode_time - prefill_time) / (generated_all)\n",
    "print(\"decode phase speed:\", '{:.4f}'.format(1/timepertoken) , 'token/s')\n",
    "print(\"the number of reloaded experts per token:\", '{:.3f}'.format(reloaded_experts / generated_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################################################################\n",
    "#Save gemlite cache\n",
    "if(backend == 'gemlite'):\n",
    "\tgemlite.core.GemLiteLinear.cache_config('/tmp/gemlite_config.json') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.profile\n",
    "\n",
    "attention使用sdpa实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 2\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        cudaMemcpyAsync        69.09%       14.734s        69.09%       14.734s       8.698ms       0.000us         0.00%       0.000us       0.000us          1694  \n",
      "                                  cudaStreamSynchronize        15.90%        3.391s        15.90%        3.391s       9.317ms       0.000us         0.00%       0.000us       0.000us           364  \n",
      "                                  cudaDeviceSynchronize        13.31%        2.839s        13.31%        2.839s      86.029ms       1.664us         0.00%       1.664us       0.050us            33  \n",
      "gemlite::gemv_revsplitK_A16fWnO16f_int32packing_forw...         0.32%      67.769ms         0.38%      80.051ms     302.078us       9.941ms         0.02%      10.240ms      38.641us           265  \n",
      "                                            aten::copy_         0.22%      47.676ms        69.40%       14.799s       7.403ms       42.004s        99.79%       42.004s      21.012ms          1999  \n",
      "                                       cudaLaunchKernel         0.21%      44.162ms         0.21%      44.162ms       8.119us       0.000us         0.00%       0.000us       0.000us          5439  \n",
      "                                               aten::mm         0.14%      29.878ms         0.19%      40.687ms      55.507us      51.179ms         0.12%      51.179ms      69.822us           733  \n",
      "                                    aten::empty_strided         0.07%      14.711ms         0.07%      14.711ms       8.894us       0.000us         0.00%       0.000us       0.000us          1654  \n",
      "                                    cudaStreamWaitEvent         0.06%      13.405ms         0.06%      13.405ms       5.535us       0.000us         0.00%       0.000us       0.000us          2422  \n",
      "                                              aten::mul         0.05%      11.699ms         0.08%      17.777ms      19.664us       1.894ms         0.00%       1.894ms       2.095us           904  \n",
      "                                         aten::_to_copy         0.04%       9.527ms        69.36%       14.791s       9.920ms       0.000us         0.00%       40.825s      27.381ms          1491  \n",
      "                                              aten::add         0.03%       6.668ms         0.05%      10.800ms      22.313us     683.972us         0.00%     683.972us       1.413us           484  \n",
      "                                            aten::index         0.03%       6.508ms         0.06%      12.927ms      38.359us      13.718ms         0.03%      13.910ms      41.276us           337  \n",
      "                                          aten::reshape         0.03%       6.496ms         0.05%      11.705ms      14.257us       0.000us         0.00%     264.138us       0.322us           821  \n",
      "                                               aten::to         0.03%       5.336ms        69.38%       14.796s       4.877ms       0.000us         0.00%       40.825s      13.456ms          3034  \n",
      "                                            aten::slice         0.02%       4.921ms         0.03%       5.733ms       5.087us       0.000us         0.00%       0.000us       0.000us          1127  \n",
      "                                            aten::empty         0.02%       4.915ms         0.02%       4.915ms       6.544us       0.000us         0.00%       0.000us       0.000us           751  \n",
      "                                             aten::topk         0.02%       4.581ms         0.04%       7.891ms      46.971us       5.844ms         0.01%       5.844ms      34.784us           168  \n",
      "                                             aten::view         0.02%       4.564ms         0.02%       4.564ms       2.429us       0.000us         0.00%       0.000us       0.000us          1879  \n",
      "                                           aten::linear         0.02%       3.986ms         0.23%      49.380ms      80.162us       0.000us         0.00%      47.529ms      77.158us           616  \n",
      "                                       aten::as_strided         0.02%       3.842ms         0.02%       3.842ms       0.869us       0.000us         0.00%       0.000us       0.000us          4424  \n",
      "                                           aten::select         0.02%       3.642ms         0.02%       4.368ms       3.953us       0.000us         0.00%       0.000us       0.000us          1105  \n",
      "                                            aten::rsqrt         0.02%       3.361ms         0.02%       4.202ms      32.325us     164.897us         0.00%     164.897us       1.268us           130  \n",
      "                                           aten::matmul         0.02%       3.298ms         0.21%      45.499ms      61.988us       0.000us         0.00%      51.524ms      70.197us           734  \n",
      "                                             aten::silu         0.02%       3.229ms         0.02%       4.847ms      24.858us     379.005us         0.00%     379.005us       1.944us           195  \n",
      "                                              aten::cat         0.02%       3.212ms         0.02%       4.517ms      23.043us     623.772us         0.00%     623.772us       3.183us           196  \n",
      "                                            aten::fill_         0.01%       3.191ms         0.04%       7.636ms      20.528us     403.615us         0.00%     403.615us       1.085us           372  \n",
      "                                        cudaEventRecord         0.01%       3.105ms         0.01%       3.105ms       1.249us     155.326us         0.00%     155.326us       0.062us          2486  \n",
      "                                         cuLaunchKernel         0.01%       2.981ms         0.01%       2.981ms       9.971us       0.000us         0.00%       0.000us       0.000us           299  \n",
      "                                              aten::pow         0.01%       2.900ms         0.02%       3.956ms      30.427us     150.824us         0.00%     150.824us       1.160us           130  \n",
      "                              aten::_local_scalar_dense         0.01%       2.844ms        15.52%        3.310s      11.655ms     405.701us         0.00%     405.701us       1.429us           284  \n",
      "                                        cudaMemsetAsync         0.01%       2.781ms         0.01%       2.781ms       8.582us       0.000us         0.00%       0.000us       0.000us           324  \n",
      "                                             aten::mean         0.01%       2.686ms         0.02%       3.661ms      28.160us     459.354us         0.00%     459.354us       3.533us           130  \n",
      "                                                aten::t         0.01%       2.453ms         0.02%       4.911ms       7.972us       0.000us         0.00%       0.000us       0.000us           616  \n",
      "                                        aten::transpose         0.01%       2.273ms         0.02%       3.640ms       3.227us       0.000us         0.00%       0.000us       0.000us          1128  \n",
      "                               aten::bitwise_left_shift         0.01%       1.823ms         0.01%       2.596ms      20.933us     148.543us         0.00%     148.543us       1.198us           124  \n",
      "                                              aten::neg         0.01%       1.719ms         0.01%       2.596ms      20.282us     277.851us         0.00%     277.851us       2.171us           128  \n",
      "                                              aten::sum         0.01%       1.641ms         0.01%       2.179ms      34.041us     174.499us         0.00%     174.499us       2.727us            64  \n",
      "                                            aten::zero_         0.01%       1.603ms         0.04%       9.039ms      25.039us       0.000us         0.00%     391.710us       1.085us           361  \n",
      "                                         aten::_softmax         0.01%       1.517ms         0.01%       2.404ms      25.306us     131.488us         0.00%     131.488us       1.384us            95  \n",
      "                         aten::_flash_attention_forward         0.01%       1.471ms         0.02%       3.855ms      60.228us     361.016us         0.00%     361.016us       5.641us            64  \n",
      "                                        aten::unsqueeze         0.01%       1.460ms         0.01%       1.845ms       4.731us       0.000us         0.00%       0.000us       0.000us           390  \n",
      "                     aten::scaled_dot_product_attention         0.01%       1.399ms         0.03%       6.406ms     100.093us       0.000us         0.00%     361.016us       5.641us            64  \n",
      "                                             aten::add_         0.01%       1.388ms         0.01%       2.361ms      18.447us     196.889us         0.00%     196.889us       1.538us           128  \n",
      "                                              aten::abs         0.01%       1.151ms         0.02%       4.173ms      28.582us      81.730us         0.00%     163.460us       1.120us           146  \n",
      "                                             aten::div_         0.01%       1.098ms         0.01%       1.607ms      25.109us      99.423us         0.00%      99.423us       1.553us            64  \n",
      "                                           aten::unbind         0.00%     779.963us         0.01%       1.250ms       9.766us       0.000us         0.00%       0.000us       0.000us           128  \n",
      "                aten::_has_compatible_shallow_copy_type         0.00%     772.761us         0.00%     772.761us       0.173us       0.000us         0.00%       0.000us       0.000us          4464  \n",
      "                                             aten::item         0.00%     761.922us        15.52%        3.311s      11.658ms       0.000us         0.00%     405.701us       1.429us           284  \n",
      "              aten::_scaled_dot_product_flash_attention         0.00%     735.883us         0.02%       5.007ms      78.237us       0.000us         0.00%     361.016us       5.641us            64  \n",
      "                                       aten::empty_like         0.00%     719.961us         0.01%       2.489ms       8.493us       0.000us         0.00%       0.000us       0.000us           293  \n",
      "                                            aten::equal         0.00%     688.515us         2.18%     465.451ms      15.015ms      42.051us         0.00%     124.325us       4.010us            31  \n",
      "                                            aten::clone         0.00%     657.266us         0.02%       4.109ms      30.662us       0.000us         0.00%     271.402us       2.025us           134  \n",
      "                                          aten::squeeze         0.00%     632.853us         0.00%     735.095us       5.743us       0.000us         0.00%       0.000us       0.000us           128  \n",
      "                                     aten::_unsafe_view         0.00%     594.881us         0.00%     594.881us       1.541us       0.000us         0.00%       0.000us       0.000us           386  \n",
      "                                               aten::ne         0.00%     532.352us         0.00%     903.747us      29.153us      36.192us         0.00%      36.192us       1.167us            31  \n",
      "                                           aten::expand         0.00%     497.330us         0.00%     576.949us       4.438us       0.000us         0.00%       0.000us       0.000us           130  \n",
      "                                              aten::any         0.00%     496.142us         0.00%     722.664us      45.166us      29.597us         0.00%      35.837us       2.240us            16  \n",
      "                                  cudaFuncGetAttributes         0.00%     482.212us         0.00%     482.212us       5.076us       0.000us         0.00%       0.000us       0.000us            95  \n",
      "                                          aten::resize_         0.00%     390.229us         0.00%     390.229us       5.203us       0.000us         0.00%       0.000us       0.000us            75  \n",
      "                                       aten::zeros_like         0.00%     380.402us         0.01%       2.861ms      29.798us       0.000us         0.00%      93.023us       0.969us            96  \n",
      "                                   cudaFuncSetAttribute         0.00%     377.027us         0.00%     377.027us       5.891us       0.000us         0.00%       0.000us       0.000us            64  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.00%     371.786us         0.00%     371.786us       0.487us       0.000us         0.00%       0.000us       0.000us           763  \n",
      "          cudaOccupancyMaxActiveBlocksPerMultiprocessor         0.00%     367.126us         0.00%     367.126us       2.824us       0.000us         0.00%       0.000us       0.000us           130  \n",
      "                                              aten::all         0.00%     339.781us         0.00%     987.251us      29.917us       6.047us         0.00%      40.959us       1.241us            33  \n",
      "                                               aten::eq         0.00%     337.413us         0.00%     553.025us      27.651us      30.721us         0.00%      30.721us       1.536us            20  \n",
      "                                          aten::softmax         0.00%     318.305us         0.01%       2.722ms      28.657us       0.000us         0.00%     131.488us       1.384us            95  \n",
      "                                          aten::permute         0.00%     267.147us         0.00%     311.283us       5.021us       0.000us         0.00%       0.000us       0.000us            62  \n",
      "                                 cudaDeviceGetAttribute         0.00%     265.073us         0.00%     265.073us       0.636us       0.000us         0.00%       0.000us       0.000us           417  \n",
      "                                            aten::addmm         0.00%     217.217us         0.00%     284.851us      47.475us      32.095us         0.00%      32.095us       5.349us             6  \n",
      "                                              aten::bmm         0.00%     162.926us         0.00%     202.670us     202.670us     345.086us         0.00%     345.086us     345.086us             1  \n",
      "                                   cudaEventElapsedTime         0.00%     142.934us         0.00%     142.934us       4.467us      81.674ms         0.19%      81.674ms       2.552ms            32  \n",
      "                                          aten::numpy_T         0.00%     123.913us         0.00%     435.196us       7.019us       0.000us         0.00%       0.000us       0.000us            62  \n",
      "                                  cudaStreamIsCapturing         0.00%     123.124us         0.00%     123.124us       1.924us       0.000us         0.00%       0.000us       0.000us            64  \n",
      "                                      aten::result_type         0.00%     121.633us         0.00%     121.633us       0.936us       0.000us         0.00%       0.000us       0.000us           130  \n",
      "                                             aten::full         0.00%     107.903us         0.00%     179.692us      44.923us       0.000us         0.00%       3.937us       0.984us             4  \n",
      "                                             aten::isin         0.00%      98.893us         0.00%     413.612us     137.871us       0.000us         0.00%      14.976us       4.992us             3  \n",
      "                                           aten::cumsum         0.00%      97.163us         0.00%     162.605us      54.202us       9.217us         0.00%       9.217us       3.072us             3  \n",
      "                                       aten::is_nonzero         0.00%      81.850us         0.00%       1.058ms      27.131us       0.000us         0.00%      53.344us       1.368us            39  \n",
      "                                              aten::sub         0.00%      74.224us         0.00%     117.780us      23.556us       6.368us         0.00%       6.368us       1.274us             5  \n",
      "                                          aten::view_as         0.00%      62.841us         0.00%     108.393us       3.011us       0.000us         0.00%       0.000us       0.000us            36  \n",
      "                                       aten::bitwise_or         0.00%      57.419us         0.00%     138.691us      34.673us       5.024us         0.00%       5.024us       1.256us             4  \n",
      "                                          aten::detach_         0.00%      53.756us         0.00%      76.433us       5.459us       0.000us         0.00%       0.000us       0.000us            14  \n",
      "                                     aten::index_select         0.00%      49.122us         0.00%      90.970us      45.485us       5.600us         0.00%       5.600us       2.800us             2  \n",
      "                                           aten::argmax         0.00%      42.451us         0.00%      69.209us      34.604us      20.448us         0.00%      20.448us      10.224us             2  \n",
      "                                              aten::max         0.00%      37.588us         0.00%      73.761us      36.880us       5.281us         0.00%       5.281us       2.640us             2  \n",
      "                                               aten::ge         0.00%      36.345us         0.00%      63.179us      31.589us       3.071us         0.00%       3.071us       1.535us             2  \n",
      "                                     aten::masked_fill_         0.00%      34.620us         0.00%      73.131us      36.565us       2.656us         0.00%       2.656us       1.328us             2  \n",
      "                                        aten::embedding         0.00%      28.254us         0.00%     126.370us      63.185us       0.000us         0.00%       5.600us       2.800us             2  \n",
      "                                      aten::bitwise_not         0.00%      25.513us         0.00%      40.853us      20.427us       2.368us         0.00%       2.368us       1.184us             2  \n",
      "                                      aten::bitwise_and         0.00%      23.546us         0.00%      44.067us      22.033us       3.904us         0.00%       3.904us       1.952us             2  \n",
      "                                               aten::lt         0.00%      23.024us         0.00%      33.027us      33.027us       1.664us         0.00%       1.664us       1.664us             1  \n",
      "                                                detach_         0.00%      22.677us         0.00%      22.677us       1.620us       0.000us         0.00%       0.000us       0.000us            14  \n",
      "                                             aten::rsub         0.00%      21.458us         0.00%      68.952us      34.476us       0.000us         0.00%       2.432us       1.216us             2  \n",
      "                                         aten::new_ones         0.00%      16.714us         0.00%      88.706us      44.353us       0.000us         0.00%       2.496us       1.248us             2  \n",
      "                                     aten::resolve_conj         0.00%      11.705us         0.00%      11.705us       0.532us       0.000us         0.00%       0.000us       0.000us            22  \n",
      "                                           aten::__or__         0.00%       9.276us         0.00%     147.967us      36.992us       0.000us         0.00%       5.024us       1.256us             4  \n",
      "                                        aten::new_empty         0.00%       8.091us         0.00%      23.201us      11.600us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                             aten::ones         0.00%       7.232us         0.00%      45.468us      45.468us       0.000us         0.00%       1.152us       1.152us             1  \n",
      "                                          aten::__and__         0.00%       6.404us         0.00%      50.471us      25.236us       0.000us         0.00%       3.904us       1.952us             2  \n",
      "                                       aten::lift_fresh         0.00%       6.346us         0.00%       6.346us       0.453us       0.000us         0.00%       0.000us       0.000us            14  \n",
      "                                      aten::resolve_neg         0.00%       5.677us         0.00%       5.677us       0.258us       0.000us         0.00%       0.000us       0.000us            22  \n",
      "                                   aten::_reshape_alias         0.00%       4.886us         0.00%       4.886us       4.886us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                        aten::ones_like         0.00%       3.697us         0.00%      26.374us      26.374us       0.000us         0.00%       1.089us       1.089us             1  \n",
      "                                    cudaPeekAtLastError         0.00%       1.329us         0.00%       1.329us       0.111us       0.000us         0.00%       0.000us       0.000us            12  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       6.974us         0.00%       6.974us       0.436us            16  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       7.168us         0.00%       7.168us       1.024us             7  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      79.907us         0.00%      79.907us       1.268us            63  \n",
      "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     183.036us         0.00%     183.036us       1.137us           161  \n",
      "                         Memcpy DtoH (Device -> Pinned)         0.00%       0.000us         0.00%       0.000us       0.000us     405.701us         0.00%     405.701us       1.429us           284  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.136us         0.00%       3.136us       1.568us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.737us         0.00%       4.737us       1.184us             4  \n",
      "void at_cuda_detail::cub::DeviceScanInitKernel<at_cu...         0.00%       0.000us         0.00%       0.000us       0.000us       3.265us         0.00%       3.265us       1.088us             3  \n",
      "void at_cuda_detail::cub::DeviceScanKernel<at_cuda_d...         0.00%       0.000us         0.00%       0.000us       0.000us       5.952us         0.00%       5.952us       1.984us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.248us         0.00%       5.248us       1.312us             4  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       1.599us         0.00%       1.599us       1.599us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       7.553us         0.00%       7.553us       1.259us             6  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.656us         0.00%       2.656us       1.328us             2  \n",
      "void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us       5.600us         0.00%       5.600us       2.800us             2  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       6.047us         0.00%       6.047us       3.023us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     362.174us         0.00%     362.174us       2.744us           132  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     150.824us         0.00%     150.824us       1.160us           130  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     459.354us         0.00%     459.354us       3.533us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     162.071us         0.00%     162.071us       1.247us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     164.897us         0.00%     164.897us       1.268us           130  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     240.737us         0.00%     240.737us       1.852us           130  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     472.514us         0.00%     472.514us       2.436us           194  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       1.313ms         0.00%       1.313ms       2.559us           513  \n",
      "ampere_fp16_s16816gemm_fp16_64x64_sliced1x2_ldg8_f2f...         0.00%       0.000us         0.00%       0.000us       0.000us       2.913ms         0.01%       2.913ms      45.523us            64  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us     140.802us         0.00%     140.802us       0.435us           324  \n",
      "ampere_fp16_s16816gemm_fp16_64x64_ldg8_f2f_stages_64...         0.00%       0.000us         0.00%       0.000us       0.000us       1.192ms         0.00%       1.192ms      18.621us            64  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      13.716ms         0.03%      13.716ms      40.821us           336  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     277.851us         0.00%     277.851us       2.171us           128  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     615.548us         0.00%     615.548us       3.206us           192  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     147.321us         0.00%     147.321us       2.302us            64  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     264.138us         0.00%     264.138us       2.064us           128  \n",
      "void pytorch_flash::flash_fwd_kernel<pytorch_flash::...         0.00%       0.000us         0.00%       0.000us       0.000us     174.621us         0.00%     174.621us       5.457us            32  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     488.816us         0.00%     488.816us       1.389us           352  \n",
      "void cutlass::Kernel2<cutlass_80_wmma_tensorop_f16_s...         0.00%       0.000us         0.00%       0.000us       0.000us     604.541us         0.00%     604.541us      18.892us            32  \n",
      "void (anonymous namespace)::softmax_warp_forward<c10...         0.00%       0.000us         0.00%       0.000us       0.000us     131.488us         0.00%     131.488us       1.384us            95  \n",
      "void at::native::sbtopk::gatherTopK<float, unsigned ...         0.00%       0.000us         0.00%       0.000us       0.000us     158.144us         0.00%     158.144us       4.942us            32  \n",
      "void at::native::bitonicSortKVInPlace<2, -1, 16, 16,...         0.00%       0.000us         0.00%       0.000us       0.000us     149.986us         0.00%     149.986us       4.687us            32  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     174.499us         0.00%     174.499us       2.727us            64  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      99.423us         0.00%      99.423us       1.553us            64  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     391.710us         0.00%     391.710us       1.085us           361  \n",
      "ampere_fp16_s16816gemm_fp16_64x64_ldg8_f2f_stages_64...         0.00%       0.000us         0.00%       0.000us       0.000us      38.024ms         0.09%      38.024ms     146.247us           260  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     379.005us         0.00%     379.005us       1.944us           195  \n",
      "          gemv_revsplitK_A16fWnO16f_int32packing_kernel         0.00%       0.000us         0.00%       0.000us       0.000us       9.941ms         0.02%       9.941ms      37.514us           265  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     334.884us         0.00%     334.884us       1.303us           257  \n",
      "                         Memcpy DtoH (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       21.001s        49.89%       21.001s      17.342ms          1211  \n",
      "                         Memcpy HtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       21.001s        49.89%       21.001s      17.342ms          1211  \n",
      "void cutlass::Kernel2<cutlass_80_tensorop_f16_s16816...         0.00%       0.000us         0.00%       0.000us       0.000us     678.620us         0.00%     678.620us     339.310us             2  \n",
      "void cublasLt::splitKreduce_kernel<32, 16, int, __ha...         0.00%       0.000us         0.00%       0.000us       0.000us      10.496us         0.00%      10.496us       5.248us             2  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us       8.224us         0.00%       8.224us       2.056us             4  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       1.280us         0.00%       1.280us       1.280us             1  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      20.448us         0.00%      20.448us      10.224us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.248us         0.00%       5.248us       1.312us             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.432us         0.00%       2.432us       1.216us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      80.061us         0.00%      80.061us       1.251us            64  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.024us         0.00%       5.024us       1.256us             4  \n",
      "void at::native::vectorized_elementwise_kernel<2, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.344us         0.00%       1.344us       1.344us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.368us         0.00%       2.368us       1.184us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       3.904us         0.00%       3.904us       1.952us             2  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       5.281us         0.00%       5.281us       2.640us             2  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.592us         0.00%       2.592us       2.592us             1  \n",
      "ampere_fp16_s16816gemm_fp16_64x64_sliced1x2_ldg8_f2f...         0.00%       0.000us         0.00%       0.000us       0.000us       2.903ms         0.01%       2.903ms      45.367us            64  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us     846.393us         0.00%     846.393us      12.633us            67  \n",
      "void pytorch_flash::flash_fwd_kernel<pytorch_flash::...         0.00%       0.000us         0.00%       0.000us       0.000us     186.395us         0.00%     186.395us       5.825us            32  \n",
      "void dot_kernel<float, 128, 0, cublasDotParams<cubla...         0.00%       0.000us         0.00%       0.000us       0.000us     123.358us         0.00%     123.358us       1.958us            63  \n",
      "void reduce_1Block_kernel<float, 128, 7, cublasGemvT...         0.00%       0.000us         0.00%       0.000us       0.000us      92.351us         0.00%      92.351us       1.466us            63  \n",
      "void at::native::sbtopk::gatherTopK<float, unsigned ...         0.00%       0.000us         0.00%       0.000us       0.000us     273.058us         0.00%     273.058us       4.334us            63  \n",
      "void at::native::bitonicSortKVInPlace<-2, -1, 16, 16...         0.00%       0.000us         0.00%       0.000us       0.000us     278.271us         0.00%     278.271us       4.417us            63  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      81.730us         0.00%      81.730us       1.120us            73  \n",
      "void at::native::sbtopk::gatherTopK<c10::Half, unsig...         0.00%       0.000us         0.00%       0.000us       0.000us       3.864ms         0.01%       3.864ms      52.933us            73  \n",
      "void at::native::radixSortKVInPlace<-2, -1, 128, 32,...         0.00%       0.000us         0.00%       0.000us       0.000us       1.120ms         0.00%       1.120ms      15.344us            73  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     141.340us         0.00%     141.340us       1.936us            73  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      77.667us         0.00%      77.667us       1.253us            62  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us      70.876us         0.00%      70.876us       1.143us            62  \n",
      "                       Memcpy DtoH (Device -> Pageable)         0.00%       0.000us         0.00%       0.000us       0.000us      31.553us         0.00%      31.553us       1.434us            22  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      18.847us         0.00%      18.847us       1.713us            11  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      29.597us         0.00%      29.597us       2.691us            11  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     191.516us         0.00%     191.516us       3.089us            62  \n",
      "void gemv2T_kernel_val<int, int, __half, __half, __h...         0.00%       0.000us         0.00%       0.000us       0.000us       1.854ms         0.00%       1.854ms      29.907us            62  \n",
      "std::enable_if<true, void>::type internal::gemvx::ke...         0.00%       0.000us         0.00%       0.000us       0.000us       2.173ms         0.01%       2.173ms      35.044us            62  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       1.313us         0.00%       1.313us       1.313us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 21.326s\n",
      "Self CUDA time total: 42.092s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 2\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 2\n",
    "test_samples = 4\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "filt_type = fineweb_path.split('.')[-1]\n",
    "fineweb = load_dataset(filt_type, data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text'][:test_samples] \n",
    "\n",
    "print(\"output length is {}\".format(output_length))\n",
    "text = fineweb_text[0]\n",
    "inputs = preprocess_data(text, tokenizer)\n",
    "\n",
    "# cached_mlp.clear_load_from_cpu_stats()\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ]\n",
    ") as p:\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(),\n",
    "            max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "print(p.key_averages().table(\n",
    "    sort_by=\"self_cpu_time_total\", row_limit=-1))\n",
    "p.export_chrome_trace(\"./offloading-3090-3gpu.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试一个正常输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "Generated length: 16 Time taken: 31.40 s prefill time: 20.90 s\n",
      "['The future of AI is here,  and it’s not as scary as you think.\\n\\nIn the past']\n",
      "decode phase speed: 1.4289  token/s\n",
      "the number of experts reload per token: 7.333333333333333\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 10\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 16\n",
    "test_samples = 1\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "generated_all, decode_time, prefill_time = 0, 0, 0\n",
    "# print(\"max output length is {}\".format(output_length))\n",
    "text = \"The future of AI is here, \"\n",
    "\n",
    "clear_prefill_time = PLLM.get_prefill_time()\n",
    "clear_experts = PLLM.get_reload_experts()\n",
    "# print(\"need to zero: \", clear_experts, clear_prefill_time)\n",
    "\n",
    "inputs = preprocess_data(text, tokenizer)\n",
    "# 测试时间\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "# 开始计时\n",
    "torch.cuda.synchronize()\n",
    "start_event.record()\n",
    "\n",
    "# 前向传播\n",
    "with torch.no_grad():\n",
    "    output = llm.generate(\n",
    "        input_ids=inputs[\"input_ids\"].cuda(device_id),\n",
    "        attention_mask=inputs[\"attention_mask\"].cuda(device_id),\n",
    "        max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "        generation_config=GenerationConfig(do_sample=False),\n",
    "        pad_token_id=tokenizer.pad_token_id, \n",
    "        # cache_implementation=\"static\" ## moe not support\n",
    "    )\n",
    "\n",
    "# 结束计时\n",
    "end_event.record()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# 计算时间\n",
    "elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "decode_time += elapsed_time\n",
    "cur_prefill_time = PLLM.get_prefill_time()\n",
    "prefill_time += cur_prefill_time\n",
    "print(f\"Generated length: {len(output[0]) - input_length}\", f\"Time taken: {elapsed_time:.2f} s\", f\"prefill time: {cur_prefill_time:.2f} s\")\n",
    "# print(output)\n",
    "print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "\n",
    "generated_all += (len(output[0]) - input_length -1)\n",
    "\n",
    "timepertoken = (decode_time - prefill_time) / (generated_all)\n",
    "print(\"decode phase speed:\", '{:.4f}'.format(1/timepertoken) , ' token/s')\n",
    "print(\"the number of experts reload per token:\", PLLM.get_reload_experts() / generated_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载到GPU上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:25<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MixtralForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "import json\n",
    "\n",
    "def get_model(model_name, device_map, dtype=torch.bfloat16):\n",
    "    llm = MixtralForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        use_cache=True,\n",
    "        torch_dtype=dtype,\n",
    "    ) \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return llm, tokenizer\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "with open('../quantize/device_map_1.json', 'r') as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "dtype = torch.float16\n",
    "llm, tokenizer = get_model(model_name, device_map, dtype=dtype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
