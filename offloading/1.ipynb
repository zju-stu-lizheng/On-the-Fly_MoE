{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先都加载到cpu上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:06<00:00,  3.14it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "from modeling_mixtral import MixtralForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import json\n",
    "\n",
    "def get_model(model_name, device_map, dtype=torch.bfloat16):\n",
    "    llm = MixtralForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        use_cache=True,\n",
    "        torch_dtype=dtype,\n",
    "    ) \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return llm, tokenizer\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "dtype = torch.float16\n",
    "llm, tokenizer = get_model(model_name, 'cpu', dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import threading\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "from queue import Queue\n",
    "\n",
    "class CachedMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, dtype, sparsity: float = 0.2):\n",
    "        super(CachedMLP, self).__init__()\n",
    "        self.sparsity = sparsity\n",
    "        self.activenum = int((1 - sparsity) * hidden_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dtype = dtype\n",
    "\n",
    "        print(\"active neural num \",self.activenum)\n",
    "\n",
    "        self.activation = nn.SiLU()\n",
    "\n",
    "        # GPU 缓存张量\n",
    "        self.w1_gpu = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda')\n",
    "        self.w2_gpu = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda')\n",
    "        self.w3_gpu = None\n",
    "\n",
    "        # 第二个专家的 GPU 缓存张量\n",
    "        self.w1_gpu_expert1 = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda')\n",
    "        self.w2_gpu_expert1 = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda')\n",
    "        self.w3_gpu_expert1 = None\n",
    "\n",
    "        # Pinned Memory 缓冲区\n",
    "        self.register_buffer('sparse_w1_cpu', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "        self.register_buffer('sparse_w2_cpu', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "\n",
    "        self.sparse_w1_cpu = self.sparse_w1_cpu.pin_memory()\n",
    "        self.sparse_w2_cpu = self.sparse_w2_cpu.pin_memory()\n",
    "\n",
    "\n",
    "        # 第二个专家的 Pinned Memory 缓冲区\n",
    "        self.register_buffer('sparse_w1_cpu_expert1', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "        self.register_buffer('sparse_w2_cpu_expert1', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "\n",
    "        self.sparse_w1_cpu_expert1 = self.sparse_w1_cpu_expert1.pin_memory()\n",
    "        self.sparse_w2_cpu_expert1 = self.sparse_w2_cpu_expert1.pin_memory()\n",
    "\n",
    "        self.expert0_weight = torch.tensor(0)\n",
    "        self.expert1_weight = torch.tensor(0)\n",
    "\n",
    "        # 统计信息\n",
    "        self.load_from_cpu_time = 0.0\n",
    "        self.load_from_cpu_calls = 0\n",
    "\n",
    "    def load_from_cpu(self, cpu_mlp, cpu_mlp_expert1, stream: torch.cuda.Stream):\n",
    "        \"\"\"\n",
    "        从CPU加载参数，并使用指定的CUDA流进行异步复制到GPU。\n",
    "        \n",
    "        参数:\n",
    "            cpu_mlp: 包含CPU上参数的字典（第一个专家）。\n",
    "            cpu_mlp_expert1: 包含CPU上参数的字典（第二个专家）。\n",
    "            stream: 用于数据传输的CUDA流。\n",
    "        \"\"\"\n",
    "        # 生成随机索引\n",
    "        random_indices = torch.randperm(cpu_mlp['w1'].data.size(0))[:self.activenum]\n",
    "        # sorted_indices = torch.sort(random_indices).values\n",
    "\n",
    "        # 从CPU加载参数（第一个专家）\n",
    "        self.sparse_w1_cpu.copy_(cpu_mlp['w1'].data[random_indices, :])\n",
    "        self.sparse_w2_cpu.copy_(cpu_mlp['w2'].data[random_indices, :])\n",
    "        # 从CPU加载参数（第二个专家）\n",
    "        self.sparse_w1_cpu_expert1.copy_(cpu_mlp_expert1['w1'].data[random_indices, :])\n",
    "        self.sparse_w2_cpu_expert1.copy_(cpu_mlp_expert1['w2'].data[random_indices, :])\n",
    "        # 异步复制到GPU\n",
    "        with torch.cuda.stream(stream):\n",
    "            self.w1_gpu.copy_(self.sparse_w1_cpu, non_blocking=True)\n",
    "            self.w2_gpu.copy_(self.sparse_w2_cpu, non_blocking=True)\n",
    "            self.w1_gpu_expert1.copy_(self.sparse_w1_cpu_expert1, non_blocking=True)\n",
    "            self.w2_gpu_expert1.copy_(self.sparse_w2_cpu_expert1, non_blocking=True)\n",
    "        \n",
    "        # 直接赋值 w3_gpu 和 w3_gpu_expert1\n",
    "        # 固定在GPU上的w3\n",
    "        self.w3_gpu = cpu_mlp['w3']\n",
    "        self.w3_gpu_expert1 = cpu_mlp_expert1['w3']\n",
    "\n",
    "    def load_expert_weights(self, expert_weights):\n",
    "        self.expert0_weight = expert_weights[0]\n",
    "        self.expert1_weight = expert_weights[1]\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        \"\"\"\n",
    "        根据hidden_states， 分别计算两个专家的输出\n",
    "        \"\"\"\n",
    "        # 第一个专家的计算\n",
    "        w3_output = torch.matmul(hidden_states, self.w3_gpu.T)[:, :self.activenum]\n",
    "        w1_output = self.activation(torch.matmul(hidden_states, self.w1_gpu.T))\n",
    "        # w2 = self.w2_gpu.T\n",
    "        hidden_states_expert0 = torch.matmul(w1_output * w3_output, self.w2_gpu)\n",
    "\n",
    "        # 第二个专家的计算\n",
    "        w3_output_expert1 = torch.matmul(hidden_states, self.w3_gpu_expert1.T)[:, :self.activenum]\n",
    "        w1_output_expert1 = self.activation(torch.matmul(hidden_states, self.w1_gpu_expert1.T))\n",
    "        # w2_expert1 = self.w2_gpu_expert1.T\n",
    "        hidden_states_expert1 = torch.matmul(w1_output_expert1 * w3_output_expert1, self.w2_gpu_expert1)\n",
    "\n",
    "        final_hidden_states = hidden_states_expert0* self.expert0_weight + hidden_states_expert1* self.expert1_weight\n",
    "        \n",
    "        return final_hidden_states\n",
    "                        \n",
    "def convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.9):\n",
    "    ### 其他部分存放在GPU上\n",
    "    llm.model.embed_tokens.cuda()\n",
    "    for i in range(len(llm.model.layers)):\n",
    "        llm.model.layers[i].self_attn.cuda()\n",
    "        llm.model.layers[i].input_layernorm.cuda()\n",
    "        llm.model.layers[i].post_attention_layernorm.cuda()\n",
    "        llm.model.layers[i].block_sparse_moe.gate.cuda()\n",
    "        for j in range(len(llm.model.layers[0].block_sparse_moe.experts)):\n",
    "            llm.model.layers[i].block_sparse_moe.experts[j].w3.cuda()\n",
    "    ### 第0层的专家存放在GPU上\n",
    "    for j in range(len(llm.model.layers[0].block_sparse_moe.experts)):\n",
    "        llm.model.layers[0].block_sparse_moe.experts[j].cuda()\n",
    "\n",
    "    llm.model.norm.cuda()\n",
    "    llm.lm_head.cuda()\n",
    "    \n",
    "    # 创建两个共享的CachedMLP实例\n",
    "    buffer0 = CachedMLP(\n",
    "        input_dim=llm.config.hidden_size,\n",
    "        hidden_dim=llm.config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "        sparsity=sparsity\n",
    "    )\n",
    "    buffer1 = CachedMLP(\n",
    "        input_dim=llm.config.hidden_size,\n",
    "        hidden_dim=llm.config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "        sparsity=sparsity\n",
    "    )\n",
    "    cached_mlps = [buffer0, buffer1]\n",
    "    \n",
    "    for i, layer in enumerate(llm.model.layers):\n",
    "        if i==0:\n",
    "            continue\n",
    "        # 将专家的forward方法替换为PipelineLLM管理的方式\n",
    "        for j, expert in enumerate(layer.block_sparse_moe.experts):\n",
    "            expert.cpu_mlp = {\n",
    "                \"w1\": expert.w1.cpu().weight,\n",
    "                \"w2\": expert.w2.cpu().weight.T.contiguous(),\n",
    "                \"w3\": expert.w3.weight,\n",
    "            }\n",
    "    return llm, cached_mlps\n",
    "\n",
    "class PipelineLLM:\n",
    "    def __init__(self, llm, cached_mlps):\n",
    "        \"\"\"\n",
    "        初始化 PipelineLLM，替换模型每一层的 forward 方法。\n",
    "        \n",
    "        参数:\n",
    "            llm: 原始的大模型\n",
    "            cached_mlps: 两个 CachedMLP 实例列表\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.cached_mlps = cached_mlps  # [buffer0, buffer1]\n",
    "        self.num_layers = len(llm.model.layers)\n",
    "        self.lock = threading.Lock()\n",
    "        self.use_buffer0 = True  # 标记当前使用哪个缓冲区\n",
    "\n",
    "        # 创建两个共享的CUDA流\n",
    "        self.stream0 = torch.cuda.Stream()\n",
    "        self.stream1 = torch.cuda.Stream()\n",
    "\n",
    "        # 初始化加载第一个和第二个层的参数\n",
    "        self._load_layer(1, buffer_index=0, expert_ids=torch.tensor([0, 1]))\n",
    "        self._load_layer(1, buffer_index=1, expert_ids=torch.tensor([0, 1]))\n",
    "        self.top_k = 2\n",
    "        self.activation = nn.SiLU()\n",
    "\n",
    "        self._replace_forward_methods()\n",
    "\n",
    "        # 用于统计时间的变量\n",
    "        self.total_prefill_time = 0.0\n",
    "        self.total_decode_time = 0.0\n",
    "\n",
    "    def _load_layer(self, layer_idx, buffer_index, expert_ids, expert_weights=torch.tensor([0, 0])):\n",
    "        \"\"\"\n",
    "        加载指定层的参数到指定的缓冲区。\n",
    "        \n",
    "        参数:\n",
    "            layer_idx: 层的索引\n",
    "            buffer_index: 缓冲区的索引（0 或 1）\n",
    "        \"\"\"\n",
    "        layer = self.llm.model.layers[layer_idx]\n",
    "        expert0 = layer.block_sparse_moe.experts[expert_ids[0]]\n",
    "        expert1 = layer.block_sparse_moe.experts[expert_ids[1]]\n",
    "        # if layer_idx == 1:\n",
    "        #     print(expert_ids[0].data, expert_ids[1].data, '{:.3f}, {:.3f}'.format(expert_weights[0], expert_weights[1]))\n",
    "\n",
    "        cpu_mlp = expert0.cpu_mlp\n",
    "        cpu_mlp_expert1 = expert1.cpu_mlp\n",
    "        buffer = self.cached_mlps[buffer_index]\n",
    "        stream = self.stream0 if buffer_index == 0 else self.stream1\n",
    "\n",
    "        buffer.load_expert_weights(expert_weights)\n",
    "        # 异步加载参数\n",
    "        buffer.load_from_cpu(cpu_mlp, cpu_mlp_expert1, stream)\n",
    "\n",
    "    def _replace_forward_methods(self):\n",
    "        \"\"\"\n",
    "        替换模型每一层的 forward 方法，添加参数预加载逻辑和注意力计算。\n",
    "        \"\"\"\n",
    "        for i, layer in enumerate(self.llm.model.layers):\n",
    "            def new_forward(hidden_states: torch.Tensor,\n",
    "                        attention_mask: Optional[torch.Tensor] = None,\n",
    "                        position_ids: Optional[torch.LongTensor] = None,\n",
    "                        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "                        output_attentions: Optional[bool] = False,\n",
    "                        output_router_logits: Optional[bool] = False,\n",
    "                        use_cache: Optional[bool] = False,\n",
    "                        cache_position: Optional[torch.LongTensor] = None,\n",
    "                        layer=layer,\n",
    "                        layer_idx=i):\n",
    "                with self.lock:\n",
    "                    batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "                    \n",
    "                    if sequence_length == 1:\n",
    "                        #### decode phase ####\n",
    "                        # 选择当前使用的缓冲区\n",
    "                        current_buffer = self.cached_mlps[0] if self.use_buffer0 else self.cached_mlps[1]\n",
    "\n",
    "                        next_buffer_index = 1 if self.use_buffer0 else 0\n",
    "\n",
    "                        next_layer_idx = layer_idx + 1\n",
    "\n",
    "                        if next_layer_idx < self.num_layers:\n",
    "                            # 预加载下一层的参数\n",
    "                            next_layer = self.llm.model.layers[next_layer_idx]\n",
    "                            router = next_layer.block_sparse_moe.gate\n",
    "\n",
    "                            # batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "                            hidden_states_flat = hidden_states.view(-1, hidden_dim)\n",
    "                            # router_logits: (batch * sequence_length, n_experts)\n",
    "                            router_logits = router(hidden_states_flat)\n",
    "\n",
    "                            routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "                            routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n",
    "                            routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n",
    "\n",
    "                            hidden_states = hidden_states_flat.reshape(batch_size, sequence_length, hidden_dim)\n",
    "\n",
    "                            self._load_layer(\n",
    "                                next_layer_idx,\n",
    "                                buffer_index=next_buffer_index,\n",
    "                                expert_ids=selected_experts[0],\n",
    "                                expert_weights=routing_weights[0]\n",
    "                            )\n",
    "\n",
    "                        # 切换缓冲区\n",
    "                        self.use_buffer0 = not self.use_buffer0\n",
    "\n",
    "                    # 处理当前层\n",
    "                    residual = hidden_states\n",
    "                    hidden_states = layer.input_layernorm(hidden_states)\n",
    "\n",
    "                    # Self Attention\n",
    "                    hidden_states, self_attn_weights, present_key_value = layer.self_attn(\n",
    "                        hidden_states=hidden_states,\n",
    "                        attention_mask=attention_mask,\n",
    "                        position_ids=position_ids,\n",
    "                        past_key_value=past_key_value,\n",
    "                        output_attentions=output_attentions,\n",
    "                        use_cache=use_cache,\n",
    "                        cache_position=cache_position,\n",
    "                    )\n",
    "                    hidden_states = residual + hidden_states\n",
    "\n",
    "                    # Fully Connected\n",
    "                    residual = hidden_states\n",
    "                    hidden_states = layer.post_attention_layernorm(hidden_states)\n",
    "\n",
    "                    if sequence_length > 1:\n",
    "                        # print(\"in prefill layer \", layer_idx)\n",
    "                        # 对于prefill阶段，仅将experts加载到GPU计算\n",
    "                        experts = layer.block_sparse_moe.experts\n",
    "\n",
    "                        # 将experts移动到GPU\n",
    "                        for expert in experts:\n",
    "                            expert.to('cuda')\n",
    "\n",
    "                        # 在GPU上进行MoE计算（gate保持在CPU）\n",
    "                        final_hidden_states, router_logits = layer.block_sparse_moe(hidden_states)\n",
    "\n",
    "                        # 计算完成后将experts移回CPU\n",
    "                        if layer_idx != 0:\n",
    "                            for expert in experts:\n",
    "                                expert.to('cpu')\n",
    "                    else:\n",
    "                        # batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "                        hidden_states_flat = hidden_states.view(-1, hidden_dim)\n",
    "                        # print(\"in decode layer\", layer_idx)\n",
    "                        if layer_idx > 0:\n",
    "                            ### 使用当前缓冲区进行 MLP 计算 ###\n",
    "                            final_hidden_states = current_buffer(hidden_states_flat)\n",
    "                        else:\n",
    "                            ### 根据router计算需要使用的专家 ###\n",
    "                            cur_layer = layer\n",
    "                            router = cur_layer.block_sparse_moe.gate\n",
    "                            # router_logits: (batch * sequence_length, n_experts)\n",
    "                            router_logits = router(hidden_states_flat)\n",
    "\n",
    "                            routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "                            routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n",
    "                            routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n",
    "                            # we cast back to the input dtype\n",
    "                            routing_weights = routing_weights.to(hidden_states_flat.dtype)\n",
    "\n",
    "                            first_expert, second_expert = selected_experts[0][0], selected_experts[0][1]\n",
    "\n",
    "                            final_hidden_states_expert0 = cur_layer.block_sparse_moe.experts[first_expert](\n",
    "                                hidden_states_flat) * routing_weights[0][0]\n",
    "\n",
    "                            final_hidden_states_expert1 = cur_layer.block_sparse_moe.experts[second_expert](\n",
    "                                hidden_states_flat) * routing_weights[0][1]\n",
    "\n",
    "                            # 将两个专家的结果相加\n",
    "                            final_hidden_states = final_hidden_states_expert0 + final_hidden_states_expert1\n",
    "\n",
    "                        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n",
    "\n",
    "                    hidden_states = residual + final_hidden_states\n",
    "\n",
    "                    outputs = (hidden_states,)\n",
    "\n",
    "                    if output_attentions:\n",
    "                        outputs += (self_attn_weights,)\n",
    "\n",
    "                    if use_cache:\n",
    "                        outputs += (present_key_value,)\n",
    "\n",
    "                    return outputs\n",
    "\n",
    "            # 替换forward方法\n",
    "            layer.forward = new_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "active neural num  2867\n",
      "active neural num  2867\n"
     ]
    }
   ],
   "source": [
    "llm, cached_mlps = convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.8)\n",
    "\n",
    "# 创建流水线模型\n",
    "PLLM = PipelineLLM(llm, cached_mlps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试时间开销"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 32\n",
      "Time taken: 3.4620 seconds\n",
      "Time taken: 3.4617 seconds\n",
      "Time taken: 3.4648 seconds\n",
      "Time taken: 3.2562 seconds\n",
      "decode time: 3.4111  s\n",
      "decode phase speed: 9.3810  token/s\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 1\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 32\n",
    "test_samples = 4\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "filt_type = fineweb_path.split('.')[-1]\n",
    "fineweb = load_dataset(filt_type, data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text'][:test_samples] \n",
    "\n",
    "prefill_time, decode_time = 0, 0\n",
    "for output_length in [output_length]:\n",
    "    print(\"output length is {}\".format(output_length))\n",
    "    for text in fineweb_text:\n",
    "        inputs = preprocess_data(text, tokenizer)\n",
    "\n",
    "        # 预热（避免第一次运行时的额外开销）\n",
    "        with torch.no_grad():\n",
    "            output = llm(input_ids=inputs[\"input_ids\"].cuda(), attention_mask=inputs[\"attention_mask\"].cuda())\n",
    "\n",
    "        # 测试时间\n",
    "        start_event = torch.cuda.Event(enable_timing=True)\n",
    "        end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        # 开始计时\n",
    "        torch.cuda.synchronize()\n",
    "        start_event.record()\n",
    "\n",
    "        # 前向传播\n",
    "        with torch.no_grad():\n",
    "            output = llm.generate(\n",
    "                input_ids=inputs[\"input_ids\"].cuda(),\n",
    "                attention_mask=inputs[\"attention_mask\"].cuda(),\n",
    "                max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "                generation_config=GenerationConfig(do_sample=False),\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        # 结束计时\n",
    "        end_event.record()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        # 计算时间\n",
    "        elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "        decode_time += elapsed_time\n",
    "        # print(f\"Generated output length: {len(output[0]) - input_length}\")\n",
    "        # print(output)\n",
    "        # print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "        print(f\"Time taken: {elapsed_time:.4f} seconds\")\n",
    "\n",
    "timepertoken = (decode_time) / (output_length) / test_samples\n",
    "print(\"decode time:\", '{:.4f}'.format((decode_time) /test_samples), ' s')\n",
    "print(\"decode phase speed:\", '{:.4f}'.format(1/timepertoken) , ' token/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.profile\n",
    "\n",
    "trace-offloading-r.json是最优，就是做完一个index就传一个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 2\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::index        56.63%     160.209ms        57.04%     161.356ms     427.999us     990.620us         0.39%     990.620us       2.628us           377  \n",
      "                                            aten::copy_        15.03%      42.509ms        16.43%      46.474ms      51.183us     209.837ms        83.14%     209.837ms     231.097us           908  \n",
      "                                       cudaLaunchKernel         5.76%      16.293ms         5.76%      16.293ms       4.602us       0.000us         0.00%       0.000us       0.000us          3540  \n",
      "                                               aten::mm         4.05%      11.444ms         5.88%      16.640ms      23.570us      19.846ms         7.86%      19.846ms      28.111us           706  \n",
      "                                         aten::randperm         2.02%       5.711ms         4.13%      11.689ms      94.268us       0.000us         0.00%       0.000us       0.000us           124  \n",
      "                                  cudaStreamSynchronize         1.84%       5.219ms         1.84%       5.219ms      37.015us       0.000us         0.00%       0.000us       0.000us           141  \n",
      "                                              aten::mul         1.59%       4.487ms         2.69%       7.623ms       9.823us       6.280ms         2.49%       6.280ms       8.093us           776  \n",
      "                                              aten::add         0.97%       2.741ms         1.63%       4.602ms      10.137us       3.227ms         1.28%       3.227ms       7.107us           454  \n",
      "                                        cudaMemcpyAsync         0.94%       2.663ms         0.94%       2.663ms       6.592us       0.000us         0.00%       0.000us       0.000us           404  \n",
      "                                            aten::slice         0.71%       2.006ms         0.89%       2.514ms       1.702us       0.000us         0.00%       0.000us       0.000us          1477  \n",
      "                                              aten::cat         0.60%       1.688ms         0.92%       2.610ms      13.318us       1.611ms         0.64%       1.611ms       8.218us           196  \n",
      "                                       aten::as_strided         0.52%       1.479ms         0.52%       1.479ms       0.396us       0.000us         0.00%       0.000us       0.000us          3738  \n",
      "                                             aten::mean         0.49%       1.388ms         0.72%       2.038ms      15.678us       1.327ms         0.53%       1.327ms      10.208us           130  \n",
      "                                    cudaLaunchKernelExC         0.48%       1.360ms         0.48%       1.360ms       5.192us       0.000us         0.00%       0.000us       0.000us           262  \n",
      "                                    aten::empty_strided         0.46%       1.291ms         0.46%       1.291ms       3.842us       0.000us         0.00%       0.000us       0.000us           336  \n",
      "                                              aten::pow         0.44%       1.239ms         0.70%       1.970ms      15.157us     837.912us         0.33%     837.912us       6.445us           130  \n",
      "                                            aten::empty         0.41%       1.154ms         0.41%       1.154ms       2.487us       0.000us         0.00%       0.000us       0.000us           464  \n",
      "                                             aten::topk         0.40%       1.123ms         0.80%       2.259ms      35.293us       1.441ms         0.57%       1.441ms      22.520us            64  \n",
      "                                           aten::matmul         0.37%       1.054ms         6.45%      18.242ms      25.839us       0.000us         0.00%      19.846ms      28.111us           706  \n",
      "                                              aten::neg         0.30%     856.306us         0.50%       1.407ms      10.995us     907.606us         0.36%     907.606us       7.091us           128  \n",
      "                         aten::_flash_attention_forward         0.30%     852.934us         0.74%       2.096ms      32.758us     788.750us         0.31%     788.750us      12.324us            64  \n",
      "                                             aten::silu         0.30%     843.048us         0.49%       1.384ms      10.816us       1.106ms         0.44%       1.106ms       8.638us           128  \n",
      "                                             aten::view         0.30%     840.005us         0.30%     840.005us       0.727us       0.000us         0.00%       0.000us       0.000us          1156  \n",
      "                                            aten::rsqrt         0.29%     831.609us         0.50%       1.414ms      10.874us     862.589us         0.34%     862.589us       6.635us           130  \n",
      "                                  cudaFuncGetAttributes         0.29%     812.794us         0.29%     812.794us       2.493us       0.000us         0.00%       0.000us       0.000us           326  \n",
      "                                          aten::reshape         0.29%     809.227us         1.21%       3.430ms       3.849us       0.000us         0.00%       1.113ms       1.249us           891  \n",
      "                                        aten::transpose         0.27%     765.129us         0.40%       1.139ms       1.347us       0.000us         0.00%       0.000us       0.000us           846  \n",
      "                                           aten::select         0.25%     712.981us         0.30%     849.697us       2.151us       0.000us         0.00%       0.000us       0.000us           395  \n",
      "                                        cudaMemsetAsync         0.25%     712.550us         0.25%     712.550us       5.318us       0.000us         0.00%       0.000us       0.000us           134  \n",
      "                                              aten::sum         0.24%     670.972us         0.34%     972.981us      15.203us     651.888us         0.26%     651.888us      10.186us            64  \n",
      "                                         aten::_to_copy         0.24%     669.643us         1.66%       4.683ms      17.409us       0.000us         0.00%       2.079ms       7.728us           269  \n",
      "                                               aten::to         0.23%     644.042us         1.88%       5.327ms       9.829us       0.000us         0.00%       2.079ms       3.836us           542  \n",
      "                                                aten::t         0.22%     616.118us         0.39%       1.097ms       3.285us       0.000us         0.00%       0.000us       0.000us           334  \n",
      "                     aten::scaled_dot_product_attention         0.22%     614.802us         1.18%       3.325ms      51.959us       0.000us         0.00%     788.750us      12.324us            64  \n",
      "                              aten::_local_scalar_dense         0.19%     541.585us         2.39%       6.751ms      49.637us       1.193ms         0.47%       1.193ms       8.769us           136  \n",
      "                                         aten::_softmax         0.18%     507.236us         0.29%     808.041us      12.626us     499.466us         0.20%     499.466us       7.804us            64  \n",
      "                                           aten::linear         0.17%     477.618us         4.07%      11.521ms      34.494us       0.000us         0.00%       6.628ms      19.844us           334  \n",
      "                                             aten::div_         0.16%     459.140us         0.28%     788.637us      12.322us     572.146us         0.23%     572.146us       8.940us            64  \n",
      "                                          aten::permute         0.15%     426.565us         0.18%     521.315us       2.102us       0.000us         0.00%       0.000us       0.000us           248  \n",
      "              aten::_scaled_dot_product_flash_attention         0.14%     383.121us         0.96%       2.711ms      42.353us       0.000us         0.00%     788.750us      12.324us            64  \n",
      "                                        aten::unsqueeze         0.13%     360.143us         0.17%     468.047us       1.786us       0.000us         0.00%       0.000us       0.000us           262  \n",
      "                                   cudaFuncSetAttribute         0.10%     294.534us         0.10%     294.534us       0.627us       0.000us         0.00%       0.000us       0.000us           470  \n",
      "                                            aten::clone         0.10%     287.889us         0.76%       2.159ms      16.115us       0.000us         0.00%       1.148ms       8.567us           134  \n",
      "                                     aten::_unsafe_view         0.09%     258.654us         0.09%     258.654us       0.670us       0.000us         0.00%       0.000us       0.000us           386  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.09%     255.894us         0.09%     255.894us       0.493us       0.000us         0.00%       0.000us       0.000us           519  \n",
      "                                  cudaDeviceSynchronize         0.09%     248.251us         0.09%     248.251us     248.251us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                       aten::empty_like         0.09%     243.844us         0.32%     914.005us       4.640us       0.000us         0.00%       0.000us       0.000us           197  \n",
      "                                           aten::expand         0.09%     242.931us         0.11%     299.882us       2.343us       0.000us         0.00%       0.000us       0.000us           128  \n",
      "                                          aten::numpy_T         0.05%     154.590us         0.24%     675.905us       2.725us       0.000us         0.00%       0.000us       0.000us           248  \n",
      "                                             aten::item         0.05%     127.416us         2.43%       6.878ms      50.574us       0.000us         0.00%       1.193ms       8.769us           136  \n",
      "                                 cudaDeviceGetAttribute         0.04%     118.278us         0.04%     118.278us       0.457us       0.000us         0.00%       0.000us       0.000us           259  \n",
      "                                    aten::scalar_tensor         0.04%     104.705us         0.04%     104.705us       1.689us       0.000us         0.00%       0.000us       0.000us            62  \n",
      "                                          aten::softmax         0.04%     103.826us         0.32%     911.867us      14.248us       0.000us         0.00%     499.466us       7.804us            64  \n",
      "                                               aten::eq         0.03%      90.715us         0.05%     150.443us      16.716us      56.515us         0.02%      56.515us       6.279us             9  \n",
      "                                  cudaStreamIsCapturing         0.02%      67.809us         0.02%      67.809us       1.060us       0.000us         0.00%       0.000us       0.000us            64  \n",
      "                                      aten::result_type         0.02%      65.170us         0.02%      65.170us       0.501us       0.000us         0.00%       0.000us       0.000us           130  \n",
      "                                             aten::isin         0.02%      63.656us         0.12%     326.017us     108.672us       0.000us         0.00%      90.081us      30.027us             3  \n",
      "                                           aten::cumsum         0.02%      62.550us         0.04%     115.260us      38.420us      36.000us         0.01%      36.000us      12.000us             3  \n",
      "                                            aten::fill_         0.02%      61.235us         0.05%     151.567us      13.779us      86.914us         0.03%      86.914us       7.901us            11  \n",
      "                                              aten::sub         0.02%      49.736us         0.03%      79.495us      15.899us      33.346us         0.01%      33.346us       6.669us             5  \n",
      "                                          aten::resize_         0.02%      46.976us         0.02%      46.976us       0.373us       0.000us         0.00%       0.000us       0.000us           126  \n",
      "                                       aten::bitwise_or         0.02%      42.963us         0.02%      64.793us      16.198us      43.936us         0.02%      43.936us      10.984us             4  \n",
      "                                           aten::argmax         0.01%      42.044us         0.02%      59.977us      29.988us      41.024us         0.02%      41.024us      20.512us             2  \n",
      "                                              aten::any         0.01%      40.367us         0.04%     110.976us      22.195us       0.000us         0.00%      27.520us       5.504us             5  \n",
      "                                     aten::index_select         0.01%      39.947us         0.02%      69.046us      34.523us       7.969us         0.00%       7.969us       3.984us             2  \n",
      "                                              aten::max         0.01%      32.168us         0.02%      54.656us      27.328us      29.088us         0.01%      29.088us      14.544us             2  \n",
      "                                     aten::masked_fill_         0.01%      27.308us         0.01%      40.663us      20.331us       4.736us         0.00%       4.736us       2.368us             2  \n",
      "                                              aten::all         0.01%      25.426us         0.02%      47.204us      23.602us       4.704us         0.00%       7.329us       3.664us             2  \n",
      "                                               aten::lt         0.01%      22.696us         0.01%      34.430us      34.430us       2.528us         0.00%       2.528us       2.528us             1  \n",
      "                                      aten::bitwise_not         0.01%      22.429us         0.01%      35.324us      17.662us      20.608us         0.01%      20.608us      10.304us             2  \n",
      "                                      aten::bitwise_and         0.01%      21.398us         0.01%      36.632us      18.316us      25.632us         0.01%      25.632us      12.816us             2  \n",
      "                                               aten::ge         0.01%      21.343us         0.01%      31.367us      15.684us      13.280us         0.01%      13.280us       6.640us             2  \n",
      "                                           aten::__or__         0.01%      18.894us         0.03%      83.687us      20.922us       0.000us         0.00%      43.936us      10.984us             4  \n",
      "                                        aten::embedding         0.01%      17.430us         0.03%      90.920us      45.460us       0.000us         0.00%       7.969us       3.984us             2  \n",
      "                                         cudaEventQuery         0.01%      15.885us         0.01%      15.885us       1.986us       0.000us         0.00%       0.000us       0.000us             8  \n",
      "                                         aten::new_ones         0.01%      14.724us         0.02%      48.630us      24.315us       0.000us         0.00%      22.528us      11.264us             2  \n",
      "                                       aten::is_nonzero         0.00%      10.627us         0.11%     308.799us      38.600us       0.000us         0.00%      45.568us       5.696us             8  \n",
      "                                             aten::rsub         0.00%       9.902us         0.02%      43.148us      21.574us       0.000us         0.00%      17.953us       8.976us             2  \n",
      "                                             aten::full         0.00%       9.148us         0.02%      52.052us      13.013us       0.000us         0.00%      38.242us       9.560us             4  \n",
      "                                          aten::__and__         0.00%       6.384us         0.02%      43.016us      21.508us       0.000us         0.00%      25.632us      12.816us             2  \n",
      "                                             aten::ones         0.00%       5.579us         0.01%      26.764us      26.764us       0.000us         0.00%       2.080us       2.080us             1  \n",
      "                                          aten::view_as         0.00%       4.642us         0.00%       7.474us       1.246us       0.000us         0.00%       0.000us       0.000us             6  \n",
      "                                          aten::detach_         0.00%       4.042us         0.00%       6.684us       2.228us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                        aten::new_empty         0.00%       2.828us         0.00%      10.122us       5.061us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                        aten::ones_like         0.00%       2.737us         0.01%      21.344us      21.344us       0.000us         0.00%       2.048us       2.048us             1  \n",
      "                                                detach_         0.00%       2.642us         0.00%       2.642us       0.881us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                    cudaPeekAtLastError         0.00%       1.537us         0.00%       1.537us       0.128us       0.000us         0.00%       0.000us       0.000us            12  \n",
      "                                       aten::lift_fresh         0.00%       0.338us         0.00%       0.338us       0.113us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       9.217us         0.00%       9.217us       1.843us             5  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      60.258us         0.02%      60.258us       8.608us             7  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.136us         0.00%       3.136us       3.136us             1  \n",
      "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      88.033us         0.03%      88.033us       5.869us            15  \n",
      "                         Memcpy DtoH (Device -> Pinned)         0.00%       0.000us         0.00%       0.000us       0.000us       1.193ms         0.47%       1.193ms       8.769us           136  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      15.808us         0.01%      15.808us       5.269us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      26.656us         0.01%      26.656us       6.664us             4  \n",
      "void at_cuda_detail::cub::DeviceScanInitKernel<at_cu...         0.00%       0.000us         0.00%       0.000us       0.000us      14.816us         0.01%      14.816us       4.939us             3  \n",
      "void at_cuda_detail::cub::DeviceScanKernel<at_cuda_d...         0.00%       0.000us         0.00%       0.000us       0.000us      21.184us         0.01%      21.184us       7.061us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      40.738us         0.02%      40.738us       8.148us             5  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      33.666us         0.01%      33.666us       5.611us             6  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.736us         0.00%       4.736us       2.368us             2  \n",
      "void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us       7.969us         0.00%       7.969us       3.984us             2  \n",
      "void dot_kernel<float, 128, 0, cublasDotParams<cubla...         0.00%       0.000us         0.00%       0.000us       0.000us     573.675us         0.23%     573.675us       8.964us            64  \n",
      "void reduce_1Block_kernel<float, 128, 7, cublasGemvT...         0.00%       0.000us         0.00%       0.000us       0.000us     533.801us         0.21%     533.801us       8.341us            64  \n",
      "void (anonymous namespace)::softmax_warp_forward<c10...         0.00%       0.000us         0.00%       0.000us       0.000us     499.466us         0.20%     499.466us       7.804us            64  \n",
      "void at::native::sbtopk::gatherTopK<float, unsigned ...         0.00%       0.000us         0.00%       0.000us       0.000us     738.028us         0.29%     738.028us      11.532us            64  \n",
      "void at::native::bitonicSortKVInPlace<-2, -1, 16, 16...         0.00%       0.000us         0.00%       0.000us       0.000us     703.246us         0.28%     703.246us      10.988us            64  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     651.888us         0.26%     651.888us      10.186us            64  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     572.146us         0.23%     572.146us       8.940us            64  \n",
      "                         Memcpy HtoD (Pinned -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     206.557ms        81.84%     206.557ms     832.892us           248  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       1.037ms         0.41%       1.037ms       7.856us           132  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     837.912us         0.33%     837.912us       6.445us           130  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       1.327ms         0.53%       1.327ms      10.208us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     877.581us         0.35%     877.581us       6.751us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     862.589us         0.34%     862.589us       6.635us           130  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       1.024ms         0.41%       1.024ms       7.874us           130  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       1.033ms         0.41%       1.033ms       7.823us           132  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.002ms         0.79%       2.002ms       7.759us           258  \n",
      "sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x...         0.00%       0.000us         0.00%       0.000us       0.000us       3.006ms         1.19%       3.006ms      23.488us           128  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       1.517ms         0.60%       1.517ms      11.853us           128  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     980.795us         0.39%     980.795us       7.662us           128  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       1.961ms         0.78%       1.961ms       7.541us           260  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     907.606us         0.36%     907.606us       7.091us           128  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     992.625us         0.39%     992.625us       7.755us           128  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.303ms         0.91%       2.303ms       7.198us           320  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       1.113ms         0.44%       1.113ms       8.691us           128  \n",
      "void pytorch_flash::flash_fwd_kernel<pytorch_flash::...         0.00%       0.000us         0.00%       0.000us       0.000us     788.750us         0.31%     788.750us      12.324us            64  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us     933.271us         0.37%     933.271us       6.965us           134  \n",
      "sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x...         0.00%       0.000us         0.00%       0.000us       0.000us       7.353ms         2.91%       7.353ms      54.873us           134  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.106ms         0.44%       1.106ms       8.638us           128  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_...         0.00%       0.000us         0.00%       0.000us       0.000us     212.292us         0.08%     212.292us      53.073us             4  \n",
      "void splitKreduce_kernel<32, 16, int, float, __half,...         0.00%       0.000us         0.00%       0.000us       0.000us      38.978us         0.02%      38.978us       9.745us             4  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       2.638ms         1.05%       2.638ms      21.273us           124  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       3.040ms         1.20%       3.040ms      24.516us           124  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       1.249ms         0.49%       1.249ms      10.071us           124  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us      43.936us         0.02%      43.936us      10.984us             4  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      41.024us         0.02%      41.024us      20.512us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      44.800us         0.02%      44.800us      11.200us             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      17.953us         0.01%      17.953us       8.976us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      20.255us         0.01%      20.255us      10.128us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      43.936us         0.02%      43.936us      10.984us             4  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       7.936us         0.00%       7.936us       7.936us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      20.608us         0.01%      20.608us      10.304us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us      25.632us         0.01%      25.632us      12.816us             2  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      29.088us         0.01%      29.088us      14.544us             2  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       9.825us         0.00%       9.825us       9.825us             1  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       4.704us         0.00%       4.704us       4.704us             1  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     574.226us         0.23%     574.226us       8.972us            64  \n",
      "void at::native::vectorized_elementwise_kernel<2, at...         0.00%       0.000us         0.00%       0.000us       0.000us      11.777us         0.00%      11.777us      11.777us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 282.879ms\n",
      "Self CUDA time total: 252.384ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 1\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 2\n",
    "test_samples = 4\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "fineweb = load_dataset(\"parquet\",data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text'][:test_samples] \n",
    "\n",
    "print(\"output length is {}\".format(output_length))\n",
    "text = fineweb_text[0]\n",
    "inputs = preprocess_data(text, tokenizer)\n",
    "\n",
    "# cached_mlp.clear_load_from_cpu_stats()\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ]\n",
    ") as p:\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(),\n",
    "            max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "print(p.key_averages().table(\n",
    "    sort_by=\"self_cpu_time_total\", row_limit=-1))\n",
    "p.export_chrome_trace(\"./trace-offloading-2.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载到GPU上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:25<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MixtralForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "import json\n",
    "\n",
    "def get_model(model_name, device_map, dtype=torch.bfloat16):\n",
    "    llm = MixtralForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        use_cache=True,\n",
    "        torch_dtype=dtype,\n",
    "    ) \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return llm, tokenizer\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "with open('../quantize/device_map_1.json', 'r') as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "dtype = torch.float16\n",
    "llm, tokenizer = get_model(model_name, device_map, dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 只传一个专家的版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import threading\n",
    "import json\n",
    "from queue import Queue\n",
    "\n",
    "class CachedMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, dtype, sparsity: float = 0.2):\n",
    "        super(CachedMLP, self).__init__()\n",
    "        self.sparsity = sparsity\n",
    "        self.activenum = int((1 - sparsity) * hidden_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dtype = dtype\n",
    "\n",
    "        # GPU 缓存张量\n",
    "        self.w1_gpu = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda')\n",
    "        self.w2_gpu = torch.empty((self.input_dim, self.activenum), dtype=self.dtype, device='cuda')\n",
    "        self.w3_gpu = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda')\n",
    "\n",
    "        # Pinned Memory 缓冲区\n",
    "        self.register_buffer('sparse_w1_cpu', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "        self.register_buffer('sparse_w2_cpu', torch.empty((self.input_dim, self.activenum), dtype=self.dtype, device='cpu'))\n",
    "        self.register_buffer('sparse_w3_cpu', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "        self.sparse_w1_cpu = self.sparse_w1_cpu.pin_memory()\n",
    "        self.sparse_w2_cpu = self.sparse_w2_cpu.pin_memory()\n",
    "        self.sparse_w3_cpu = self.sparse_w3_cpu.pin_memory()\n",
    "\n",
    "        # 统计信息\n",
    "        self.load_from_cpu_time = 0.0\n",
    "        self.load_from_cpu_calls = 0\n",
    "\n",
    "    def load_from_cpu(self, cpu_mlp, stream: torch.cuda.Stream):\n",
    "        \"\"\"\n",
    "        从CPU加载参数，并使用指定的CUDA流进行异步复制到GPU。\n",
    "        \n",
    "        参数:\n",
    "            cpu_mlp: 包含CPU上参数的字典。\n",
    "            stream: 用于数据传输的CUDA流。\n",
    "        \"\"\"\n",
    "        # 从CPU加载参数\n",
    "        self.sparse_w1_cpu.copy_(cpu_mlp['w1'].data[:self.activenum, :])\n",
    "        self.sparse_w2_cpu.copy_(cpu_mlp['w2'].data[:, :self.activenum])\n",
    "        self.sparse_w3_cpu.copy_(cpu_mlp['w3'].data[:self.activenum, :])\n",
    "\n",
    "        # 异步复制到GPU\n",
    "        with torch.cuda.stream(stream):\n",
    "            self.w1_gpu.copy_(self.sparse_w1_cpu, non_blocking=True)\n",
    "            self.w2_gpu.copy_(self.sparse_w2_cpu, non_blocking=True)\n",
    "            self.w3_gpu.copy_(self.sparse_w3_cpu, non_blocking=True)\n",
    "\n",
    "    def get_load_from_cpu_stats(self):\n",
    "        if self.load_from_cpu_calls == 0:\n",
    "            return 0.0, 0.0\n",
    "        avg_time = self.load_from_cpu_time / self.load_from_cpu_calls\n",
    "        return self.load_from_cpu_time, avg_time\n",
    "\n",
    "    def clear_load_from_cpu_stats(self):\n",
    "        self.load_from_cpu_time = 0.0\n",
    "        self.load_from_cpu_calls = 0\n",
    "\n",
    "def convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.9):\n",
    "    ### 其他部分存放在GPU上\n",
    "    llm.model.embed_tokens.cuda()\n",
    "    for i in range(len(llm.model.layers)):\n",
    "        llm.model.layers[i].self_attn.cuda()\n",
    "        llm.model.layers[i].input_layernorm.cuda()\n",
    "        llm.model.layers[i].post_attention_layernorm.cuda()\n",
    "        llm.model.layers[i].block_sparse_moe.gate.cuda()\n",
    "    ### 第0层的专家存放在GPU上\n",
    "    for j in range(len(llm.model.layers[0].block_sparse_moe.experts)):\n",
    "        llm.model.layers[0].block_sparse_moe.experts[j].cuda()\n",
    "\n",
    "    llm.model.norm.cuda()\n",
    "    llm.lm_head.cuda()\n",
    "    \n",
    "    # 创建两个共享的CachedMLP实例\n",
    "    buffer0 = CachedMLP(\n",
    "        input_dim=llm.config.hidden_size,\n",
    "        hidden_dim=llm.config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "        sparsity=sparsity\n",
    "    )\n",
    "    buffer1 = CachedMLP(\n",
    "        input_dim=llm.config.hidden_size,\n",
    "        hidden_dim=llm.config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "        sparsity=sparsity\n",
    "    )\n",
    "    cached_mlps = [buffer0, buffer1]\n",
    "    \n",
    "    for i, layer in enumerate(llm.model.layers):\n",
    "        if i==0:\n",
    "            continue\n",
    "        # 将专家的forward方法替换为PipelineLLM管理的方式\n",
    "        for j, expert in enumerate(layer.block_sparse_moe.experts):\n",
    "            expert.cpu_mlp = {\n",
    "                \"w1\": expert.w1.cpu().weight,\n",
    "                \"w2\": expert.w2.cpu().weight,\n",
    "                \"w3\": expert.w3.cpu().weight,\n",
    "            }\n",
    "            # 替换forward方法为直接调用CachedMLP的forward（需要在pipelineLLM里面替换)\n",
    "            # expert.forward = lambda x, cached_mlp=cached_mlp, cpu_mlp=expert.cpu_mlp: cached_mlp(x, cpu_mlp)\n",
    "    return llm, cached_mlps\n",
    "\n",
    "class PipelineLLM:\n",
    "    def __init__(self, llm, cached_mlps):\n",
    "        \"\"\"\n",
    "        初始化 PipelineLLM，替换模型每一层的 forward 方法。\n",
    "        \n",
    "        参数:\n",
    "            llm: 原始的大模型\n",
    "            cached_mlps: 两个 CachedMLP 实例列表\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.cached_mlps = cached_mlps  # [buffer0, buffer1]\n",
    "        self.num_layers = len(llm.model.layers)\n",
    "        self.lock = threading.Lock()\n",
    "        self.use_buffer0 = True  # 标记当前使用哪个缓冲区\n",
    "\n",
    "        # 创建两个共享的CUDA流\n",
    "        self.stream0 = torch.cuda.Stream()\n",
    "        self.stream1 = torch.cuda.Stream()\n",
    "\n",
    "        # 初始化加载第一个和第二个层的参数\n",
    "        self._load_layer(1, buffer_index=0)\n",
    "        self._load_layer(1, buffer_index=1)\n",
    "\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "        self._replace_forward_methods()\n",
    "    \n",
    "    def _load_layer(self, layer_idx, buffer_index):\n",
    "        \"\"\"\n",
    "        加载指定层的参数到指定的缓冲区。\n",
    "        \n",
    "        参数:\n",
    "            layer_idx: 层的索引\n",
    "            buffer_index: 缓冲区的索引（0 或 1）\n",
    "        \"\"\"\n",
    "        layer = self.llm.model.layers[layer_idx]\n",
    "        expert = layer.block_sparse_moe.experts[0]\n",
    "        cpu_mlp = expert.cpu_mlp\n",
    "        buffer = self.cached_mlps[buffer_index]\n",
    "        stream = self.stream0 if buffer_index == 0 else self.stream1\n",
    "\n",
    "        # 异步加载参数\n",
    "        buffer.load_from_cpu(cpu_mlp, stream)\n",
    "\n",
    "    def _replace_forward_methods(self):\n",
    "        \"\"\"\n",
    "        替换模型每一层的 forward 方法，添加参数预加载逻辑和注意力计算。\n",
    "        \"\"\"\n",
    "        for i, layer in enumerate(self.llm.model.layers):\n",
    "            def new_forward(hidden_states: torch.Tensor,\n",
    "                            attention_mask: Optional[torch.Tensor] = None,\n",
    "                            position_ids: Optional[torch.LongTensor] = None,\n",
    "                            past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "                            output_attentions: Optional[bool] = False,\n",
    "                            output_router_logits: Optional[bool] = False,\n",
    "                            use_cache: Optional[bool] = False,\n",
    "                            cache_position: Optional[torch.LongTensor] = None,\n",
    "                            layer_idx=i):\n",
    "                # print(f\"in layer {layer_idx}\")\n",
    "                with self.lock:\n",
    "                    # 选择当前使用的缓冲区\n",
    "                    current_buffer = self.cached_mlps[0] if self.use_buffer0 else self.cached_mlps[1]\n",
    "                    current_stream = self.stream0 if self.use_buffer0 else self.stream1\n",
    "\n",
    "                    # 切换缓冲区用于下一次\n",
    "                    next_buffer_index = 1 if self.use_buffer0 else 0\n",
    "                    next_buffer = self.cached_mlps[next_buffer_index]\n",
    "                    next_stream = self.stream1 if self.use_buffer0 else self.stream0\n",
    "\n",
    "                    # 预加载下一层的参数\n",
    "                    next_layer_idx = layer_idx + 1\n",
    "                    if next_layer_idx < self.num_layers:\n",
    "                        self._load_layer(next_layer_idx, buffer_index=next_buffer_index)\n",
    "                    \n",
    "                    # 切换缓冲区\n",
    "                    self.use_buffer0 = not self.use_buffer0\n",
    "\n",
    "                    # 处理当前层\n",
    "                    residual = hidden_states\n",
    "                    hidden_states = layer.input_layernorm(hidden_states)\n",
    "\n",
    "                    # Self Attention\n",
    "                    hidden_states, self_attn_weights, present_key_value = layer.self_attn(\n",
    "                        hidden_states=hidden_states,\n",
    "                        attention_mask=attention_mask,\n",
    "                        position_ids=position_ids,\n",
    "                        past_key_value=past_key_value,\n",
    "                        output_attentions=output_attentions,\n",
    "                        use_cache=use_cache,\n",
    "                        cache_position=cache_position,\n",
    "                    )\n",
    "                    hidden_states = residual + hidden_states\n",
    "\n",
    "                    # Fully Connected\n",
    "                    residual = hidden_states\n",
    "                    hidden_states = layer.post_attention_layernorm(hidden_states)\n",
    "\n",
    "                    # 使用当前缓冲区进行 MLP 计算\n",
    "                    batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "                    hidden_states = hidden_states.view(-1, hidden_dim)\n",
    "                    # 仅使用第一个专家\n",
    "                    expert_layer = layer.block_sparse_moe.experts[0]\n",
    "\n",
    "                    w3_output = torch.matmul(hidden_states, current_buffer.w3_gpu.T)\n",
    "                    w1_output = self.activation(torch.matmul(hidden_states, current_buffer.w1_gpu.T))\n",
    "                    w2 = current_buffer.w2_gpu.T\n",
    "                    final_hidden_states = torch.matmul(w1_output * w3_output, w2)\n",
    "\n",
    "                    final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n",
    "\n",
    "                    hidden_states = residual + final_hidden_states\n",
    "\n",
    "                    outputs = (hidden_states,)\n",
    "\n",
    "                    if output_attentions:\n",
    "                        outputs += (self_attn_weights,)\n",
    "\n",
    "                    if use_cache:\n",
    "                        outputs += (present_key_value,)\n",
    "\n",
    "                    return outputs\n",
    "            # 替换 forward 方法\n",
    "            layer.forward = new_forward\n",
    "\n",
    "    def _async_load(self, layer_idx, buffer_index):\n",
    "        \"\"\"\n",
    "        异步加载 MLP 参数到指定缓冲区，使用共享的CUDA流。\n",
    "        \"\"\"\n",
    "        self._load_layer(layer_idx, buffer_index)\n",
    "\n",
    "# 将模型转换为使用CachedMLP的版本\n",
    "llm, cached_mlps = convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.9)\n",
    "\n",
    "# 创建流水线模型\n",
    "pipeline_llm = PipelineLLM(llm, cached_mlps).llm\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
