{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先都加载到cpu上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using atten... sdpa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/venv/dilab/floe/hqq/hqq/models/base.py:251: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(cls.get_weight_file(save_dir), map_location=map_location)\n",
      "100%|██████████| 32/32 [00:00<00:00, 219.54it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 1188.29it/s]\n",
      "100%|██████████| 32/32 [03:53<00:00,  7.31s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"\n",
    "from modeling_mixtral import MixtralForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import json\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "dtype = torch.float16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "### HQQ量化\n",
    "from hqq.core.quantize import *\n",
    "from hqq.models.hf.mixtral import MixtralPatch\n",
    "import transformers\n",
    "from hqq.models.base import BaseHQQModel\n",
    "from accelerate import init_empty_weights\n",
    "\n",
    "save_dir = './hqqsaved'\n",
    "\n",
    "class BaseHQQHFModel(BaseHQQModel):\n",
    "    # Save model architecture\n",
    "    @classmethod\n",
    "    def cache_model(cls, model, save_dir):\n",
    "        model.config.save_pretrained(save_dir)\n",
    "\n",
    "    # Create empty model from config\n",
    "    @classmethod\n",
    "    def create_model(cls, save_dir, kwargs):\n",
    "        model_kwargs = {}\n",
    "        for key in [\"attn_implementation\"]:\n",
    "            if key in kwargs:\n",
    "                model_kwargs[key] = kwargs[key]\n",
    "\n",
    "        config = transformers.AutoConfig.from_pretrained(\n",
    "            cls.get_config_file(save_dir)\n",
    "        )\n",
    "\n",
    "        with init_empty_weights():\n",
    "            model = MixtralForCausalLM._from_config(config, **model_kwargs)\n",
    "\n",
    "        return model\n",
    "\n",
    "class MixtralHQQ(MixtralPatch, BaseHQQHFModel):\n",
    "    pass\n",
    "\n",
    "### 从保存的权重中加载\n",
    "llm = MixtralHQQ.from_quantized(save_dir, compute_dtype=dtype, device='cpu')\n",
    "HQQLinear.set_backend(HQQBackend.PYTORCH)\n",
    "\n",
    "backend       = \"gemlite\" #'torchao_int4' #\"torchao_int4\" (4-bit only) or \"gemlite\" (4-bit + 2-bit)\n",
    "# #Optimize\n",
    "from hqq.utils.patching import prepare_for_inference\n",
    "prepare_for_inference(llm, backend=backend, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "active neural num  2867\n",
      "active neural num  2867\n"
     ]
    }
   ],
   "source": [
    "from pipelinellm import convert_mixtral_to_cached_mlp, PipelineLLM\n",
    "llm, cached_mlps = convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.8, backends=backend)\n",
    "\n",
    "# 创建流水线模型\n",
    "PLLM = PipelineLLM(llm, cached_mlps, 1, 3, training_epoch=20, print_layer_info=False) ### use ep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 尝试替换torch.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w3_forward = torch.compile(llm.model.layers[0].block_sparse_moe.experts[1].w3.forward, fullgraph=True, mode=\"reduce-overhead\")\n",
    "# w3 = llm.model.layers[0].block_sparse_moe.experts[1].w3\n",
    "# w3.cuda(0)\n",
    "# w3.forward = w3_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试时间开销"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warm up ...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "\n",
    "input_length = 10\n",
    "MAX_LENGTH = 1\n",
    "output_length = 10\n",
    "test_samples = 1\n",
    "device_id = 0\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "filt_type = fineweb_path.split('.')[-1]\n",
    "fineweb = load_dataset(filt_type, data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text']\n",
    "\n",
    "print(\"warm up ...\")\n",
    "# 预热（避免第一次运行时的额外开销）\n",
    "for text in fineweb_text[:10]:\n",
    "    inputs = preprocess_data(text, tokenizer)\n",
    "    with torch.no_grad():\n",
    "        output = llm(input_ids=inputs[\"input_ids\"].cuda(device_id), attention_mask=inputs[\"attention_mask\"].cuda(device_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "\n",
    "input_length = 10\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 256\n",
    "test_samples = 10\n",
    "device_id = 0\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "# print(\"warm up ...\")\n",
    "# # 预热（避免第一次运行时的额外开销）\n",
    "# for text in fineweb_text[:5] :\n",
    "#     inputs = preprocess_data(text, tokenizer)\n",
    "#     with torch.no_grad():\n",
    "#         output = llm(input_ids=inputs[\"input_ids\"].cuda(device_id), attention_mask=inputs[\"attention_mask\"].cuda(device_id))\n",
    "\n",
    "generated_all = 0\n",
    "prefill_time, decode_time = 0, 0\n",
    "reloaded_experts = 0\n",
    "print(\"output length is {}\".format(output_length))\n",
    "for text in fineweb_text[2:2+test_samples] :\n",
    "    inputs = preprocess_data(text, tokenizer)\n",
    "    ### 清空统计数据\n",
    "    PLLM.get_prefill_time()\n",
    "    PLLM.get_reload_experts()\n",
    "\n",
    "    # 测试时间\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # 开始计时\n",
    "    torch.cuda.synchronize()\n",
    "    start_event.record()\n",
    "\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(device_id),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(device_id),\n",
    "            max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.pad_token_id, \n",
    "            # cache_implementation=\"static\" ## moe not support\n",
    "        )\n",
    "\n",
    "    # 结束计时\n",
    "    end_event.record()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # 计算时间\n",
    "    elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "    decode_time += elapsed_time\n",
    "    cur_prefill_time = PLLM.get_prefill_time()\n",
    "    prefill_time += cur_prefill_time\n",
    "    print(f\"Generated length: {len(output[0]) - input_length}\", f\"Time taken: {elapsed_time:.2f} s,\", f\"prefill time: {cur_prefill_time:.2f} s\")\n",
    "    # print(output)\n",
    "    print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "\n",
    "    generated_all += (len(output[0]) - input_length - 1)\n",
    "    reloaded_experts += PLLM.get_reload_experts()\n",
    "\n",
    "timepertoken = (decode_time - prefill_time) / (generated_all)\n",
    "print(\"decode phase speed:\", '{:.4f}'.format(1/timepertoken) , 'token/s')\n",
    "print(\"the number of reloaded experts per token:\", '{:.3f}'.format(reloaded_experts / generated_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################################################################\n",
    "#Save gemlite cache\n",
    "if(backend == 'gemlite'):\n",
    "\tgemlite.core.GemLiteLinear.cache_config('/tmp/gemlite_config.json') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.profile\n",
    "\n",
    "attention使用sdpa实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 2\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::index        43.78%     299.804ms        44.92%     307.630ms     465.400us       1.682ms         0.33%       2.450ms       3.706us           661  \n",
      "                                  cudaStreamSynchronize        19.20%     131.464ms        19.20%     131.464ms     140.154us       0.000us         0.00%       0.000us       0.000us           938  \n",
      "                                            aten::copy_        10.37%      70.993ms        14.49%      99.254ms      54.416us     452.721ms        88.67%     452.721ms     248.202us          1824  \n",
      "gemlite::gemv_revsplitK_A16fWnO16f_int32packing_forw...         7.15%      48.956ms         8.27%      56.661ms     170.665us       8.637ms         1.69%       9.999ms      30.118us           332  \n",
      "                                       cudaLaunchKernel         3.88%      26.541ms         3.88%      26.541ms       5.111us       0.000us         0.00%       0.000us       0.000us          5193  \n",
      "                                        cudaMemcpyAsync         3.72%      25.469ms         3.72%      25.469ms      18.389us       0.000us         0.00%       0.000us       0.000us          1385  \n",
      "                                               aten::mm         1.69%      11.605ms         2.41%      16.530ms      26.072us      10.588ms         2.07%      10.588ms      16.701us           634  \n",
      "                                             aten::topk         0.81%       5.553ms         1.45%       9.908ms      30.024us      17.959ms         3.52%      17.959ms      54.422us           330  \n",
      "                                              aten::mul         0.76%       5.203ms         1.27%       8.665ms      11.166us       3.911ms         0.77%       3.911ms       5.040us           776  \n",
      "                                    aten::empty_strided         0.62%       4.229ms         0.62%       4.229ms       5.247us       0.000us         0.00%       0.000us       0.000us           806  \n",
      "                                            aten::empty         0.49%       3.375ms         0.49%       3.375ms       3.599us       0.000us         0.00%       0.000us       0.000us           938  \n",
      "                                            aten::slice         0.49%       3.362ms         0.59%       4.052ms       2.793us       0.000us         0.00%       0.000us       0.000us          1451  \n",
      "                                              aten::add         0.45%       3.061ms         0.74%       5.100ms      11.234us       1.979ms         0.39%       1.979ms       4.358us           454  \n",
      "                                           aten::select         0.39%       2.654ms         0.46%       3.171ms       2.645us       0.000us         0.00%       0.000us       0.000us          1199  \n",
      "                                       aten::as_strided         0.34%       2.333ms         0.34%       2.333ms       0.492us       0.000us         0.00%       0.000us       0.000us          4740  \n",
      "                                             aten::view         0.34%       2.307ms         0.34%       2.307ms       0.947us       0.000us         0.00%       0.000us       0.000us          2436  \n",
      "                                         aten::_to_copy         0.30%       2.068ms         4.76%      32.619ms      44.140us       0.000us         0.00%       4.677ms       6.329us           739  \n",
      "                                              aten::cat         0.27%       1.816ms         0.41%       2.789ms      14.228us       1.215ms         0.24%       1.215ms       6.198us           196  \n",
      "                              aten::_local_scalar_dense         0.25%       1.722ms         2.03%      13.893ms      32.845us       2.661ms         0.52%       2.661ms       6.290us           423  \n",
      "                                              aten::abs         0.24%       1.659ms         0.96%       6.579ms      16.125us     855.982us         0.17%       1.712ms       4.196us           408  \n",
      "                                             aten::mean         0.23%       1.581ms         0.32%       2.218ms      17.059us     931.450us         0.18%     931.450us       7.165us           130  \n",
      "                                               aten::to         0.23%       1.567ms         4.99%      34.186ms      33.780us       0.000us         0.00%       4.677ms       4.621us          1012  \n",
      "                                         cuLaunchKernel         0.23%       1.563ms         0.23%       1.563ms       4.708us       0.000us         0.00%       0.000us       0.000us           332  \n",
      "                                            aten::fill_         0.22%       1.528ms         0.54%       3.669ms      10.696us       1.387ms         0.27%       1.387ms       4.045us           343  \n",
      "                                              aten::pow         0.21%       1.458ms         0.32%       2.189ms      16.838us     567.906us         0.11%     567.906us       4.369us           130  \n",
      "                                          aten::reshape         0.19%       1.273ms         0.64%       4.364ms       3.714us       0.000us         0.00%     685.160us       0.583us          1175  \n",
      "                                           aten::matmul         0.18%       1.264ms         2.69%      18.392ms      29.009us       0.000us         0.00%      10.588ms      16.701us           634  \n",
      "                                         aten::_softmax         0.17%       1.185ms         0.28%       1.886ms      14.965us     659.621us         0.13%     659.621us       5.235us           126  \n",
      "                                               aten::eq         0.16%       1.117ms         0.27%       1.881ms      12.213us     905.844us         0.18%     905.844us       5.882us           154  \n",
      "                                             aten::silu         0.16%       1.099ms         0.26%       1.796ms      13.406us     608.768us         0.12%     608.768us       4.543us           134  \n",
      "                         aten::_flash_attention_forward         0.15%       1.016ms         0.35%       2.407ms      37.602us     584.873us         0.11%     584.873us       9.139us            64  \n",
      "                                        aten::transpose         0.15%       1.015ms         0.22%       1.484ms       1.630us       0.000us         0.00%       0.000us       0.000us           910  \n",
      "                                              aten::neg         0.14%     961.435us         0.22%       1.528ms      11.939us     695.950us         0.14%     695.950us       5.437us           128  \n",
      "                                            aten::zero_         0.14%     942.676us         0.65%       4.461ms      13.438us       0.000us         0.00%       1.362ms       4.104us           332  \n",
      "                                            aten::rsqrt         0.14%     933.266us         0.22%       1.523ms      11.715us     569.582us         0.11%     569.582us       4.381us           130  \n",
      "                                              aten::sum         0.13%     868.989us         0.18%       1.230ms      19.222us     552.269us         0.11%     552.269us       8.629us            64  \n",
      "                                  cudaFuncGetAttributes         0.12%     805.503us         0.12%     805.503us       3.098us       0.000us         0.00%       0.000us       0.000us           260  \n",
      "                                           aten::linear         0.12%     791.068us         2.33%      15.989ms      40.175us       0.000us         0.00%       6.064ms      15.237us           398  \n",
      "                                    cudaLaunchKernelExC         0.11%     747.056us         0.11%     747.056us       5.575us       0.000us         0.00%       0.000us       0.000us           134  \n",
      "                                                aten::t         0.11%     732.398us         0.20%       1.390ms       3.492us       0.000us         0.00%       0.000us       0.000us           398  \n",
      "                                               aten::ne         0.10%     674.119us         0.18%       1.227ms      19.794us     237.471us         0.05%     237.471us       3.830us            62  \n",
      "                     aten::scaled_dot_product_attention         0.10%     659.172us         0.56%       3.822ms      59.724us       0.000us         0.00%     584.873us       9.139us            64  \n",
      "                                          aten::resize_         0.09%     612.300us         0.09%     612.300us       2.972us       0.000us         0.00%       0.000us       0.000us           206  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.08%     563.574us         0.08%     563.574us       0.549us       0.000us         0.00%       0.000us       0.000us          1027  \n",
      "                                             aten::div_         0.08%     561.908us         0.13%     899.048us      14.048us     440.134us         0.09%     440.134us       6.877us            64  \n",
      "                                             aten::item         0.07%     464.188us         2.10%      14.358ms      33.942us       0.000us         0.00%       2.661ms       6.290us           423  \n",
      "              aten::_scaled_dot_product_flash_attention         0.06%     440.341us         0.46%       3.163ms      49.425us       0.000us         0.00%     584.873us       9.139us            64  \n",
      "                                        aten::unsqueeze         0.06%     426.058us         0.08%     557.140us       2.126us       0.000us         0.00%       0.000us       0.000us           262  \n",
      "                                            aten::addmm         0.06%     404.345us         0.07%     473.858us      39.488us     101.922us         0.02%     101.922us       8.493us            12  \n",
      "                                          aten::permute         0.05%     320.562us         0.05%     375.306us       3.027us       0.000us         0.00%       0.000us       0.000us           124  \n",
      "                                            aten::clone         0.04%     298.016us         0.34%       2.298ms      17.149us       0.000us         0.00%     700.391us       5.227us           134  \n",
      "                                     aten::_unsafe_view         0.04%     290.203us         0.04%     290.203us       0.752us       0.000us         0.00%       0.000us       0.000us           386  \n",
      "                                       aten::empty_like         0.04%     282.730us         0.15%       1.008ms       5.115us       0.000us         0.00%       0.000us       0.000us           197  \n",
      "                                           aten::expand         0.04%     276.662us         0.05%     342.444us       2.675us       0.000us         0.00%       0.000us       0.000us           128  \n",
      "                                       aten::is_nonzero         0.04%     250.108us         1.17%       7.986ms      37.145us       0.000us         0.00%       1.491ms       6.936us           215  \n",
      "                                 cudaDeviceGetAttribute         0.03%     238.182us         0.03%     238.182us       0.470us       0.000us         0.00%       0.000us       0.000us           507  \n",
      "                                          aten::softmax         0.03%     229.019us         0.31%       2.115ms      16.783us       0.000us         0.00%     659.621us       5.235us           126  \n",
      "                                          aten::numpy_T         0.02%     138.856us         0.08%     514.162us       4.146us       0.000us         0.00%       0.000us       0.000us           124  \n",
      "                                   cudaFuncSetAttribute         0.02%     121.786us         0.02%     121.786us       1.416us       0.000us         0.00%       0.000us       0.000us            86  \n",
      "                                  cudaStreamIsCapturing         0.01%      70.742us         0.01%      70.742us       1.105us       0.000us         0.00%       0.000us       0.000us            64  \n",
      "                                           aten::cumsum         0.01%      67.706us         0.02%     115.416us      38.472us      15.712us         0.00%      15.712us       5.237us             3  \n",
      "                                             aten::isin         0.01%      66.028us         0.04%     286.196us      95.399us       0.000us         0.00%      28.416us       9.472us             3  \n",
      "                                      aten::result_type         0.01%      63.942us         0.01%      63.942us       0.492us       0.000us         0.00%       0.000us       0.000us           130  \n",
      "                                        cudaMemsetAsync         0.01%      54.918us         0.01%      54.918us       9.153us       0.000us         0.00%       0.000us       0.000us             6  \n",
      "                                              aten::sub         0.01%      54.438us         0.01%      82.777us      16.555us      11.905us         0.00%      11.905us       2.381us             5  \n",
      "                                       aten::bitwise_or         0.01%      46.084us         0.01%      68.778us      17.194us       9.664us         0.00%       9.664us       2.416us             4  \n",
      "                                              aten::any         0.01%      42.860us         0.02%     103.952us      20.790us       0.000us         0.00%      11.485us       2.297us             5  \n",
      "                                     aten::index_select         0.01%      42.668us         0.01%      72.218us      36.109us       8.194us         0.00%       8.194us       4.097us             2  \n",
      "                                           aten::argmax         0.01%      37.943us         0.01%      51.967us      25.983us      25.473us         0.00%      25.473us      12.736us             2  \n",
      "                                     aten::masked_fill_         0.01%      35.425us         0.01%      49.368us      24.684us       4.704us         0.00%       4.704us       2.352us             2  \n",
      "                                              aten::max         0.00%      33.149us         0.01%      53.193us      26.597us       9.153us         0.00%       9.153us       4.577us             2  \n",
      "                                         cudaEventQuery         0.00%      30.823us         0.00%      30.823us       1.926us       0.000us         0.00%       0.000us       0.000us            16  \n",
      "                                               aten::ge         0.00%      26.866us         0.01%      38.514us      19.257us       4.704us         0.00%       4.704us       2.352us             2  \n",
      "                                              aten::all         0.00%      24.730us         0.01%      42.218us      21.109us       4.512us         0.00%       6.816us       3.408us             2  \n",
      "                                               aten::lt         0.00%      22.745us         0.00%      29.739us      29.739us       2.465us         0.00%       2.465us       2.465us             1  \n",
      "                                      aten::bitwise_not         0.00%      22.248us         0.00%      32.511us      16.255us       4.831us         0.00%       4.831us       2.416us             2  \n",
      "                                      aten::bitwise_and         0.00%      20.693us         0.00%      32.568us      16.284us       5.793us         0.00%       5.793us       2.896us             2  \n",
      "                                  cudaDeviceSynchronize         0.00%      20.524us         0.00%      20.524us      20.524us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                        aten::embedding         0.00%      19.399us         0.01%      96.367us      48.184us       0.000us         0.00%       8.194us       4.097us             2  \n",
      "                                     aten::resolve_conj         0.00%      16.259us         0.00%      16.259us       0.203us       0.000us         0.00%       0.000us       0.000us            80  \n",
      "                                      aten::resolve_neg         0.00%      13.042us         0.00%      13.042us       0.163us       0.000us         0.00%       0.000us       0.000us            80  \n",
      "                                             aten::rsub         0.00%      12.524us         0.01%      46.207us      23.103us       0.000us         0.00%       5.057us       2.529us             2  \n",
      "                                         aten::new_ones         0.00%      10.259us         0.01%      51.206us      25.603us       0.000us         0.00%       5.216us       2.608us             2  \n",
      "                                             aten::full         0.00%       9.299us         0.01%      55.679us      13.920us       0.000us         0.00%       8.801us       2.200us             4  \n",
      "                                          aten::view_as         0.00%       6.851us         0.00%      11.417us       1.903us       0.000us         0.00%       0.000us       0.000us             6  \n",
      "                                          aten::detach_         0.00%       6.237us         0.00%       9.059us       3.020us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                           aten::__or__         0.00%       5.580us         0.01%      74.358us      18.589us       0.000us         0.00%       9.664us       2.416us             4  \n",
      "                                             aten::ones         0.00%       5.452us         0.00%      23.827us      23.827us       0.000us         0.00%       2.080us       2.080us             1  \n",
      "                                        aten::new_empty         0.00%       3.874us         0.00%      11.607us       5.804us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                          aten::__and__         0.00%       3.602us         0.01%      36.170us      18.085us       0.000us         0.00%       5.793us       2.896us             2  \n",
      "                                                detach_         0.00%       2.822us         0.00%       2.822us       0.941us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                        aten::ones_like         0.00%       2.772us         0.00%      22.029us      22.029us       0.000us         0.00%       2.240us       2.240us             1  \n",
      "                                    cudaPeekAtLastError         0.00%       1.203us         0.00%       1.203us       0.100us       0.000us         0.00%       0.000us       0.000us            12  \n",
      "                                       aten::lift_fresh         0.00%       0.392us         0.00%       0.392us       0.131us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     777.360us         0.15%     777.360us       6.026us           129  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      15.522us         0.00%      15.522us       2.217us             7  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     630.469us         0.12%     630.469us       5.044us           125  \n",
      "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     613.705us         0.12%     613.705us       4.353us           141  \n",
      "                         Memcpy DtoH (Device -> Pinned)         0.00%       0.000us         0.00%       0.000us       0.000us       2.661ms         0.52%       2.661ms       6.290us           423  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       7.169us         0.00%       7.169us       2.390us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       9.536us         0.00%       9.536us       2.384us             4  \n",
      "void at_cuda_detail::cub::DeviceScanInitKernel<at_cu...         0.00%       0.000us         0.00%       0.000us       0.000us       7.296us         0.00%       7.296us       2.432us             3  \n",
      "void at_cuda_detail::cub::DeviceScanKernel<at_cuda_d...         0.00%       0.000us         0.00%       0.000us       0.000us       8.416us         0.00%       8.416us       2.805us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      12.128us         0.00%      12.128us       2.426us             5  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      14.464us         0.00%      14.464us       2.411us             6  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.704us         0.00%       4.704us       2.352us             2  \n",
      "void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us       8.194us         0.00%       8.194us       4.097us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     696.229us         0.14%     696.229us       5.274us           132  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     567.906us         0.11%     567.906us       4.369us           130  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     931.450us         0.18%     931.450us       7.165us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     558.313us         0.11%     558.313us       4.295us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     569.582us         0.11%     569.582us       4.381us           130  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     670.926us         0.13%     670.926us       5.161us           130  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       1.128ms         0.22%       1.128ms       5.814us           194  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.102ms         0.22%       1.102ms       4.271us           258  \n",
      "sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x...         0.00%       0.000us         0.00%       0.000us       0.000us       2.599ms         0.51%       2.599ms      20.303us           128  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       1.338ms         0.26%       1.338ms       9.983us           134  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.678ms         0.33%       1.678ms       6.659us           252  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       2.127ms         0.42%       2.127ms       5.540us           384  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     695.950us         0.14%     695.950us       5.437us           128  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     796.520us         0.16%     796.520us       6.223us           128  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.410ms         0.28%       1.410ms       4.406us           320  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     685.160us         0.13%     685.160us       5.353us           128  \n",
      "void pytorch_flash::flash_fwd_kernel<pytorch_flash::...         0.00%       0.000us         0.00%       0.000us       0.000us     584.873us         0.11%     584.873us       9.139us            64  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     608.768us         0.12%     608.768us       4.543us           134  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       2.133ms         0.42%       2.133ms      16.407us           130  \n",
      "void (anonymous namespace)::softmax_warp_forward<c10...         0.00%       0.000us         0.00%       0.000us       0.000us     659.621us         0.13%     659.621us       5.235us           126  \n",
      "void at::native::sbtopk::gatherTopK<float, unsigned ...         0.00%       0.000us         0.00%       0.000us       0.000us       1.227ms         0.24%       1.227ms       9.740us           126  \n",
      "void at::native::bitonicSortKVInPlace<-2, -1, 16, 16...         0.00%       0.000us         0.00%       0.000us       0.000us       1.133ms         0.22%       1.133ms       8.995us           126  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.362ms         0.27%       1.362ms       4.104us           332  \n",
      "          gemv_revsplitK_A16fWnO16f_int32packing_kernel         0.00%       0.000us         0.00%       0.000us       0.000us       8.637ms         1.69%       8.637ms      26.014us           332  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     855.982us         0.17%     855.982us       4.196us           204  \n",
      "void at::native::sbtopk::gatherTopK<c10::Half, unsig...         0.00%       0.000us         0.00%       0.000us       0.000us      11.694ms         2.29%      11.694ms      57.322us           204  \n",
      "void at::native::radixSortKVInPlace<-2, -1, 128, 32,...         0.00%       0.000us         0.00%       0.000us       0.000us       3.905ms         0.76%       3.905ms      19.142us           204  \n",
      "                       Memcpy DtoH (Device -> Pageable)         0.00%       0.000us         0.00%       0.000us       0.000us       2.075ms         0.41%       2.075ms       7.307us           284  \n",
      "                         Memcpy HtoD (Pinned -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     446.745ms        87.50%     446.745ms       1.095ms           408  \n",
      "void dot_kernel<float, 128, 0, cublasDotParams<cubla...         0.00%       0.000us         0.00%       0.000us       0.000us     730.704us         0.14%     730.704us       6.089us           120  \n",
      "void reduce_1Block_kernel<float, 128, 7, cublasGemvT...         0.00%       0.000us         0.00%       0.000us       0.000us     658.537us         0.13%     658.537us       5.488us           120  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     552.269us         0.11%     552.269us       8.629us            64  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     440.134us         0.09%     440.134us       6.877us            64  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us      36.000us         0.01%      36.000us       6.000us             6  \n",
      "sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x...         0.00%       0.000us         0.00%       0.000us       0.000us     406.216us         0.08%     406.216us      67.703us             6  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_...         0.00%       0.000us         0.00%       0.000us       0.000us     214.628us         0.04%     214.628us      53.657us             4  \n",
      "void splitKreduce_kernel<32, 16, int, float, __half,...         0.00%       0.000us         0.00%       0.000us       0.000us      39.905us         0.01%      39.905us       9.976us             4  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     495.758us         0.10%     495.758us       5.902us            84  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       2.535ms         0.50%       2.535ms      20.441us           124  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us      13.920us         0.00%      13.920us       3.480us             4  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      25.473us         0.00%      25.473us      12.736us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      10.625us         0.00%      10.625us       2.656us             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.057us         0.00%       5.057us       2.529us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.216us         0.00%       5.216us       2.608us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       9.664us         0.00%       9.664us       2.416us             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.831us         0.00%       4.831us       2.416us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       5.793us         0.00%       5.793us       2.896us             2  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       9.153us         0.00%       9.153us       4.577us             2  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.488us         0.00%       3.488us       3.488us             1  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       4.512us         0.00%       4.512us       4.512us             1  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     404.456us         0.08%     404.456us       6.320us            64  \n",
      "void at::native::vectorized_elementwise_kernel<2, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.624us         0.00%       2.624us       2.624us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 684.779ms\n",
      "Self CUDA time total: 510.558ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 1\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 2\n",
    "test_samples = 4\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "fineweb = load_dataset(\"parquet\",data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text'][:test_samples] \n",
    "\n",
    "print(\"output length is {}\".format(output_length))\n",
    "text = fineweb_text[0]\n",
    "inputs = preprocess_data(text, tokenizer)\n",
    "\n",
    "# cached_mlp.clear_load_from_cpu_stats()\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ]\n",
    ") as p:\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(),\n",
    "            max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "print(p.key_averages().table(\n",
    "    sort_by=\"self_cpu_time_total\", row_limit=-1))\n",
    "p.export_chrome_trace(\"./offloading-hqq-sdpa-eq.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试一个正常输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "in prefill layer  2\n",
      "in prefill layer  3\n",
      "in prefill layer  4\n",
      "in prefill layer  5\n",
      "in prefill layer  6\n",
      "in prefill layer  7\n",
      "in prefill layer  8\n",
      "in prefill layer  9\n",
      "in prefill layer  10\n",
      "in prefill layer  11\n",
      "in prefill layer  12\n",
      "in prefill layer  13\n",
      "in prefill layer  14\n",
      "in prefill layer  15\n",
      "in prefill layer  16\n",
      "in prefill layer  17\n",
      "in prefill layer  18\n",
      "in prefill layer  19\n",
      "in prefill layer  20\n",
      "in prefill layer  21\n",
      "in prefill layer  22\n",
      "in prefill layer  23\n",
      "in prefill layer  24\n",
      "in prefill layer  25\n",
      "in prefill layer  26\n",
      "in prefill layer  27\n",
      "in prefill layer  28\n",
      "in prefill layer  29\n",
      "in prefill layer  30\n",
      "in prefill layer  31\n",
      "Generated length: 32 Time taken: 71.98 s prefill time: 51.36 s\n",
      "['The future of AI is here,  and it’s not just about the technology.\\n\\nIt’s also about the way we think and work.\\nAI is a new technology that is']\n",
      "decode phase speed: 1.5033  token/s\n",
      "the number of experts reload per token: 10.774193548387096\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 10\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 32\n",
    "device_id = 0\n",
    "test_samples = 1\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "generated_all, decode_time, prefill_time = 0, 0, 0\n",
    "# print(\"max output length is {}\".format(output_length))\n",
    "text = \"The future of AI is here, \"\n",
    "\n",
    "clear_prefill_time = PLLM.get_prefill_time()\n",
    "clear_experts = PLLM.get_reload_experts()\n",
    "# print(\"need to zero: \", clear_experts, clear_prefill_time)\n",
    "\n",
    "inputs = preprocess_data(text, tokenizer)\n",
    "# 测试时间\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "# 开始计时\n",
    "torch.cuda.synchronize()\n",
    "start_event.record()\n",
    "\n",
    "# 前向传播\n",
    "with torch.no_grad():\n",
    "    output = llm.generate(\n",
    "        input_ids=inputs[\"input_ids\"].cuda(device_id),\n",
    "        attention_mask=inputs[\"attention_mask\"].cuda(device_id),\n",
    "        max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "        generation_config=GenerationConfig(do_sample=False),\n",
    "        pad_token_id=tokenizer.pad_token_id, \n",
    "        # cache_implementation=\"static\" ## moe not support\n",
    "    )\n",
    "\n",
    "# 结束计时\n",
    "end_event.record()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# 计算时间\n",
    "elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "decode_time += elapsed_time\n",
    "cur_prefill_time = PLLM.get_prefill_time()\n",
    "prefill_time += cur_prefill_time\n",
    "print(f\"Generated length: {len(output[0]) - input_length}\", f\"Time taken: {elapsed_time:.2f} s\", f\"prefill time: {cur_prefill_time:.2f} s\")\n",
    "# print(output)\n",
    "print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "\n",
    "generated_all += (len(output[0]) - input_length -1)\n",
    "\n",
    "timepertoken = (decode_time - prefill_time) / (generated_all)\n",
    "print(\"decode phase speed:\", '{:.4f}'.format(1/timepertoken) , ' token/s')\n",
    "print(\"the number of experts reload per token:\", PLLM.get_reload_experts() / generated_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载到GPU上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:25<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MixtralForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "import json\n",
    "\n",
    "def get_model(model_name, device_map, dtype=torch.bfloat16):\n",
    "    llm = MixtralForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        use_cache=True,\n",
    "        torch_dtype=dtype,\n",
    "    ) \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return llm, tokenizer\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "with open('../quantize/device_map_1.json', 'r') as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "dtype = torch.float16\n",
    "llm, tokenizer = get_model(model_name, device_map, dtype=dtype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
