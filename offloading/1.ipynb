{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先都加载到cpu上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"\n",
    "from modeling_mixtral import MixtralForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import json\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "with open(\"../quantize/device_map.json\", \"r\") as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "dtype = torch.float16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# llm, tokenizer = get_model(model_name, 'cpu', dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 2272.60it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 1089.38it/s]\n",
      "100%|██████████| 32/32 [02:54<00:00,  5.46s/it]\n"
     ]
    }
   ],
   "source": [
    "from hqq.core.quantize import *\n",
    "from hqq.models.hf.mixtral import MixtralHQQ\n",
    "\n",
    "save_dir = './hqqsaved'\n",
    "### 第一次加载\n",
    "# q3_config    = BaseQuantizeConfig(nbits=2, group_size=64)\n",
    "# quant_config      = {'block_sparse_moe.experts.w3'   : q3_config}\n",
    "# llm = MixtralForCausalLM.from_pretrained(\n",
    "#         model_name,\n",
    "#         device_map='cpu',\n",
    "#         use_cache=True,\n",
    "#         torch_dtype=dtype,\n",
    "#     ) \n",
    "# MixtralHQQ.quantize_model(llm, quant_config=quant_config, compute_dtype=dtype, device='cuda:0')\n",
    "#### 先放CUDA量化，然后再传回CPU\n",
    "# MixtralHQQ.save_quantized(llm, save_dir)\n",
    "\n",
    "### 从保存的权重中加载\n",
    "llm = MixtralHQQ.from_quantized(save_dir, compute_dtype=dtype, device='cpu')\n",
    "HQQLinear.set_backend(HQQBackend.PYTORCH)\n",
    "\n",
    "backend       = \"gemlite\" #'torchao_int4' #\"torchao_int4\" (4-bit only) or \"gemlite\" (4-bit + 2-bit)\n",
    "#Optimize\n",
    "from hqq.utils.patching import prepare_for_inference\n",
    "prepare_for_inference(llm, backend=backend, verbose=True)\n",
    "#Load GemLite cache\n",
    "if(backend == 'gemlite'):\n",
    "\timport gemlite\n",
    "\tgemlite.core.GEMLITE_TRITON_RESTRICT_M = True\n",
    "\tgemlite.core.GemLiteLinear.load_config('/tmp/gemlite_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import threading\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CachedMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, dtype, sparsity: float = 0.2):\n",
    "        super(CachedMLP, self).__init__()\n",
    "        self.sparsity = sparsity\n",
    "        self.activenum = int((1 - sparsity) * hidden_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dtype = dtype\n",
    "        print(\"active neural num \",self.activenum)\n",
    "\n",
    "        self.activation = nn.SiLU()\n",
    "\n",
    "        #### 中间变量\n",
    "        self.w3_result1 = None\n",
    "        self.w3_result2 = None\n",
    "\n",
    "        # 将GPU缓存张量改为列表存储\n",
    "        self.w_gpu = [\n",
    "            torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda:0') for _ in range(4)\n",
    "        ]  # [w1_gpu, w2_gpu, w1_gpu_expert1, w2_gpu_expert1]\n",
    "\n",
    "\n",
    "        # 将Pinned Memory缓冲区改为列表存储\n",
    "        self.sparse_w_cpu = [\n",
    "            torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu').pin_memory() for _ in range(4)\n",
    "        ]  # [sparse_w1_cpu, sparse_w2_cpu, sparse_w1_cpu_expert1, sparse_w2_cpu_expert1]\n",
    "\n",
    "        self.expert0_weight = torch.tensor(0)\n",
    "        self.expert1_weight = torch.tensor(0)\n",
    "\n",
    "    def load_from_cpu(self, cpu_mlp, cpu_mlp_expert1, stream: torch.cuda.Stream, hidden_states):\n",
    "        \"\"\"\n",
    "        从CPU加载参数，并使用指定的CUDA流进行异步复制到GPU。\n",
    "        \n",
    "        参数:\n",
    "            cpu_mlp: 包含CPU上参数的字典（第一个专家）\n",
    "            cpu_mlp_expert1: 包含CPU上参数的字典（第二个专家）。\n",
    "            stream: 用于数据传输的CUDA流。\n",
    "        \"\"\"\n",
    "        ### 根据up计算的结果进行稀疏化\n",
    "        up_result1 = cpu_mlp['w3'](hidden_states)\n",
    "        up_result2 = cpu_mlp_expert1['w3'](hidden_states)\n",
    "        # 提取 up_result1 的值并计算 top-k 索引\n",
    "        _, indices1 = torch.topk(up_result1, self.activenum, dim=1)  # 在第二个维度上取 top-k\n",
    "        # 对 w1 进行索引操作\n",
    "        self.w3_result1 = up_result1[: , indices1[0]]\n",
    "        indices1 = indices1[0].cpu()\n",
    "\n",
    "        _, indices2 = torch.topk(up_result2, self.activenum, dim=1)  # 在第二个维度上取 top-k\n",
    "        self.w3_result2 = up_result2[: , indices2[0]]\n",
    "        indices2 = indices2[0].cpu()  # 去除多余的维度，得到形状为 [k] 的索引张量\n",
    "\n",
    "        # 使用列表索引更新CPU数据\n",
    "        self.sparse_w_cpu[0].copy_(cpu_mlp['w1'].data[indices1, :])\n",
    "        self.sparse_w_cpu[1].copy_(cpu_mlp['w2'].data[indices1, :])\n",
    "        self.sparse_w_cpu[2].copy_(cpu_mlp_expert1['w1'].data[indices2, :])\n",
    "        self.sparse_w_cpu[3].copy_(cpu_mlp_expert1['w2'].data[indices2, :])\n",
    "        \n",
    "        # 异步复制到GPU\n",
    "        with torch.cuda.stream(stream):\n",
    "            self.w_gpu[0].copy_(self.sparse_w_cpu[0], non_blocking=True)\n",
    "            self.w_gpu[1].copy_(self.sparse_w_cpu[1], non_blocking=True)\n",
    "            self.w_gpu[2].copy_(self.sparse_w_cpu[2], non_blocking=True)\n",
    "            self.w_gpu[3].copy_(self.sparse_w_cpu[3], non_blocking=True)\n",
    "\n",
    "    def load_expert_weights(self, expert_weights):\n",
    "        self.expert0_weight = expert_weights[0]\n",
    "        self.expert1_weight = expert_weights[1]\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        \"\"\"\n",
    "        根据hidden_states， 分别计算两个专家的输出\n",
    "        \"\"\"\n",
    "        w3_output = self.w3_result1\n",
    "        w1_output = self.activation(torch.matmul(hidden_states, self.w_gpu[0].T))\n",
    "        hidden_states_expert0 = torch.matmul(w1_output * w3_output, self.w_gpu[1])\n",
    "\n",
    "        # 第二个专家的计算\n",
    "        w3_output_expert1 = self.w3_result2\n",
    "        w1_output_expert1 = self.activation(torch.matmul(hidden_states, self.w_gpu[2].T))\n",
    "        hidden_states_expert1 = torch.matmul(w1_output_expert1 * w3_output_expert1, self.w_gpu[3])\n",
    "\n",
    "        final_hidden_states = hidden_states_expert0 * self.expert0_weight + hidden_states_expert1 * self.expert1_weight\n",
    "        return final_hidden_states\n",
    "                        \n",
    "def convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.9):\n",
    "    ### 其他部分存放在GPU上\n",
    "    llm.model.embed_tokens.cuda(0)\n",
    "    for i in range(len(llm.model.layers)):\n",
    "        llm.model.layers[i].self_attn.cuda(0)\n",
    "        llm.model.layers[i].input_layernorm.cuda(0)\n",
    "        llm.model.layers[i].post_attention_layernorm.cuda(0)\n",
    "        llm.model.layers[i].block_sparse_moe.gate.cuda(0)\n",
    "        for j in range(len(llm.model.layers[0].block_sparse_moe.experts)):\n",
    "            llm.model.layers[i].block_sparse_moe.experts[j].w3.cuda(0)\n",
    "    ### 第0层的专家存放在GPU上\n",
    "    for j in range(len(llm.model.layers[0].block_sparse_moe.experts)):\n",
    "        llm.model.layers[0].block_sparse_moe.experts[j].cuda(0)\n",
    "\n",
    "    llm.model.norm.cuda(0)\n",
    "    llm.lm_head.cuda(0)\n",
    "    \n",
    "    # 创建两个共享的CachedMLP实例\n",
    "    buffer0 = CachedMLP(\n",
    "        input_dim=llm.config.hidden_size,\n",
    "        hidden_dim=llm.config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "        sparsity=sparsity\n",
    "    )\n",
    "    buffer1 = CachedMLP(\n",
    "        input_dim=llm.config.hidden_size,\n",
    "        hidden_dim=llm.config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "        sparsity=sparsity\n",
    "    )\n",
    "    cached_mlps = [buffer0, buffer1]\n",
    "    \n",
    "    for i, layer in enumerate(llm.model.layers):\n",
    "        if i==0:\n",
    "            continue\n",
    "        # 将专家的forward方法替换为PipelineLLM管理的方式\n",
    "        for j, expert in enumerate(layer.block_sparse_moe.experts):\n",
    "            expert.cpu_mlp = {\n",
    "                \"w1\": expert.w1.cpu().weight,\n",
    "                \"w2\": expert.w2.cpu().weight.T.contiguous(),\n",
    "                \"w3\": expert.w3,\n",
    "            }\n",
    "    return llm, cached_mlps\n",
    "\n",
    "class PipelineLLM:\n",
    "    def __init__(self, llm, cached_mlps):\n",
    "        \"\"\"\n",
    "        初始化 PipelineLLM，替换模型每一层的 forward 方法。\n",
    "        \n",
    "        参数:\n",
    "            llm: 原始的大模型\n",
    "            cached_mlps: 两个 CachedMLP 实例列表\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.cached_mlps = cached_mlps  # [buffer0, buffer1]\n",
    "        self.num_layers = len(llm.model.layers)\n",
    "        self.lock = threading.Lock()\n",
    "        self.use_buffer0 = True  # 标记当前使用哪个缓冲区\n",
    "\n",
    "        # 创建两个共享的CUDA流\n",
    "        self.stream0 = torch.cuda.Stream()\n",
    "        self.stream1 = torch.cuda.Stream()\n",
    "\n",
    "        self.top_k = 2\n",
    "        self.activation = nn.SiLU()\n",
    "\n",
    "        self._replace_forward_methods()\n",
    "\n",
    "        # 用于统计时间的变量\n",
    "        self.total_prefill_time = 0.0\n",
    "        self.total_decode_time = 0.0\n",
    "\n",
    "    def _load_layer(self, layer_idx, buffer_index, expert_ids, expert_weights,\n",
    "                    hidden_states):\n",
    "        \"\"\"\n",
    "        加载指定层的参数到指定的缓冲区。\n",
    "        \n",
    "        参数:\n",
    "            layer_idx: 层的索引\n",
    "            buffer_index: 缓冲区的索引（0 或 1）\n",
    "        \"\"\"\n",
    "        layer = self.llm.model.layers[layer_idx]\n",
    "        expert0 = layer.block_sparse_moe.experts[expert_ids[0]]\n",
    "        expert1 = layer.block_sparse_moe.experts[expert_ids[1]]\n",
    "\n",
    "        cpu_mlp = expert0.cpu_mlp\n",
    "        cpu_mlp_expert1 = expert1.cpu_mlp\n",
    "        buffer = self.cached_mlps[buffer_index]\n",
    "        stream = self.stream0 if buffer_index == 0 else self.stream1\n",
    "\n",
    "        buffer.load_expert_weights(expert_weights)\n",
    "        # 异步加载参数\n",
    "        buffer.load_from_cpu(cpu_mlp, cpu_mlp_expert1, stream, hidden_states)\n",
    "\n",
    "    def _replace_forward_methods(self):\n",
    "        \"\"\"\n",
    "        替换模型每一层的 forward 方法，添加参数预加载逻辑和注意力计算。\n",
    "        \"\"\"\n",
    "        for i, layer in enumerate(self.llm.model.layers):\n",
    "            def new_forward(hidden_states: torch.Tensor,\n",
    "                        attention_mask: Optional[torch.Tensor] = None,\n",
    "                        position_ids: Optional[torch.LongTensor] = None,\n",
    "                        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "                        output_attentions: Optional[bool] = False,\n",
    "                        output_router_logits: Optional[bool] = False,\n",
    "                        use_cache: Optional[bool] = False,\n",
    "                        cache_position: Optional[torch.LongTensor] = None,\n",
    "                        layer=layer,\n",
    "                        layer_idx=i):\n",
    "                with self.lock:\n",
    "                    batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "                    \n",
    "                    if sequence_length == 1:\n",
    "                        #### decode phase ####\n",
    "                        # 选择当前使用的缓冲区\n",
    "                        current_buffer = self.cached_mlps[0] if self.use_buffer0 else self.cached_mlps[1]\n",
    "\n",
    "                        next_buffer_index = 1 if self.use_buffer0 else 0\n",
    "\n",
    "                        next_layer_idx = layer_idx + 1\n",
    "\n",
    "                        if next_layer_idx < self.num_layers:\n",
    "                            # 预加载下一层的参数\n",
    "                            next_layer = self.llm.model.layers[next_layer_idx]\n",
    "                            router = next_layer.block_sparse_moe.gate\n",
    "\n",
    "                            # batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "                            hidden_states_flat = hidden_states.view(-1, hidden_dim)\n",
    "                            # router_logits: (batch * sequence_length, n_experts)\n",
    "                            router_logits = router(hidden_states_flat)\n",
    "\n",
    "                            routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "                            routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n",
    "                            routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n",
    "\n",
    "                            self._load_layer(\n",
    "                                next_layer_idx,\n",
    "                                buffer_index=next_buffer_index,\n",
    "                                expert_ids=selected_experts[0],\n",
    "                                expert_weights=routing_weights[0],\n",
    "                                hidden_states=hidden_states_flat,\n",
    "                            )\n",
    "\n",
    "                            hidden_states = hidden_states_flat.reshape(batch_size, sequence_length, hidden_dim)\n",
    "\n",
    "                        # 切换缓冲区\n",
    "                        self.use_buffer0 = not self.use_buffer0\n",
    "\n",
    "                    # 处理当前层\n",
    "                    residual = hidden_states\n",
    "                    hidden_states = layer.input_layernorm(hidden_states)\n",
    "\n",
    "                    # Self Attention\n",
    "                    hidden_states, self_attn_weights, present_key_value = layer.self_attn(\n",
    "                        hidden_states=hidden_states,\n",
    "                        attention_mask=attention_mask,\n",
    "                        position_ids=position_ids,\n",
    "                        past_key_value=past_key_value,\n",
    "                        output_attentions=output_attentions,\n",
    "                        use_cache=use_cache,\n",
    "                        cache_position=cache_position,\n",
    "                    )\n",
    "                    hidden_states = residual + hidden_states\n",
    "\n",
    "                    # Fully Connected\n",
    "                    residual = hidden_states\n",
    "                    hidden_states = layer.post_attention_layernorm(hidden_states)\n",
    "                    \n",
    "                    if sequence_length > 1:\n",
    "                        print(\"in prefill layer \", layer_idx)\n",
    "                        # 对于prefill阶段，仅将experts加载到GPU计算\n",
    "                        experts = layer.block_sparse_moe.experts\n",
    "\n",
    "                        # 将experts移动到GPU\n",
    "                        if layer_idx != 0:\n",
    "                            for expert in experts:\n",
    "                                expert.cuda(0)\n",
    "\n",
    "                        # 在GPU上进行MoE计算（gate保持在CPU）\n",
    "                        final_hidden_states, router_logits = layer.block_sparse_moe(hidden_states)\n",
    "\n",
    "                        # 计算完成后将experts移回CPU\n",
    "                        if layer_idx != 0:\n",
    "                            for expert in experts:\n",
    "                                expert.w1.to('cpu')\n",
    "                                expert.w2.to('cpu')\n",
    "                    else:\n",
    "                        # batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "                        hidden_states_flat = hidden_states.view(-1, hidden_dim)\n",
    "                        # print(\"in decode layer\", layer_idx)\n",
    "                        if layer_idx > 0:\n",
    "                            ### 使用当前缓冲区进行 MLP 计算 ###\n",
    "                            final_hidden_states = current_buffer(hidden_states_flat)\n",
    "                        else:\n",
    "                            ### 根据router计算需要使用的专家 ###\n",
    "                            cur_layer = layer\n",
    "                            router = cur_layer.block_sparse_moe.gate\n",
    "                            # router_logits: (batch * sequence_length, n_experts)\n",
    "                            router_logits = router(hidden_states_flat)\n",
    "\n",
    "                            routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "                            routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n",
    "                            routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n",
    "                            # we cast back to the input dtype\n",
    "                            routing_weights = routing_weights.to(hidden_states_flat.dtype)\n",
    "\n",
    "                            first_expert, second_expert = selected_experts[0][0], selected_experts[0][1]\n",
    "\n",
    "                            final_hidden_states_expert0 = cur_layer.block_sparse_moe.experts[first_expert](\n",
    "                                hidden_states_flat) * routing_weights[0][0]\n",
    "\n",
    "                            final_hidden_states_expert1 = cur_layer.block_sparse_moe.experts[second_expert](\n",
    "                                hidden_states_flat) * routing_weights[0][1]\n",
    "\n",
    "                            # 将两个专家的结果相加\n",
    "                            final_hidden_states = final_hidden_states_expert0 + final_hidden_states_expert1\n",
    "\n",
    "                        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n",
    "\n",
    "                    hidden_states = residual + final_hidden_states\n",
    "\n",
    "                    outputs = (hidden_states,)\n",
    "\n",
    "                    if output_attentions:\n",
    "                        outputs += (self_attn_weights,)\n",
    "\n",
    "                    if use_cache:\n",
    "                        outputs += (present_key_value,)\n",
    "\n",
    "                    return outputs\n",
    "\n",
    "            # 替换forward方法\n",
    "            layer.forward = new_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "active neural num  2867\n",
      "active neural num  2867\n"
     ]
    }
   ],
   "source": [
    "llm, cached_mlps = convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.8)\n",
    "\n",
    "# 创建流水线模型\n",
    "PLLM = PipelineLLM(llm, cached_mlps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试时间开销"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 32\n",
      "Generated output length: 32 Time taken: 5.6117 seconds\n",
      "['3 coursofs ch shift Form Space training --(edi pom shopistrzostvin flagtainment Shop Murpdev Murray booksond览 Unitos руerr esponbookwidet Saf']\n",
      "Generated output length: 32 Time taken: 4.8852 seconds\n",
      "['time involving po R  Ec /******/sburg codenlyical /******/decesscriptoressage /******/ǐVIistribute /******/ packageuspendges inclusiongorithogue proxim rele /******/ oppon up quand']\n",
      "Generated output length: 32 Time taken: 4.8049 seconds\n",
      "['hfol topITERases showsphan wingsBook pooliens Elementbian spont douronogenrielP convenient At tasteskbons collectckiciglia Rtoberantry Fed']\n",
      "Generated output length: 32 Time taken: 4.7167 seconds\n",
      "['frontows Harr Tagtegerej... experiment critique PropTypes糊 Ge inter --(\\xadoc   initialGEMS inter N emergencyisy the upon /******/úblic riied respons age']\n",
      "Generated output length: 32 Time taken: 4.6777 seconds\n",
      "['und Con expend Luvud selectree features /******/ R ofck featuresnaiocriptorriz prior dur Chcembre︎ tro /******/ S costess tort Selemiz hoplaatst']\n",
      "Generated output length: 32 Time taken: 4.6476 seconds\n",
      "['s nov!!!ory [Up Vality --(agma collectrics venschaft holas sets /******/ count advertisvo /******/ toni po /******/ístockensefore oprequ']\n",
      "Generated output length: 32 Time taken: 4.6077 seconds\n",
      "['bu Febru Goldenrael During countch M /******/ among!\\\\ BB --( winds sendrose --( figure Loldecess ToimeqSI send Books bolcentted # Imagesijd last']\n",
      "Generated output length: 32 Time taken: 4.5737 seconds\n",
      "['vac influence to]} visible /******/ Fal fall Oriagemloat remotkind /******/ /******/ doub represents communicationement unlike concerning typical object /******/ judge half byipage /******/ /******/ fuckingcrate']\n",
      "Generated output length: 32 Time taken: 4.5801 seconds\n",
      "['litt M mid--) Icon for soon stra ess /******/ /******/z while /******/ /******/ dut pselesh /******/ seam sleep havet /******/ tracesse when thous /******/ stepping /******/ /******/ /******/']\n",
      "Generated output length: 32 Time taken: 4.5473 seconds\n",
      "['Hol Pr Str top /******/ oper serialized Writ Gro pose ,sembly /******/ er How Crities /******/cció!\\\\ instrument /******/ opp and[ half nowrysrcu flag Justiceen']\n",
      "decode phase speed: 6.7153  token/s\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "\n",
    "input_length = 1\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 32\n",
    "test_samples = 10\n",
    "device_id = 0\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "filt_type = fineweb_path.split('.')[-1]\n",
    "fineweb = load_dataset(filt_type, data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text']\n",
    "\n",
    "# 预热（避免第一次运行时的额外开销）\n",
    "for text in fineweb_text[:5] :\n",
    "    inputs = preprocess_data(text, tokenizer)\n",
    "    with torch.no_grad():\n",
    "        output = llm(input_ids=inputs[\"input_ids\"].cuda(device_id), attention_mask=inputs[\"attention_mask\"].cuda(device_id))\n",
    "\n",
    "generated_all = 0\n",
    "prefill_time, decode_time = 0, 0\n",
    "print(\"output length is {}\".format(output_length))\n",
    "for text in fineweb_text[:test_samples] :\n",
    "    inputs = preprocess_data(text, tokenizer)\n",
    "\n",
    "    # 预热（避免第一次运行时的额外开销）\n",
    "    with torch.no_grad():\n",
    "        output = llm(input_ids=inputs[\"input_ids\"].cuda(device_id), attention_mask=inputs[\"attention_mask\"].cuda(device_id))\n",
    "\n",
    "    # 测试时间\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # 开始计时\n",
    "    torch.cuda.synchronize()\n",
    "    start_event.record()\n",
    "\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(device_id),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(device_id),\n",
    "            max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.pad_token_id, \n",
    "            # cache_implementation=\"static\" ## moe not support\n",
    "        )\n",
    "\n",
    "    # 结束计时\n",
    "    end_event.record()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # 计算时间\n",
    "    elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "    decode_time += elapsed_time\n",
    "    print(f\"Generated output length: {len(output[0]) - input_length}\", f\"Time taken: {elapsed_time:.4f} seconds\")\n",
    "    # print(output)\n",
    "    print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "\n",
    "    generated_all += len(output[0]) - input_length\n",
    "\n",
    "timepertoken = (decode_time) / (generated_all)\n",
    "# print(\"decode time:\", '{:.4f}'.format((decode_time) /test_samples), ' s')\n",
    "print(\"decode phase speed:\", '{:.4f}'.format(1/timepertoken) , ' token/s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################################################################\n",
    "#Save gemlite cache\n",
    "if(backend == 'gemlite'):\n",
    "\tgemlite.core.GemLiteLinear.cache_config('/tmp/gemlite_config.json') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.profile\n",
    "\n",
    "trace-offloading-r.json是最优，就是做完一个index就传一个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 2\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::index        38.64%     123.023ms        39.30%     125.122ms     249.745us       1.850ms         0.72%       1.850ms       3.693us           501  \n",
      "                                            aten::copy_        25.34%      80.685ms        29.32%      93.360ms      84.795us     215.178ms        83.29%     215.178ms     195.439us          1101  \n",
      "gemlite::gemv_revsplitK_A16fWnO16f_int32packing_forw...         6.51%      20.726ms         7.46%      23.765ms     185.665us       3.433ms         1.33%       4.137ms      32.318us           128  \n",
      "                                       cudaLaunchKernel         6.22%      19.814ms         6.22%      19.814ms       4.525us       0.000us         0.00%       0.000us       0.000us          4379  \n",
      "                                        cudaMemcpyAsync         3.44%      10.956ms         3.44%      10.956ms      20.633us       0.000us         0.00%       0.000us       0.000us           531  \n",
      "                                               aten::mm         2.73%       8.692ms         3.80%      12.108ms      20.948us      10.350ms         4.01%      10.350ms      17.907us           578  \n",
      "                                              aten::mul         1.50%       4.773ms         2.48%       7.902ms      10.183us       4.525ms         1.75%       4.525ms       5.831us           776  \n",
      "                                  cudaStreamSynchronize         1.17%       3.731ms         1.17%       3.731ms      14.187us       0.000us         0.00%       0.000us       0.000us           263  \n",
      "                                              aten::add         1.03%       3.276ms         1.72%       5.473ms      10.526us       2.526ms         0.98%       2.526ms       4.859us           520  \n",
      "                                             aten::topk         0.90%       2.863ms         1.64%       5.219ms      27.762us      11.261ms         4.36%      11.261ms      59.899us           188  \n",
      "                                    aten::empty_strided         0.86%       2.743ms         0.86%       2.743ms       5.937us       0.000us         0.00%       0.000us       0.000us           462  \n",
      "                                               aten::to         0.80%       2.561ms         5.97%      18.999ms      26.062us       0.000us         0.00%       3.155ms       4.328us           729  \n",
      "                                            aten::slice         0.74%       2.343ms         0.93%       2.952ms       1.957us       0.000us         0.00%       0.000us       0.000us          1509  \n",
      "                                           aten::matmul         0.59%       1.882ms         5.53%      17.603ms      24.934us       0.000us         0.00%      11.062ms      15.669us           706  \n",
      "                                       aten::as_strided         0.59%       1.868ms         0.59%       1.868ms       0.457us       0.000us         0.00%       0.000us       0.000us          4087  \n",
      "                                              aten::cat         0.57%       1.805ms         0.85%       2.707ms      13.809us       1.238ms         0.48%       1.238ms       6.314us           196  \n",
      "                                              aten::bmm         0.52%       1.667ms         0.71%       2.261ms      17.668us     711.851us         0.28%     711.851us       5.561us           128  \n",
      "                                          aten::reshape         0.48%       1.535ms         1.44%       4.595ms       3.432us       0.000us         0.00%     769.908us       0.575us          1339  \n",
      "                                             aten::view         0.47%       1.503ms         0.47%       1.503ms       0.795us       0.000us         0.00%       0.000us       0.000us          1891  \n",
      "                                             aten::mean         0.47%       1.490ms         0.66%       2.100ms      16.156us     939.131us         0.36%     939.131us       7.224us           130  \n",
      "                                         aten::_to_copy         0.43%       1.372ms         5.16%      16.438ms      35.970us       0.000us         0.00%       3.155ms       6.904us           457  \n",
      "                                              aten::pow         0.41%       1.320ms         0.64%       2.042ms      15.712us     602.696us         0.23%     602.696us       4.636us           130  \n",
      "                                            aten::empty         0.38%       1.224ms         0.38%       1.224ms       4.340us       0.000us         0.00%       0.000us       0.000us           282  \n",
      "                                           aten::select         0.37%       1.182ms         0.45%       1.441ms       2.242us       0.000us         0.00%       0.000us       0.000us           643  \n",
      "                                         aten::_softmax         0.29%     932.479us         0.47%       1.488ms      11.622us     643.048us         0.25%     643.048us       5.024us           128  \n",
      "                                              aten::neg         0.28%     887.853us         0.45%       1.419ms      11.086us     721.905us         0.28%     721.905us       5.640us           128  \n",
      "                                            aten::rsqrt         0.27%     852.378us         0.44%       1.410ms      10.847us     635.112us         0.25%     635.112us       4.885us           130  \n",
      "                                           aten::linear         0.26%     842.251us         3.88%      12.355ms      37.440us       0.000us         0.00%       5.433ms      16.465us           330  \n",
      "                                             aten::silu         0.26%     822.639us         0.43%       1.368ms      10.687us     705.067us         0.27%     705.067us       5.508us           128  \n",
      "                                        aten::transpose         0.26%     819.716us         0.37%       1.170ms       1.801us       0.000us         0.00%       0.000us       0.000us           650  \n",
      "                                  cudaFuncGetAttributes         0.23%     730.629us         0.23%     730.629us       3.690us       0.000us         0.00%       0.000us       0.000us           198  \n",
      "                                    cudaLaunchKernelExC         0.21%     660.627us         0.21%     660.627us       4.930us       0.000us         0.00%       0.000us       0.000us           134  \n",
      "                                              aten::sum         0.20%     646.332us         0.30%     941.236us      14.707us     486.405us         0.19%     486.405us       7.600us            64  \n",
      "                                                aten::t         0.20%     628.990us         0.38%       1.207ms       3.658us       0.000us         0.00%       0.000us       0.000us           330  \n",
      "                                            aten::fill_         0.19%     616.909us         0.48%       1.537ms      10.901us     743.341us         0.29%     743.341us       5.272us           141  \n",
      "                                         cuLaunchKernel         0.19%     594.853us         0.19%     594.853us       4.647us       0.000us         0.00%       0.000us       0.000us           128  \n",
      "                                              aten::div         0.18%     581.687us         0.28%     879.379us      13.740us     308.586us         0.12%     308.586us       4.822us            64  \n",
      "                              aten::_local_scalar_dense         0.17%     551.493us         1.55%       4.942ms      36.882us     971.575us         0.38%     971.575us       7.251us           134  \n",
      "                                           aten::expand         0.16%     520.681us         0.21%     660.793us       1.712us       0.000us         0.00%       0.000us       0.000us           386  \n",
      "                                        aten::unsqueeze         0.16%     510.945us         0.20%     632.488us       2.343us       0.000us         0.00%       0.000us       0.000us           270  \n",
      "                                             aten::div_         0.14%     433.950us         0.22%     710.112us      11.095us     343.339us         0.13%     343.339us       5.365us            64  \n",
      "                                            aten::zero_         0.12%     395.537us         0.56%       1.779ms      13.897us       0.000us         0.00%     703.853us       5.499us           128  \n",
      "                                     aten::_unsafe_view         0.12%     375.445us         0.12%     375.445us       0.730us       0.000us         0.00%       0.000us       0.000us           514  \n",
      "                                            aten::clone         0.09%     285.814us         0.72%       2.282ms      16.538us       0.000us         0.00%     802.869us       5.818us           138  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.09%     273.669us         0.09%     273.669us       0.527us       0.000us         0.00%       0.000us       0.000us           519  \n",
      "                                          aten::permute         0.08%     247.358us         0.09%     296.973us       2.395us       0.000us         0.00%       0.000us       0.000us           124  \n",
      "                                             aten::item         0.06%     202.276us         1.62%       5.144ms      38.391us       0.000us         0.00%     971.575us       7.251us           134  \n",
      "                                       aten::empty_like         0.06%     186.419us         0.22%     688.002us       5.096us       0.000us         0.00%       0.000us       0.000us           135  \n",
      "                                          aten::softmax         0.05%     175.027us         0.52%       1.663ms      12.990us       0.000us         0.00%     643.048us       5.024us           128  \n",
      "                                 cudaDeviceGetAttribute         0.04%     121.658us         0.04%     121.658us       0.470us       0.000us         0.00%       0.000us       0.000us           259  \n",
      "                                  cudaDeviceSynchronize         0.04%     117.374us         0.04%     117.374us     117.374us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                          aten::numpy_T         0.03%     110.020us         0.13%     406.993us       3.282us       0.000us         0.00%       0.000us       0.000us           124  \n",
      "                                     aten::masked_fill_         0.03%     108.542us         0.05%     154.496us      38.624us       9.600us         0.00%       9.600us       2.400us             4  \n",
      "                                               aten::eq         0.03%     106.845us         0.05%     167.373us      18.597us      21.376us         0.01%      21.376us       2.375us             9  \n",
      "                                             aten::isin         0.02%      72.370us         0.10%     309.841us     103.280us       0.000us         0.00%      28.033us       9.344us             3  \n",
      "                                      aten::result_type         0.02%      66.268us         0.02%      66.268us       0.510us       0.000us         0.00%       0.000us       0.000us           130  \n",
      "                                        cudaMemsetAsync         0.02%      64.670us         0.02%      64.670us      10.778us       0.000us         0.00%       0.000us       0.000us             6  \n",
      "                                              aten::sub         0.02%      64.621us         0.03%      96.720us      13.817us      19.233us         0.01%      19.233us       2.748us             7  \n",
      "                                           aten::cumsum         0.02%      60.477us         0.03%     103.036us      34.345us      16.032us         0.01%      16.032us       5.344us             3  \n",
      "                                   aten::_reshape_alias         0.02%      60.213us         0.02%      60.213us       1.882us       0.000us         0.00%       0.000us       0.000us            32  \n",
      "                                      aten::masked_fill         0.01%      46.815us         0.07%     221.431us     110.716us       0.000us         0.00%      10.944us       5.472us             2  \n",
      "                                              aten::any         0.01%      42.883us         0.04%     113.693us      22.739us       0.000us         0.00%      11.649us       2.330us             5  \n",
      "                                       aten::bitwise_or         0.01%      39.135us         0.02%      57.477us      14.369us       9.857us         0.00%       9.857us       2.464us             4  \n",
      "                                     aten::index_select         0.01%      38.457us         0.02%      67.393us      33.697us       7.681us         0.00%       7.681us       3.840us             2  \n",
      "                                           aten::argmax         0.01%      32.394us         0.01%      46.883us      23.442us      33.312us         0.01%      33.312us      16.656us             2  \n",
      "                                           aten::arange         0.01%      30.736us         0.03%     105.151us      13.144us       8.577us         0.00%      17.154us       2.144us             8  \n",
      "                                              aten::max         0.01%      29.551us         0.02%      49.037us      24.519us       9.216us         0.00%       9.216us       4.608us             2  \n",
      "                                          aten::dropout         0.01%      26.036us         0.01%      26.036us       0.407us       0.000us         0.00%       0.000us       0.000us            64  \n",
      "                                               aten::gt         0.01%      24.440us         0.01%      37.132us      18.566us       5.312us         0.00%       5.312us       2.656us             2  \n",
      "                                        aten::embedding         0.01%      22.733us         0.03%      94.440us      47.220us       0.000us         0.00%       7.681us       3.840us             2  \n",
      "                                      aten::bitwise_not         0.01%      22.396us         0.01%      35.530us      17.765us       4.481us         0.00%       4.481us       2.240us             2  \n",
      "                                               aten::ge         0.01%      22.363us         0.01%      31.820us      15.910us       4.416us         0.00%       4.416us       2.208us             2  \n",
      "                                      aten::bitwise_or_         0.01%      22.334us         0.01%      33.190us      16.595us       5.056us         0.00%       5.056us       2.528us             2  \n",
      "                                   cudaFuncSetAttribute         0.01%      21.195us         0.01%      21.195us       0.963us       0.000us         0.00%       0.000us       0.000us            22  \n",
      "                                      aten::bitwise_and         0.01%      20.362us         0.01%      32.781us      16.391us       5.472us         0.00%       5.472us       2.736us             2  \n",
      "                                             aten::mul_         0.01%      20.330us         0.01%      32.513us      16.257us       5.472us         0.00%       5.472us       2.736us             2  \n",
      "                                               aten::lt         0.01%      20.026us         0.01%      28.376us      28.376us       2.208us         0.00%       2.208us       2.208us             1  \n",
      "                                          aten::resize_         0.01%      17.101us         0.01%      17.101us       2.850us       0.000us         0.00%       0.000us       0.000us             6  \n",
      "                                             aten::full         0.00%      15.247us         0.03%      92.741us      15.457us       0.000us         0.00%      19.265us       3.211us             6  \n",
      "                                         aten::new_ones         0.00%      15.125us         0.02%      53.648us      26.824us       0.000us         0.00%       9.664us       4.832us             2  \n",
      "                                               aten::le         0.00%      14.395us         0.01%      21.297us      10.649us       5.761us         0.00%       5.761us       2.881us             2  \n",
      "                                             aten::rsub         0.00%      11.416us         0.01%      46.604us      23.302us       0.000us         0.00%       8.000us       4.000us             2  \n",
      "                                       aten::is_nonzero         0.00%      10.768us         0.05%     144.044us      24.007us       0.000us         0.00%      14.688us       2.448us             6  \n",
      "                                           aten::__or__         0.00%       8.097us         0.02%      65.574us      16.393us       0.000us         0.00%       9.857us       2.464us             4  \n",
      "                                          aten::__and__         0.00%       7.060us         0.01%      39.841us      19.920us       0.000us         0.00%       5.472us       2.736us             2  \n",
      "                                          aten::view_as         0.00%       5.031us         0.00%       6.853us       1.371us       0.000us         0.00%       0.000us       0.000us             5  \n",
      "                                             aten::ones         0.00%       4.705us         0.01%      22.432us      22.432us       0.000us         0.00%       2.240us       2.240us             1  \n",
      "                                          aten::detach_         0.00%       4.357us         0.00%       6.092us       2.031us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                        aten::new_empty         0.00%       3.368us         0.00%      11.359us       5.679us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                        aten::ones_like         0.00%       2.887us         0.00%      15.325us      15.325us       0.000us         0.00%       2.048us       2.048us             1  \n",
      "                                                detach_         0.00%       1.735us         0.00%       1.735us       0.578us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                    cudaPeekAtLastError         0.00%       1.197us         0.00%       1.197us       0.100us       0.000us         0.00%       0.000us       0.000us            12  \n",
      "                                       aten::lift_fresh         0.00%       0.542us         0.00%       0.542us       0.181us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       8.577us         0.00%       8.577us       1.715us             5  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      21.312us         0.01%      21.312us       3.045us             7  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.464us         0.00%       2.464us       2.464us             1  \n",
      "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      56.706us         0.02%      56.706us       2.835us            20  \n",
      "                         Memcpy DtoH (Device -> Pinned)         0.00%       0.000us         0.00%       0.000us       0.000us     971.575us         0.38%     971.575us       7.251us           134  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       6.624us         0.00%       6.624us       2.208us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      13.952us         0.01%      13.952us       3.488us             4  \n",
      "void at_cuda_detail::cub::DeviceScanInitKernel<at_cu...         0.00%       0.000us         0.00%       0.000us       0.000us       7.200us         0.00%       7.200us       2.400us             3  \n",
      "void at_cuda_detail::cub::DeviceScanKernel<at_cuda_d...         0.00%       0.000us         0.00%       0.000us       0.000us       8.832us         0.00%       8.832us       2.944us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      23.138us         0.01%      23.138us       3.305us             7  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       9.312us         0.00%       9.312us       2.328us             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.056us         0.00%       5.056us       2.528us             2  \n",
      "void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us       7.681us         0.00%       7.681us       3.840us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     708.077us         0.27%     708.077us       5.447us           130  \n",
      "void (anonymous namespace)::elementwise_kernel_with_...         0.00%       0.000us         0.00%       0.000us       0.000us       8.577us         0.00%       8.577us       2.144us             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.896us         0.00%       4.896us       2.448us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      14.913us         0.01%      14.913us       2.486us             6  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       5.472us         0.00%       5.472us       2.736us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       5.537us         0.00%       5.537us       2.769us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.768us         0.00%       4.768us       2.384us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.544us         0.00%       4.544us       2.272us             2  \n",
      "void dot_kernel<float, 128, 0, cublasDotParams<cubla...         0.00%       0.000us         0.00%       0.000us       0.000us     394.311us         0.15%     394.311us       6.161us            64  \n",
      "void reduce_1Block_kernel<float, 128, 7, cublasGemvT...         0.00%       0.000us         0.00%       0.000us       0.000us     352.709us         0.14%     352.709us       5.511us            64  \n",
      "void (anonymous namespace)::softmax_warp_forward<c10...         0.00%       0.000us         0.00%       0.000us       0.000us     311.810us         0.12%     311.810us       4.872us            64  \n",
      "void at::native::sbtopk::gatherTopK<float, unsigned ...         0.00%       0.000us         0.00%       0.000us       0.000us     573.197us         0.22%     573.197us       8.956us            64  \n",
      "void at::native::bitonicSortKVInPlace<-2, -1, 16, 16...         0.00%       0.000us         0.00%       0.000us       0.000us     551.812us         0.21%     551.812us       8.622us            64  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     486.405us         0.19%     486.405us       7.600us            64  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     343.339us         0.13%     343.339us       5.365us            64  \n",
      "          gemv_revsplitK_A16fWnO16f_int32packing_kernel         0.00%       0.000us         0.00%       0.000us       0.000us       3.433ms         1.33%       3.433ms      26.819us           128  \n",
      "void at::native::sbtopk::gatherTopK<c10::Half, unsig...         0.00%       0.000us         0.00%       0.000us       0.000us       7.388ms         2.86%       7.388ms      59.577us           124  \n",
      "void at::native::radixSortKVInPlace<-2, -1, 128, 32,...         0.00%       0.000us         0.00%       0.000us       0.000us       2.748ms         1.06%       2.748ms      22.165us           124  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.847ms         0.71%       1.847ms       7.328us           252  \n",
      "                       Memcpy DtoH (Device -> Pageable)         0.00%       0.000us         0.00%       0.000us       0.000us       1.205ms         0.47%       1.205ms       9.722us           124  \n",
      "                         Memcpy HtoD (Pinned -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     211.197ms        81.75%     211.197ms     851.600us           248  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     778.582us         0.30%     778.582us       5.898us           132  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     602.696us         0.23%     602.696us       4.636us           130  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     939.131us         0.36%     939.131us       7.224us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     622.137us         0.24%     622.137us       4.786us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     635.112us         0.25%     635.112us       4.885us           130  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     722.344us         0.28%     722.344us       5.556us           130  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       1.162ms         0.45%       1.162ms       5.930us           196  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.341ms         0.52%       1.341ms       5.196us           258  \n",
      "sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x...         0.00%       0.000us         0.00%       0.000us       0.000us       2.728ms         1.06%       2.728ms      21.311us           128  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       1.292ms         0.50%       1.292ms      10.090us           128  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       1.526ms         0.59%       1.526ms       5.869us           260  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     721.905us         0.28%     721.905us       5.640us           128  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     821.429us         0.32%     821.429us       6.417us           128  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.525ms         0.59%       1.525ms       4.767us           320  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     769.908us         0.30%     769.908us       6.015us           128  \n",
      "void gemv2T_kernel_val<int, int, __half, __half, __h...         0.00%       0.000us         0.00%       0.000us       0.000us     358.149us         0.14%     358.149us       5.596us            64  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     308.586us         0.12%     308.586us       4.822us            64  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     353.257us         0.14%     353.257us       5.520us            64  \n",
      "void (anonymous namespace)::softmax_warp_forward<c10...         0.00%       0.000us         0.00%       0.000us       0.000us     171.491us         0.07%     171.491us       5.359us            32  \n",
      "void gemmk1_kernel<int, float, 256, 5, false, false,...         0.00%       0.000us         0.00%       0.000us       0.000us     166.785us         0.06%     166.785us       5.212us            32  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us      37.889us         0.01%      37.889us       6.315us             6  \n",
      "sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x...         0.00%       0.000us         0.00%       0.000us       0.000us     415.976us         0.16%     415.976us      69.329us             6  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     705.067us         0.27%     705.067us       5.508us           128  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_...         0.00%       0.000us         0.00%       0.000us       0.000us     191.204us         0.07%     191.204us      47.801us             4  \n",
      "void splitKreduce_kernel<32, 16, int, float, __half,...         0.00%       0.000us         0.00%       0.000us       0.000us      22.016us         0.01%      22.016us       5.504us             4  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       2.224ms         0.86%       2.224ms      17.939us           124  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       2.692ms         1.04%       2.692ms      21.711us           124  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     918.801us         0.36%     918.801us       7.410us           124  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us      23.616us         0.01%      23.616us       5.904us             4  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      33.312us         0.01%      33.312us      16.656us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      17.504us         0.01%      17.504us       4.376us             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       8.000us         0.00%       8.000us       4.000us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       8.288us         0.00%       8.288us       4.144us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       2.400us         0.00%       2.400us       2.400us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.481us         0.00%       4.481us       2.240us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       5.472us         0.00%       5.472us       2.736us             2  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       9.216us         0.00%       9.216us       4.608us             2  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.424us         0.00%       3.424us       3.424us             1  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       6.177us         0.00%       6.177us       3.089us             2  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     392.553us         0.15%     392.553us       6.134us            64  \n",
      "void (anonymous namespace)::softmax_warp_forward<c10...         0.00%       0.000us         0.00%       0.000us       0.000us     159.747us         0.06%     159.747us       4.992us            32  \n",
      "void gemv2N_kernel<int, int, __half, __half, __half,...         0.00%       0.000us         0.00%       0.000us       0.000us     186.917us         0.07%     186.917us       5.841us            32  \n",
      "void at::native::vectorized_elementwise_kernel<2, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.432us         0.00%       2.432us       2.432us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 318.368ms\n",
      "Self CUDA time total: 258.347ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 1\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 2\n",
    "test_samples = 4\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "fineweb = load_dataset(\"parquet\",data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text'][:test_samples] \n",
    "\n",
    "print(\"output length is {}\".format(output_length))\n",
    "text = fineweb_text[0]\n",
    "inputs = preprocess_data(text, tokenizer)\n",
    "\n",
    "# cached_mlp.clear_load_from_cpu_stats()\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ]\n",
    ") as p:\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(),\n",
    "            max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "print(p.key_averages().table(\n",
    "    sort_by=\"self_cpu_time_total\", row_limit=-1))\n",
    "p.export_chrome_trace(\"./offloading-hqq2-ping.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试一个正常输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 32\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "experts to GPU done...\n",
      "in prefill layer  2\n",
      "experts to GPU done...\n",
      "in prefill layer  3\n",
      "experts to GPU done...\n",
      "in prefill layer  4\n",
      "experts to GPU done...\n",
      "in prefill layer  5\n",
      "experts to GPU done...\n",
      "in prefill layer  6\n",
      "experts to GPU done...\n",
      "in prefill layer  7\n",
      "experts to GPU done...\n",
      "in prefill layer  8\n",
      "experts to GPU done...\n",
      "in prefill layer  9\n",
      "experts to GPU done...\n",
      "in prefill layer  10\n",
      "experts to GPU done...\n",
      "in prefill layer  11\n",
      "experts to GPU done...\n",
      "in prefill layer  12\n",
      "experts to GPU done...\n",
      "in prefill layer  13\n",
      "experts to GPU done...\n",
      "in prefill layer  14\n",
      "experts to GPU done...\n",
      "in prefill layer  15\n",
      "experts to GPU done...\n",
      "in prefill layer  16\n",
      "experts to GPU done...\n",
      "in prefill layer  17\n",
      "experts to GPU done...\n",
      "in prefill layer  18\n",
      "experts to GPU done...\n",
      "in prefill layer  19\n",
      "experts to GPU done...\n",
      "in prefill layer  20\n",
      "experts to GPU done...\n",
      "in prefill layer  21\n",
      "experts to GPU done...\n",
      "in prefill layer  22\n",
      "experts to GPU done...\n",
      "in prefill layer  23\n",
      "experts to GPU done...\n",
      "in prefill layer  24\n",
      "experts to GPU done...\n",
      "in prefill layer  25\n",
      "experts to GPU done...\n",
      "in prefill layer  26\n",
      "experts to GPU done...\n",
      "in prefill layer  27\n",
      "experts to GPU done...\n",
      "in prefill layer  28\n",
      "experts to GPU done...\n",
      "in prefill layer  29\n",
      "experts to GPU done...\n",
      "in prefill layer  30\n",
      "experts to GPU done...\n",
      "in prefill layer  31\n",
      "experts to GPU done...\n",
      "in prefill layer  0\n",
      "in prefill layer  1\n",
      "experts to GPU done...\n",
      "in prefill layer  2\n",
      "experts to GPU done...\n",
      "in prefill layer  3\n",
      "experts to GPU done...\n",
      "in prefill layer  4\n",
      "experts to GPU done...\n",
      "in prefill layer  5\n",
      "experts to GPU done...\n",
      "in prefill layer  6\n",
      "experts to GPU done...\n",
      "in prefill layer  7\n",
      "experts to GPU done...\n",
      "in prefill layer  8\n",
      "experts to GPU done...\n",
      "in prefill layer  9\n",
      "experts to GPU done...\n",
      "in prefill layer  10\n",
      "experts to GPU done...\n",
      "in prefill layer  11\n",
      "experts to GPU done...\n",
      "in prefill layer  12\n",
      "experts to GPU done...\n",
      "in prefill layer  13\n",
      "experts to GPU done...\n",
      "in prefill layer  14\n",
      "experts to GPU done...\n",
      "in prefill layer  15\n",
      "experts to GPU done...\n",
      "in prefill layer  16\n",
      "experts to GPU done...\n",
      "in prefill layer  17\n",
      "experts to GPU done...\n",
      "in prefill layer  18\n",
      "experts to GPU done...\n",
      "in prefill layer  19\n",
      "experts to GPU done...\n",
      "in prefill layer  20\n",
      "experts to GPU done...\n",
      "in prefill layer  21\n",
      "experts to GPU done...\n",
      "in prefill layer  22\n",
      "experts to GPU done...\n",
      "in prefill layer  23\n",
      "experts to GPU done...\n",
      "in prefill layer  24\n",
      "experts to GPU done...\n",
      "in prefill layer  25\n",
      "experts to GPU done...\n",
      "in prefill layer  26\n",
      "experts to GPU done...\n",
      "in prefill layer  27\n",
      "experts to GPU done...\n",
      "in prefill layer  28\n",
      "experts to GPU done...\n",
      "in prefill layer  29\n",
      "experts to GPU done...\n",
      "in prefill layer  30\n",
      "experts to GPU done...\n",
      "in prefill layer  31\n",
      "experts to GPU done...\n",
      "in decode layer 0\n",
      "in decode layer 1\n",
      "in decode layer 2\n",
      "in decode layer 3\n",
      "in decode layer 4\n",
      "in decode layer 5\n",
      "in decode layer 6\n",
      "in decode layer 7\n",
      "in decode layer 8\n",
      "in decode layer 9\n",
      "in decode layer 10\n",
      "in decode layer 11\n",
      "in decode layer 12\n",
      "in decode layer 13\n",
      "in decode layer 14\n",
      "in decode layer 15\n",
      "in decode layer 16\n",
      "in decode layer 17\n",
      "in decode layer 18\n",
      "in decode layer 19\n",
      "in decode layer 20\n",
      "in decode layer 21\n",
      "in decode layer 22\n",
      "in decode layer 23\n",
      "in decode layer 24\n",
      "in decode layer 25\n",
      "in decode layer 26\n",
      "in decode layer 27\n",
      "in decode layer 28\n",
      "in decode layer 29\n",
      "in decode layer 30\n",
      "in decode layer 31\n",
      "in decode layer 0\n",
      "in decode layer 1\n",
      "in decode layer 2\n",
      "in decode layer 3\n",
      "in decode layer 4\n",
      "in decode layer 5\n",
      "in decode layer 6\n",
      "in decode layer 7\n",
      "in decode layer 8\n",
      "in decode layer 9\n",
      "in decode layer 10\n",
      "in decode layer 11\n",
      "in decode layer 12\n",
      "in decode layer 13\n",
      "in decode layer 14\n",
      "in decode layer 15\n",
      "in decode layer 16\n",
      "in decode layer 17\n",
      "in decode layer 18\n",
      "in decode layer 19\n",
      "in decode layer 20\n",
      "in decode layer 21\n",
      "in decode layer 22\n",
      "in decode layer 23\n",
      "in decode layer 24\n",
      "in decode layer 25\n",
      "in decode layer 26\n",
      "in decode layer 27\n",
      "in decode layer 28\n",
      "in decode layer 29\n",
      "in decode layer 30\n",
      "in decode layer 31\n",
      "in decode layer 0\n",
      "in decode layer 1\n",
      "in decode layer 2\n",
      "in decode layer 3\n",
      "in decode layer 4\n",
      "in decode layer 5\n",
      "in decode layer 6\n",
      "in decode layer 7\n",
      "in decode layer 8\n",
      "in decode layer 9\n",
      "in decode layer 10\n",
      "in decode layer 11\n",
      "in decode layer 12\n",
      "in decode layer 13\n",
      "in decode layer 14\n",
      "in decode layer 15\n",
      "in decode layer 16\n",
      "in decode layer 17\n",
      "in decode layer 18\n",
      "in decode layer 19\n",
      "in decode layer 20\n",
      "in decode layer 21\n",
      "in decode layer 22\n",
      "in decode layer 23\n",
      "in decode layer 24\n",
      "in decode layer 25\n",
      "in decode layer 26\n",
      "in decode layer 27\n",
      "in decode layer 28\n",
      "in decode layer 29\n",
      "in decode layer 30\n",
      "in decode layer 31\n",
      "in decode layer 0\n",
      "in decode layer 1\n",
      "in decode layer 2\n",
      "in decode layer 3\n",
      "in decode layer 4\n",
      "in decode layer 5\n",
      "in decode layer 6\n",
      "in decode layer 7\n",
      "in decode layer 8\n",
      "in decode layer 9\n",
      "in decode layer 10\n",
      "in decode layer 11\n",
      "in decode layer 12\n",
      "in decode layer 13\n",
      "in decode layer 14\n",
      "in decode layer 15\n",
      "in decode layer 16\n",
      "in decode layer 17\n",
      "in decode layer 18\n",
      "in decode layer 19\n",
      "in decode layer 20\n",
      "in decode layer 21\n",
      "in decode layer 22\n",
      "in decode layer 23\n",
      "in decode layer 24\n",
      "in decode layer 25\n",
      "in decode layer 26\n",
      "in decode layer 27\n",
      "in decode layer 28\n",
      "in decode layer 29\n",
      "in decode layer 30\n",
      "in decode layer 31\n",
      "in decode layer 0\n",
      "in decode layer 1\n",
      "in decode layer 2\n",
      "in decode layer 3\n",
      "in decode layer 4\n",
      "in decode layer 5\n",
      "in decode layer 6\n",
      "in decode layer 7\n",
      "in decode layer 8\n",
      "in decode layer 9\n",
      "in decode layer 10\n",
      "in decode layer 11\n",
      "in decode layer 12\n",
      "in decode layer 13\n",
      "in decode layer 14\n",
      "in decode layer 15\n",
      "in decode layer 16\n",
      "in decode layer 17\n",
      "in decode layer 18\n",
      "in decode layer 19\n",
      "in decode layer 20\n",
      "in decode layer 21\n",
      "in decode layer 22\n",
      "in decode layer 23\n",
      "in decode layer 24\n",
      "in decode layer 25\n",
      "in decode layer 26\n",
      "in decode layer 27\n",
      "in decode layer 28\n",
      "in decode layer 29\n",
      "in decode layer 30\n",
      "in decode layer 31\n",
      "in decode layer 0\n",
      "in decode layer 1\n",
      "in decode layer 2\n",
      "in decode layer 3\n",
      "in decode layer 4\n",
      "in decode layer 5\n",
      "in decode layer 6\n",
      "in decode layer 7\n",
      "in decode layer 8\n",
      "in decode layer 9\n",
      "in decode layer 10\n",
      "in decode layer 11\n",
      "in decode layer 12\n",
      "in decode layer 13\n",
      "in decode layer 14\n",
      "in decode layer 15\n",
      "in decode layer 16\n",
      "in decode layer 17\n",
      "in decode layer 18\n",
      "in decode layer 19\n",
      "in decode layer 20\n",
      "in decode layer 21\n",
      "in decode layer 22\n",
      "in decode layer 23\n",
      "in decode layer 24\n",
      "in decode layer 25\n",
      "in decode layer 26\n",
      "in decode layer 27\n",
      "in decode layer 28\n",
      "in decode layer 29\n",
      "in decode layer 30\n",
      "in decode layer 31\n",
      "in decode layer 0\n",
      "in decode layer 1\n",
      "in decode layer 2\n",
      "in decode layer 3\n",
      "in decode layer 4\n",
      "in decode layer 5\n",
      "in decode layer 6\n",
      "in decode layer 7\n",
      "in decode layer 8\n",
      "in decode layer 9\n",
      "in decode layer 10\n",
      "in decode layer 11\n",
      "in decode layer 12\n",
      "in decode layer 13\n",
      "in decode layer 14\n",
      "in decode layer 15\n",
      "in decode layer 16\n",
      "in decode layer 17\n",
      "in decode layer 18\n",
      "in decode layer 19\n",
      "in decode layer 20\n",
      "in decode layer 21\n",
      "in decode layer 22\n",
      "in decode layer 23\n",
      "in decode layer 24\n",
      "in decode layer 25\n",
      "in decode layer 26\n",
      "in decode layer 27\n",
      "in decode layer 28\n",
      "in decode layer 29\n",
      "in decode layer 30\n",
      "in decode layer 31\n",
      "in decode layer 0\n",
      "in decode layer 1\n",
      "in decode layer 2\n",
      "in decode layer 3\n",
      "in decode layer 4\n",
      "in decode layer 5\n",
      "in decode layer 6\n",
      "in decode layer 7\n",
      "in decode layer 8\n",
      "in decode layer 9\n",
      "in decode layer 10\n",
      "in decode layer 11\n",
      "in decode layer 12\n",
      "in decode layer 13\n",
      "in decode layer 14\n",
      "in decode layer 15\n",
      "in decode layer 16\n",
      "in decode layer 17\n",
      "in decode layer 18\n",
      "in decode layer 19\n",
      "in decode layer 20\n",
      "in decode layer 21\n",
      "in decode layer 22\n",
      "in decode layer 23\n",
      "in decode layer 24\n",
      "in decode layer 25\n",
      "in decode layer 26\n",
      "in decode layer 27\n",
      "in decode layer 28\n",
      "in decode layer 29\n",
      "in decode layer 30\n",
      "in decode layer 31\n",
      "in decode layer 0\n",
      "in decode layer 1\n",
      "in decode layer 2\n",
      "in decode layer 3\n",
      "in decode layer 4\n",
      "in decode layer 5\n",
      "in decode layer 6\n",
      "in decode layer 7\n",
      "in decode layer 8\n",
      "in decode layer 9\n",
      "in decode layer 10\n",
      "in decode layer 11\n",
      "in decode layer 12\n",
      "in decode layer 13\n",
      "in decode layer 14\n",
      "in decode layer 15\n",
      "in decode layer 16\n",
      "in decode layer 17\n",
      "in decode layer 18\n",
      "in decode layer 19\n",
      "in decode layer 20\n",
      "in decode layer 21\n",
      "in decode layer 22\n",
      "in decode layer 23\n",
      "in decode layer 24\n",
      "in decode layer 25\n",
      "in decode layer 26\n",
      "in decode layer 27\n",
      "in decode layer 28\n",
      "in decode layer 29\n",
      "in decode layer 30\n",
      "in decode layer 31\n",
      "in decode layer 0\n",
      "in decode layer 1\n",
      "in decode layer 2\n",
      "in decode layer 3\n",
      "in decode layer 4\n",
      "in decode layer 5\n",
      "in decode layer 6\n",
      "in decode layer 7\n",
      "in decode layer 8\n",
      "in decode layer 9\n",
      "in decode layer 10\n",
      "in decode layer 11\n",
      "in decode layer 12\n",
      "in decode layer 13\n",
      "in decode layer 14\n",
      "in decode layer 15\n",
      "in decode layer 16\n",
      "in decode layer 17\n",
      "in decode layer 18\n",
      "in decode layer 19\n",
      "in decode layer 20\n",
      "in decode layer 21\n",
      "in decode layer 22\n",
      "in decode layer 23\n",
      "in decode layer 24\n",
      "in decode layer 25\n",
      "in decode layer 26\n",
      "in decode layer 27\n",
      "in decode layer 28\n",
      "in decode layer 29\n",
      "in decode layer 30\n",
      "in decode layer 31\n",
      "in decode layer 0\n",
      "in decode layer 1\n",
      "in decode layer 2\n",
      "in decode layer 3\n",
      "in decode layer 4\n",
      "in decode layer 5\n",
      "in decode layer 6\n",
      "in decode layer 7\n",
      "in decode layer 8\n",
      "in decode layer 9\n",
      "in decode layer 10\n",
      "in decode layer 11\n",
      "in decode layer 12\n",
      "in decode layer 13\n",
      "in decode layer 14\n",
      "in decode layer 15\n",
      "in decode layer 16\n",
      "in decode layer 17\n",
      "in decode layer 18\n",
      "in decode layer 19\n",
      "in decode layer 20\n",
      "in decode layer 21\n",
      "in decode layer 22\n",
      "in decode layer 23\n",
      "in decode layer 24\n",
      "in decode layer 25\n",
      "in decode layer 26\n",
      "in decode layer 27\n",
      "in decode layer 28\n",
      "in decode layer 29\n",
      "in decode layer 30\n",
      "in decode layer 31\n",
      "in decode layer 0\n",
      "in decode layer 1\n",
      "in decode layer 2\n",
      "in decode layer 3\n",
      "in decode layer 4\n",
      "in decode layer 5\n",
      "in decode layer 6\n",
      "in decode layer 7\n",
      "in decode layer 8\n",
      "in decode layer 9\n",
      "in decode layer 10\n",
      "in decode layer 11\n",
      "in decode layer 12\n",
      "in decode layer 13\n",
      "in decode layer 14\n",
      "in decode layer 15\n",
      "in decode layer 16\n",
      "in decode layer 17\n",
      "in decode layer 18\n",
      "in decode layer 19\n",
      "in decode layer 20\n",
      "in decode layer 21\n",
      "in decode layer 22\n",
      "in decode layer 23\n",
      "in decode layer 24\n",
      "in decode layer 25\n",
      "in decode layer 26\n",
      "in decode layer 27\n",
      "in decode layer 28\n",
      "in decode layer 29\n",
      "in decode layer 30\n",
      "in decode layer 31\n",
      "in decode layer 0\n",
      "in decode layer 1\n",
      "in decode layer 2\n",
      "in decode layer 3\n",
      "in decode layer 4\n",
      "in decode layer 5\n",
      "in decode layer 6\n",
      "in decode layer 7\n",
      "in decode layer 8\n",
      "in decode layer 9\n",
      "in decode layer 10\n",
      "in decode layer 11\n",
      "in decode layer 12\n",
      "in decode layer 13\n",
      "in decode layer 14\n",
      "in decode layer 15\n",
      "in decode layer 16\n",
      "in decode layer 17\n",
      "in decode layer 18\n",
      "in decode layer 19\n",
      "in decode layer 20\n",
      "in decode layer 21\n",
      "in decode layer 22\n",
      "in decode layer 23\n",
      "in decode layer 24\n",
      "in decode layer 25\n",
      "in decode layer 26\n",
      "in decode layer 27\n",
      "in decode layer 28\n",
      "in decode layer 29\n",
      "in decode layer 30\n",
      "in decode layer 31\n",
      "in decode layer 0\n",
      "in decode layer 1\n",
      "in decode layer 2\n",
      "in decode layer 3\n",
      "in decode layer 4\n",
      "in decode layer 5\n",
      "in decode layer 6\n",
      "in decode layer 7\n",
      "in decode layer 8\n",
      "in decode layer 9\n",
      "in decode layer 10\n",
      "in decode layer 11\n",
      "in decode layer 12\n",
      "in decode layer 13\n",
      "in decode layer 14\n",
      "in decode layer 15\n",
      "in decode layer 16\n",
      "in decode layer 17\n",
      "in decode layer 18\n",
      "in decode layer 19\n",
      "in decode layer 20\n",
      "in decode layer 21\n",
      "in decode layer 22\n",
      "in decode layer 23\n",
      "in decode layer 24\n",
      "in decode layer 25\n",
      "in decode layer 26\n",
      "in decode layer 27\n",
      "in decode layer 28\n",
      "in decode layer 29\n",
      "in decode layer 30\n",
      "in decode layer 31\n",
      "in decode layer 0\n",
      "in decode layer 1\n",
      "in decode layer 2\n",
      "in decode layer 3\n",
      "in decode layer 4\n",
      "in decode layer 5\n",
      "in decode layer 6\n",
      "in decode layer 7\n",
      "in decode layer 8\n",
      "in decode layer 9\n",
      "in decode layer 10\n",
      "in decode layer 11\n",
      "in decode layer 12\n",
      "in decode layer 13\n",
      "in decode layer 14\n",
      "in decode layer 15\n",
      "in decode layer 16\n",
      "in decode layer 17\n",
      "in decode layer 18\n",
      "in decode layer 19\n",
      "in decode layer 20\n",
      "in decode layer 21\n",
      "in decode layer 22\n",
      "in decode layer 23\n",
      "in decode layer 24\n",
      "in decode layer 25\n",
      "in decode layer 26\n",
      "in decode layer 27\n",
      "in decode layer 28\n",
      "in decode layer 29\n",
      "in decode layer 30\n",
      "in decode layer 31\n",
      "in decode layer 0\n",
      "in decode layer 1\n",
      "in decode layer 2\n",
      "in decode layer 3\n",
      "in decode layer 4\n",
      "in decode layer 5\n",
      "in decode layer 6\n",
      "in decode layer 7\n",
      "in decode layer 8\n",
      "in decode layer 9\n",
      "in decode layer 10\n",
      "in decode layer 11\n",
      "in decode layer 12\n",
      "in decode layer 13\n",
      "in decode layer 14\n",
      "in decode layer 15\n",
      "in decode layer 16\n",
      "in decode layer 17\n",
      "in decode layer 18\n",
      "in decode layer 19\n",
      "in decode layer 20\n",
      "in decode layer 21\n",
      "in decode layer 22\n",
      "in decode layer 23\n",
      "in decode layer 24\n",
      "in decode layer 25\n",
      "in decode layer 26\n",
      "in decode layer 27\n",
      "in decode layer 28\n",
      "in decode layer 29\n",
      "in decode layer 30\n",
      "in decode layer 31\n",
      "in decode layer 0\n",
      "in decode layer 1\n",
      "in decode layer 2\n",
      "in decode layer 3\n",
      "in decode layer 4\n",
      "in decode layer 5\n",
      "in decode layer 6\n",
      "in decode layer 7\n",
      "in decode layer 8\n",
      "in decode layer 9\n",
      "in decode layer 10\n",
      "in decode layer 11\n",
      "in decode layer 12\n",
      "in decode layer 13\n",
      "in decode layer 14\n",
      "in decode layer 15\n",
      "in decode layer 16\n",
      "in decode layer 17\n",
      "in decode layer 18\n",
      "in decode layer 19\n",
      "in decode layer 20\n",
      "in decode layer 21\n",
      "in decode layer 22\n",
      "in decode layer 23\n",
      "in decode layer 24\n",
      "in decode layer 25\n",
      "in decode layer 26\n",
      "in decode layer 27\n",
      "in decode layer 28\n",
      "in decode layer 29\n",
      "in decode layer 30\n",
      "in decode layer 31\n",
      "in decode layer 0\n",
      "in decode layer 1\n",
      "in decode layer 2\n",
      "in decode layer 3\n",
      "in decode layer 4\n",
      "in decode layer 5\n",
      "in decode layer 6\n",
      "in decode layer 7\n",
      "in decode layer 8\n",
      "in decode layer 9\n",
      "in decode layer 10\n",
      "in decode layer 11\n",
      "in decode layer 12\n",
      "in decode layer 13\n",
      "in decode layer 14\n",
      "in decode layer 15\n",
      "in decode layer 16\n",
      "in decode layer 17\n",
      "in decode layer 18\n",
      "in decode layer 19\n",
      "in decode layer 20\n",
      "in decode layer 21\n",
      "in decode layer 22\n",
      "in decode layer 23\n",
      "in decode layer 24\n",
      "in decode layer 25\n",
      "in decode layer 26\n",
      "in decode layer 27\n",
      "in decode layer 28\n",
      "in decode layer 29\n",
      "in decode layer 30\n",
      "in decode layer 31\n",
      "in decode layer 0\n",
      "in decode layer 1\n",
      "in decode layer 2\n",
      "in decode layer 3\n",
      "in decode layer 4\n",
      "in decode layer 5\n",
      "in decode layer 6\n",
      "in decode layer 7\n",
      "in decode layer 8\n",
      "in decode layer 9\n",
      "in decode layer 10\n",
      "in decode layer 11\n",
      "in decode layer 12\n",
      "in decode layer 13\n",
      "in decode layer 14\n",
      "in decode layer 15\n",
      "in decode layer 16\n",
      "in decode layer 17\n",
      "in decode layer 18\n",
      "in decode layer 19\n",
      "in decode layer 20\n",
      "in decode layer 21\n",
      "in decode layer 22\n",
      "in decode layer 23\n",
      "in decode layer 24\n",
      "in decode layer 25\n",
      "in decode layer 26\n",
      "in decode layer 27\n",
      "in decode layer 28\n",
      "in decode layer 29\n",
      "in decode layer 30\n",
      "in decode layer 31\n",
      "in decode layer 0\n",
      "in decode layer 1\n",
      "in decode layer 2\n",
      "in decode layer 3\n",
      "in decode layer 4\n",
      "in decode layer 5\n",
      "in decode layer 6\n",
      "in decode layer 7\n",
      "in decode layer 8\n",
      "in decode layer 9\n",
      "in decode layer 10\n",
      "in decode layer 11\n",
      "in decode layer 12\n",
      "in decode layer 13\n",
      "in decode layer 14\n",
      "in decode layer 15\n",
      "in decode layer 16\n",
      "in decode layer 17\n",
      "in decode layer 18\n",
      "in decode layer 19\n",
      "in decode layer 20\n",
      "in decode layer 21\n",
      "in decode layer 22\n",
      "in decode layer 23\n",
      "in decode layer 24\n",
      "in decode layer 25\n",
      "in decode layer 26\n",
      "in decode layer 27\n",
      "in decode layer 28\n",
      "in decode layer 29\n",
      "in decode layer 30\n",
      "in decode layer 31\n",
      "in decode layer 0\n",
      "in decode layer 1\n",
      "in decode layer 2\n",
      "in decode layer 3\n",
      "in decode layer 4\n",
      "in decode layer 5\n",
      "in decode layer 6\n",
      "in decode layer 7\n",
      "in decode layer 8\n",
      "in decode layer 9\n",
      "in decode layer 10\n",
      "in decode layer 11\n",
      "in decode layer 12\n",
      "in decode layer 13\n",
      "in decode layer 14\n",
      "in decode layer 15\n",
      "in decode layer 16\n",
      "in decode layer 17\n",
      "in decode layer 18\n",
      "in decode layer 19\n",
      "in decode layer 20\n",
      "in decode layer 21\n",
      "in decode layer 22\n",
      "in decode layer 23\n",
      "in decode layer 24\n",
      "in decode layer 25\n",
      "in decode layer 26\n",
      "in decode layer 27\n",
      "in decode layer 28\n",
      "in decode layer 29\n",
      "in decode layer 30\n",
      "in decode layer 31\n",
      "in decode layer 0\n",
      "in decode layer 1\n",
      "in decode layer 2\n",
      "in decode layer 3\n",
      "in decode layer 4\n",
      "in decode layer 5\n",
      "in decode layer 6\n",
      "in decode layer 7\n",
      "in decode layer 8\n",
      "in decode layer 9\n",
      "in decode layer 10\n",
      "in decode layer 11\n",
      "in decode layer 12\n",
      "in decode layer 13\n",
      "in decode layer 14\n",
      "in decode layer 15\n",
      "in decode layer 16\n",
      "in decode layer 17\n",
      "in decode layer 18\n",
      "in decode layer 19\n",
      "in decode layer 20\n",
      "in decode layer 21\n",
      "in decode layer 22\n",
      "in decode layer 23\n",
      "in decode layer 24\n",
      "in decode layer 25\n",
      "in decode layer 26\n",
      "in decode layer 27\n",
      "in decode layer 28\n",
      "in decode layer 29\n",
      "in decode layer 30\n",
      "in decode layer 31\n",
      "in decode layer 0\n",
      "in decode layer 1\n",
      "in decode layer 2\n",
      "in decode layer 3\n",
      "in decode layer 4\n",
      "in decode layer 5\n",
      "in decode layer 6\n",
      "in decode layer 7\n",
      "in decode layer 8\n",
      "in decode layer 9\n",
      "in decode layer 10\n",
      "in decode layer 11\n",
      "in decode layer 12\n",
      "in decode layer 13\n",
      "in decode layer 14\n",
      "in decode layer 15\n",
      "in decode layer 16\n",
      "in decode layer 17\n",
      "in decode layer 18\n",
      "in decode layer 19\n",
      "in decode layer 20\n",
      "in decode layer 21\n",
      "in decode layer 22\n",
      "in decode layer 23\n",
      "in decode layer 24\n",
      "in decode layer 25\n",
      "in decode layer 26\n",
      "in decode layer 27\n",
      "in decode layer 28\n",
      "in decode layer 29\n",
      "in decode layer 30\n",
      "in decode layer 31\n",
      "in decode layer 0\n",
      "in decode layer 1\n",
      "in decode layer 2\n",
      "in decode layer 3\n",
      "in decode layer 4\n",
      "in decode layer 5\n",
      "in decode layer 6\n",
      "in decode layer 7\n",
      "in decode layer 8\n",
      "in decode layer 9\n",
      "in decode layer 10\n",
      "in decode layer 11\n",
      "in decode layer 12\n",
      "in decode layer 13\n",
      "in decode layer 14\n",
      "in decode layer 15\n",
      "in decode layer 16\n",
      "in decode layer 17\n",
      "in decode layer 18\n",
      "in decode layer 19\n",
      "in decode layer 20\n",
      "in decode layer 21\n",
      "in decode layer 22\n",
      "in decode layer 23\n",
      "in decode layer 24\n",
      "in decode layer 25\n",
      "in decode layer 26\n",
      "in decode layer 27\n",
      "in decode layer 28\n",
      "in decode layer 29\n",
      "in decode layer 30\n",
      "in decode layer 31\n",
      "in decode layer 0\n",
      "in decode layer 1\n",
      "in decode layer 2\n",
      "in decode layer 3\n",
      "in decode layer 4\n",
      "in decode layer 5\n",
      "in decode layer 6\n",
      "in decode layer 7\n",
      "in decode layer 8\n",
      "in decode layer 9\n",
      "in decode layer 10\n",
      "in decode layer 11\n",
      "in decode layer 12\n",
      "in decode layer 13\n",
      "in decode layer 14\n",
      "in decode layer 15\n",
      "in decode layer 16\n",
      "in decode layer 17\n",
      "in decode layer 18\n",
      "in decode layer 19\n",
      "in decode layer 20\n",
      "in decode layer 21\n",
      "in decode layer 22\n",
      "in decode layer 23\n",
      "in decode layer 24\n",
      "in decode layer 25\n",
      "in decode layer 26\n",
      "in decode layer 27\n",
      "in decode layer 28\n",
      "in decode layer 29\n",
      "in decode layer 30\n",
      "in decode layer 31\n",
      "in decode layer 0\n",
      "in decode layer 1\n",
      "in decode layer 2\n",
      "in decode layer 3\n",
      "in decode layer 4\n",
      "in decode layer 5\n",
      "in decode layer 6\n",
      "in decode layer 7\n",
      "in decode layer 8\n",
      "in decode layer 9\n",
      "in decode layer 10\n",
      "in decode layer 11\n",
      "in decode layer 12\n",
      "in decode layer 13\n",
      "in decode layer 14\n",
      "in decode layer 15\n",
      "in decode layer 16\n",
      "in decode layer 17\n",
      "in decode layer 18\n",
      "in decode layer 19\n",
      "in decode layer 20\n",
      "in decode layer 21\n",
      "in decode layer 22\n",
      "in decode layer 23\n",
      "in decode layer 24\n",
      "in decode layer 25\n",
      "in decode layer 26\n",
      "in decode layer 27\n",
      "in decode layer 28\n",
      "in decode layer 29\n",
      "in decode layer 30\n",
      "in decode layer 31\n",
      "in decode layer 0\n",
      "in decode layer 1\n",
      "in decode layer 2\n",
      "in decode layer 3\n",
      "in decode layer 4\n",
      "in decode layer 5\n",
      "in decode layer 6\n",
      "in decode layer 7\n",
      "in decode layer 8\n",
      "in decode layer 9\n",
      "in decode layer 10\n",
      "in decode layer 11\n",
      "in decode layer 12\n",
      "in decode layer 13\n",
      "in decode layer 14\n",
      "in decode layer 15\n",
      "in decode layer 16\n",
      "in decode layer 17\n",
      "in decode layer 18\n",
      "in decode layer 19\n",
      "in decode layer 20\n",
      "in decode layer 21\n",
      "in decode layer 22\n",
      "in decode layer 23\n",
      "in decode layer 24\n",
      "in decode layer 25\n",
      "in decode layer 26\n",
      "in decode layer 27\n",
      "in decode layer 28\n",
      "in decode layer 29\n",
      "in decode layer 30\n",
      "in decode layer 31\n",
      "in decode layer 0\n",
      "in decode layer 1\n",
      "in decode layer 2\n",
      "in decode layer 3\n",
      "in decode layer 4\n",
      "in decode layer 5\n",
      "in decode layer 6\n",
      "in decode layer 7\n",
      "in decode layer 8\n",
      "in decode layer 9\n",
      "in decode layer 10\n",
      "in decode layer 11\n",
      "in decode layer 12\n",
      "in decode layer 13\n",
      "in decode layer 14\n",
      "in decode layer 15\n",
      "in decode layer 16\n",
      "in decode layer 17\n",
      "in decode layer 18\n",
      "in decode layer 19\n",
      "in decode layer 20\n",
      "in decode layer 21\n",
      "in decode layer 22\n",
      "in decode layer 23\n",
      "in decode layer 24\n",
      "in decode layer 25\n",
      "in decode layer 26\n",
      "in decode layer 27\n",
      "in decode layer 28\n",
      "in decode layer 29\n",
      "in decode layer 30\n",
      "in decode layer 31\n",
      "in decode layer 0\n",
      "in decode layer 1\n",
      "in decode layer 2\n",
      "in decode layer 3\n",
      "in decode layer 4\n",
      "in decode layer 5\n",
      "in decode layer 6\n",
      "in decode layer 7\n",
      "in decode layer 8\n",
      "in decode layer 9\n",
      "in decode layer 10\n",
      "in decode layer 11\n",
      "in decode layer 12\n",
      "in decode layer 13\n",
      "in decode layer 14\n",
      "in decode layer 15\n",
      "in decode layer 16\n",
      "in decode layer 17\n",
      "in decode layer 18\n",
      "in decode layer 19\n",
      "in decode layer 20\n",
      "in decode layer 21\n",
      "in decode layer 22\n",
      "in decode layer 23\n",
      "in decode layer 24\n",
      "in decode layer 25\n",
      "in decode layer 26\n",
      "in decode layer 27\n",
      "in decode layer 28\n",
      "in decode layer 29\n",
      "in decode layer 30\n",
      "in decode layer 31\n",
      "in decode layer 0\n",
      "in decode layer 1\n",
      "in decode layer 2\n",
      "in decode layer 3\n",
      "in decode layer 4\n",
      "in decode layer 5\n",
      "in decode layer 6\n",
      "in decode layer 7\n",
      "in decode layer 8\n",
      "in decode layer 9\n",
      "in decode layer 10\n",
      "in decode layer 11\n",
      "in decode layer 12\n",
      "in decode layer 13\n",
      "in decode layer 14\n",
      "in decode layer 15\n",
      "in decode layer 16\n",
      "in decode layer 17\n",
      "in decode layer 18\n",
      "in decode layer 19\n",
      "in decode layer 20\n",
      "in decode layer 21\n",
      "in decode layer 22\n",
      "in decode layer 23\n",
      "in decode layer 24\n",
      "in decode layer 25\n",
      "in decode layer 26\n",
      "in decode layer 27\n",
      "in decode layer 28\n",
      "in decode layer 29\n",
      "in decode layer 30\n",
      "in decode layer 31\n",
      "in decode layer 0\n",
      "in decode layer 1\n",
      "in decode layer 2\n",
      "in decode layer 3\n",
      "in decode layer 4\n",
      "in decode layer 5\n",
      "in decode layer 6\n",
      "in decode layer 7\n",
      "in decode layer 8\n",
      "in decode layer 9\n",
      "in decode layer 10\n",
      "in decode layer 11\n",
      "in decode layer 12\n",
      "in decode layer 13\n",
      "in decode layer 14\n",
      "in decode layer 15\n",
      "in decode layer 16\n",
      "in decode layer 17\n",
      "in decode layer 18\n",
      "in decode layer 19\n",
      "in decode layer 20\n",
      "in decode layer 21\n",
      "in decode layer 22\n",
      "in decode layer 23\n",
      "in decode layer 24\n",
      "in decode layer 25\n",
      "in decode layer 26\n",
      "in decode layer 27\n",
      "in decode layer 28\n",
      "in decode layer 29\n",
      "in decode layer 30\n",
      "in decode layer 31\n",
      "Generated output length: 32 Time taken: 5.9743 seconds\n",
      "['The future of AI is hereiasm desperate tuneauc lamp’ Q adn /******/ field Po BdŹ att Vtik\\\\\\\\ con` clientfields sav Tia\"                 treatEs.']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "\n",
    "input_length = 6\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 32\n",
    "device_id = 0\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "filt_type = fineweb_path.split('.')[-1]\n",
    "fineweb = load_dataset(filt_type, data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text']\n",
    "\n",
    "generated_all = 0\n",
    "prefill_time, decode_time = 0, 0\n",
    "print(\"output length is {}\".format(output_length))\n",
    "text = \"The future of AI is \"\n",
    "inputs = preprocess_data(text, tokenizer)\n",
    "\n",
    "# 预热（避免第一次运行时的额外开销）\n",
    "with torch.no_grad():\n",
    "    output = llm(input_ids=inputs[\"input_ids\"].cuda(device_id), attention_mask=inputs[\"attention_mask\"].cuda(device_id))\n",
    "\n",
    "# 测试时间\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "# 开始计时\n",
    "torch.cuda.synchronize()\n",
    "start_event.record()\n",
    "\n",
    "# 前向传播\n",
    "with torch.no_grad():\n",
    "    output = llm.generate(\n",
    "        input_ids=inputs[\"input_ids\"].cuda(device_id),\n",
    "        attention_mask=inputs[\"attention_mask\"].cuda(device_id),\n",
    "        max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "        generation_config=GenerationConfig(do_sample=False),\n",
    "        pad_token_id=tokenizer.pad_token_id, \n",
    "        # cache_implementation=\"static\" ## moe not support\n",
    "    )\n",
    "\n",
    "# 结束计时\n",
    "end_event.record()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# 计算时间\n",
    "elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "decode_time += elapsed_time\n",
    "print(f\"Generated output length: {len(output[0]) - input_length}\", f\"Time taken: {elapsed_time:.4f} seconds\")\n",
    "# print(output)\n",
    "print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "\n",
    "generated_all += len(output[0]) - input_length\n",
    "\n",
    "# timepertoken = (decode_time) / (generated_all)\n",
    "# print(\"decode time:\", '{:.4f}'.format((decode_time) /test_samples), ' s')\n",
    "# print(\"decode phase speed:\", '{:.4f}'.format(1/timepertoken) , ' token/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载到GPU上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:25<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MixtralForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "import json\n",
    "\n",
    "def get_model(model_name, device_map, dtype=torch.bfloat16):\n",
    "    llm = MixtralForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        use_cache=True,\n",
    "        torch_dtype=dtype,\n",
    "    ) \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return llm, tokenizer\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "with open('../quantize/device_map_1.json', 'r') as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "dtype = torch.float16\n",
    "llm, tokenizer = get_model(model_name, device_map, dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 只传一个专家的版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import threading\n",
    "import json\n",
    "from queue import Queue\n",
    "\n",
    "class CachedMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, dtype, sparsity: float = 0.2):\n",
    "        super(CachedMLP, self).__init__()\n",
    "        self.sparsity = sparsity\n",
    "        self.activenum = int((1 - sparsity) * hidden_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dtype = dtype\n",
    "\n",
    "        # GPU 缓存张量\n",
    "        self.w1_gpu = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda:0')\n",
    "        self.w2_gpu = torch.empty((self.input_dim, self.activenum), dtype=self.dtype, device='cuda:0')\n",
    "        self.w3_gpu = torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda:0')\n",
    "\n",
    "        # Pinned Memory 缓冲区\n",
    "        self.register_buffer('sparse_w1_cpu', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "        self.register_buffer('sparse_w2_cpu', torch.empty((self.input_dim, self.activenum), dtype=self.dtype, device='cpu'))\n",
    "        self.register_buffer('sparse_w3_cpu', torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu'))\n",
    "        self.sparse_w1_cpu = self.sparse_w1_cpu.pin_memory()\n",
    "        self.sparse_w2_cpu = self.sparse_w2_cpu.pin_memory()\n",
    "        self.sparse_w3_cpu = self.sparse_w3_cpu.pin_memory()\n",
    "\n",
    "        # 统计信息\n",
    "        self.load_from_cpu_time = 0.0\n",
    "        self.load_from_cpu_calls = 0\n",
    "\n",
    "    def load_from_cpu(self, cpu_mlp, stream: torch.cuda.Stream):\n",
    "        \"\"\"\n",
    "        从CPU加载参数，并使用指定的CUDA流进行异步复制到GPU。\n",
    "        \n",
    "        参数:\n",
    "            cpu_mlp: 包含CPU上参数的字典。\n",
    "            stream: 用于数据传输的CUDA流。\n",
    "        \"\"\"\n",
    "        # 从CPU加载参数\n",
    "        self.sparse_w1_cpu.copy_(cpu_mlp['w1'].data[:self.activenum, :])\n",
    "        self.sparse_w2_cpu.copy_(cpu_mlp['w2'].data[:, :self.activenum])\n",
    "        self.sparse_w3_cpu.copy_(cpu_mlp['w3'].data[:self.activenum, :])\n",
    "\n",
    "        # 异步复制到GPU\n",
    "        with torch.cuda.stream(stream):\n",
    "            self.w1_gpu.copy_(self.sparse_w1_cpu, non_blocking=True)\n",
    "            self.w2_gpu.copy_(self.sparse_w2_cpu, non_blocking=True)\n",
    "            self.w3_gpu.copy_(self.sparse_w3_cpu, non_blocking=True)\n",
    "\n",
    "    def get_load_from_cpu_stats(self):\n",
    "        if self.load_from_cpu_calls == 0:\n",
    "            return 0.0, 0.0\n",
    "        avg_time = self.load_from_cpu_time / self.load_from_cpu_calls\n",
    "        return self.load_from_cpu_time, avg_time\n",
    "\n",
    "    def clear_load_from_cpu_stats(self):\n",
    "        self.load_from_cpu_time = 0.0\n",
    "        self.load_from_cpu_calls = 0\n",
    "\n",
    "def convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.9):\n",
    "    ### 其他部分存放在GPU上\n",
    "    llm.model.embed_tokens.cuda()\n",
    "    for i in range(len(llm.model.layers)):\n",
    "        llm.model.layers[i].self_attn.cuda()\n",
    "        llm.model.layers[i].input_layernorm.cuda()\n",
    "        llm.model.layers[i].post_attention_layernorm.cuda()\n",
    "        llm.model.layers[i].block_sparse_moe.gate.cuda()\n",
    "    ### 第0层的专家存放在GPU上\n",
    "    for j in range(len(llm.model.layers[0].block_sparse_moe.experts)):\n",
    "        llm.model.layers[0].block_sparse_moe.experts[j].cuda()\n",
    "\n",
    "    llm.model.norm.cuda()\n",
    "    llm.lm_head.cuda()\n",
    "    \n",
    "    # 创建两个共享的CachedMLP实例\n",
    "    buffer0 = CachedMLP(\n",
    "        input_dim=llm.config.hidden_size,\n",
    "        hidden_dim=llm.config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "        sparsity=sparsity\n",
    "    )\n",
    "    buffer1 = CachedMLP(\n",
    "        input_dim=llm.config.hidden_size,\n",
    "        hidden_dim=llm.config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "        sparsity=sparsity\n",
    "    )\n",
    "    cached_mlps = [buffer0, buffer1]\n",
    "    \n",
    "    for i, layer in enumerate(llm.model.layers):\n",
    "        if i==0:\n",
    "            continue\n",
    "        # 将专家的forward方法替换为PipelineLLM管理的方式\n",
    "        for j, expert in enumerate(layer.block_sparse_moe.experts):\n",
    "            expert.cpu_mlp = {\n",
    "                \"w1\": expert.w1.cpu().weight,\n",
    "                \"w2\": expert.w2.cpu().weight,\n",
    "                \"w3\": expert.w3.cpu().weight,\n",
    "            }\n",
    "            # 替换forward方法为直接调用CachedMLP的forward（需要在pipelineLLM里面替换)\n",
    "            # expert.forward = lambda x, cached_mlp=cached_mlp, cpu_mlp=expert.cpu_mlp: cached_mlp(x, cpu_mlp)\n",
    "    return llm, cached_mlps\n",
    "\n",
    "class PipelineLLM:\n",
    "    def __init__(self, llm, cached_mlps):\n",
    "        \"\"\"\n",
    "        初始化 PipelineLLM，替换模型每一层的 forward 方法。\n",
    "        \n",
    "        参数:\n",
    "            llm: 原始的大模型\n",
    "            cached_mlps: 两个 CachedMLP 实例列表\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.cached_mlps = cached_mlps  # [buffer0, buffer1]\n",
    "        self.num_layers = len(llm.model.layers)\n",
    "        self.lock = threading.Lock()\n",
    "        self.use_buffer0 = True  # 标记当前使用哪个缓冲区\n",
    "\n",
    "        # 创建两个共享的CUDA流\n",
    "        self.stream0 = torch.cuda.Stream()\n",
    "        self.stream1 = torch.cuda.Stream()\n",
    "\n",
    "        # 初始化加载第一个和第二个层的参数\n",
    "        self._load_layer(1, buffer_index=0)\n",
    "        self._load_layer(1, buffer_index=1)\n",
    "\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "        self._replace_forward_methods()\n",
    "    \n",
    "    def _load_layer(self, layer_idx, buffer_index):\n",
    "        \"\"\"\n",
    "        加载指定层的参数到指定的缓冲区。\n",
    "        \n",
    "        参数:\n",
    "            layer_idx: 层的索引\n",
    "            buffer_index: 缓冲区的索引（0 或 1）\n",
    "        \"\"\"\n",
    "        layer = self.llm.model.layers[layer_idx]\n",
    "        expert = layer.block_sparse_moe.experts[0]\n",
    "        cpu_mlp = expert.cpu_mlp\n",
    "        buffer = self.cached_mlps[buffer_index]\n",
    "        stream = self.stream0 if buffer_index == 0 else self.stream1\n",
    "\n",
    "        # 异步加载参数\n",
    "        buffer.load_from_cpu(cpu_mlp, stream)\n",
    "\n",
    "    def _replace_forward_methods(self):\n",
    "        \"\"\"\n",
    "        替换模型每一层的 forward 方法，添加参数预加载逻辑和注意力计算。\n",
    "        \"\"\"\n",
    "        for i, layer in enumerate(self.llm.model.layers):\n",
    "            def new_forward(hidden_states: torch.Tensor,\n",
    "                            attention_mask: Optional[torch.Tensor] = None,\n",
    "                            position_ids: Optional[torch.LongTensor] = None,\n",
    "                            past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "                            output_attentions: Optional[bool] = False,\n",
    "                            output_router_logits: Optional[bool] = False,\n",
    "                            use_cache: Optional[bool] = False,\n",
    "                            cache_position: Optional[torch.LongTensor] = None,\n",
    "                            layer_idx=i):\n",
    "                # print(f\"in layer {layer_idx}\")\n",
    "                with self.lock:\n",
    "                    # 选择当前使用的缓冲区\n",
    "                    current_buffer = self.cached_mlps[0] if self.use_buffer0 else self.cached_mlps[1]\n",
    "                    current_stream = self.stream0 if self.use_buffer0 else self.stream1\n",
    "\n",
    "                    # 切换缓冲区用于下一次\n",
    "                    next_buffer_index = 1 if self.use_buffer0 else 0\n",
    "                    next_buffer = self.cached_mlps[next_buffer_index]\n",
    "                    next_stream = self.stream1 if self.use_buffer0 else self.stream0\n",
    "\n",
    "                    # 预加载下一层的参数\n",
    "                    next_layer_idx = layer_idx + 1\n",
    "                    if next_layer_idx < self.num_layers:\n",
    "                        self._load_layer(next_layer_idx, buffer_index=next_buffer_index)\n",
    "                    \n",
    "                    # 切换缓冲区\n",
    "                    self.use_buffer0 = not self.use_buffer0\n",
    "\n",
    "                    # 处理当前层\n",
    "                    residual = hidden_states\n",
    "                    hidden_states = layer.input_layernorm(hidden_states)\n",
    "\n",
    "                    # Self Attention\n",
    "                    hidden_states, self_attn_weights, present_key_value = layer.self_attn(\n",
    "                        hidden_states=hidden_states,\n",
    "                        attention_mask=attention_mask,\n",
    "                        position_ids=position_ids,\n",
    "                        past_key_value=past_key_value,\n",
    "                        output_attentions=output_attentions,\n",
    "                        use_cache=use_cache,\n",
    "                        cache_position=cache_position,\n",
    "                    )\n",
    "                    hidden_states = residual + hidden_states\n",
    "\n",
    "                    # Fully Connected\n",
    "                    residual = hidden_states\n",
    "                    hidden_states = layer.post_attention_layernorm(hidden_states)\n",
    "\n",
    "                    # 使用当前缓冲区进行 MLP 计算\n",
    "                    batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "                    hidden_states = hidden_states.view(-1, hidden_dim)\n",
    "                    # 仅使用第一个专家\n",
    "                    expert_layer = layer.block_sparse_moe.experts[0]\n",
    "\n",
    "                    w3_output = torch.matmul(hidden_states, current_buffer.w3_gpu.T)\n",
    "                    w1_output = self.activation(torch.matmul(hidden_states, current_buffer.w1_gpu.T))\n",
    "                    w2 = current_buffer.w2_gpu.T\n",
    "                    final_hidden_states = torch.matmul(w1_output * w3_output, w2)\n",
    "\n",
    "                    final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n",
    "\n",
    "                    hidden_states = residual + final_hidden_states\n",
    "\n",
    "                    outputs = (hidden_states,)\n",
    "\n",
    "                    if output_attentions:\n",
    "                        outputs += (self_attn_weights,)\n",
    "\n",
    "                    if use_cache:\n",
    "                        outputs += (present_key_value,)\n",
    "\n",
    "                    return outputs\n",
    "            # 替换 forward 方法\n",
    "            layer.forward = new_forward\n",
    "\n",
    "    def _async_load(self, layer_idx, buffer_index):\n",
    "        \"\"\"\n",
    "        异步加载 MLP 参数到指定缓冲区，使用共享的CUDA流。\n",
    "        \"\"\"\n",
    "        self._load_layer(layer_idx, buffer_index)\n",
    "\n",
    "# 将模型转换为使用CachedMLP的版本\n",
    "llm, cached_mlps = convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.9)\n",
    "\n",
    "# 创建流水线模型\n",
    "pipeline_llm = PipelineLLM(llm, cached_mlps).llm\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
