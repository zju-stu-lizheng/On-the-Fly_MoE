{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先都加载到cpu上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"\n",
    "# from modeling_mixtral import MixtralForCausalLM\n",
    "from transformers import AutoTokenizer, MixtralForCausalLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import json\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "with open(\"../quantize/device_map.json\", \"r\") as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "def get_model(model_name, device_map, dtype=torch.bfloat16, use_cache=True):\n",
    "    llm = MixtralForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        use_cache=use_cache,\n",
    "        torch_dtype=dtype,\n",
    "    ) \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return llm, tokenizer\n",
    "\n",
    "dtype = torch.float16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# llm, tokenizer = get_model(model_name, 'cpu', dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/venv/dilab/floe/hqq/hqq/models/base.py:251: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(cls.get_weight_file(save_dir), map_location=map_location)\n",
      "100%|██████████| 32/32 [00:00<00:00, 424.80it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 1123.33it/s]\n"
     ]
    }
   ],
   "source": [
    "### HQQ量化\n",
    "from hqq.core.quantize import *\n",
    "from hqq.models.hf.mixtral import MixtralHQQ\n",
    "\n",
    "save_dir = './hqqsaved'\n",
    "### 第一次加载\n",
    "# q3_config    = BaseQuantizeConfig(nbits=2, group_size=64)\n",
    "# quant_config      = {'block_sparse_moe.experts.w3'   : q3_config}\n",
    "# llm = MixtralForCausalLM.from_pretrained(\n",
    "#         model_name,\n",
    "#         device_map='cpu',\n",
    "#         use_cache=True,\n",
    "#         torch_dtype=dtype,\n",
    "#     ) \n",
    "# MixtralHQQ.quantize_model(llm, quant_config=quant_config, compute_dtype=dtype, device='cuda:0')\n",
    "#### 先放CUDA量化，然后再传回CPU\n",
    "# MixtralHQQ.save_quantized(llm, save_dir)\n",
    "\n",
    "### 从保存的权重中加载\n",
    "llm = MixtralHQQ.from_quantized(save_dir, compute_dtype=dtype, device='cpu')\n",
    "HQQLinear.set_backend(HQQBackend.PYTORCH)\n",
    "\n",
    "# backend       = \"gemlite\" #'torchao_int4' #\"torchao_int4\" (4-bit only) or \"gemlite\" (4-bit + 2-bit)\n",
    "# #Optimize\n",
    "# from hqq.utils.patching import prepare_for_inference\n",
    "# prepare_for_inference(llm, backend=backend, verbose=True)\n",
    "# #Load GemLite cache\n",
    "# if(backend == 'gemlite'):\n",
    "# \timport gemlite\n",
    "# \tgemlite.core.GEMLITE_TRITON_RESTRICT_M = True\n",
    "# \tgemlite.core.GemLiteLinear.load_config('/tmp/gemlite_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import threading\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Expert_Predictor(nn.Module):\n",
    "    def __init__(self, layer_idx: int = 0, input_dim: int = 4096, hidden_dim: int = 512, output_dim: int = 8, dtype = torch.float16):\n",
    "        super(Expert_Predictor, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim, dtype=dtype)\n",
    "        self.activation = nn.SiLU() \n",
    "        self.linear2 = nn.Linear(hidden_dim,output_dim, dtype=dtype) \n",
    "\n",
    "        self.load_state_dict(torch.load(f'/home/bcds/On-the-Fly_MoE_Inference/expert_predictor/training/{layer_idx}.pth'))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.linear1(hidden_states)\n",
    "        hidden_states = self.activation(hidden_states)\n",
    "        return self.linear2(hidden_states)\n",
    "\n",
    "#### 增加专家preload失败后的重新加载\n",
    "class CachedMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, dtype, sparsity: float = 0.2):\n",
    "        super(CachedMLP, self).__init__()\n",
    "        self.sparsity = sparsity\n",
    "        self.activenum = int((1 - sparsity) * hidden_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dtype = dtype\n",
    "        print(\"active neural num \",self.activenum)\n",
    "\n",
    "        self.activation = nn.SiLU()\n",
    "\n",
    "        self.w3_expert0 = None\n",
    "        self.w3_expert1 = None\n",
    "\n",
    "        self.indices0 = None\n",
    "        self.indices1 = None\n",
    "\n",
    "        # 将GPU缓存张量改为列表存储\n",
    "        self.w_gpu = [\n",
    "            torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cuda:0') for _ in range(4)\n",
    "        ]  # [w1_gpu, w2_gpu, w1_gpu_expert1, w2_gpu_expert1]\n",
    "\n",
    "        # 将Pinned Memory缓冲区改为列表存储\n",
    "        self.sparse_w_cpu = [\n",
    "            torch.empty((self.activenum, self.input_dim), dtype=self.dtype, device='cpu').pin_memory() for _ in range(4)\n",
    "        ]  # [sparse_w1_cpu, sparse_w2_cpu, sparse_w1_cpu_expert1, sparse_w2_cpu_expert1]\n",
    "\n",
    "        ### 增加两个专家序号\n",
    "        self.expert_ids = torch.tensor([0,1])\n",
    "\n",
    "    def get_predict_experts(self):\n",
    "        return self.expert_ids\n",
    "    \n",
    "    def load_conflict_cpu(self, cpu_mlp_new, stream: torch.cuda.Stream, hidden_states, replace_idx):\n",
    "        # 只更新需要加载的专家权重\n",
    "        _, indices = torch.topk(torch.abs(cpu_mlp_new['w3'](hidden_states)), self.activenum, dim=1)\n",
    "        indices = indices[0].cpu()\n",
    "        if replace_idx == 0:\n",
    "            # 更新CPU数据\n",
    "            self.w3_expert0 = cpu_mlp_new['w3']\n",
    "            self.indices0 = indices\n",
    "\n",
    "            self.sparse_w_cpu[0].copy_(cpu_mlp_new['w1'].data[indices, :])\n",
    "            self.sparse_w_cpu[1].copy_(cpu_mlp_new['w2'].data[indices, :])\n",
    "            \n",
    "            # 异步复制到GPU\n",
    "            with torch.cuda.stream(stream):\n",
    "                self.w_gpu[0].copy_(self.sparse_w_cpu[0], non_blocking=True)\n",
    "                self.w_gpu[1].copy_(self.sparse_w_cpu[1], non_blocking=True)\n",
    "        else:\n",
    "            # 更新CPU数据\n",
    "            self.w3_expert1 = cpu_mlp_new['w3']\n",
    "            self.indices1 = indices\n",
    "            self.sparse_w_cpu[2].copy_(cpu_mlp_new['w1'].data[indices, :])\n",
    "            self.sparse_w_cpu[3].copy_(cpu_mlp_new['w2'].data[indices, :])\n",
    "            \n",
    "            # 异步复制到GPU\n",
    "            with torch.cuda.stream(stream):\n",
    "                self.w_gpu[2].copy_(self.sparse_w_cpu[2], non_blocking=True)\n",
    "                self.w_gpu[3].copy_(self.sparse_w_cpu[3], non_blocking=True)\n",
    "\n",
    "    def load_from_cpu(self, cpu_mlp, cpu_mlp_expert1, stream: torch.cuda.Stream, hidden_states):\n",
    "        \"\"\"\n",
    "        从CPU加载参数，并使用指定的CUDA流进行异步复制到GPU。\n",
    "        \n",
    "        参数:\n",
    "            cpu_mlp: 包含CPU上参数的字典（第一个专家）\n",
    "            cpu_mlp_expert1: 包含CPU上参数的字典（第二个专家）。\n",
    "            stream: 用于数据传输的CUDA流。\n",
    "        \"\"\"\n",
    "        ### 根据up计算的结果进行稀疏化\n",
    "        self.w3_expert0 = cpu_mlp['w3']\n",
    "        self.w3_expert1 = cpu_mlp_expert1['w3']\n",
    "        up_result0 = self.w3_expert0(hidden_states)\n",
    "        up_result1 = self.w3_expert1(hidden_states)\n",
    "        # 提取 up_result0 的值并计算 top-k 索引\n",
    "        _, indices0 = torch.topk(torch.abs(up_result0), self.activenum, dim=1)  # 在第二个维度上取 top-k\n",
    "        indices0 = indices0[0].cpu()\n",
    "\n",
    "        _, indices1 = torch.topk(torch.abs(up_result1), self.activenum, dim=1)  # 在第二个维度上取 top-k\n",
    "        indices1 = indices1[0].cpu() \n",
    "\n",
    "        self.indices0 = indices0\n",
    "        self.indices1 = indices1\n",
    "\n",
    "        # 使用列表索引更新CPU数据\n",
    "        self.sparse_w_cpu[0].copy_(cpu_mlp['w1'].data[indices0, :])\n",
    "        self.sparse_w_cpu[1].copy_(cpu_mlp['w2'].data[indices0, :])\n",
    "\n",
    "        # 异步复制到GPU\n",
    "        with torch.cuda.stream(stream):\n",
    "            self.w_gpu[0].copy_(self.sparse_w_cpu[0], non_blocking=True)\n",
    "            self.w_gpu[1].copy_(self.sparse_w_cpu[1], non_blocking=True)\n",
    "\n",
    "        self.sparse_w_cpu[2].copy_(cpu_mlp_expert1['w1'].data[indices1, :])\n",
    "        self.sparse_w_cpu[3].copy_(cpu_mlp_expert1['w2'].data[indices1, :])\n",
    "        \n",
    "        # 异步复制到GPU\n",
    "        with torch.cuda.stream(stream):\n",
    "            self.w_gpu[2].copy_(self.sparse_w_cpu[2], non_blocking=True)\n",
    "            self.w_gpu[3].copy_(self.sparse_w_cpu[3], non_blocking=True)\n",
    "\n",
    "    def load_expert_weights(self, expert_ids):\n",
    "        # print('loading next expert: ', expert_ids)   ## tensor([7, 6], device='cuda:0')\n",
    "        self.expert_ids = expert_ids\n",
    "\n",
    "    def forward(self, hidden_states, expert_weights, expert_ids):\n",
    "        \"\"\"\n",
    "        根据hidden_states， 分别计算两个专家的输出\n",
    "        \"\"\"\n",
    "        # print(expert_weights, expert_ids)\n",
    "        # print(self.expert_ids)\n",
    "        ### 如果取出来的专家 第一个对不上，可能是预测的 1，2号专家顺序颠倒了，替换\n",
    "        if self.expert_ids[0] != expert_ids[0]:\n",
    "            # print(\"----replace expert weights\")\n",
    "            expert_weights[0], expert_weights[1] = expert_weights[1], expert_weights[0]\n",
    "\n",
    "        w3_output = self.w3_expert0(hidden_states)[:, self.indices0]\n",
    "        w1_output = self.activation(torch.matmul(hidden_states, self.w_gpu[0].T))\n",
    "        hidden_states_expert0 = torch.matmul(w1_output * w3_output, self.w_gpu[1])\n",
    "\n",
    "        # 第二个专家的计算\n",
    "        w3_output_expert1 = self.w3_expert1(hidden_states)[:, self.indices0]\n",
    "        w1_output_expert1 = self.activation(torch.matmul(hidden_states, self.w_gpu[2].T))\n",
    "        hidden_states_expert1 = torch.matmul(w1_output_expert1 * w3_output_expert1, self.w_gpu[3])\n",
    "\n",
    "        final_hidden_states = hidden_states_expert0 * expert_weights[0] + hidden_states_expert1 * expert_weights[1]\n",
    "        return final_hidden_states\n",
    "                    \n",
    "\n",
    "class PipelineLLM:\n",
    "    def __init__(self, llm, cached_mlps):\n",
    "        \"\"\"\n",
    "        初始化 PipelineLLM，替换模型每一层的 forward 方法。\n",
    "        \n",
    "        参数:\n",
    "            llm: 原始的大模型\n",
    "            cached_mlps: 两个 CachedMLP 实例列表\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.cached_mlps = cached_mlps  # [buffer0, buffer1]\n",
    "        self.num_layers = len(llm.model.layers)\n",
    "        self.lock = threading.Lock()\n",
    "        self.use_buffer0 = True  # 标记当前使用哪个缓冲区\n",
    "\n",
    "        self.stream0 = torch.cuda.Stream()\n",
    "        self.stream1 = torch.cuda.Stream()\n",
    "\n",
    "        self.top_k = 2\n",
    "        self.activation = nn.SiLU()\n",
    "\n",
    "        # 用于统计时间的变量\n",
    "        self.total_reload_experts = 0\n",
    "\n",
    "        self.prefill_time = 0\n",
    "\n",
    "        #### 增加[1,3]的Expert_Predictor\n",
    "        self.eps = []\n",
    "        self.start_layer = 1\n",
    "        self.end_layer = 3\n",
    "        for i in range(self.start_layer, self.end_layer+1):\n",
    "            ep = Expert_Predictor(layer_idx=i).cuda(0)\n",
    "            self.eps.append(ep)\n",
    "\n",
    "        self._replace_forward_methods()\n",
    "    \n",
    "    def get_reload_experts(self):\n",
    "        tmp = self.total_reload_experts\n",
    "        self.total_reload_experts = 0\n",
    "        return tmp\n",
    "\n",
    "    def get_prefill_time(self):\n",
    "        tmp = self.prefill_time\n",
    "        self.prefill_time = 0\n",
    "        return tmp\n",
    "\n",
    "    def _load_layer(self, layer_idx, buffer, expert_ids, stream,\n",
    "                    hidden_states):\n",
    "        \"\"\"\n",
    "        加载指定层的参数到指定的缓冲区。\n",
    "        \n",
    "        参数:\n",
    "            layer_idx: 层的索引\n",
    "            buffer_index: 缓冲区的索引（0 或 1）\n",
    "        \"\"\"\n",
    "        layer = self.llm.model.layers[layer_idx]\n",
    "        expert0 = layer.block_sparse_moe.experts[expert_ids[0]]\n",
    "        expert1 = layer.block_sparse_moe.experts[expert_ids[1]]\n",
    "\n",
    "        cpu_mlp = expert0.cpu_mlp\n",
    "        cpu_mlp_expert1 = expert1.cpu_mlp\n",
    "\n",
    "        ### weights应该用正确的来算\n",
    "        buffer.load_expert_weights(expert_ids)\n",
    "        # 异步加载参数\n",
    "        buffer.load_from_cpu(cpu_mlp, cpu_mlp_expert1, stream, hidden_states)\n",
    "\n",
    "    def _load_conflict_layer(self, layer_idx, buffer, expert_ids, hidden_states):\n",
    "        \"\"\"\n",
    "        处理专家预测冲突的情况，尽可能复用已加载的专家权重\n",
    "        \n",
    "        参数:\n",
    "            layer_idx: 层索引\n",
    "            buffer_index: 缓冲区索引\n",
    "            expert_ids: 实际需要的专家ID\n",
    "            predict_experts: 预测的专家ID\n",
    "            hidden_states: 输入hidden_states\n",
    "        \"\"\"\n",
    "        predict_experts = buffer.get_predict_experts()\n",
    "        # print(f\"predict_experts in layer {layer_idx}: {predict_experts}\")\n",
    "        # 找出需要加载的新专家\n",
    "        required_experts = set(expert_ids.tolist()) - set(predict_experts.tolist())\n",
    "        \n",
    "        # 如果只需要加载一个专家\n",
    "        if len(required_experts) == 1:\n",
    "            new_expert_id = list(required_experts)[0]\n",
    "            # 找出需要被替换的专家位置\n",
    "            if predict_experts[0] in expert_ids:\n",
    "                replace_idx = 1   \n",
    "                ### 更新buffer中保存的专家序号\n",
    "                new_expert_ids = torch.tensor([predict_experts[0], new_expert_id])\n",
    "            else:\n",
    "                replace_idx = 0\n",
    "                new_expert_ids = torch.tensor([new_expert_id, predict_experts[1]])\n",
    "\n",
    "            # 更新专家ID\n",
    "            buffer.load_expert_weights(new_expert_ids)\n",
    "\n",
    "            # print(f\"reloading 1 experts, {new_expert_id}\")\n",
    "            self.total_reload_experts += 1\n",
    "            \n",
    "            # 获取需要加载的专家\n",
    "            layer = self.llm.model.layers[layer_idx]\n",
    "            new_expert = layer.block_sparse_moe.experts[new_expert_id]\n",
    "            cpu_mlp_new = new_expert.cpu_mlp\n",
    "            \n",
    "            # 只更新需要加载的专家权重\n",
    "            buffer.load_conflict_cpu(cpu_mlp_new, self.stream1, hidden_states, replace_idx)\n",
    "            \n",
    "        else:\n",
    "            # print(f\"reloading 2 experts, {expert_ids[:]}\")\n",
    "            self.total_reload_experts += 2\n",
    "            # 需要加载两个专家，直接调用原始方法\n",
    "            self._load_layer(layer_idx, buffer, expert_ids, self.stream1, hidden_states)\n",
    "\n",
    "    def _replace_forward_methods(self):\n",
    "        \"\"\"\n",
    "        替换模型每一层的 forward 方法，添加参数预加载逻辑和注意力计算。\n",
    "        \"\"\"\n",
    "        for i, layer in enumerate(self.llm.model.layers):\n",
    "            def new_forward(hidden_states: torch.Tensor,\n",
    "                        attention_mask: Optional[torch.Tensor] = None,\n",
    "                        position_ids: Optional[torch.LongTensor] = None,\n",
    "                        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "                        output_attentions: Optional[bool] = False,\n",
    "                        output_router_logits: Optional[bool] = False,\n",
    "                        use_cache: Optional[bool] = False,\n",
    "                        cache_position: Optional[torch.LongTensor] = None,\n",
    "                        layer=layer,\n",
    "                        layer_idx=i):\n",
    "                batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "                \n",
    "                if sequence_length == 1:\n",
    "                    #### decode phase ####\n",
    "                    # 选择当前使用的缓冲区\n",
    "                    current_buffer = self.cached_mlps[0] if self.use_buffer0 else self.cached_mlps[1]\n",
    "                    \n",
    "                    next_buffer_index = 1 if self.use_buffer0 else 0\n",
    "                    next_layer_idx = layer_idx + 1\n",
    "                    if next_layer_idx < self.num_layers:\n",
    "                        ### 使用下一个缓冲区进行加载\n",
    "                        next_buffer = self.cached_mlps[next_buffer_index]\n",
    "\n",
    "                        # 预加载下一层的参数\n",
    "                        # batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "                        hidden_states = hidden_states.view(-1, hidden_dim)\n",
    "                        # router_logits: (batch * sequence_length, n_experts)\n",
    "                        if self.start_layer <= next_layer_idx <= self.end_layer:\n",
    "                            ### 使用训练好的专家预测矩阵\n",
    "                            router = self.eps[next_layer_idx - self.start_layer]\n",
    "                        else:\n",
    "                            ### 使用next_layer_idx对应的gate矩阵\n",
    "                            router = self.llm.model.layers[next_layer_idx].block_sparse_moe.gate\n",
    "                        router_logits = router(hidden_states)\n",
    "\n",
    "                        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "                        _, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n",
    "                        # routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n",
    "\n",
    "                        self._load_layer(\n",
    "                            next_layer_idx,\n",
    "                            buffer=next_buffer,\n",
    "                            expert_ids=selected_experts[0],\n",
    "                            stream=self.stream0,\n",
    "                            hidden_states=hidden_states,\n",
    "                        )\n",
    "                        hidden_states = hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n",
    "\n",
    "                        # 切换缓冲区\n",
    "                        self.use_buffer0 = not self.use_buffer0\n",
    "\n",
    "                # 处理当前层\n",
    "                residual = hidden_states\n",
    "                hidden_states = layer.input_layernorm(hidden_states)\n",
    "\n",
    "                # Self Attention\n",
    "                hidden_states, self_attn_weights, present_key_value = layer.self_attn(\n",
    "                    hidden_states=hidden_states,\n",
    "                    attention_mask=attention_mask,\n",
    "                    position_ids=position_ids,\n",
    "                    past_key_value=past_key_value,\n",
    "                    output_attentions=output_attentions,\n",
    "                    use_cache=use_cache,\n",
    "                    cache_position=cache_position,\n",
    "                )\n",
    "                hidden_states = residual + hidden_states\n",
    "\n",
    "                # Fully Connected\n",
    "                residual = hidden_states\n",
    "                hidden_states = layer.post_attention_layernorm(hidden_states)\n",
    "                \n",
    "                if sequence_length > 1:\n",
    "                    start_event = torch.cuda.Event(enable_timing=True)\n",
    "                    end_event = torch.cuda.Event(enable_timing=True)\n",
    "                    start_event.record()\n",
    "                    # print(\"in prefill layer \", layer_idx)\n",
    "                    # 对于prefill阶段，仅将experts加载到GPU计算\n",
    "                    experts = layer.block_sparse_moe.experts\n",
    "\n",
    "                    # 将experts移动到GPU\n",
    "                    if layer_idx != 0:\n",
    "                        for expert in experts:\n",
    "                            expert.cuda(0)\n",
    "\n",
    "                    # 在GPU上进行MoE计算（gate保持在CPU）\n",
    "                    final_hidden_states, router_logits = layer.block_sparse_moe(hidden_states)\n",
    "\n",
    "                    # 计算完成后将experts移回CPU\n",
    "                    if layer_idx != 0:\n",
    "                        for expert in experts:\n",
    "                            expert.w1.to('cpu')\n",
    "                            expert.w2.to('cpu')\n",
    "                    end_event.record()\n",
    "                    torch.cuda.synchronize()\n",
    "\n",
    "                    # 计算时间\n",
    "                    self.prefill_time += start_event.elapsed_time(end_event) / 1000 \n",
    "                else:\n",
    "                    # print(\"in decode layer\", layer_idx)\n",
    "                    hidden_states = hidden_states.view(-1, hidden_dim)\n",
    "                    ### 根据router计算需要使用的专家 ###\n",
    "                    router = layer.block_sparse_moe.gate\n",
    "                    # router_logits: (batch * sequence_length, n_experts)\n",
    "                    router_logits = router(hidden_states)\n",
    "\n",
    "                    routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "                    routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n",
    "                    routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n",
    "                    # we cast back to the input dtype\n",
    "                    routing_weights = routing_weights.to(hidden_states.dtype)\n",
    "                    expert_ids = selected_experts[0]\n",
    "                    expert_weights = routing_weights[0]\n",
    "                    # print('the right experts is', expert_ids)\n",
    "                    ## tensor([6, 7], device='cuda:0')\n",
    "                    if layer_idx > 0:\n",
    "                        ### 等待完全加载\n",
    "                        self.stream0.synchronize()  # 等待 self.stream0(传下一层的) 中的所有操作完成\n",
    "                        ### 判断加载是否正确\n",
    "                        predict_experts = current_buffer.get_predict_experts()\n",
    "                        \n",
    "                        # 判断expert_ids和predict_experts是否包含相同数据（忽略顺序）\n",
    "                        if not torch.equal(torch.sort(expert_ids)[0], torch.sort(predict_experts)[0]):\n",
    "                            ### 不吻合，重新加载\n",
    "                            self._load_conflict_layer(\n",
    "                                layer_idx,\n",
    "                                buffer=current_buffer,## 用cur_buffer_index\n",
    "                                expert_ids=expert_ids,\n",
    "                                hidden_states=hidden_states,\n",
    "                            )\n",
    "                            self.stream1.synchronize()  # 等待当前层的参数传递                          \n",
    "                        \n",
    "                        ### 使用当前缓冲区进行 MLP 计算 ###\n",
    "                        final_hidden_states = current_buffer(hidden_states, expert_weights, expert_ids)\n",
    "                    else:\n",
    "                        final_hidden_states_expert0 = layer.block_sparse_moe.experts[expert_ids[0]](\n",
    "                            hidden_states) * expert_weights[0]\n",
    "\n",
    "                        final_hidden_states_expert1 = layer.block_sparse_moe.experts[expert_ids[1]](\n",
    "                            hidden_states) * expert_weights[1]\n",
    "\n",
    "                        # 将两个专家的结果相加\n",
    "                        final_hidden_states = final_hidden_states_expert0 + final_hidden_states_expert1\n",
    "\n",
    "                    final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n",
    "\n",
    "                hidden_states = residual + final_hidden_states\n",
    "\n",
    "                outputs = (hidden_states,)\n",
    "\n",
    "                if output_attentions:\n",
    "                    outputs += (self_attn_weights,)\n",
    "\n",
    "                if use_cache:\n",
    "                    outputs += (present_key_value,)\n",
    "\n",
    "                return outputs\n",
    "\n",
    "            # 替换forward方法\n",
    "            layer.forward = new_forward\n",
    "\n",
    "def convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.9):\n",
    "    ### todo: 加载训练后的router\n",
    "\n",
    "    ### 其他部分存放在GPU上\n",
    "    llm.model.embed_tokens.cuda(0)\n",
    "    for i in range(len(llm.model.layers)):\n",
    "        llm.model.layers[i].self_attn.cuda(0)\n",
    "        llm.model.layers[i].input_layernorm.cuda(0)\n",
    "        llm.model.layers[i].post_attention_layernorm.cuda(0)\n",
    "        ### 原始的gate\n",
    "        llm.model.layers[i].block_sparse_moe.gate.cuda(0)\n",
    "        for j in range(len(llm.model.layers[0].block_sparse_moe.experts)):\n",
    "            llm.model.layers[i].block_sparse_moe.experts[j].w3.cuda(0)\n",
    "    ### 第0层的专家存放在GPU上\n",
    "    for j in range(len(llm.model.layers[0].block_sparse_moe.experts)):\n",
    "        llm.model.layers[0].block_sparse_moe.experts[j].cuda(0)\n",
    "\n",
    "    llm.model.norm.cuda(0)\n",
    "    llm.lm_head.cuda(0)\n",
    "    \n",
    "    # 创建两个共享的CachedMLP实例\n",
    "    buffer0 = CachedMLP(\n",
    "        input_dim=llm.config.hidden_size,\n",
    "        hidden_dim=llm.config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "        sparsity=sparsity\n",
    "    )\n",
    "    buffer1 = CachedMLP(\n",
    "        input_dim=llm.config.hidden_size,\n",
    "        hidden_dim=llm.config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "        sparsity=sparsity\n",
    "    )\n",
    "    cached_mlps = [buffer0, buffer1]\n",
    "    \n",
    "    for i, layer in enumerate(llm.model.layers):\n",
    "        if i==0:\n",
    "            continue\n",
    "        # 将专家的forward方法替换为PipelineLLM管理的方式\n",
    "        for j, expert in enumerate(layer.block_sparse_moe.experts):\n",
    "            expert.cpu_mlp = {\n",
    "                \"w1\": expert.w1.cpu().weight,\n",
    "                \"w2\": expert.w2.cpu().weight.T.contiguous(),\n",
    "                \"w3\": expert.w3,\n",
    "            }\n",
    "    return llm, cached_mlps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "active neural num  2867\n",
      "active neural num  2867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4054221/571230474.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(f'/home/bcds/On-the-Fly_MoE_Inference/expert_predictor/training/{layer_idx}.pth'))\n"
     ]
    }
   ],
   "source": [
    "llm, cached_mlps = convert_mixtral_to_cached_mlp(llm, dtype, sparsity=0.8)\n",
    "\n",
    "# 创建流水线模型\n",
    "PLLM = PipelineLLM(llm, cached_mlps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试时间开销"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 32\n",
      "Generated length: 32 Time taken: 65.29 s, prefill time: 52.84 s\n",
      "['How do you get HIV?\\nHIV is transmitted sexually through contact sexual, through contact sexual, through contact sexual, through contact sexual, through contact sexual, through contact sexual, through contact sexual']\n",
      "Generated length: 32 Time taken: 54.77 s, prefill time: 43.21 s\n",
      "['CTComms sends on average 2 million emails a day to 100,000 employees across 100 different countries. That is, on a daily basis, the equivalent of']\n",
      "decode phase speed: 2.5825 token/s\n",
      "the number of reloaded experts per token: 22.06451612903226\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "\n",
    "input_length = 10\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 32\n",
    "test_samples = 2\n",
    "device_id = 0\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "filt_type = fineweb_path.split('.')[-1]\n",
    "fineweb = load_dataset(filt_type, data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text']\n",
    "\n",
    "# 预热（避免第一次运行时的额外开销）\n",
    "# for text in fineweb_text[:5] :\n",
    "#     inputs = preprocess_data(text, tokenizer)\n",
    "#     with torch.no_grad():\n",
    "#         output = llm(input_ids=inputs[\"input_ids\"].cuda(device_id), attention_mask=inputs[\"attention_mask\"].cuda(device_id))\n",
    "\n",
    "generated_all = 0\n",
    "prefill_time, decode_time = 0, 0\n",
    "reloaded_experts = 0\n",
    "print(\"output length is {}\".format(output_length))\n",
    "for text in fineweb_text[2:2+test_samples] :\n",
    "    inputs = preprocess_data(text, tokenizer)\n",
    "    ### 清空统计数据\n",
    "    PLLM.get_prefill_time()\n",
    "    PLLM.get_reload_experts()\n",
    "\n",
    "    # 测试时间\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # 开始计时\n",
    "    torch.cuda.synchronize()\n",
    "    start_event.record()\n",
    "\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(device_id),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(device_id),\n",
    "            max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.pad_token_id, \n",
    "            # cache_implementation=\"static\" ## moe not support\n",
    "        )\n",
    "\n",
    "    # 结束计时\n",
    "    end_event.record()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # 计算时间\n",
    "    elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "    decode_time += elapsed_time\n",
    "    cur_prefill_time = PLLM.get_prefill_time()\n",
    "    prefill_time += cur_prefill_time\n",
    "    print(f\"Generated length: {len(output[0]) - input_length}\", f\"Time taken: {elapsed_time:.2f} s,\", f\"prefill time: {cur_prefill_time:.2f} s\")\n",
    "    # print(output)\n",
    "    print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "\n",
    "    generated_all += (len(output[0]) - input_length - 1)\n",
    "    reloaded_experts += PLLM.get_reload_experts()\n",
    "\n",
    "timepertoken = (decode_time - prefill_time) / (generated_all)\n",
    "print(\"decode phase speed:\", '{:.4f}'.format(1/timepertoken) , 'token/s')\n",
    "print(\"the number of reloaded experts per token:\", reloaded_experts / generated_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################################################################\n",
    "#Save gemlite cache\n",
    "if(backend == 'gemlite'):\n",
    "\tgemlite.core.GemLiteLinear.cache_config('/tmp/gemlite_config.json') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.profile\n",
    "\n",
    "trace-offloading-r.json是最优，就是做完一个index就传一个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length is 2\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::index        45.06%     470.669ms        50.12%     523.527ms     848.505us       1.602ms         0.29%       1.888ms       3.059us           617  \n",
      "                                  cudaStreamSynchronize        15.14%     158.114ms        15.14%     158.114ms     180.496us     870.353us         0.16%     870.353us       0.994us           876  \n",
      "                                            aten::copy_        11.86%     123.879ms        25.50%     266.310ms      79.997us     357.476ms        63.81%     357.476ms     107.382us          3329  \n",
      "                                        cudaMemcpyAsync         8.32%      86.877ms         8.32%      86.877ms      55.584us       0.000us         0.00%       0.000us       0.000us          1563  \n",
      "                                       cudaLaunchKernel         5.11%      53.392ms         5.11%      53.392ms       5.641us       0.000us         0.00%       0.000us       0.000us          9465  \n",
      "                                    HQQMatmulNoCacheMul         2.14%      22.381ms         9.00%      94.007ms     303.247us       0.000us         0.00%     200.158ms     645.670us           310  \n",
      "                                               aten::mm         2.07%      21.670ms         3.09%      32.246ms      33.944us      27.081ms         4.83%      27.081ms      28.506us           950  \n",
      "                                      aten::bitwise_and         0.98%      10.248ms         1.59%      16.650ms      13.406us      23.977ms         4.28%      23.977ms      19.305us          1242  \n",
      "                                              aten::mul         0.72%       7.525ms         1.31%      13.717ms      12.631us      53.472ms         9.54%      53.472ms      49.237us          1086  \n",
      "                                            aten::empty         0.55%       5.706ms         0.55%       5.706ms       3.279us       0.000us         0.00%       0.000us       0.000us          1740  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.47%       4.908ms         0.47%       4.908ms       2.440us       0.000us         0.00%       0.000us       0.000us          2012  \n",
      "                                       aten::__rshift__         0.47%       4.878ms         1.17%      12.271ms      13.195us      17.627ms         3.15%      17.627ms      18.954us           930  \n",
      "                                            aten::slice         0.46%       4.851ms         0.57%       6.002ms       2.095us       0.000us         0.00%       0.000us       0.000us          2865  \n",
      "                                    aten::empty_strided         0.45%       4.738ms         0.45%       4.738ms       5.894us       0.000us         0.00%       0.000us       0.000us           804  \n",
      "                                             aten::topk         0.44%       4.630ms         0.87%       9.038ms      29.344us      15.520ms         2.77%      15.520ms      50.390us           308  \n",
      "                                              aten::add         0.34%       3.534ms         0.61%       6.389ms      12.287us       2.437ms         0.43%       2.437ms       4.686us           520  \n",
      "                                       aten::as_strided         0.29%       3.051ms         0.29%       3.051ms       0.482us       0.000us         0.00%       0.000us       0.000us          6331  \n",
      "                                    cudaLaunchKernelExC         0.28%       2.926ms         0.28%       2.926ms       6.591us       0.000us         0.00%       0.000us       0.000us           444  \n",
      "                                              aten::sub         0.26%       2.668ms         0.42%       4.386ms      13.837us      49.896ms         8.91%      49.896ms     157.401us           317  \n",
      "                                           aten::matmul         0.21%       2.244ms         4.18%      43.615ms      40.460us       0.000us         0.00%      27.869ms      25.852us          1078  \n",
      "                                        cudaMemsetAsync         0.21%       2.170ms         0.21%       2.170ms       6.868us       0.000us         0.00%       0.000us       0.000us           316  \n",
      "                                           aten::select         0.20%       2.044ms         0.23%       2.448ms       2.601us       0.000us         0.00%       0.000us       0.000us           941  \n",
      "                                         aten::_to_copy         0.19%       2.013ms        13.33%     139.189ms     174.204us       0.000us         0.00%       3.610ms       4.518us           799  \n",
      "                                          aten::reshape         0.19%       1.985ms         0.58%       6.048ms       3.427us       0.000us         0.00%     766.928us       0.435us          1765  \n",
      "                                              aten::cat         0.19%       1.958ms         0.31%       3.203ms      16.344us       1.272ms         0.23%       1.272ms       6.488us           196  \n",
      "                                             aten::sort         0.19%       1.936ms         0.86%       9.003ms      72.606us     605.385us         0.11%       1.717ms      13.850us           124  \n",
      "                                  cudaFuncGetAttributes         0.18%       1.919ms         0.18%       1.919ms       2.765us       0.000us         0.00%       0.000us       0.000us           694  \n",
      "                                              aten::bmm         0.18%       1.896ms         0.69%       7.259ms      56.712us     788.240us         0.14%     788.240us       6.158us           128  \n",
      "                                             aten::view         0.18%       1.868ms         0.18%       1.868ms       0.908us       0.000us         0.00%       0.000us       0.000us          2057  \n",
      "                                             aten::mean         0.16%       1.655ms         0.24%       2.462ms      18.938us       1.108ms         0.20%       1.108ms       8.520us           130  \n",
      "                                                aten::t         0.15%       1.582ms         0.28%       2.874ms       4.094us       0.000us         0.00%       0.000us       0.000us           702  \n",
      "                                              aten::pow         0.14%       1.466ms         0.23%       2.414ms      18.568us     708.336us         0.13%     708.336us       5.449us           130  \n",
      "                                         aten::_softmax         0.14%       1.466ms         0.27%       2.834ms      14.918us     887.763us         0.16%     887.763us       4.672us           190  \n",
      "                              aten::_local_scalar_dense         0.14%       1.440ms         0.75%       7.806ms      21.926us     854.607us         0.15%     854.607us       2.401us           356  \n",
      "                                              aten::abs         0.13%       1.402ms         0.54%       5.651ms      15.524us     521.061us         0.09%       1.042ms       2.863us           364  \n",
      "                                               aten::to         0.13%       1.309ms        13.45%     140.497ms     126.460us       0.000us         0.00%       3.610ms       3.250us          1111  \n",
      "                                        aten::transpose         0.11%       1.108ms         0.18%       1.879ms       1.839us       0.000us         0.00%       0.000us       0.000us          1022  \n",
      "                                              aten::neg         0.10%       1.001ms         0.16%       1.721ms      13.449us     776.940us         0.14%     776.940us       6.070us           128  \n",
      "                                          aten::__and__         0.09%     942.744us         1.68%      17.593ms      14.165us       0.000us         0.00%      23.977ms      19.305us          1242  \n",
      "                                             aten::silu         0.09%     924.570us         0.16%       1.633ms      12.758us     385.704us         0.07%     385.704us       3.013us           128  \n",
      "                                            aten::rsqrt         0.09%     912.245us         0.16%       1.686ms      12.968us     717.964us         0.13%     717.964us       5.523us           130  \n",
      "                                              aten::div         0.08%     875.227us         0.12%       1.283ms      20.051us     343.306us         0.06%     343.306us       5.364us            64  \n",
      "                                            aten::equal         0.08%     804.504us         0.35%       3.617ms      58.334us     145.763us         0.03%     595.145us       9.599us            62  \n",
      "                                           aten::arange         0.07%     783.036us         0.34%       3.587ms      14.013us     272.385us         0.05%     544.770us       2.128us           256  \n",
      "                                          aten::resize_         0.07%     778.507us         0.07%     778.507us       2.495us       0.000us         0.00%       0.000us       0.000us           312  \n",
      "                                               aten::ne         0.07%     750.877us         0.13%       1.381ms      22.273us     161.476us         0.03%     161.476us       2.604us            62  \n",
      "                                           aten::linear         0.07%     713.838us         1.49%      15.558ms      39.689us       0.000us         0.00%       5.974ms      15.240us           392  \n",
      "                                              aten::all         0.06%     648.568us         0.10%       1.017ms      16.411us     279.587us         0.05%     279.587us       4.509us            62  \n",
      "                                              aten::sum         0.06%     645.660us         0.10%       1.017ms      15.898us     476.873us         0.09%     476.873us       7.451us            64  \n",
      "                                   cudaFuncSetAttribute         0.06%     597.197us         0.06%     597.197us       0.627us       0.000us         0.00%       0.000us       0.000us           952  \n",
      "                                           aten::expand         0.06%     585.346us         0.07%     726.658us       1.883us       0.000us         0.00%       0.000us       0.000us           386  \n",
      "                                             aten::div_         0.04%     454.232us         0.08%     818.582us      12.790us     361.567us         0.06%     361.567us       5.649us            64  \n",
      "                                             aten::item         0.04%     439.584us         0.79%       8.245ms      23.161us       0.000us         0.00%     854.607us       2.401us           356  \n",
      "                                        aten::unsqueeze         0.04%     426.633us         0.05%     551.619us       2.043us       0.000us         0.00%       0.000us       0.000us           270  \n",
      "                                              aten::any         0.04%     413.754us         0.07%     732.604us      16.280us     180.389us         0.03%     191.204us       4.249us            45  \n",
      "                                 cudaDeviceGetAttribute         0.04%     413.533us         0.04%     413.533us       0.412us       0.000us         0.00%       0.000us       0.000us          1003  \n",
      "                                     aten::_unsafe_view         0.04%     407.995us         0.04%     407.995us       0.794us       0.000us         0.00%       0.000us       0.000us           514  \n",
      "                                               aten::eq         0.03%     363.360us         0.07%     691.011us      14.102us     135.973us         0.02%     135.973us       2.775us            49  \n",
      "                                            aten::clone         0.03%     317.935us         0.24%       2.541ms      18.410us       0.000us         0.00%     790.037us       5.725us           138  \n",
      "                                          aten::permute         0.03%     311.269us         0.03%     351.291us       2.833us       0.000us         0.00%       0.000us       0.000us           124  \n",
      "                                       aten::empty_like         0.03%     268.138us         0.07%     764.010us       5.659us       0.000us         0.00%       0.000us       0.000us           135  \n",
      "                                          aten::softmax         0.03%     267.255us         0.30%       3.102ms      16.325us       0.000us         0.00%     887.763us       4.672us           190  \n",
      "                                          aten::numpy_T         0.01%     128.536us         0.05%     479.827us       3.870us       0.000us         0.00%       0.000us       0.000us           124  \n",
      "                                  cudaDeviceSynchronize         0.01%     126.966us         0.01%     126.966us     126.966us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                          aten::detach_         0.01%     105.098us         0.01%     137.772us       3.204us       0.000us         0.00%       0.000us       0.000us            43  \n",
      "                                       aten::is_nonzero         0.01%      86.188us         0.15%       1.576ms      23.172us       0.000us         0.00%     175.713us       2.584us            68  \n",
      "                                            aten::fill_         0.01%      86.156us         0.02%     201.819us      15.525us      27.649us         0.00%      27.649us       2.127us            13  \n",
      "                                   aten::_reshape_alias         0.01%      83.285us         0.01%      83.285us       2.603us       0.000us         0.00%       0.000us       0.000us            32  \n",
      "                                             aten::isin         0.01%      76.243us         0.03%     332.211us     110.737us       0.000us         0.00%      28.034us       9.345us             3  \n",
      "                                      aten::result_type         0.01%      65.826us         0.01%      65.826us       0.506us       0.000us         0.00%       0.000us       0.000us           130  \n",
      "                                           aten::cumsum         0.01%      63.018us         0.01%     127.306us      42.435us      15.582us         0.00%      15.582us       5.194us             3  \n",
      "                                     aten::index_select         0.00%      43.806us         0.01%      71.523us      35.762us       7.584us         0.00%       7.584us       3.792us             2  \n",
      "                                     aten::masked_fill_         0.00%      41.257us         0.01%      66.210us      16.553us       9.632us         0.00%       9.632us       2.408us             4  \n",
      "                                       aten::bitwise_or         0.00%      40.083us         0.01%      64.053us      16.013us      10.113us         0.00%      10.113us       2.528us             4  \n",
      "                                          aten::dropout         0.00%      33.856us         0.00%      33.856us       0.529us       0.000us         0.00%       0.000us       0.000us            64  \n",
      "                                                detach_         0.00%      32.674us         0.00%      32.674us       0.760us       0.000us         0.00%       0.000us       0.000us            43  \n",
      "                                           aten::argmax         0.00%      31.590us         0.00%      51.600us      25.800us      25.473us         0.00%      25.473us      12.736us             2  \n",
      "                                              aten::max         0.00%      31.229us         0.01%      62.379us      31.190us      10.432us         0.00%      10.432us       5.216us             2  \n",
      "                                      aten::bitwise_not         0.00%      28.605us         0.00%      49.623us      24.812us       4.480us         0.00%       4.480us       2.240us             2  \n",
      "                                         aten::new_ones         0.00%      27.598us         0.01%      70.401us      35.201us       0.000us         0.00%       4.640us       2.320us             2  \n",
      "                                     aten::resolve_conj         0.00%      27.277us         0.00%      27.277us       0.278us       0.000us         0.00%       0.000us       0.000us            98  \n",
      "                                               aten::gt         0.00%      26.880us         0.01%      98.697us      49.349us       5.408us         0.00%       5.408us       2.704us             2  \n",
      "                                               aten::ge         0.00%      25.618us         0.00%      36.931us      18.466us       4.352us         0.00%       4.352us       2.176us             2  \n",
      "                                      aten::bitwise_or_         0.00%      23.583us         0.00%      34.781us      17.390us       4.896us         0.00%       4.896us       2.448us             2  \n",
      "                                             aten::mul_         0.00%      19.310us         0.00%      34.728us      17.364us       5.184us         0.00%       5.184us       2.592us             2  \n",
      "                                               aten::lt         0.00%      18.308us         0.00%      26.741us      26.741us       2.369us         0.00%       2.369us       2.369us             1  \n",
      "                                        aten::embedding         0.00%      17.399us         0.01%      92.550us      46.275us       0.000us         0.00%       7.584us       3.792us             2  \n",
      "                                      aten::resolve_neg         0.00%      17.349us         0.00%      17.349us       0.177us       0.000us         0.00%       0.000us       0.000us            98  \n",
      "                                             aten::full         0.00%      14.560us         0.01%     117.477us      19.580us       0.000us         0.00%      12.480us       2.080us             6  \n",
      "                                               aten::le         0.00%      13.991us         0.00%      21.173us      10.587us       5.792us         0.00%       5.792us       2.896us             2  \n",
      "                                           aten::__or__         0.00%      12.267us         0.01%      76.320us      19.080us       0.000us         0.00%      10.113us       2.528us             4  \n",
      "                                       aten::lift_fresh         0.00%      12.122us         0.00%      12.122us       0.282us       0.000us         0.00%       0.000us       0.000us            43  \n",
      "                                      aten::masked_fill         0.00%      10.708us         0.01%      64.997us      32.498us       0.000us         0.00%       8.864us       4.432us             2  \n",
      "                                             aten::rsub         0.00%      10.268us         0.01%      65.920us      32.960us       0.000us         0.00%       4.929us       2.465us             2  \n",
      "                                          aten::view_as         0.00%       5.406us         0.00%       7.072us       1.414us       0.000us         0.00%       0.000us       0.000us             5  \n",
      "                                             aten::ones         0.00%       3.988us         0.00%      23.141us      23.141us       0.000us         0.00%       2.048us       2.048us             1  \n",
      "                                        aten::new_empty         0.00%       3.911us         0.00%      11.351us       5.675us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                        aten::ones_like         0.00%       3.466us         0.00%      16.463us      16.463us       0.000us         0.00%       2.017us       2.017us             1  \n",
      "                                    cudaPeekAtLastError         0.00%       1.156us         0.00%       1.156us       0.096us       0.000us         0.00%       0.000us       0.000us            12  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     294.376us         0.05%     294.376us       2.282us           129  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      14.528us         0.00%      14.528us       2.075us             7  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     201.445us         0.04%     201.445us       2.370us            85  \n",
      "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     996.269us         0.18%     996.269us       2.296us           434  \n",
      "                         Memcpy DtoH (Device -> Pinned)         0.00%       0.000us         0.00%       0.000us       0.000us     854.607us         0.15%     854.607us       2.401us           356  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       6.721us         0.00%       6.721us       2.240us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       8.705us         0.00%       8.705us       2.176us             4  \n",
      "void at_cuda_detail::cub::DeviceScanInitKernel<at_cu...         0.00%       0.000us         0.00%       0.000us       0.000us       7.424us         0.00%       7.424us       2.475us             3  \n",
      "void at_cuda_detail::cub::DeviceScanKernel<at_cuda_d...         0.00%       0.000us         0.00%       0.000us       0.000us       8.158us         0.00%       8.158us       2.719us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      16.864us         0.00%      16.864us       2.409us             7  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     118.051us         0.02%     118.051us       2.683us            44  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.896us         0.00%       4.896us       2.448us             2  \n",
      "void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us       7.584us         0.00%       7.584us       3.792us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.416us         0.00%       4.416us       2.208us             2  \n",
      "void (anonymous namespace)::elementwise_kernel_with_...         0.00%       0.000us         0.00%       0.000us       0.000us     272.385us         0.05%     272.385us       2.128us           128  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.055us         0.00%       5.055us       2.527us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      15.009us         0.00%      15.009us       2.502us             6  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       5.184us         0.00%       5.184us       2.592us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       5.472us         0.00%       5.472us       2.736us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.481us         0.00%       4.481us       2.240us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.736us         0.00%       4.736us       2.368us             2  \n",
      "void dot_kernel<float, 128, 0, cublasDotParams<cubla...         0.00%       0.000us         0.00%       0.000us       0.000us     645.457us         0.12%     645.457us       5.123us           126  \n",
      "void reduce_1Block_kernel<float, 128, 7, cublasGemvT...         0.00%       0.000us         0.00%       0.000us       0.000us     552.494us         0.10%     552.494us       4.385us           126  \n",
      "void (anonymous namespace)::softmax_warp_forward<c10...         0.00%       0.000us         0.00%       0.000us       0.000us     513.230us         0.09%     513.230us       4.073us           126  \n",
      "void at::native::sbtopk::gatherTopK<float, unsigned ...         0.00%       0.000us         0.00%       0.000us       0.000us       1.012ms         0.18%       1.012ms       8.035us           126  \n",
      "void at::native::bitonicSortKVInPlace<-2, -1, 16, 16...         0.00%       0.000us         0.00%       0.000us       0.000us     926.703us         0.17%     926.703us       7.355us           126  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      23.972ms         4.28%      23.972ms      19.332us          1240  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      17.627ms         3.15%      17.627ms      18.954us           930  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us      44.038ms         7.86%      44.038ms      29.398us          1498  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      50.266ms         8.97%      50.266ms     134.401us           374  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      51.585ms         9.21%      51.585ms      74.329us           694  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us     505.671us         0.09%     505.671us       1.600us           316  \n",
      "sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x...         0.00%       0.000us         0.00%       0.000us       0.000us      16.786ms         3.00%      16.786ms      53.121us           316  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     521.061us         0.09%     521.061us       2.863us           182  \n",
      "void at::native::sbtopk::gatherTopK<c10::Half, unsig...         0.00%       0.000us         0.00%       0.000us       0.000us      10.091ms         1.80%      10.091ms      55.445us           182  \n",
      "void at::native::radixSortKVInPlace<-2, -1, 128, 32,...         0.00%       0.000us         0.00%       0.000us       0.000us       3.490ms         0.62%       3.490ms      19.175us           182  \n",
      "                       Memcpy DtoH (Device -> Pageable)         0.00%       0.000us         0.00%       0.000us       0.000us     778.058us         0.14%     778.058us       2.779us           280  \n",
      "                         Memcpy HtoD (Pinned -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     309.704ms        55.28%     309.704ms     850.834us           364  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     898.544us         0.16%     898.544us       6.807us           132  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     708.336us         0.13%     708.336us       5.449us           130  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       1.108ms         0.20%       1.108ms       8.520us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     704.264us         0.13%     704.264us       5.417us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     717.964us         0.13%     717.964us       5.523us           130  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     795.248us         0.14%     795.248us       6.117us           130  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.088ms         0.19%       1.088ms       4.216us           258  \n",
      "sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x...         0.00%       0.000us         0.00%       0.000us       0.000us       2.816ms         0.50%       2.816ms      21.998us           128  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       1.379ms         0.25%       1.379ms      10.771us           128  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.598ms         0.29%       1.598ms       6.342us           252  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     776.940us         0.14%     776.940us       6.070us           128  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     884.119us         0.16%     884.119us       6.907us           128  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.330ms         0.24%       1.330ms       4.156us           320  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     766.928us         0.14%     766.928us       5.992us           128  \n",
      "void gemv2T_kernel_val<int, int, __half, __half, __h...         0.00%       0.000us         0.00%       0.000us       0.000us     376.557us         0.07%     376.557us       5.884us            64  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     343.306us         0.06%     343.306us       5.364us            64  \n",
      "void (anonymous namespace)::softmax_warp_forward<c10...         0.00%       0.000us         0.00%       0.000us       0.000us     188.706us         0.03%     188.706us       5.897us            32  \n",
      "void gemmk1_kernel<int, float, 256, 5, false, false,...         0.00%       0.000us         0.00%       0.000us       0.000us     197.153us         0.04%     197.153us       6.161us            32  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     476.873us         0.09%     476.873us       7.451us            64  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     361.567us         0.06%     361.567us       5.649us            64  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     385.704us         0.07%     385.704us       3.013us           128  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_...         0.00%       0.000us         0.00%       0.000us       0.000us     178.083us         0.03%     178.083us      44.521us             4  \n",
      "void splitKreduce_kernel<32, 16, int, float, __half,...         0.00%       0.000us         0.00%       0.000us       0.000us      14.754us         0.00%      14.754us       3.689us             4  \n",
      "void at::native::bitonicSortKVInPlace<-2, -1, 16, 16...         0.00%       0.000us         0.00%       0.000us       0.000us     605.385us         0.11%     605.385us       4.882us           124  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     279.587us         0.05%     279.587us       4.509us            62  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     114.242us         0.02%     114.242us       2.856us            40  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     180.389us         0.03%     180.389us       4.510us            40  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       1.901ms         0.34%       1.901ms      15.329us           124  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       2.303ms         0.41%       2.303ms      18.569us           124  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us      14.880us         0.00%      14.880us       3.720us             4  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      25.473us         0.00%      25.473us      12.736us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      10.496us         0.00%      10.496us       2.624us             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.929us         0.00%       4.929us       2.465us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.344us         0.00%       5.344us       2.672us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       2.528us         0.00%       2.528us       2.528us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.480us         0.00%       4.480us       2.240us             2  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       5.248us         0.00%       5.248us       2.624us             2  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      10.432us         0.00%      10.432us       5.216us             2  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.360us         0.00%       3.360us       3.360us             1  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       6.145us         0.00%       6.145us       3.072us             2  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     372.580us         0.07%     372.580us       5.822us            64  \n",
      "void (anonymous namespace)::softmax_warp_forward<c10...         0.00%       0.000us         0.00%       0.000us       0.000us     185.827us         0.03%     185.827us       5.807us            32  \n",
      "void gemv2N_kernel<int, int, __half, __half, __half,...         0.00%       0.000us         0.00%       0.000us       0.000us     217.026us         0.04%     217.026us       6.782us            32  \n",
      "void at::native::vectorized_elementwise_kernel<2, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.465us         0.00%       2.465us       2.465us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.045s\n",
      "Self CUDA time total: 560.217ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 1\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 2\n",
    "test_samples = 4\n",
    "\n",
    "with open(\"../path.json\", \"r\") as f:\n",
    "    paths = json.load(f)\n",
    "    fineweb_path = paths[\"fineweb\"]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "fineweb = load_dataset(\"parquet\",data_files=fineweb_path) #726000\n",
    "fineweb_text = fineweb['train']['text'][:test_samples] \n",
    "\n",
    "print(\"output length is {}\".format(output_length))\n",
    "text = fineweb_text[0]\n",
    "inputs = preprocess_data(text, tokenizer)\n",
    "\n",
    "# cached_mlp.clear_load_from_cpu_stats()\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ]\n",
    ") as p:\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        output = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"].cuda(),\n",
    "            attention_mask=inputs[\"attention_mask\"].cuda(),\n",
    "            max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "            generation_config=GenerationConfig(do_sample=False),\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "print(p.key_averages().table(\n",
    "    sort_by=\"self_cpu_time_total\", row_limit=-1))\n",
    "p.export_chrome_trace(\"./offloading-hqq2-reload.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试一个正常输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated length: 12 Time taken: 42.10 s prefill time: 36.94 s\n",
      "['The future of AI is here and now.\\n\\nThe future of AI is here']\n",
      "decode phase speed: 2.1345  token/s\n",
      "the number of experts reload per token: 23.454545454545453\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "input_length = 6\n",
    "MAX_LENGTH = input_length\n",
    "output_length = 12\n",
    "device_id = 0\n",
    "test_samples = 1\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "\t# 使用 tokenizer 将文本数据转换为模型输入\n",
    "\tinputs = tokenizer(data, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\tinputs[\"labels\"] = inputs.input_ids.clone()\n",
    "\treturn inputs\n",
    "\n",
    "generated_all, decode_time, prefill_time = 0, 0, 0\n",
    "# print(\"max output length is {}\".format(output_length))\n",
    "text = \"The future of AI is \"\n",
    "\n",
    "clear_prefill_time = PLLM.get_prefill_time()\n",
    "clear_experts = PLLM.get_reload_experts()\n",
    "# print(\"need to zero: \", clear_experts, clear_prefill_time)\n",
    "\n",
    "inputs = preprocess_data(text, tokenizer)\n",
    "# 测试时间\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "# 开始计时\n",
    "torch.cuda.synchronize()\n",
    "start_event.record()\n",
    "\n",
    "# 前向传播\n",
    "with torch.no_grad():\n",
    "    output = llm.generate(\n",
    "        input_ids=inputs[\"input_ids\"].cuda(device_id),\n",
    "        attention_mask=inputs[\"attention_mask\"].cuda(device_id),\n",
    "        max_length=input_length + output_length,  # 总长度为输入长度 + 输出长度\n",
    "        generation_config=GenerationConfig(do_sample=False),\n",
    "        pad_token_id=tokenizer.pad_token_id, \n",
    "        # cache_implementation=\"static\" ## moe not support\n",
    "    )\n",
    "\n",
    "# 结束计时\n",
    "end_event.record()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# 计算时间\n",
    "elapsed_time = start_event.elapsed_time(end_event) / 1000  # 转换为秒\n",
    "decode_time += elapsed_time\n",
    "cur_prefill_time = PLLM.get_prefill_time()\n",
    "prefill_time += cur_prefill_time\n",
    "print(f\"Generated length: {len(output[0]) - input_length}\", f\"Time taken: {elapsed_time:.2f} s\", f\"prefill time: {cur_prefill_time:.2f} s\")\n",
    "# print(output)\n",
    "print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "\n",
    "generated_all += (len(output[0]) - input_length -1)\n",
    "\n",
    "timepertoken = (decode_time - prefill_time) / (generated_all)\n",
    "print(\"decode phase speed:\", '{:.4f}'.format(1/timepertoken) , ' token/s')\n",
    "print(\"the number of experts reload per token:\", PLLM.get_reload_experts() / generated_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载到GPU上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:25<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MixtralForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "import json\n",
    "\n",
    "def get_model(model_name, device_map, dtype=torch.bfloat16):\n",
    "    llm = MixtralForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        use_cache=True,\n",
    "        torch_dtype=dtype,\n",
    "    ) \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return llm, tokenizer\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    # threshold_path = path[threshold_path_name]\n",
    "\n",
    "with open('../quantize/device_map_1.json', 'r') as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "dtype = torch.float16\n",
    "llm, tokenizer = get_model(model_name, device_map, dtype=dtype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
