{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('/home/bcds/On-the-Fly_MoE_Inference/bagel-v0.5/bagel-clean-v0.5.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df.sample(frac=0.95,random_state=200)\n",
    "df_eval=df.drop(df_train.index)\n",
    "# df_train.to_json(\"bagel_train.jsonl\", orient=\"records\", lines=False)\n",
    "df_eval.to_json(\"bagel_eval.json\", orient=\"records\", lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"conversation prompt templates\"\"\"\n",
    "\n",
    "import dataclasses\n",
    "from enum import auto, Enum\n",
    "from typing import List, Any\n",
    "\n",
    "\n",
    "class SeparatorStyle(Enum):\n",
    "    \"\"\"Different separator style.\"\"\"\n",
    "\n",
    "    ADD_COLON_SINGLE = auto()\n",
    "    ADD_COLON_TWO = auto()\n",
    "    NO_COLON_SINGLE = auto()\n",
    "    BAIZE = auto()\n",
    "    DOLLY = auto()\n",
    "    RWKV = auto()\n",
    "IGNORE_TOKEN_ID = -100\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Conversation:\n",
    "    \"\"\"A class that keeps all conversation history.\"\"\"\n",
    "\n",
    "    # System prompts\n",
    "    system: str\n",
    "    # Two roles\n",
    "    roles: List[str]\n",
    "    # All messages\n",
    "    messages: List[List[str]]\n",
    "    # Offset of few shot examples\n",
    "    offset: int\n",
    "    # Separator\n",
    "    sep_style: SeparatorStyle\n",
    "    sep: str\n",
    "    sep2: str = None\n",
    "    # Stop criteria (the default one is EOS token)\n",
    "    stop_str: str = None\n",
    "    # Stops generation if meeting any token in this list\n",
    "    stop_token_ids: List[int] = None\n",
    "\n",
    "    # Used for the state in the gradio servers.\n",
    "    conv_id: Any = None\n",
    "    skip_next: bool = False\n",
    "    model_name: str = None\n",
    "\n",
    "    def get_prompt(self):\n",
    "        \"\"\"Get the prompt for generation.\"\"\"\n",
    "        if self.sep_style == SeparatorStyle.ADD_COLON_SINGLE:\n",
    "            ret = self.system + self.sep\n",
    "            for role, message in self.messages:\n",
    "                if message:\n",
    "                    ret += role + \": \" + message + self.sep\n",
    "                else:\n",
    "                    ret += role + \":\"\n",
    "            return ret\n",
    "        if self.sep_style == SeparatorStyle.ADD_COLON_TWO:\n",
    "            seps = [self.sep, self.sep2]\n",
    "            ret = self.system + seps[0]\n",
    "            for i, (role, message) in enumerate(self.messages):\n",
    "                if message:\n",
    "                    ret += role + \": \" + message + seps[i % 2]\n",
    "                else:\n",
    "                    ret += role + \":\"\n",
    "            return ret\n",
    "        raise ValueError(f\"Invalid style: {self.sep_style}\")\n",
    "\n",
    "    def append_message(self, role, message):\n",
    "        \"\"\"Append a new message.\"\"\"\n",
    "        self.messages.append([role, message])\n",
    "\n",
    "    def to_openai_api_messages(self):\n",
    "        \"\"\"Convert the conversation to OpenAI chat completion format.\"\"\"\n",
    "        ret = [{\"role\": \"system\", \"content\": self.system}]\n",
    "\n",
    "        for i, (_, msg) in enumerate(self.messages[self.offset:]):\n",
    "            if i % 2 == 0:\n",
    "                ret.append({\"role\": \"user\", \"content\": msg})\n",
    "            else:\n",
    "                if msg is not None:\n",
    "                    ret.append({\"role\": \"assistant\", \"content\": msg})\n",
    "        return ret\n",
    "\n",
    "    def copy(self):\n",
    "        return Conversation(\n",
    "            system=self.system,\n",
    "            roles=self.roles,\n",
    "            messages=[[x, y] for x, y in self.messages],\n",
    "            offset=self.offset,\n",
    "            sep_style=self.sep_style,\n",
    "            sep=self.sep,\n",
    "            sep2=self.sep2,\n",
    "            stop_str=self.stop_str,\n",
    "            stop_token_ids=self.stop_token_ids,\n",
    "            conv_id=self.conv_id,\n",
    "            model_name=self.model_name,\n",
    "        )\n",
    "\n",
    "    def dict(self):\n",
    "        return {\n",
    "            \"system\": self.system,\n",
    "            \"roles\": self.roles,\n",
    "            \"messages\": self.messages,\n",
    "            \"offset\": self.offset,\n",
    "            \"conv_id\": self.conv_id,\n",
    "            \"model_name\": self.model_name,\n",
    "        }\n",
    "\n",
    "\n",
    "def get_default_conv_template(conv_name):\n",
    "    return Conversation(\n",
    "        system=\"A chat between a curious user and an artificial intelligence assistant. \"\n",
    "        \"The assistant gives helpful, detailed, and polite answers to the user's questions.\",\n",
    "        roles=(\"USER\", \"ASSISTANT\"),\n",
    "        messages=(),\n",
    "        offset=0,\n",
    "        sep_style=SeparatorStyle.ADD_COLON_TWO,\n",
    "        sep=\" \",\n",
    "        sep2=\"</s>\",\n",
    "    )\n",
    "import numpy as np\n",
    "def preprocess(sources, tokenizer, seq_length):\n",
    "    \"\"\"conversation preprocess.\"\"\"\n",
    "    conv = get_default_conv_template(\"vicuna\").copy()\n",
    "    roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n",
    "    print(conv.roles[0], conv.roles[1])\n",
    "\n",
    "    # Apply prompt templates\n",
    "    conversations = []\n",
    "    for i, source in enumerate(sources):\n",
    "        print(source[0].get(\"from\"))\n",
    "        if roles.get(source[0].get(\"from\")) != conv.roles[0]:\n",
    "            # Skip the first one if it is not from human\n",
    "            source = source[1:]\n",
    "\n",
    "        conv.messages = []\n",
    "        for j, sentence in enumerate(source):\n",
    "            role = roles.get(sentence.get(\"from\"))\n",
    "            if role != conv.roles[j % 2]:\n",
    "                raise ValueError(f\"sources[{i}] is wrong.\")\n",
    "            conv.append_message(role, sentence[\"value\"])\n",
    "        conversations.append(conv.get_prompt())\n",
    "\n",
    "    sep = conv.sep + conv.roles[1] + \": \"\n",
    "    # Tokenize conversations\n",
    "    input_ids = []\n",
    "    targets = []\n",
    "    # attention_mask = []\n",
    "    for conversation in conversations:\n",
    "        rounds = conversation.split(conv.sep2)\n",
    "        ids = [tokenizer.bos_token_id]\n",
    "        mask = [1]\n",
    "        for i, rou in enumerate(rounds):\n",
    "            if rou == \"\":\n",
    "                break\n",
    "            conv_out = tokenizer(rou)\n",
    "            ids.extend(conv_out['input_ids'][1:])\n",
    "            mask.extend(conv_out['attention_mask'][1:])\n",
    "        d = {'input_ids': ids, 'attention_mask': mask}\n",
    "        # pylint: disable=W0212\n",
    "        d = tokenizer._pad(d, max_length=seq_length, padding_strategy='max_length')\n",
    "        input_ids.append(d['input_ids'][:seq_length])\n",
    "        # attention_mask.append(d['attention_mask'])\n",
    "\n",
    "        target = np.array(d['input_ids'])\n",
    "        total_len = int(np.not_equal(target, tokenizer.pad_token_id).sum())\n",
    "        cur_len = 1\n",
    "        target[:cur_len] = IGNORE_TOKEN_ID\n",
    "        for i, rou in enumerate(rounds):\n",
    "            if rou == \"\":\n",
    "                break\n",
    "            parts = rou.split(sep)\n",
    "            if len(parts) != 2:\n",
    "                break\n",
    "            parts[0] += sep\n",
    "            round_len = len(tokenizer(rou)['input_ids']) - 1\n",
    "            instruction_len = len(tokenizer(parts[0])['input_ids']) - 3\n",
    "\n",
    "            target[cur_len : cur_len + instruction_len] = IGNORE_TOKEN_ID\n",
    "\n",
    "            cur_len += round_len\n",
    "        target[cur_len:] = IGNORE_TOKEN_ID\n",
    "\n",
    "        if cur_len < seq_length:\n",
    "            if cur_len != total_len:\n",
    "                target[:] = IGNORE_TOKEN_ID\n",
    "        else:\n",
    "            target = target[:seq_length]\n",
    "        targets.append(target.tolist())\n",
    "\n",
    "    input_ids = np.array(input_ids, dtype=np.int32)\n",
    "    targets = np.array(targets, dtype=np.int32)\n",
    "\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=targets,\n",
    "    )\n",
    "\n",
    "class SupervisedDataset:\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, raw_data, tokenizer, seq_length):\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "\n",
    "        sources = [example[\"conversations\"] for example in raw_data][:100]\n",
    "        data_dict = preprocess(sources, tokenizer, seq_length)\n",
    "\n",
    "        self.input_ids = data_dict.get(\"input_ids\", None)\n",
    "        self.labels = data_dict.get(\"labels\", None)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return dict(\n",
    "            input_ids=self.input_ids[i],\n",
    "            labels=self.labels[i]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "with open('./bagel_eval.json', 'r') as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "bagel = SupervisedDataset(raw_data=raw_data, tokenizer=tokenizer, seq_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = [example[\"conversations\"] for example in raw_data][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 假设你的JSON文件名为data.json\n",
    "with open('/home/bcds/On-the-Fly_MoE_Inference/OpenHermes-2.5/openhermes2_5.json', 'r', encoding='utf-8') as file:\n",
    "    raw_data = json.load(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据处理完成，已保存为 processed_data.json\n"
     ]
    }
   ],
   "source": [
    "# 处理数据并拼接成指定格式\n",
    "processed_data = []\n",
    "data = [example[\"conversations\"] for example in raw_data]\n",
    "for item in data:\n",
    "    try:\n",
    "        # print(item)\n",
    "        user_message = item[0]['value']\n",
    "        assistant_message = item[1]['value']\n",
    "        formatted_message = f\"</s>USER: {user_message} </s>ASSISTANT: {assistant_message}\"\n",
    "        processed_data.append({\"text\": formatted_message})  # 按照要求格式保存\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# 将处理后的数据保存为新的JSON文件\n",
    "with open('/home/bcds/On-the-Fly_MoE_Inference/OpenHermes-2.5/processed_data.json', 'w', encoding='utf-8') as output_file:\n",
    "    json.dump(processed_data, output_file, ensure_ascii=True)  # 保存为JSON格式\n",
    "\n",
    "print(\"数据处理完成，已保存为 processed_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据处理完成，已保存为 processed_data.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 假设你的JSON文件名为data.json\n",
    "with open('/home/bcds/On-the-Fly_MoE_Inference/bagel-v0.5/bagel_eval.json', 'r', encoding='utf-8') as file:\n",
    "    raw_data = json.load(file)\n",
    "\n",
    "# 处理数据并拼接成指定格式\n",
    "processed_data = []\n",
    "data = [example[\"conversations\"] for example in raw_data]\n",
    "for item in data:\n",
    "    try:\n",
    "        # print(item)\n",
    "        user_message = item[0]['value']\n",
    "        assistant_message = item[1]['value']\n",
    "        formatted_message = f\"</s>USER: {user_message} </s>ASSISTANT: {assistant_message}\"\n",
    "        processed_data.append({\"text\": formatted_message})  # 按照要求格式保存\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# 将处理后的数据保存为新的JSON文件\n",
    "with open('/home/bcds/On-the-Fly_MoE_Inference/bagel-v0.5/processed_data.json', 'w', encoding='utf-8') as output_file:\n",
    "    json.dump(processed_data, output_file, ensure_ascii=True)  # 保存为JSON格式\n",
    "\n",
    "print(\"数据处理完成，已保存为 processed_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 38807 examples [00:01, 26122.88 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 38807\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 加载处理后的 JSON 文件\n",
    "dataset = load_dataset('json', data_files='/home/bcds/On-the-Fly_MoE_Inference/bagel-v0.5/processed_data.json')\n",
    "\n",
    "# 查看数据集内容\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n",
      "human\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/bcds/On-the-Fly_MoE_Inference/quantize/recover.ipynb 单元格 6\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bh100/home/bcds/On-the-Fly_MoE_Inference/quantize/recover.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m conversations \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bh100/home/bcds/On-the-Fly_MoE_Inference/quantize/recover.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, source \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(sources):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bh100/home/bcds/On-the-Fly_MoE_Inference/quantize/recover.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(source[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mfrom\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bh100/home/bcds/On-the-Fly_MoE_Inference/quantize/recover.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mif\u001b[39;00m roles\u001b[39m.\u001b[39mget(source[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mfrom\u001b[39m\u001b[39m\"\u001b[39m)) \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bh100/home/bcds/On-the-Fly_MoE_Inference/quantize/recover.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m         \u001b[39m# Skip the first one if it is not from human\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bh100/home/bcds/On-the-Fly_MoE_Inference/quantize/recover.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m         source \u001b[39m=\u001b[39m source[\u001b[39m1\u001b[39m:]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# \"USER\", \"ASSISTANT\"\n",
    "roles = {\"human\": \"USER\", \"gpt\": \"ASSISTANT\"}\n",
    "# Apply prompt templates\n",
    "conversations = []\n",
    "for i, source in enumerate(sources):\n",
    "    print(source[0].get(\"from\"))\n",
    "    if roles.get(source[0].get(\"from\")) != \"human\":\n",
    "        # Skip the first one if it is not from human\n",
    "        source = source[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! export HF_ENDPOINT=\"https://hf-mirror.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lz/miniconda3/envs/hqq/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/lz/On-the-Fly_MoE_Inference/quantize/recover.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blza100/home/lz/On-the-Fly_MoE_Inference/quantize/recover.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blza100/home/lz/On-the-Fly_MoE_Inference/quantize/recover.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlm_eval\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvllm_causallms\u001b[39;00m \u001b[39mimport\u001b[39;00m VLLM\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blza100/home/lz/On-the-Fly_MoE_Inference/quantize/recover.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blza100/home/lz/On-the-Fly_MoE_Inference/quantize/recover.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m0,1\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/lm_eval/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mevaluator\u001b[39;00m \u001b[39mimport\u001b[39;00m evaluate, simple_evaluate\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/lm_eval/evaluator.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlm_eval\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlm_eval\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mregistry\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlm_eval\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/lm_eval/api/metrics.py:169\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39m@register_metric\u001b[39m(\n\u001b[1;32m    160\u001b[0m     metric\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39macc_mutual_info\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    161\u001b[0m     higher_is_better\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    164\u001b[0m )\n\u001b[1;32m    165\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39macc_mutual_info_fn\u001b[39m(items):  \u001b[39m# This is a passthrough function\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39mreturn\u001b[39;00m items\n\u001b[0;32m--> 169\u001b[0m exact_match \u001b[39m=\u001b[39m hf_evaluate\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mexact_match\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    172\u001b[0m \u001b[39m@register_metric\u001b[39m(\n\u001b[1;32m    173\u001b[0m     metric\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexact_match\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    174\u001b[0m     higher_is_better\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m )\n\u001b[1;32m    178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexact_match_fn\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    179\u001b[0m     \u001b[39mreturn\u001b[39;00m exact_match\u001b[39m.\u001b[39mcompute(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/evaluate/loading.py:748\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, config_name, module_type, process_id, num_process, cache_dir, experiment_id, keep_in_memory, download_config, download_mode, revision, **init_kwargs)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Load a [`~evaluate.EvaluationModule`].\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \n\u001b[1;32m    705\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[39m    ```\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    747\u001b[0m download_mode \u001b[39m=\u001b[39m DownloadMode(download_mode \u001b[39mor\u001b[39;00m DownloadMode\u001b[39m.\u001b[39mREUSE_DATASET_IF_EXISTS)\n\u001b[0;32m--> 748\u001b[0m evaluation_module \u001b[39m=\u001b[39m evaluation_module_factory(\n\u001b[1;32m    749\u001b[0m     path, module_type\u001b[39m=\u001b[39;49mmodule_type, revision\u001b[39m=\u001b[39;49mrevision, download_config\u001b[39m=\u001b[39;49mdownload_config, download_mode\u001b[39m=\u001b[39;49mdownload_mode\n\u001b[1;32m    750\u001b[0m )\n\u001b[1;32m    751\u001b[0m evaluation_cls \u001b[39m=\u001b[39m import_main_class(evaluation_module\u001b[39m.\u001b[39mmodule_path)\n\u001b[1;32m    752\u001b[0m evaluation_instance \u001b[39m=\u001b[39m evaluation_cls(\n\u001b[1;32m    753\u001b[0m     config_name\u001b[39m=\u001b[39mconfig_name,\n\u001b[1;32m    754\u001b[0m     process_id\u001b[39m=\u001b[39mprocess_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39minit_kwargs,\n\u001b[1;32m    761\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/evaluate/loading.py:639\u001b[0m, in \u001b[0;36mevaluation_module_factory\u001b[0;34m(path, module_type, revision, download_config, download_mode, force_local_path, dynamic_modules_path, **download_kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mfor\u001b[39;00m current_type \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mmetric\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcomparison\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmeasurement\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    632\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    633\u001b[0m         \u001b[39mreturn\u001b[39;00m HubEvaluationModuleFactory(\n\u001b[1;32m    634\u001b[0m             \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mevaluate-\u001b[39;49m\u001b[39m{\u001b[39;49;00mcurrent_type\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mpath\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    635\u001b[0m             revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    636\u001b[0m             download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m    637\u001b[0m             download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m    638\u001b[0m             dynamic_modules_path\u001b[39m=\u001b[39;49mdynamic_modules_path,\n\u001b[0;32m--> 639\u001b[0m         )\u001b[39m.\u001b[39;49mget_module()\n\u001b[1;32m    640\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m         \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/evaluate/loading.py:479\u001b[0m, in \u001b[0;36mHubEvaluationModuleFactory.get_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39m# get script and other files\u001b[39;00m\n\u001b[1;32m    478\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 479\u001b[0m     local_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownload_loading_script(revision)\n\u001b[1;32m    480\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    481\u001b[0m     \u001b[39m# if there is no file found with current revision tag try to load main\u001b[39;00m\n\u001b[1;32m    482\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrevision \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m os\u001b[39m.\u001b[39mgetenv(\u001b[39m\"\u001b[39m\u001b[39mHF_SCRIPTS_VERSION\u001b[39m\u001b[39m\"\u001b[39m, SCRIPTS_VERSION) \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmain\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/evaluate/loading.py:469\u001b[0m, in \u001b[0;36mHubEvaluationModuleFactory.download_loading_script\u001b[0;34m(self, revision)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[39mif\u001b[39;00m download_config\u001b[39m.\u001b[39mdownload_desc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    468\u001b[0m     download_config\u001b[39m.\u001b[39mdownload_desc \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDownloading builder script\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 469\u001b[0m \u001b[39mreturn\u001b[39;00m cached_path(file_path, download_config\u001b[39m=\u001b[39;49mdownload_config)\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/evaluate/utils/file_utils.py:175\u001b[0m, in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, download_config, **download_kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     url_or_filename \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(url_or_filename)\n\u001b[1;32m    173\u001b[0m \u001b[39mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[1;32m    174\u001b[0m     \u001b[39m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     output_path \u001b[39m=\u001b[39m get_from_cache(\n\u001b[1;32m    176\u001b[0m         url_or_filename,\n\u001b[1;32m    177\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    178\u001b[0m         force_download\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mforce_download,\n\u001b[1;32m    179\u001b[0m         proxies\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mproxies,\n\u001b[1;32m    180\u001b[0m         resume_download\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mresume_download,\n\u001b[1;32m    181\u001b[0m         user_agent\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49muser_agent,\n\u001b[1;32m    182\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mlocal_files_only,\n\u001b[1;32m    183\u001b[0m         use_etag\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49muse_etag,\n\u001b[1;32m    184\u001b[0m         max_retries\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    185\u001b[0m         token\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mtoken,\n\u001b[1;32m    186\u001b[0m         download_desc\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mdownload_desc,\n\u001b[1;32m    187\u001b[0m     )\n\u001b[1;32m    188\u001b[0m \u001b[39melif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(url_or_filename):\n\u001b[1;32m    189\u001b[0m     \u001b[39m# File, and it exists.\u001b[39;00m\n\u001b[1;32m    190\u001b[0m     output_path \u001b[39m=\u001b[39m url_or_filename\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/evaluate/utils/file_utils.py:457\u001b[0m, in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, token, download_desc)\u001b[0m\n\u001b[1;32m    455\u001b[0m     connected \u001b[39m=\u001b[39m ftp_head(url)\n\u001b[1;32m    456\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 457\u001b[0m     response \u001b[39m=\u001b[39m http_head(\n\u001b[1;32m    458\u001b[0m         url,\n\u001b[1;32m    459\u001b[0m         allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    460\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    461\u001b[0m         timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[1;32m    462\u001b[0m         max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m    463\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    464\u001b[0m     )\n\u001b[1;32m    465\u001b[0m     \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m200\u001b[39m:  \u001b[39m# ok\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         etag \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mETag\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mif\u001b[39;00m use_etag \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/evaluate/utils/file_utils.py:378\u001b[0m, in \u001b[0;36mhttp_head\u001b[0;34m(url, proxies, headers, cookies, allow_redirects, timeout, max_retries)\u001b[0m\n\u001b[1;32m    376\u001b[0m headers \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(headers) \u001b[39mor\u001b[39;00m {}\n\u001b[1;32m    377\u001b[0m headers[\u001b[39m\"\u001b[39m\u001b[39muser-agent\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m get_datasets_user_agent(user_agent\u001b[39m=\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39muser-agent\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m--> 378\u001b[0m response \u001b[39m=\u001b[39m _request_with_retry(\n\u001b[1;32m    379\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHEAD\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    380\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    381\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    382\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    383\u001b[0m     cookies\u001b[39m=\u001b[39;49mcookies,\n\u001b[1;32m    384\u001b[0m     allow_redirects\u001b[39m=\u001b[39;49mallow_redirects,\n\u001b[1;32m    385\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    386\u001b[0m     max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m    387\u001b[0m )\n\u001b[1;32m    388\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/evaluate/utils/file_utils.py:307\u001b[0m, in \u001b[0;36m_request_with_retry\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, **params)\u001b[0m\n\u001b[1;32m    305\u001b[0m tries \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    306\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 307\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod\u001b[39m.\u001b[39;49mupper(), url\u001b[39m=\u001b[39;49murl, timeout\u001b[39m=\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    308\u001b[0m     success \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[39mexcept\u001b[39;00m (requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mConnectTimeout, requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mConnectionError) \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    668\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    669\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    670\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    671\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    672\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    673\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    674\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    675\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    676\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    677\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    678\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    679\u001b[0m     )\n\u001b[1;32m    681\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    788\u001b[0m     conn,\n\u001b[1;32m    789\u001b[0m     method,\n\u001b[1;32m    790\u001b[0m     url,\n\u001b[1;32m    791\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    792\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    793\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    794\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    795\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[1;32m    796\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[1;32m    797\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    798\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    799\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    800\u001b[0m )\n\u001b[1;32m    802\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/urllib3/connectionpool.py:464\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     \u001b[39m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 464\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[1;32m    465\u001b[0m     \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    466\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mconn\u001b[39m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/urllib3/connectionpool.py:1093\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[39m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m \u001b[39mif\u001b[39;00m conn\u001b[39m.\u001b[39mis_closed:\n\u001b[0;32m-> 1093\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m   1095\u001b[0m \u001b[39m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mproxy_is_verified:\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/urllib3/connection.py:704\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     sock: socket\u001b[39m.\u001b[39msocket \u001b[39m|\u001b[39m ssl\u001b[39m.\u001b[39mSSLSocket\n\u001b[0;32m--> 704\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m sock \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[1;32m    705\u001b[0m     server_hostname: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n\u001b[1;32m    706\u001b[0m     tls_in_tls \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/urllib3/connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \n\u001b[1;32m    195\u001b[0m \u001b[39m:return: New socket connection.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m     sock \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    199\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport),\n\u001b[1;32m    200\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout,\n\u001b[1;32m    201\u001b[0m         source_address\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msource_address,\n\u001b[1;32m    202\u001b[0m         socket_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket_options,\n\u001b[1;32m    203\u001b[0m     )\n\u001b[1;32m    204\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39mgaierror \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    205\u001b[0m     \u001b[39mraise\u001b[39;00m NameResolutionError(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost, \u001b[39mself\u001b[39m, e) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mif\u001b[39;00m source_address:\n\u001b[1;32m     72\u001b[0m     sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m sock\u001b[39m.\u001b[39;49mconnect(sa)\n\u001b[1;32m     74\u001b[0m \u001b[39m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[1;32m     75\u001b[0m err \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "from lm_eval.models.vllm_causallms import VLLM\n",
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "\n",
    "lora_save_path = '/home/lz/On-the-Fly_MoE_Inference/quantize/saved/training/nohqq_noaver/checkpoint-200'\n",
    "VLLM(pretrained=model_name, lora_local_path=lora_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Sparsity:  2.5000\n",
      "Max Sparsity: 4.0000\n",
      "Min Sparsity: 1.0000\n"
     ]
    }
   ],
   "source": [
    "avg_list = [1,2,3,4]\n",
    "\n",
    "print('Average Sparsity: ', f'{sum(avg_list)/len(avg_list):.4f}')\n",
    "print('Max Sparsity: {:.4f}'.format(max(avg_list)))\n",
    "print('Min Sparsity: {:.4f}'.format(min(avg_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lz/miniconda3/envs/hqq/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set profile_threshold to False\n",
      "Thresholds loaded from /home/lz/On-the-Fly_MoE_Inference/saving/threshold/c4_mixtral_up/thresholds_0_8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:24<00:00,  1.27s/it]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MixtralForCausalLM' object has no attribute '_split_kwargs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/lz/On-the-Fly_MoE_Inference/quantize/recover.ipynb Cell 1\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blza100/home/lz/On-the-Fly_MoE_Inference/quantize/recover.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m llm, tokenizer \u001b[39m=\u001b[39m get_model(model_name, device_map, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blza100/home/lz/On-the-Fly_MoE_Inference/quantize/recover.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# PeftUtils.load_lora_weights(llm, lora_save_path)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blza100/home/lz/On-the-Fly_MoE_Inference/quantize/recover.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# PeftUtils.cast_lora_weights(llm, dtype)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blza100/home/lz/On-the-Fly_MoE_Inference/quantize/recover.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m PeftModelForCausalLM\u001b[39m.\u001b[39;49mload_adapter(llm, lora_save_path, \u001b[39m'\u001b[39;49m\u001b[39mdefault\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/peft/peft_model.py:1215\u001b[0m, in \u001b[0;36mPeftModel.load_adapter\u001b[0;34m(self, model_id, adapter_name, is_trainable, torch_device, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, **kwargs)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m \u001b[39mLoad a trained adapter into the model.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[39m        Additional arguments to modify the way the adapter is loaded, e.g. the token for Hugging Face Hub.\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmapping\u001b[39;00m \u001b[39mimport\u001b[39;00m PEFT_TYPE_TO_CONFIG_MAPPING\n\u001b[0;32m-> 1215\u001b[0m hf_hub_download_kwargs, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_split_kwargs(kwargs)\n\u001b[1;32m   1216\u001b[0m \u001b[39mif\u001b[39;00m torch_device \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1217\u001b[0m     torch_device \u001b[39m=\u001b[39m infer_device()\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/torch/nn/modules/module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1708\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1709\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MixtralForCausalLM' object has no attribute '_split_kwargs'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from modeling_mixtral import set_profile_mode, load_thresholds\n",
    "from utils import myevaluate, get_model\n",
    "import json \n",
    "import argparse\n",
    "from peft import PeftModelForCausalLM\n",
    "\n",
    "# def doeval(dtype, lora_save_path, args):\n",
    "dtype = torch.float16\n",
    "threshold_path_name='chess_up_threshold'\n",
    "use_average = True\n",
    "lora_save_path = '/home/lz/On-the-Fly_MoE_Inference/quantize/saved/training/nohqq/checkpoint-200'\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    threshold_path = path[threshold_path_name]\n",
    "\n",
    "with open('./device_map_1.json', 'r') as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "## 开启稀疏模式\n",
    "set_profile_mode(False)\n",
    "load_thresholds(f'{threshold_path}/thresholds_0_8.pt', use_average=use_average)\t\n",
    "llm, tokenizer = get_model(model_name, device_map, dtype=dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MixtralForCausalLM(\n",
       "      (model): MixtralModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x MixtralDecoderLayer(\n",
       "            (self_attn): MixtralSdpaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): MixtralRotaryEmbedding()\n",
       "            )\n",
       "            (block_sparse_moe): MixtralSparseMoeBlock(\n",
       "              (gate): Linear(in_features=4096, out_features=8, bias=False)\n",
       "              (experts): ModuleList(\n",
       "                (0-7): 8 x MixtralBlockSparseTop2MLP(\n",
       "                  (w1): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.01, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=64, out_features=14336, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (w2): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.01, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=14336, out_features=64, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (w3): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.01, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=64, out_features=14336, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (act_fn): SiLU()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (input_layernorm): MixtralRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): MixtralRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): MixtralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "PeftModelForCausalLM.from_pretrained(llm, lora_save_path, 'default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default.weight Parameter containing:\n",
      "tensor([[-0.0019, -0.0103,  0.0009,  ...,  0.0043,  0.0092, -0.0057],\n",
      "        [ 0.0047,  0.0046, -0.0017,  ...,  0.0062,  0.0033, -0.0004],\n",
      "        [-0.0011,  0.0035, -0.0030,  ..., -0.0140, -0.0139,  0.0035],\n",
      "        ...,\n",
      "        [-0.0109,  0.0147,  0.0099,  ..., -0.0129,  0.0135,  0.0080],\n",
      "        [ 0.0067, -0.0071, -0.0059,  ..., -0.0132, -0.0062, -0.0133],\n",
      "        [ 0.0104,  0.0120,  0.0111,  ...,  0.0022,  0.0015,  0.0138]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for name,para in llm.base_model.layers[0].self_attn.q_proj.lora_A.named_parameters():\n",
    "    print(name, para)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "lora = torch.load('/home/lz/On-the-Fly_MoE_Inference/quantize/saved/training/test/checkpoint-300_lora_combine.pt')\n",
    "print(lora['parameters'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lz/miniconda3/envs/hqq/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set profile_threshold to False\n",
      "Thresholds loaded from /home/lz/On-the-Fly_MoE_Inference/quantize/threshold/c4_mixtral_up/thresholds_0_8.pt\n",
      "using  torch.bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:34<00:00,  1.83s/it]\n",
      "100%|██████████| 32/32 [00:00<00:00, 557.99it/s]\n",
      "100%|██████████| 32/32 [00:15<00:00,  2.03it/s]\n",
      "100%|██████████| 32/32 [00:13<00:00,  2.46it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 33.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 31 done...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6\"\n",
    "import transformers\n",
    "from modeling_mixtral import set_profile_mode, load_thresholds\n",
    "import json\n",
    "from utils import get_model, CompensatedModel\n",
    "from hqq.core.quantize import *\n",
    "from hqq.models.hf.mixtral import MixtralHQQ\n",
    "from hqq.core.peft import PeftUtils\n",
    "from datasets import load_dataset, Dataset\n",
    "import functools\n",
    "\n",
    "# # 加载 C4 数据集的验证集\n",
    "with open('../path.json', 'r') as file:\n",
    "    paths = json.load(file)\n",
    "    fineweb_path = paths.get('fineweb', '')\n",
    "    model_name = paths.get('mixtral','')\n",
    "    threshold_path = paths.get('chess_up_sparsity_threshold','')\n",
    "\n",
    "with open('./device_map.json', 'r') as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "set_profile_mode(False)\n",
    "load_thresholds(f'{threshold_path}/thresholds_0_8.pt')\n",
    "dtype = torch.bfloat16\n",
    "print('using ',dtype)\n",
    "llm, tokenizer = get_model(model_name, device_map, dtype=dtype)\n",
    "\n",
    "q4_config    = BaseQuantizeConfig(nbits=8, group_size=64) \n",
    "q3_config    = BaseQuantizeConfig(nbits=2, group_size=64)\n",
    "\n",
    "quant_config      = {'block_sparse_moe.experts.w3'   : q3_config}\n",
    "MixtralHQQ.quantize_model(llm, quant_config=quant_config, compute_dtype=dtype, device=device_map)\n",
    "\n",
    "base_lora_params = {'lora_type':'default', 'r':128, 'lora_alpha':128, 'dropout':0.05, 'train_dtype':dtype}\n",
    "\n",
    "lora_params      = {'self_attn.q_proj': base_lora_params,\n",
    "                    'self_attn.k_proj': base_lora_params,\n",
    "                    'self_attn.v_proj': base_lora_params,\n",
    "                    'self_attn.o_proj': base_lora_params,\n",
    "                    'block_sparse_moe.experts.w1'   : base_lora_params,\n",
    "                    'block_sparse_moe.experts.w3'   : base_lora_params,\n",
    "                    'block_sparse_moe.experts.w2'   : base_lora_params}\n",
    "\n",
    "\n",
    "PeftUtils.add_lora(llm, lora_params)\n",
    "lora = '/home/lz/On-the-Fly_MoE_Inference/quantize/saved/training/test/checkpoint-300_lora_combine.pt'\n",
    "PeftUtils.load_lora_weights(llm, lora)\n",
    "class CompensatedModel(torch.nn.Module):\n",
    "    def __init__(self, model, path, layerid, expertid):\n",
    "        super(CompensatedModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.A = torch.load(path + f'A_{layerid}_{expertid}.pt').to(dtype)\n",
    "        self.B_prime = torch.load(path + f'B_prime_{layerid}_{expertid}.pt').to(dtype)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        outputs = self.model(input_ids)\n",
    "        residual = (input_ids @ self.A.T) @ self.B_prime.T\n",
    "        outputs += residual\n",
    "    \n",
    "        return outputs\n",
    "for i in range(32):\n",
    "    if i == 31:\n",
    "        print(f\"Layer {i} done...\")\n",
    "    for j in range(8):\n",
    "        llmdevice = llm.model.layers[i].block_sparse_moe.experts[j].w3.linear_layer.device\n",
    "        llm.model.layers[i].block_sparse_moe.experts[j].w3.linear_layer = \\\n",
    "        CompensatedModel(llm.model.layers[i].block_sparse_moe.experts[j].w3.linear_layer, '/home/lz/On-the-Fly_MoE_Inference/quantize/saved/eora/', layerid=i, expertid=j).to(llmdevice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HQQLinearLoRA(\n",
       "  (linear_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "  (peft_drop): Dropout(p=0.05, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.model.layers[0].block_sparse_moe.experts[0].w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "import functools\n",
    "\n",
    "def preprocess_data(batch, tokenizer):\n",
    "    # 使用 tokenizer 将文本数据转换为模型输入\n",
    "    inputs = tokenizer(batch['text'], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    inputs[\"labels\"] = inputs.input_ids.clone()\n",
    "    return inputs\n",
    "\n",
    "with open('../path.json', 'r') as file:\n",
    "    paths = json.load(file)\n",
    "    fineweb_path = paths.get('fineweb', '')\n",
    "openmath = load_dataset(\"/home/lz/web-math/\",data_files=\"/home/lz/web-math/openmath1.json\")\n",
    "fineweb = load_dataset(fineweb_path)\n",
    "openmath_text = openmath['train']['text'][:4000] \n",
    "fineweb_text = fineweb['train']['text'][:12000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21411"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/lz/On-the-Fly_MoE_Inference/quantize/recover.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blza100/home/lz/On-the-Fly_MoE_Inference/quantize/recover.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m train_data \u001b[39m=\u001b[39m combined_train[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blza100/home/lz/On-the-Fly_MoE_Inference/quantize/recover.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m test_data \u001b[39m=\u001b[39m combined_train[\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blza100/home/lz/On-the-Fly_MoE_Inference/quantize/recover.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m new_train_data \u001b[39m=\u001b[39m train_data\u001b[39m.\u001b[39mmap(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blza100/home/lz/On-the-Fly_MoE_Inference/quantize/recover.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     functools\u001b[39m.\u001b[39mpartial(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blza100/home/lz/On-the-Fly_MoE_Inference/quantize/recover.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     preprocess_data,\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blza100/home/lz/On-the-Fly_MoE_Inference/quantize/recover.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     tokenizer\u001b[39m=\u001b[39mtokenizer\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blza100/home/lz/On-the-Fly_MoE_Inference/quantize/recover.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m ), batched\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blza100/home/lz/On-the-Fly_MoE_Inference/quantize/recover.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m new_test_data \u001b[39m=\u001b[39m test_data\u001b[39m.\u001b[39mmap(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blza100/home/lz/On-the-Fly_MoE_Inference/quantize/recover.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     functools\u001b[39m.\u001b[39mpartial(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blza100/home/lz/On-the-Fly_MoE_Inference/quantize/recover.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     preprocess_data,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blza100/home/lz/On-the-Fly_MoE_Inference/quantize/recover.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     tokenizer\u001b[39m=\u001b[39mtokenizer\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blza100/home/lz/On-the-Fly_MoE_Inference/quantize/recover.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m ), batched\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blza100/home/lz/On-the-Fly_MoE_Inference/quantize/recover.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m new_train_data\u001b[39m.\u001b[39mshuffle(seed)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "test_num = 0.1\n",
    "seed = 42\n",
    "\n",
    "combined_text = openmath_text + fineweb_text\n",
    "combined_dataset = Dataset.from_dict({\"text\": combined_text})\n",
    "combined_train = combined_dataset.train_test_split(test_size=test_num, seed=seed)\n",
    "train_data = combined_train['train']\n",
    "test_data = combined_train['test']\n",
    "\n",
    "new_train_data = train_data.map(\n",
    "    functools.partial(\n",
    "    preprocess_data,\n",
    "    tokenizer=tokenizer\n",
    "), batched=True)\n",
    "new_test_data = test_data.map(\n",
    "    functools.partial(\n",
    "    preprocess_data,\n",
    "    tokenizer=tokenizer\n",
    "), batched=True)\n",
    "new_train_data.shuffle(seed)\n",
    "new_test_data.shuffle(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lz/miniconda3/envs/hqq/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "2025-01-08:09:27:50,984 WARNING  [other.py:331] Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:41, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 1097.59it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 3088.38it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 736.16it/s]\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "100%|██████████| 32/32 [00:00<00:00, 1061.88it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 2894.18it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 382.07it/s]\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8, training_loss=1.8526314496994019, metrics={'train_runtime': 46.1998, 'train_samples_per_second': 1.299, 'train_steps_per_second': 0.173, 'total_flos': 6860099687546880.0, 'train_loss': 1.8526314496994019, 'epoch': 2.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hqq.core.peft import PeftUtils\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, AdamW\n",
    "from transformers import (\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "class CustomTrainer(transformers.Trainer):\n",
    "    def save_model(self, output_dir=None, _internal_call=False):\n",
    "        # 如果没有指定output_dir，则使用训练参数中的输出目录\n",
    "        if output_dir is None:\n",
    "            output_dir = self.args.output_dir #这里的args不是该脚本的输入，而是TrainerArgs\n",
    "\n",
    "        # 确保输出目录存在\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # 保存完整的模型参数\n",
    "        # torch.save(self.model.state_dict(), os.path.join(output_dir, 'pytorch_model.bin'))\n",
    "        \n",
    "        self.model.eval()\n",
    "        PeftUtils.cast_lora_weights(self.model, dtype=torch.float16)\n",
    "\n",
    "        #Save LoRA weights\n",
    "        PeftUtils.save_lora_weights(self.model, output_dir+'_lora_combine.pt')\n",
    "\n",
    "        PeftUtils.cast_lora_weights(self.model, dtype=torch.bfloat16)\n",
    "        self.model.train()\n",
    "\n",
    "        # 保存配置文件和tokenizer\n",
    "        self.model.config.save_pretrained(output_dir)\n",
    "        if self.tokenizer is not None:\n",
    "            self.tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "model_save_path='./saved/training/less2'\n",
    "learning_rate = 1e-4\n",
    "micro_batch_size=8\n",
    "epochs=2\n",
    "save_steps = 5\n",
    "save_total_limit = 6\n",
    "sample_num = len(new_train_data)\n",
    "optimizer=AdamW(filter(lambda p : p.requires_grad, llm.parameters()),lr=learning_rate)\n",
    "linear_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(sample_num*epochs) // micro_batch_size)\n",
    "args = TrainingArguments(\n",
    "    output_dir=model_save_path,\n",
    "    num_train_epochs=epochs,\n",
    "    # max_steps=opt.max_steps,\n",
    "    # fp16=True,\n",
    "    bf16=True,\n",
    "    optim=\"adamw_torch\",# paged_adamw_8bit\n",
    "    learning_rate=learning_rate,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    per_device_train_batch_size=micro_batch_size,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=False,   ### 先设置成False\n",
    "    group_by_length=False,\n",
    "    logging_steps=50,\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_only_model=True,\n",
    "    save_steps=save_steps,\n",
    "    save_total_limit=save_total_limit,\n",
    "    disable_tqdm=False,\n",
    "    report_to='tensorboard',\n",
    "    logging_dir='/home/lz/On-the-Fly_MoE_Inference/quantize/saved/logs/'\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=llm,\n",
    "    train_dataset=new_train_data.select(range(30)),\n",
    "    eval_dataset=new_test_data.select(range(4)),\n",
    "    args=args,\n",
    "    optimizers=(optimizer, linear_scheduler),\n",
    "    data_collator=DataCollatorForSeq2Seq(\n",
    "    tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True),\n",
    ")\n",
    "\n",
    "# silence the warnings. re-enable for inference!\n",
    "llm.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-01-27 10:05:09,569\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set profile_threshold to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/On-the-Fly_MoE_Inference/quantize/modeling_mixtral.py:93: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  up_th = torch.load(threshold_path, map_location='cuda')[\"up_proj_states_thresholds\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thresholds loaded from /home/bcds/On-the-Fly_MoE_Inference/saving/threshold/c4_mixtral/thresholds_0_8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:12<00:00,  1.51it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 2355.44it/s]\n",
      "100%|██████████| 32/32 [00:09<00:00,  3.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MixtralForCausalLM(\n",
       "  (model): MixtralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MixtralDecoderLayer(\n",
       "        (self_attn): MixtralSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MixtralRotaryEmbedding()\n",
       "        )\n",
       "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
       "          (gate): Linear(in_features=4096, out_features=8, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-7): 8 x MixtralBlockSparseTop2MLP(\n",
       "              (w1): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (w2): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "              (w3): HQQLinear(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): MixtralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MixtralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MixtralRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6\"\n",
    "from modeling_mixtral import MixtralForCausalLM, set_profile_mode, load_thresholds\n",
    "import json\n",
    "from utils import get_model\n",
    "\n",
    "# # 加载 C4 数据集的验证集\n",
    "with open('../path.json', 'r') as file:\n",
    "    paths = json.load(file)\n",
    "    fineweb_path = paths.get('c4', '')\n",
    "    model_name = paths.get('mixtral','')\n",
    "    threshold_path = paths.get('mixtral_threshold','')\n",
    "\n",
    "with open('./device_map.json', 'r') as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "set_profile_mode(True)\n",
    "sparsity_level=0.8\n",
    "filepath = str(sparsity_level).replace('.', '_')\n",
    "load_thresholds(f'{threshold_path}/thresholds_{filepath}.pt', use_average=False)\n",
    "llm, tokenizer = get_model(model_name, device_map)\n",
    "# %%\n",
    "#Quantize\n",
    "from hqq.core.quantize import *\n",
    "q4_config    = BaseQuantizeConfig(nbits=8, group_size=64) \n",
    "q3_config    = BaseQuantizeConfig(nbits=2, group_size=64)\n",
    "\n",
    "quant_config = {\n",
    "  'block_sparse_moe.experts.w3'  :q3_config,\n",
    "}\n",
    "from hqq.models.hf.mixtral import MixtralHQQ\n",
    "MixtralHQQ.quantize_model(llm, quant_config=quant_config, compute_dtype=torch.float16, device=device_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/929 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 929/929 [00:09<00:00, 97.35it/s] \n",
      "100%|██████████| 929/929 [00:00<00:00, 159327.30it/s]\n"
     ]
    }
   ],
   "source": [
    "from hqq.core.peft import PeftUtils\n",
    "PeftUtils.load_lora_weights(llm, '/home/lz/On-the-Fly_MoE_Inference/quantize/saved/training/less2/checkpoint-750_lora_combine.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/364608 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 364608/364608 [01:03<00:00, 5776.10 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from datasets import load_dataset\n",
    "def preprocess_data(batch):\n",
    "    # 使用 tokenizer 将文本数据转换为模型输入\n",
    "    inputs = tokenizer(batch['text'], padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    inputs[\"labels\"] = inputs.input_ids.clone()\n",
    "    return inputs\n",
    "\n",
    "# 定义一个函数来选择特征并丢弃不需要的\n",
    "def select_features(example):\n",
    "    return {\n",
    "        'input_ids': example['input_ids'],\n",
    "        'attention_mask': example['attention_mask'],\n",
    "        'labels': example['labels']\n",
    "    }\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "with open('../path.json', 'r') as file:\n",
    "    paths = json.load(file)\n",
    "    c4_path = paths.get('c4', '')\n",
    "c4 = load_dataset(c4_path)\n",
    "# 对数据集进行预处理\n",
    "c4_dataset = c4.map(preprocess_data, batched=True)\n",
    "# c4_dataset = c4_dataset.map(select_features, batched=True)\n",
    "c4_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "# c4_dataset\n",
    "top_four_thousand_data = c4_dataset['validation'].select(range(10000))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "set_seed(42)\n",
    "\n",
    "# 定义数据加载器\n",
    "batch_size = 8\n",
    "# dataloader = DataLoader(c4_dataset['validation'], batch_size=batch_size)\n",
    "dataloader = DataLoader(top_four_thousand_data, batch_size=batch_size)\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:09<00:00,  2.10it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "llm_base = MixtralForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map='cpu',\n",
    "    use_cache=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    # attn_implementation=\"flash_attention_2\"\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eora恢复"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直接从文件中读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class CompensatedModel(torch.nn.Module):\n",
    "    def __init__(self, model, path, layerid, expertid):\n",
    "        super(CompensatedModel, self).__init__()\n",
    "        self.model = model\n",
    "        ### self.A and self.B_prime are initialized as the values loaded from the file\n",
    "        self.A = torch.load(path + f'A_{layerid}_{expertid}.pt').to(torch.float16)\n",
    "        self.B_prime = torch.load(path + f'B_prime_{layerid}_{expertid}.pt').to(torch.float16)\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        outputs = self.model(input_ids)\n",
    "        residual = (input_ids @ self.A.T) @ self.B_prime.T\n",
    "        outputs += residual\n",
    "    \n",
    "        return outputs\n",
    "\n",
    "for i in range(32):\n",
    "    print(f\"Layer {i} done...\")\n",
    "    for j in range(8):\n",
    "        llmdevice = llm.model.layers[i].block_sparse_moe.experts[j].w3.device\n",
    "        llm.model.layers[i].block_sparse_moe.experts[j].w3 = \\\n",
    "        CompensatedModel(llm.model.layers[i].block_sparse_moe.experts[j].w3, '/home/lz/On-the-Fly_MoE_Inference/quantize/saved/', layerid=i, expertid=j).to(llmdevice)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一次计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start obtaining the whitening matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [57:54<00:00,  2.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Cholesky Decomposition...\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def profle_svdllm(name, model, calib_loader, dev):\n",
    "    # model.to(dev)\n",
    "    if \"llama\" in name or \"mixtral\" in name or \"vicuna\" in name:\n",
    "        layers = model.model.layers\n",
    "    print(\"Start obtaining the whitening matrix...\")\n",
    "    def hook(module, input, output):\n",
    "        inp = input[0].detach().float()\n",
    "        if inp.dim() == 2:   # for opt\n",
    "            inp = inp.unsqueeze(0)\n",
    "        adds = torch.matmul(inp.transpose(1,2), inp)\n",
    "        adds_sum = torch.sum(adds, dim=0)\n",
    "        module.raw_scaling_diag_matrix += adds_sum\n",
    "        del inp, adds, adds_sum\n",
    "        torch.cuda.empty_cache()\n",
    "    for name, module in model.named_modules():\n",
    "        if \"w3\" in name:\n",
    "            # print(name)\n",
    "            module.raw_scaling_diag_matrix = 0\n",
    "            module.register_forward_hook(hook)\n",
    "            \n",
    "    for batch in tqdm(calib_loader):\n",
    "        inputs = batch['input_ids'].to(llm.device)\n",
    "        model(inputs)\n",
    "    for name, module in model.named_modules():\n",
    "        if \"w3\" in name:\n",
    "            module._forward_hooks.clear()\n",
    "            # print(module.raw_scaling_diag_matrix)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    profiling_mat = {}\n",
    "    print(\"Start Cholesky Decomposition...\")\n",
    "    \n",
    "    layer_profile = {}\n",
    "    for name, module in model.named_modules():\n",
    "        if \"w3\" in name:\n",
    "            covariance = module.raw_scaling_diag_matrix.double().to(dev)\n",
    "            if not torch.allclose(covariance, covariance.t(), atol=1e-6):\n",
    "                raise ValueError(\"Covariance matrix is not symmetric.\")\n",
    "                    # Perform eigen decomposition\n",
    "            Lambda, Q = torch.linalg.eigh(covariance, UPLO='U')\n",
    "            if torch.isnan(Lambda).any() or torch.isinf(Lambda).any():\n",
    "                raise ValueError(\"Lambda contains NaN or Inf values.\")\n",
    "\n",
    "            # 检查 Lambda 是否包含负值\n",
    "            if (Lambda < 0).any():\n",
    "                print(\"Lambda contains negative values. Clamping to zero.\")\n",
    "                eigenvalues = torch.linalg.eigvalsh(covariance)\n",
    "                covariance += (- eigenvalues[0] + 2e-6) * torch.eye(covariance.shape[0]).cuda()\n",
    "                Lambda, Q = torch.linalg.eigh(covariance, UPLO='U')\n",
    "                print(f\"Lambda min: {Lambda.min().item()}, Lambda max: {Lambda.max().item()}\")\n",
    "            # 现在进行平方根操作\n",
    "            Lambda_diag = torch.diag(torch.sqrt(Lambda))\n",
    "            # Sort eigenvalues and eigenvectors in descending order\n",
    "            indices = torch.argsort(Lambda, descending=True)\n",
    "            Lambda = Lambda[indices]\n",
    "            Q = Q[:, indices]\n",
    "\n",
    "            # Compute Q_prime = Q * sqrt(Lambda)\n",
    "            Lambda_diag = torch.diag(torch.sqrt(Lambda))\n",
    "            Q_prime = torch.matmul(Q, Lambda_diag)\n",
    "            layer_profile[name] = Q_prime.cpu()\n",
    "            profiling_mat[name] = layer_profile\n",
    "    return profiling_mat\n",
    "\n",
    "profiling_mat=profle_svdllm(\"mixtral\", llm, dataloader, \"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2817685/1358780285.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.B_prime = torch.nn.Parameter(torch.tensor(B_prime)).to(torch.float16)\n",
      "/tmp/ipykernel_2817685/1358780285.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.A = torch.nn.Parameter(torch.tensor(A)).to(torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 done...\n",
      "Layer 2 done...\n",
      "Layer 3 done...\n",
      "Layer 4 done...\n",
      "Layer 5 done...\n",
      "Layer 6 done...\n",
      "Layer 7 done...\n",
      "Layer 8 done...\n",
      "Layer 9 done...\n",
      "Layer 10 done...\n",
      "Layer 11 done...\n",
      "Layer 12 done...\n",
      "Layer 13 done...\n",
      "Layer 14 done...\n",
      "Layer 15 done...\n",
      "Layer 16 done...\n",
      "Layer 17 done...\n",
      "Layer 18 done...\n",
      "Layer 19 done...\n",
      "Layer 20 done...\n",
      "Layer 21 done...\n",
      "Layer 22 done...\n",
      "Layer 23 done...\n",
      "Layer 24 done...\n",
      "Layer 25 done...\n",
      "Layer 26 done...\n",
      "Layer 27 done...\n",
      "Layer 28 done...\n",
      "Layer 29 done...\n",
      "Layer 30 done...\n",
      "Layer 31 done...\n"
     ]
    }
   ],
   "source": [
    "class CompensatedModel(torch.nn.Module):\n",
    "    def __init__(self, model, B_prime, A):\n",
    "        super(CompensatedModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.B_prime = torch.nn.Parameter(torch.tensor(B_prime)).to(torch.float16)\n",
    "        self.A = torch.nn.Parameter(torch.tensor(A)).to(torch.float16)\n",
    "        # print(self.A.shape,self.B_prime.shape)\n",
    "    def forward(self, input_ids):\n",
    "        outputs = self.model(input_ids)\n",
    "        residual = (input_ids @ self.A.T) @ self.B_prime.T\n",
    "        torch.add(outputs, residual, out = outputs)\n",
    "    \n",
    "        return outputs\n",
    "    \n",
    "for i in range(32):\n",
    "    print(f\"Layer {i} done...\")\n",
    "    for j in range(8):\n",
    "        llmdevice = llm.model.layers[i].block_sparse_moe.experts[j].w3.device\n",
    "        Delta_W = llm_base.model.layers[i].block_sparse_moe.experts[j].w3.weight.to(llmdevice) - llm.model.layers[i].block_sparse_moe.experts[j].w3.dequantize()\n",
    "        Q_prime = profiling_mat[f\"model.layers.{i}.block_sparse_moe.experts.{j}.w3\"][f\"model.layers.{i}.block_sparse_moe.experts.{j}.w3\"].cuda().float()\n",
    "        Delta_W_prime =  Delta_W.to(torch.float32).to(llmdevice) @ Q_prime.to(torch.float32).to(llmdevice)\n",
    "        llm_base.model.layers[i].block_sparse_moe.experts[j].w3.cpu()\n",
    "        # 步骤5: 进行SVD分解并取前r个奇异值\n",
    "        rank = 64  # 设置 desired rank\n",
    "        U_prime, Sigma_prime, V_prime = torch.linalg.svd(Delta_W_prime, full_matrices=False)\n",
    "        U_prime = U_prime[:, :rank]\n",
    "        Sigma_prime = Sigma_prime[:rank]\n",
    "        V_prime = V_prime[:rank, :]\n",
    "\n",
    "        B_prime = U_prime @ torch.diag(Sigma_prime)\n",
    "        A_prime = V_prime\n",
    "\n",
    "        # 步骤6: 投影回原空间\n",
    "        A = A_prime.to(llmdevice) @ torch.linalg.inv(Q_prime).to(llmdevice)\n",
    "        llm.model.layers[i].block_sparse_moe.experts[j].w3 = CompensatedModel(llm.model.layers[i].block_sparse_moe.experts[j].w3, B_prime, A).to(llmdevice)\n",
    "        torch.save(B_prime, f\"./saved/B_prime_{i}_{j}.pt\")\n",
    "        torch.save(A, f\"./saved/A_{i}_{j}.pt\")\n",
    "\n",
    "del llm_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set profile_threshold to True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "datasets = torch.load('../saving/threshold/chess/datasets.pt')\n",
    "set_profile_mode(True)\n",
    "def get_batch(data, batch_size, block_size):\n",
    "    start_idxs = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in start_idxs])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in start_idxs])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "14\n",
      "16\n",
      "18\n",
      "20\n",
      "22\n",
      "24\n",
      "26\n",
      "28\n",
      "30\n",
      "32\n",
      "34\n",
      "36\n",
      "38\n",
      "40\n",
      "42\n",
      "44\n",
      "46\n",
      "48\n",
      "50\n",
      "52\n",
      "54\n",
      "56\n",
      "58\n",
      "60\n",
      "62\n",
      "64\n",
      "66\n",
      "68\n",
      "70\n",
      "72\n",
      "74\n",
      "76\n",
      "78\n",
      "80\n",
      "82\n",
      "84\n",
      "86\n",
      "88\n",
      "90\n",
      "92\n",
      "94\n",
      "96\n",
      "98\n",
      "100\n",
      "102\n",
      "104\n",
      "106\n",
      "108\n",
      "110\n",
      "112\n",
      "114\n",
      "116\n",
      "118\n",
      "120\n",
      "122\n",
      "124\n",
      "126\n"
     ]
    }
   ],
   "source": [
    "sparsity_level = 0.8\n",
    "# device = 'cuda:1'\n",
    "device_2 = 'cpu'\n",
    "avg_loss = 0.0\n",
    "n_batch = 64 * 2\n",
    "# accum_steps = 4 \n",
    "accum_steps = 2\n",
    "batch_size = 1\n",
    "block_size = 2048\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model = llm\n",
    "\n",
    "n_layers = len(model.model.layers)\n",
    "n_experts = len(model.model.layers[0].block_sparse_moe.experts)\n",
    "\n",
    "up_proj_states_thresholds = [torch.zeros([n_experts,]) for _ in range(n_layers)]\n",
    "gate_proj_states_mean_squares = [[torch.zeros(model.config.intermediate_size) for _ in range(n_experts)] for _ in range(n_layers)]\n",
    "\n",
    "up_states = [[torch.zeros([accum_steps * batch_size * block_size //2, model.config.intermediate_size]) for _ in range(n_experts)] for _ in range(n_layers)]\n",
    "gate_states = [[torch.zeros([accum_steps * batch_size * block_size //2, model.config.intermediate_size]) for _ in range(n_experts)] for _ in range(n_layers)]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step in range(n_batch // accum_steps):\n",
    "        print(step * accum_steps)\n",
    "        all_counts = [0 for _ in range(n_layers * n_experts)]\n",
    "        for batch_idx in range(accum_steps):\n",
    "            # print('batch_idx:', batch_idx)\n",
    "            inputs, labels = get_batch(datasets['validation'], batch_size, block_size)\n",
    "            inputs = inputs.cuda()\n",
    "            outputs = model(inputs, labels=inputs)\n",
    "            avg_loss = avg_loss + outputs.loss / n_batch\n",
    "\n",
    "            for layer_idx in range(n_layers):\n",
    "                for expert_idx in range(n_experts):\n",
    "                    counts = all_counts[layer_idx * n_experts + expert_idx]\n",
    "\n",
    "                    states = model.model.layers[layer_idx].block_sparse_moe.experts[expert_idx].up_proj_states.reshape(-1, model.config.intermediate_size)\n",
    "                    cur_counts = states.size(0)\n",
    "                    # print('counts and cur_counts:',counts, cur_counts)\n",
    "                    # print(states.size())\n",
    "                    # print(up_states[layer_idx][expert_idx][counts : counts+cur_counts, :].size())\n",
    "                    up_states[layer_idx][expert_idx][counts : counts+cur_counts, :] = states\n",
    "\n",
    "                    states = model.model.layers[layer_idx].block_sparse_moe.experts[expert_idx].gate_proj_states.reshape(-1, model.config.intermediate_size)\n",
    "                    gate_states[layer_idx][expert_idx][counts : counts+cur_counts, :] = states\n",
    "                    # counts += cur_counts\n",
    "                    all_counts[layer_idx * n_experts + expert_idx] += cur_counts\n",
    "\n",
    "        for layer_idx in range(n_layers):   \n",
    "            for expert_idx in range(n_experts):\n",
    "                # print('layer_idx:', layer_idx, 'expert_idx:', expert_idx)\n",
    "                useful_num = all_counts[layer_idx * n_experts + expert_idx]\n",
    "                topk_num = int(useful_num * model.config.intermediate_size * sparsity_level)\n",
    "                up_proj_states_thresholds[layer_idx][expert_idx] += up_states[layer_idx][expert_idx][0:useful_num,:].to(device_2).abs().flatten().kthvalue(topk_num).values.to('cpu')\n",
    "                gate_proj_states_mean_squares[layer_idx][expert_idx] += (torch.sum(gate_states[layer_idx][expert_idx][0:useful_num,:].to(dev\n",
    "                \n",
    "                \n",
    "                ice_2) ** 2, dim=0).to('cpu') / useful_num).to('cpu')\n",
    "\n",
    "for layer_idx in range(n_layers):\n",
    "    for expert_idx in range(n_experts):\n",
    "        gate_proj_states_mean_squares[layer_idx][expert_idx] /= n_batch // accum_steps\n",
    "        up_proj_states_thresholds[layer_idx][expert_idx] /= n_batch // accum_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "14\n",
      "16\n",
      "18\n",
      "20\n",
      "22\n",
      "24\n",
      "26\n",
      "28\n",
      "30\n",
      "32\n",
      "34\n",
      "36\n",
      "38\n",
      "40\n",
      "42\n",
      "44\n",
      "46\n",
      "48\n",
      "50\n",
      "52\n",
      "54\n",
      "56\n",
      "58\n",
      "60\n",
      "62\n",
      "64\n",
      "66\n",
      "68\n",
      "70\n",
      "72\n",
      "74\n",
      "76\n",
      "78\n",
      "80\n",
      "82\n",
      "84\n",
      "86\n",
      "88\n",
      "90\n",
      "92\n",
      "94\n",
      "96\n",
      "98\n",
      "100\n",
      "102\n",
      "104\n",
      "106\n",
      "108\n",
      "110\n",
      "112\n",
      "114\n",
      "116\n",
      "118\n",
      "120\n",
      "122\n",
      "124\n",
      "126\n"
     ]
    }
   ],
   "source": [
    "importance_thresholds = [torch.zeros([n_experts,]) for _ in range(n_layers)]\n",
    "up_proj_states_thresholds_2 = [[torch.zeros(model.config.intermediate_size) for _ in range(n_experts)] for _ in range(n_layers)]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step in range(n_batch // accum_steps):\n",
    "        print(step * accum_steps)\n",
    "        all_counts = [0 for _ in range(n_layers * n_experts)]\n",
    "        for batch_idx in range(accum_steps):\n",
    "            inputs, labels = get_batch(datasets['validation'], batch_size, block_size)\n",
    "            inputs = inputs.cuda()\n",
    "            outputs = model(inputs, labels=inputs)\n",
    "            avg_loss = avg_loss + outputs.loss / n_batch\n",
    "\n",
    "            for layer_idx in range(n_layers):\n",
    "                for expert_idx in range(n_experts):\n",
    "                    counts = all_counts[layer_idx * n_experts + expert_idx]\n",
    "                    states = model.model.layers[layer_idx].block_sparse_moe.experts[expert_idx].up_proj_states.reshape(-1, states.size(-1))\n",
    "                    cur_counts = states.size(0)\n",
    "                    up_states[layer_idx][expert_idx][counts:cur_counts+counts, :] = states\n",
    "                    # counts += cur_counts\n",
    "                    all_counts[layer_idx * n_experts + expert_idx] += cur_counts\n",
    "                \n",
    "        for layer_idx in range(n_layers):   \n",
    "            for expert_idx in range(n_experts):\n",
    "                useful_num = all_counts[layer_idx * n_experts + expert_idx]\n",
    "                importance_scores = up_states[layer_idx][expert_idx][:useful_num,:] ** 2 * gate_proj_states_mean_squares[layer_idx][expert_idx]\n",
    "                importance_thresholds[layer_idx][expert_idx] += importance_scores.to(device_2).flatten().kthvalue(int(importance_scores.numel() * sparsity_level)).values.to('cpu')\n",
    "\n",
    "for layer_idx in range(n_layers):\n",
    "    for expert_idx in range(n_experts):\n",
    "        importance_thresholds[layer_idx][expert_idx] /= n_batch // accum_steps\n",
    "        up_proj_states_thresholds_2[layer_idx][expert_idx] = (importance_thresholds[layer_idx][expert_idx].expand_as(up_proj_states_thresholds_2[layer_idx][expert_idx]) / gate_proj_states_mean_squares[layer_idx][expert_idx]) ** 0.5\n",
    "\n",
    "thresholds = {'up_proj_states_thresholds': up_proj_states_thresholds, 'up_proj_states_thresholds_2': up_proj_states_thresholds_2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save in: ./threshold/training_up\n"
     ]
    }
   ],
   "source": [
    "save_path = './threshold/training_up'\n",
    "\n",
    "sp = str(sparsity_level).replace('.', '_')\n",
    "print('save in:', save_path)\n",
    "torch.save(thresholds, f'{save_path}/thresholds_{sp}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 364608/364608 [01:30<00:00, 4040.35 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from datasets import load_dataset\n",
    "def preprocess_data(batch):\n",
    "    # 使用 tokenizer 将文本数据转换为模型输入\n",
    "    inputs = tokenizer(batch['text'], padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "    inputs[\"labels\"] = inputs.input_ids.clone()\n",
    "    return inputs\n",
    "\n",
    "# 定义一个函数来选择特征并丢弃不需要的\n",
    "def select_features(example):\n",
    "    return {\n",
    "        'input_ids': example['input_ids'],\n",
    "        'attention_mask': example['attention_mask'],\n",
    "        'labels': example['labels']\n",
    "    }\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# with open('../path.json', 'r') as file:\n",
    "#     paths = json.load(file)\n",
    "#     c4_path = paths.get('c4', '')\n",
    "# c4 = load_dataset(c4_path)\n",
    "# # 对数据集进行预处理\n",
    "c4_dataset = c4.map(preprocess_data, batched=True)\n",
    "# c4_dataset = c4_dataset.map(select_features, batched=True)\n",
    "c4_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "# # c4_dataset\n",
    "top_four_thousand_data = c4_dataset['validation'].select(range(4000))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "set_seed(42)\n",
    "\n",
    "# 定义数据加载器\n",
    "batch_size = 8\n",
    "# dataloader = DataLoader(c4_dataset['validation'], batch_size=batch_size)\n",
    "dataloader = DataLoader(top_four_thousand_data, batch_size=batch_size)\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 100/500 [02:08<07:53,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100], Eval Loss: 2.7280060803890227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 200/500 [04:05<05:53,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200], Eval Loss: 2.7090926414728163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 300/500 [05:57<03:55,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300], Eval Loss: 2.7416156327724455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 400/500 [07:54<01:52,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[400], Eval Loss: 2.7384608909487724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [09:52<00:00,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500], Eval Loss: 2.7316660568714144\n",
      "Eval Loss: 2.7316660568714144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# 计算评估损失\n",
    "total_loss = 0.0\n",
    "num_batches = 0\n",
    "\n",
    "for batch in tqdm(dataloader):\n",
    "    input_ids = batch['input_ids'].to(llm.device)\n",
    "    attention_mask = batch['attention_mask'].to(llm.device)\n",
    "    labels = batch['labels'].to(llm.device)\n",
    "    \n",
    "    # 禁用梯度计算\n",
    "    with torch.no_grad():\n",
    "        outputs = llm(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        if num_batches % 100 == 0:\n",
    "            print(f\"[{num_batches}], Eval Loss: {total_loss / (num_batches)}\")\n",
    "\n",
    "# 计算平均损失\n",
    "eval_loss = total_loss / num_batches\n",
    "print(f\"Eval Loss: {eval_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.model.layers[0].block_sparse_moe.experts[0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0 expert 0 ratio: 0.2167\n",
      "layer 0 expert 1 ratio: 0.1075\n",
      "layer 0 expert 2 ratio: 0.0871\n",
      "layer 0 expert 3 ratio: 0.2104\n",
      "layer 0 expert 4 ratio: 0.2017\n",
      "layer 0 expert 5 ratio: 0.2117\n",
      "layer 0 expert 6 ratio: 0.2164\n",
      "layer 0 expert 7 ratio: 0.2175\n",
      "layer 1 expert 0 ratio: 0.2209\n",
      "layer 1 expert 1 ratio: 0.2114\n",
      "layer 1 expert 2 ratio: 0.3353\n",
      "layer 1 expert 3 ratio: 0.2220\n",
      "layer 1 expert 4 ratio: 0.2407\n",
      "layer 1 expert 5 ratio: 0.3093\n",
      "layer 1 expert 6 ratio: 0.2343\n",
      "layer 1 expert 7 ratio: 0.2426\n",
      "layer 2 expert 0 ratio: 0.2640\n",
      "layer 2 expert 1 ratio: 0.2186\n",
      "layer 2 expert 2 ratio: 0.3159\n",
      "layer 2 expert 3 ratio: 0.3497\n",
      "layer 2 expert 4 ratio: 0.2149\n",
      "layer 2 expert 5 ratio: 0.2662\n",
      "layer 2 expert 6 ratio: 0.2368\n",
      "layer 2 expert 7 ratio: 0.2575\n",
      "layer 3 expert 0 ratio: 0.3555\n",
      "layer 3 expert 1 ratio: 0.2156\n",
      "layer 3 expert 2 ratio: 0.2297\n",
      "layer 3 expert 3 ratio: 0.2700\n",
      "layer 3 expert 4 ratio: 0.2890\n",
      "layer 3 expert 5 ratio: 0.2718\n",
      "layer 3 expert 6 ratio: 0.2421\n",
      "layer 3 expert 7 ratio: 0.3535\n",
      "layer 4 expert 0 ratio: 0.3492\n",
      "layer 4 expert 1 ratio: 0.2416\n",
      "layer 4 expert 2 ratio: 0.2716\n",
      "layer 4 expert 3 ratio: 0.3119\n",
      "layer 4 expert 4 ratio: 0.2632\n",
      "layer 4 expert 5 ratio: 0.3584\n",
      "layer 4 expert 6 ratio: 0.2183\n",
      "layer 4 expert 7 ratio: 0.3338\n",
      "layer 5 expert 0 ratio: 0.2305\n",
      "layer 5 expert 1 ratio: 0.2402\n",
      "layer 5 expert 2 ratio: 0.2377\n",
      "layer 5 expert 3 ratio: 0.2541\n",
      "layer 5 expert 4 ratio: 0.2931\n",
      "layer 5 expert 5 ratio: 0.2606\n",
      "layer 5 expert 6 ratio: 0.2186\n",
      "layer 5 expert 7 ratio: 0.2987\n",
      "layer 6 expert 0 ratio: 0.2579\n",
      "layer 6 expert 1 ratio: 0.2437\n",
      "layer 6 expert 2 ratio: 0.2908\n",
      "layer 6 expert 3 ratio: 0.2551\n",
      "layer 6 expert 4 ratio: 0.2546\n",
      "layer 6 expert 5 ratio: 0.2885\n",
      "layer 6 expert 6 ratio: 0.2213\n",
      "layer 6 expert 7 ratio: 0.2433\n",
      "layer 7 expert 0 ratio: 0.2440\n",
      "layer 7 expert 1 ratio: 0.2280\n",
      "layer 7 expert 2 ratio: 0.2376\n",
      "layer 7 expert 3 ratio: 0.2595\n",
      "layer 7 expert 4 ratio: 0.2468\n",
      "layer 7 expert 5 ratio: 0.2363\n",
      "layer 7 expert 6 ratio: 0.2458\n",
      "layer 7 expert 7 ratio: 0.2173\n",
      "layer 8 expert 0 ratio: 0.2298\n",
      "layer 8 expert 1 ratio: 0.2468\n",
      "layer 8 expert 2 ratio: 0.2369\n",
      "layer 8 expert 3 ratio: 0.2416\n",
      "layer 8 expert 4 ratio: 0.2393\n",
      "layer 8 expert 5 ratio: 0.2341\n",
      "layer 8 expert 6 ratio: 0.2378\n",
      "layer 8 expert 7 ratio: 0.2419\n",
      "layer 9 expert 0 ratio: 0.2343\n",
      "layer 9 expert 1 ratio: 0.2381\n",
      "layer 9 expert 2 ratio: 0.2315\n",
      "layer 9 expert 3 ratio: 0.2411\n",
      "layer 9 expert 4 ratio: 0.2284\n",
      "layer 9 expert 5 ratio: 0.2391\n",
      "layer 9 expert 6 ratio: 0.2319\n",
      "layer 9 expert 7 ratio: 0.2369\n",
      "layer 10 expert 0 ratio: 0.2271\n",
      "layer 10 expert 1 ratio: 0.2263\n",
      "layer 10 expert 2 ratio: 0.2302\n",
      "layer 10 expert 3 ratio: 0.2309\n",
      "layer 10 expert 4 ratio: 0.2269\n",
      "layer 10 expert 5 ratio: 0.2212\n",
      "layer 10 expert 6 ratio: 0.2255\n",
      "layer 10 expert 7 ratio: 0.2308\n",
      "layer 11 expert 0 ratio: 0.2250\n",
      "layer 11 expert 1 ratio: 0.2425\n",
      "layer 11 expert 2 ratio: 0.2303\n",
      "layer 11 expert 3 ratio: 0.2016\n",
      "layer 11 expert 4 ratio: 0.2332\n",
      "layer 11 expert 5 ratio: 0.2239\n",
      "layer 11 expert 6 ratio: 0.2303\n",
      "layer 11 expert 7 ratio: 0.2280\n",
      "layer 12 expert 0 ratio: 0.2178\n",
      "layer 12 expert 1 ratio: 0.2241\n",
      "layer 12 expert 2 ratio: 0.2268\n",
      "layer 12 expert 3 ratio: 0.2299\n",
      "layer 12 expert 4 ratio: 0.2261\n",
      "layer 12 expert 5 ratio: 0.2139\n",
      "layer 12 expert 6 ratio: 0.2301\n",
      "layer 12 expert 7 ratio: 0.2126\n",
      "layer 13 expert 0 ratio: 0.2233\n",
      "layer 13 expert 1 ratio: 0.2344\n",
      "layer 13 expert 2 ratio: 0.2324\n",
      "layer 13 expert 3 ratio: 0.2167\n",
      "layer 13 expert 4 ratio: 0.2211\n",
      "layer 13 expert 5 ratio: 0.2216\n",
      "layer 13 expert 6 ratio: 0.2221\n",
      "layer 13 expert 7 ratio: 0.2242\n",
      "layer 14 expert 0 ratio: 0.2204\n",
      "layer 14 expert 1 ratio: 0.2333\n",
      "layer 14 expert 2 ratio: 0.2187\n",
      "layer 14 expert 3 ratio: 0.2223\n",
      "layer 14 expert 4 ratio: 0.2266\n",
      "layer 14 expert 5 ratio: 0.2292\n",
      "layer 14 expert 6 ratio: 0.2242\n",
      "layer 14 expert 7 ratio: 0.2272\n",
      "layer 15 expert 0 ratio: 0.2323\n",
      "layer 15 expert 1 ratio: 0.2298\n",
      "layer 15 expert 2 ratio: 0.2195\n",
      "layer 15 expert 3 ratio: 0.2271\n",
      "layer 15 expert 4 ratio: 0.2193\n",
      "layer 15 expert 5 ratio: 0.2357\n",
      "layer 15 expert 6 ratio: 0.2274\n",
      "layer 15 expert 7 ratio: 0.2184\n",
      "layer 16 expert 0 ratio: 0.2153\n",
      "layer 16 expert 1 ratio: 0.2198\n",
      "layer 16 expert 2 ratio: 0.2185\n",
      "layer 16 expert 3 ratio: 0.2194\n",
      "layer 16 expert 4 ratio: 0.2257\n",
      "layer 16 expert 5 ratio: 0.2283\n",
      "layer 16 expert 6 ratio: 0.2221\n",
      "layer 16 expert 7 ratio: 0.2146\n",
      "layer 17 expert 0 ratio: 0.1951\n",
      "layer 17 expert 1 ratio: 0.2199\n",
      "layer 17 expert 2 ratio: 0.2179\n",
      "layer 17 expert 3 ratio: 0.2173\n",
      "layer 17 expert 4 ratio: 0.2267\n",
      "layer 17 expert 5 ratio: 0.2109\n",
      "layer 17 expert 6 ratio: 0.2225\n",
      "layer 17 expert 7 ratio: 0.2211\n",
      "layer 18 expert 0 ratio: 0.2289\n",
      "layer 18 expert 1 ratio: 0.2247\n",
      "layer 18 expert 2 ratio: 0.2228\n",
      "layer 18 expert 3 ratio: 0.2197\n",
      "layer 18 expert 4 ratio: 0.2201\n",
      "layer 18 expert 5 ratio: 0.2226\n",
      "layer 18 expert 6 ratio: 0.2081\n",
      "layer 18 expert 7 ratio: 0.2117\n",
      "layer 19 expert 0 ratio: 0.2000\n",
      "layer 19 expert 1 ratio: 0.2034\n",
      "layer 19 expert 2 ratio: 0.1949\n",
      "layer 19 expert 3 ratio: 0.1997\n",
      "layer 19 expert 4 ratio: 0.2208\n",
      "layer 19 expert 5 ratio: 0.2027\n",
      "layer 19 expert 6 ratio: 0.2111\n",
      "layer 19 expert 7 ratio: 0.2047\n",
      "layer 20 expert 0 ratio: 0.2178\n",
      "layer 20 expert 1 ratio: 0.1900\n",
      "layer 20 expert 2 ratio: 0.2077\n",
      "layer 20 expert 3 ratio: 0.1725\n",
      "layer 20 expert 4 ratio: 0.2186\n",
      "layer 20 expert 5 ratio: 0.2004\n",
      "layer 20 expert 6 ratio: 0.1913\n",
      "layer 20 expert 7 ratio: 0.2018\n",
      "layer 21 expert 0 ratio: 0.1862\n",
      "layer 21 expert 1 ratio: 0.1916\n",
      "layer 21 expert 2 ratio: 0.2224\n",
      "layer 21 expert 3 ratio: 0.2075\n",
      "layer 21 expert 4 ratio: 0.1779\n",
      "layer 21 expert 5 ratio: 0.1913\n",
      "layer 21 expert 6 ratio: 0.2235\n",
      "layer 21 expert 7 ratio: 0.1701\n",
      "layer 22 expert 0 ratio: 0.2049\n",
      "layer 22 expert 1 ratio: 0.2042\n",
      "layer 22 expert 2 ratio: 0.1815\n",
      "layer 22 expert 3 ratio: 0.2068\n",
      "layer 22 expert 4 ratio: 0.1681\n",
      "layer 22 expert 5 ratio: 0.2089\n",
      "layer 22 expert 6 ratio: 0.1860\n",
      "layer 22 expert 7 ratio: 0.2005\n",
      "layer 23 expert 0 ratio: 0.1930\n",
      "layer 23 expert 1 ratio: 0.1929\n",
      "layer 23 expert 2 ratio: 0.1583\n",
      "layer 23 expert 3 ratio: 0.1872\n",
      "layer 23 expert 4 ratio: 0.2176\n",
      "layer 23 expert 5 ratio: 0.1622\n",
      "layer 23 expert 6 ratio: 0.1987\n",
      "layer 23 expert 7 ratio: 0.1999\n",
      "layer 24 expert 0 ratio: 0.1848\n",
      "layer 24 expert 1 ratio: 0.1713\n",
      "layer 24 expert 2 ratio: 0.1695\n",
      "layer 24 expert 3 ratio: 0.1727\n",
      "layer 24 expert 4 ratio: 0.2012\n",
      "layer 24 expert 5 ratio: 0.1837\n",
      "layer 24 expert 6 ratio: 0.2015\n",
      "layer 24 expert 7 ratio: 0.1827\n",
      "layer 25 expert 0 ratio: 0.1668\n",
      "layer 25 expert 1 ratio: 0.1966\n",
      "layer 25 expert 2 ratio: 0.2055\n",
      "layer 25 expert 3 ratio: 0.2026\n",
      "layer 25 expert 4 ratio: 0.1772\n",
      "layer 25 expert 5 ratio: 0.2016\n",
      "layer 25 expert 6 ratio: 0.1646\n",
      "layer 25 expert 7 ratio: 0.1826\n",
      "layer 26 expert 0 ratio: 0.1745\n",
      "layer 26 expert 1 ratio: 0.2131\n",
      "layer 26 expert 2 ratio: 0.1526\n",
      "layer 26 expert 3 ratio: 0.2053\n",
      "layer 26 expert 4 ratio: 0.1692\n",
      "layer 26 expert 5 ratio: 0.1764\n",
      "layer 26 expert 6 ratio: 0.1643\n",
      "layer 26 expert 7 ratio: 0.2197\n",
      "layer 27 expert 0 ratio: 0.1578\n",
      "layer 27 expert 1 ratio: 0.1898\n",
      "layer 27 expert 2 ratio: 0.1826\n",
      "layer 27 expert 3 ratio: 0.1639\n",
      "layer 27 expert 4 ratio: 0.1610\n",
      "layer 27 expert 5 ratio: 0.1995\n",
      "layer 27 expert 6 ratio: 0.2002\n",
      "layer 27 expert 7 ratio: 0.2148\n",
      "layer 28 expert 0 ratio: 0.1564\n",
      "layer 28 expert 1 ratio: 0.1879\n",
      "layer 28 expert 2 ratio: 0.1810\n",
      "layer 28 expert 3 ratio: 0.1909\n",
      "layer 28 expert 4 ratio: 0.2137\n",
      "layer 28 expert 5 ratio: 0.1635\n",
      "layer 28 expert 6 ratio: 0.2119\n",
      "layer 28 expert 7 ratio: 0.1742\n",
      "layer 29 expert 0 ratio: 0.1979\n",
      "layer 29 expert 1 ratio: 0.1816\n",
      "layer 29 expert 2 ratio: 0.1984\n",
      "layer 29 expert 3 ratio: 0.2036\n",
      "layer 29 expert 4 ratio: 0.1918\n",
      "layer 29 expert 5 ratio: 0.1726\n",
      "layer 29 expert 6 ratio: 0.1919\n",
      "layer 29 expert 7 ratio: 0.1467\n",
      "layer 30 expert 0 ratio: 0.2143\n",
      "layer 30 expert 1 ratio: 0.2203\n",
      "layer 30 expert 2 ratio: 0.2172\n",
      "layer 30 expert 3 ratio: 0.2179\n",
      "layer 30 expert 4 ratio: 0.2201\n",
      "layer 30 expert 5 ratio: 0.1934\n",
      "layer 30 expert 6 ratio: 0.1919\n",
      "layer 30 expert 7 ratio: 0.1765\n",
      "layer 31 expert 0 ratio: 0.2272\n",
      "layer 31 expert 1 ratio: 0.2044\n",
      "layer 31 expert 2 ratio: 0.2166\n",
      "layer 31 expert 3 ratio: 0.2298\n",
      "layer 31 expert 4 ratio: 0.2461\n",
      "layer 31 expert 5 ratio: 0.2361\n",
      "layer 31 expert 6 ratio: 0.2506\n",
      "layer 31 expert 7 ratio: 0.2367\n"
     ]
    }
   ],
   "source": [
    "for layerid in range(32):\n",
    "    for expertid in range(8):\n",
    "        llm.model.layers[layerid].block_sparse_moe.experts[expertid].print_ratio()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_ENDPOINT\"]=\"https://hf-mirror.com\"\n",
    "\n",
    "\n",
    "import lm_eval\n",
    "from lm_eval.models.huggingface import HFLM\n",
    "from lm_eval import evaluator\n",
    "del dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-03:13:11:00,185 WARNING  [huggingface.py:121] `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "2025-01-03:13:11:00,251 WARNING  [huggingface.py:349] Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "2025-01-03:13:11:00,259 INFO     [evaluator.py:152] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2025-01-03:13:11:00,261 INFO     [evaluator.py:203] Using pre-initialized model\n",
      "Using the latest cached version of the module from /home/lz/.cache/huggingface/modules/datasets_modules/datasets/winogrande/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2 (last modified on Thu Jan  2 22:35:53 2025) since it couldn't be found locally at winogrande, or remotely on the Hugging Face Hub.\n",
      "2025-01-03:13:13:13,531 WARNING  [load.py:1407] Using the latest cached version of the module from /home/lz/.cache/huggingface/modules/datasets_modules/datasets/winogrande/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2 (last modified on Thu Jan  2 22:35:53 2025) since it couldn't be found locally at winogrande, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "def evaluate(task_name_list, model, tokenizer, num_fewshot, device):\n",
    "    hflm = HFLM(pretrained=llm, tokenizer=tokenizer)\n",
    "    results = evaluator.simple_evaluate(\n",
    "    model=hflm,\n",
    "    tasks=task_name_list,\n",
    "    num_fewshot=num_fewshot)\n",
    "    print(results['results'])\n",
    "\n",
    "\n",
    "\n",
    "# triviaqa\n",
    "task_list=['winogrande','sciq','openbookqa','arc_challenge','arc_easy']\n",
    "# 'boolq',\n",
    "# task_list=['truthfulqa_gen','triviaqa_gen']\n",
    "evaluate(task_list, llm, tokenizer, 0, \"cuda\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
