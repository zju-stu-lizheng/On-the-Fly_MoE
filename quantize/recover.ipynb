{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set profile_threshold to False\n",
      "Thresholds loaded from /home/lz/On-the-Fly_MoE_Inference/quantize/threshold/c4_mixtral_up/thresholds_0_8.pt\n",
      "using  torch.bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:33<00:00,  1.77s/it]\n",
      "100%|██████████| 354/354 [00:00<00:00, 11383.67it/s]\n",
      "100%|██████████| 929/929 [00:15<00:00, 61.27it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6\"\n",
    "import transformers\n",
    "from modeling_mixtral import set_profile_mode, load_thresholds\n",
    "import json\n",
    "from utils import get_model, CompensatedModel\n",
    "from hqq.core.quantize import *\n",
    "from hqq.models.hf.base import AutoHQQHFModel\n",
    "from hqq.core.peft import PeftUtils\n",
    "from datasets import load_dataset, Dataset\n",
    "import functools\n",
    "\n",
    "\n",
    "# # 加载 C4 数据集的验证集\n",
    "with open('../path.json', 'r') as file:\n",
    "    paths = json.load(file)\n",
    "    fineweb_path = paths.get('fineweb', '')\n",
    "    model_name = paths.get('mixtral','')\n",
    "    threshold_path = paths.get('chess_up_sparsity_threshold','')\n",
    "\n",
    "with open('./device_map.json', 'r') as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "set_profile_mode(False)\n",
    "load_thresholds(f'{threshold_path}/thresholds_0_8.pt')\n",
    "dtype = torch.bfloat16\n",
    "print('using ',dtype)\n",
    "llm, tokenizer = get_model(model_name, device_map, dtype=dtype)\n",
    "\n",
    "q4_config    = BaseQuantizeConfig(nbits=8, group_size=64) \n",
    "q3_config    = BaseQuantizeConfig(nbits=2, group_size=64)\n",
    "\n",
    "quant_config      = {'block_sparse_moe.experts.w3'   : q3_config}\n",
    "AutoHQQHFModel.quantize_model(llm, quant_config=quant_config, compute_dtype=dtype, device=device_map)\n",
    "\n",
    "base_lora_params = {'lora_type':'default', 'r':128, 'lora_alpha':128, 'dropout':0.05, 'train_dtype':dtype}\n",
    "\n",
    "lora_params      = {'self_attn.q_proj': base_lora_params,\n",
    "                    'self_attn.k_proj': base_lora_params,\n",
    "                    'self_attn.v_proj': base_lora_params,\n",
    "                    'self_attn.o_proj': base_lora_params,\n",
    "                    'block_sparse_moe.experts.w1'   : base_lora_params,\n",
    "                    'block_sparse_moe.experts.w3'   : base_lora_params,\n",
    "                    'block_sparse_moe.experts.w2'   : base_lora_params}\n",
    "\n",
    "\n",
    "PeftUtils.add_lora(llm, lora_params)\n",
    "class CompensatedModel(torch.nn.Module):\n",
    "    def __init__(self, model, path, layerid, expertid):\n",
    "        super(CompensatedModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.A = torch.load(path + f'A_{layerid}_{expertid}.pt').to(dtype)\n",
    "        self.B_prime = torch.load(path + f'B_prime_{layerid}_{expertid}.pt').to(dtype)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        outputs = self.model(input_ids)\n",
    "        residual = (input_ids @ self.A.T) @ self.B_prime.T\n",
    "        outputs += residual\n",
    "    \n",
    "        return outputs\n",
    "for i in range(32):\n",
    "    if i == 31:\n",
    "        print(f\"Layer {i} done...\")\n",
    "    for j in range(8):\n",
    "        llmdevice = llm.model.layers[i].block_sparse_moe.experts[j].w3.linear_layer.device\n",
    "        llm.model.layers[i].block_sparse_moe.experts[j].w3.linear_layer = \\\n",
    "        CompensatedModel(llm.model.layers[i].block_sparse_moe.experts[j].w3.linear_layer, '/home/lz/On-the-Fly_MoE_Inference/quantize/saved/eora/', layerid=i, expertid=j).to(llmdevice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HQQLinear(in_features=4096, out_features=14336, bias=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.model.layers[0].block_sparse_moe.experts[0].w3.linear_layer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "2025-01-06:21:13:38,297 WARNING  [repocard.py:108] Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "import functools\n",
    "\n",
    "def preprocess_data(batch, tokenizer):\n",
    "    # 使用 tokenizer 将文本数据转换为模型输入\n",
    "    inputs = tokenizer(batch['text'], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    inputs[\"labels\"] = inputs.input_ids.clone()\n",
    "    return inputs\n",
    "\n",
    "\n",
    "openmath = load_dataset(\"/home/lz/web-math/\",data_files=\"/home/lz/web-math/openmath1.json\")\n",
    "fineweb = load_dataset(fineweb_path)\n",
    "openmath_text = openmath['train']['text'][:2000] \n",
    "fineweb_text = fineweb['train']['text'][:6000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7200/7200 [00:05<00:00, 1381.09 examples/s]\n",
      "Map: 100%|██████████| 800/800 [00:00<00:00, 1357.93 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 800\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_num = 0.1\n",
    "seed = 42\n",
    "\n",
    "combined_text = openmath_text + fineweb_text\n",
    "combined_dataset = Dataset.from_dict({\"text\": combined_text})\n",
    "combined_train = combined_dataset.train_test_split(test_size=test_num, seed=seed)\n",
    "train_data = combined_train['train']\n",
    "test_data = combined_train['test']\n",
    "\n",
    "new_train_data = train_data.map(\n",
    "    functools.partial(\n",
    "    preprocess_data,\n",
    "    tokenizer=tokenizer\n",
    "), batched=True)\n",
    "new_test_data = test_data.map(\n",
    "    functools.partial(\n",
    "    preprocess_data,\n",
    "    tokenizer=tokenizer\n",
    "), batched=True)\n",
    "new_train_data.shuffle(seed)\n",
    "new_test_data.shuffle(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lz/miniconda3/envs/hqq/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "2025-01-06:21:13:46,796 WARNING  [other.py:331] Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 37:42, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.770800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.666900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.719000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.644700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.674300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.682200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.662200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.397200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.062400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.145700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.120700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.102900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.123600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.122800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.122300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 929/929 [00:00<00:00, 214813.85it/s]\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "100%|██████████| 929/929 [00:00<00:00, 144582.87it/s]\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "100%|██████████| 929/929 [00:00<00:00, 195000.92it/s]\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "100%|██████████| 929/929 [00:00<00:00, 205241.42it/s]\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "100%|██████████| 929/929 [00:00<00:00, 145207.89it/s]\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "100%|██████████| 929/929 [00:00<00:00, 208804.91it/s]\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "100%|██████████| 929/929 [00:00<00:00, 211766.76it/s]\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "100%|██████████| 929/929 [00:00<00:00, 137749.09it/s]\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=750, training_loss=1.401180679321289, metrics={'train_runtime': 2267.8098, 'train_samples_per_second': 2.646, 'train_steps_per_second': 0.331, 'total_flos': 6.74877413523456e+17, 'train_loss': 1.401180679321289, 'epoch': 2.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hqq.core.peft import PeftUtils\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, AdamW\n",
    "from transformers import (\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "class CustomTrainer(transformers.Trainer):\n",
    "    def save_model(self, output_dir=None, _internal_call=False):\n",
    "        # 如果没有指定output_dir，则使用训练参数中的输出目录\n",
    "        if output_dir is None:\n",
    "            output_dir = self.args.output_dir #这里的args不是该脚本的输入，而是TrainerArgs\n",
    "\n",
    "        # 确保输出目录存在\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # 保存完整的模型参数\n",
    "        # torch.save(self.model.state_dict(), os.path.join(output_dir, 'pytorch_model.bin'))\n",
    "        \n",
    "        # PeftUtils.cast_lora_weights(self.model, dtype=torch.bfloat16)\n",
    "\n",
    "        #Save LoRA weights\n",
    "        PeftUtils.save_lora_weights(self.model, output_dir+'_lora_combine.pt')\n",
    "\n",
    "        # 保存配置文件和tokenizer\n",
    "        self.model.config.save_pretrained(output_dir)\n",
    "        if self.tokenizer is not None:\n",
    "            self.tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "model_save_path='./saved/training/less2'\n",
    "learning_rate = 1e-4\n",
    "micro_batch_size=8\n",
    "epochs=2\n",
    "save_steps = 100\n",
    "save_total_limit = 6\n",
    "sample_num = len(new_train_data)\n",
    "optimizer=AdamW(filter(lambda p : p.requires_grad, llm.parameters()),lr=learning_rate)\n",
    "linear_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(sample_num*epochs) // micro_batch_size)\n",
    "args = TrainingArguments(\n",
    "    output_dir=model_save_path,\n",
    "    num_train_epochs=epochs,\n",
    "    # max_steps=opt.max_steps,\n",
    "    # fp16=True,\n",
    "    bf16=True,\n",
    "    optim=\"adamw_torch\",# paged_adamw_8bit\n",
    "    learning_rate=learning_rate,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    per_device_train_batch_size=micro_batch_size,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=False,   ### 先设置成False\n",
    "    group_by_length=False,\n",
    "    logging_steps=50,\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_only_model=True,\n",
    "    save_steps=save_steps,\n",
    "    save_total_limit=save_total_limit,\n",
    "    disable_tqdm=False,\n",
    "    report_to='tensorboard',\n",
    "    logging_dir='/home/lz/On-the-Fly_MoE_Inference/quantize/saved/logs/'\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=llm,\n",
    "    train_dataset=new_train_data.select(range(3000)),\n",
    "    eval_dataset=new_test_data.select(range(300)),\n",
    "    args=args,\n",
    "    optimizers=(optimizer, linear_scheduler),\n",
    "    data_collator=DataCollatorForSeq2Seq(\n",
    "    tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True),\n",
    ")\n",
    "\n",
    "# silence the warnings. re-enable for inference!\n",
    "llm.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lz/miniconda3/envs/hqq/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set profile_threshold to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:35<00:00,  1.85s/it]\n",
      "100%|██████████| 354/354 [00:00<00:00, 10211.37it/s]\n",
      "100%|██████████| 929/929 [00:15<00:00, 60.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MixtralForCausalLM(\n",
       "  (model): MixtralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MixtralDecoderLayer(\n",
       "        (self_attn): MixtralSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MixtralRotaryEmbedding()\n",
       "        )\n",
       "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
       "          (gate): Linear(in_features=4096, out_features=8, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-7): 8 x MixtralBlockSparseTop2MLP(\n",
       "              (w1): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (w2): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "              (w3): HQQLinear(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): MixtralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MixtralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MixtralRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6\"\n",
    "from modeling_mixtral import MixtralForCausalLM, set_profile_mode\n",
    "import json\n",
    "from utils import get_model\n",
    "\n",
    "# # 加载 C4 数据集的验证集\n",
    "with open('../path.json', 'r') as file:\n",
    "    paths = json.load(file)\n",
    "    fineweb_path = paths.get('c4', '')\n",
    "    model_name = paths.get('mixtral','')\n",
    "\n",
    "with open('./device_map.json', 'r') as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "set_profile_mode(True)\n",
    "llm, tokenizer = get_model(model_name, device_map)\n",
    "# %%\n",
    "#Quantize\n",
    "from hqq.core.quantize import *\n",
    "q4_config    = BaseQuantizeConfig(nbits=8, group_size=64) \n",
    "q3_config    = BaseQuantizeConfig(nbits=2, group_size=64)\n",
    "\n",
    "quant_config = {\n",
    "  'block_sparse_moe.experts.w3'  :q3_config,\n",
    "}\n",
    "from hqq.models.hf.base import AutoHQQHFModel\n",
    "AutoHQQHFModel.quantize_model(llm, quant_config=quant_config, compute_dtype=torch.float16, device=device_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/929 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 929/929 [00:09<00:00, 97.35it/s] \n",
      "100%|██████████| 929/929 [00:00<00:00, 159327.30it/s]\n"
     ]
    }
   ],
   "source": [
    "from hqq.core.peft import PeftUtils\n",
    "PeftUtils.load_lora_weights(llm, '/home/lz/On-the-Fly_MoE_Inference/quantize/saved/training/less2/checkpoint-750_lora_combine.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/45576 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 45576/45576 [00:13<00:00, 3333.84 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from datasets import load_dataset\n",
    "def preprocess_data(batch):\n",
    "    # 使用 tokenizer 将文本数据转换为模型输入\n",
    "    inputs = tokenizer(batch['text'], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    inputs[\"labels\"] = inputs.input_ids.clone()\n",
    "    return inputs\n",
    "\n",
    "# 定义一个函数来选择特征并丢弃不需要的\n",
    "def select_features(example):\n",
    "    return {\n",
    "        'input_ids': example['input_ids'],\n",
    "        'attention_mask': example['attention_mask'],\n",
    "        'labels': example['labels']\n",
    "    }\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "with open('../path.json', 'r') as file:\n",
    "    paths = json.load(file)\n",
    "    c4_path = paths.get('c4', '')\n",
    "c4 = load_dataset(c4_path)\n",
    "# 对数据集进行预处理\n",
    "c4_dataset = c4.map(preprocess_data, batched=True)\n",
    "# c4_dataset = c4_dataset.map(select_features, batched=True)\n",
    "c4_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "# c4_dataset\n",
    "top_four_thousand_data = c4_dataset['validation'].select(range(400))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "set_seed(42)\n",
    "\n",
    "# 定义数据加载器\n",
    "batch_size = 8\n",
    "# dataloader = DataLoader(c4_dataset['validation'], batch_size=batch_size)\n",
    "dataloader = DataLoader(top_four_thousand_data, batch_size=batch_size)\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "llm_base = MixtralForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map='cpu',\n",
    "    use_cache=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    # attn_implementation=\"flash_attention_2\"\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eora恢复"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直接从文件中读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 done...\n",
      "Layer 1 done...\n",
      "Layer 2 done...\n",
      "Layer 3 done...\n",
      "Layer 4 done...\n",
      "Layer 5 done...\n",
      "Layer 6 done...\n",
      "Layer 7 done...\n",
      "Layer 8 done...\n",
      "Layer 9 done...\n",
      "Layer 10 done...\n",
      "Layer 11 done...\n",
      "Layer 12 done...\n",
      "Layer 13 done...\n",
      "Layer 14 done...\n",
      "Layer 15 done...\n",
      "Layer 16 done...\n",
      "Layer 17 done...\n",
      "Layer 18 done...\n",
      "Layer 19 done...\n",
      "Layer 20 done...\n",
      "Layer 21 done...\n",
      "Layer 22 done...\n",
      "Layer 23 done...\n",
      "Layer 24 done...\n",
      "Layer 25 done...\n",
      "Layer 26 done...\n",
      "Layer 27 done...\n",
      "Layer 28 done...\n",
      "Layer 29 done...\n",
      "Layer 30 done...\n",
      "Layer 31 done...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class CompensatedModel(torch.nn.Module):\n",
    "    def __init__(self, model, path, layerid, expertid):\n",
    "        super(CompensatedModel, self).__init__()\n",
    "        self.model = model\n",
    "        ### self.A and self.B_prime are initialized as the values loaded from the file\n",
    "        self.A = torch.load(path + f'A_{layerid}_{expertid}.pt').to(torch.float16)\n",
    "        self.B_prime = torch.load(path + f'B_prime_{layerid}_{expertid}.pt').to(torch.float16)\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        outputs = self.model(input_ids)\n",
    "        residual = (input_ids @ self.A.T) @ self.B_prime.T\n",
    "        outputs += residual\n",
    "    \n",
    "        return outputs\n",
    "\n",
    "for i in range(32):\n",
    "    print(f\"Layer {i} done...\")\n",
    "    for j in range(8):\n",
    "        llmdevice = llm.model.layers[i].block_sparse_moe.experts[j].w3.device\n",
    "        llm.model.layers[i].block_sparse_moe.experts[j].w3 = \\\n",
    "        CompensatedModel(llm.model.layers[i].block_sparse_moe.experts[j].w3, '/home/lz/On-the-Fly_MoE_Inference/quantize/saved/', layerid=i, expertid=j).to(llmdevice)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一次计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def profle_svdllm(name, model, calib_loader, dev):\n",
    "    # model.to(dev)\n",
    "    if \"llama\" in name or \"mixtral\" in name or \"vicuna\" in name:\n",
    "        layers = model.model.layers\n",
    "    print(\"Start obtaining the whitening matrix...\")\n",
    "    def hook(module, input, output):\n",
    "        inp = input[0].detach().float()\n",
    "        if inp.dim() == 2:   # for opt\n",
    "            inp = inp.unsqueeze(0)\n",
    "        adds = torch.matmul(inp.transpose(1,2), inp)\n",
    "        adds_sum = torch.sum(adds, dim=0)\n",
    "        module.raw_scaling_diag_matrix += adds_sum\n",
    "        del inp, adds, adds_sum\n",
    "        torch.cuda.empty_cache()\n",
    "    for name, module in model.named_modules():\n",
    "        if \"w3\" in name:\n",
    "            # print(name)\n",
    "            module.raw_scaling_diag_matrix = 0\n",
    "            module.register_forward_hook(hook)\n",
    "            \n",
    "    for batch in tqdm(calib_loader):\n",
    "        inputs = batch['input_ids'].to(llm.device)\n",
    "        model(inputs)\n",
    "    for name, module in model.named_modules():\n",
    "        if \"w3\" in name:\n",
    "            module._forward_hooks.clear()\n",
    "            # print(module.raw_scaling_diag_matrix)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    profiling_mat = {}\n",
    "    print(\"Start Cholesky Decomposition...\")\n",
    "    \n",
    "    layer_profile = {}\n",
    "    for name, module in model.named_modules():\n",
    "        if \"w3\" in name:\n",
    "            covariance = module.raw_scaling_diag_matrix.double().to(dev)\n",
    "            if not torch.allclose(covariance, covariance.t(), atol=1e-6):\n",
    "                raise ValueError(\"Covariance matrix is not symmetric.\")\n",
    "                    # Perform eigen decomposition\n",
    "            Lambda, Q = torch.linalg.eigh(covariance, UPLO='U')\n",
    "            if torch.isnan(Lambda).any() or torch.isinf(Lambda).any():\n",
    "                raise ValueError(\"Lambda contains NaN or Inf values.\")\n",
    "\n",
    "            # 检查 Lambda 是否包含负值\n",
    "            if (Lambda < 0).any():\n",
    "                print(\"Lambda contains negative values. Clamping to zero.\")\n",
    "                eigenvalues = torch.linalg.eigvalsh(covariance)\n",
    "                covariance += (- eigenvalues[0] + 2e-6) * torch.eye(covariance.shape[0]).cuda()\n",
    "                Lambda, Q = torch.linalg.eigh(covariance, UPLO='U')\n",
    "                print(f\"Lambda min: {Lambda.min().item()}, Lambda max: {Lambda.max().item()}\")\n",
    "            # 现在进行平方根操作\n",
    "            Lambda_diag = torch.diag(torch.sqrt(Lambda))\n",
    "            # Sort eigenvalues and eigenvectors in descending order\n",
    "            indices = torch.argsort(Lambda, descending=True)\n",
    "            Lambda = Lambda[indices]\n",
    "            Q = Q[:, indices]\n",
    "\n",
    "            # Compute Q_prime = Q * sqrt(Lambda)\n",
    "            Lambda_diag = torch.diag(torch.sqrt(Lambda))\n",
    "            Q_prime = torch.matmul(Q, Lambda_diag)\n",
    "            layer_profile[name] = Q_prime.cpu()\n",
    "            profiling_mat[name] = layer_profile\n",
    "    return profiling_mat\n",
    "\n",
    "profiling_mat=profle_svdllm(\"mixtral\", llm, dataloader, \"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompensatedModel(torch.nn.Module):\n",
    "    def __init__(self, model, B_prime, A):\n",
    "        super(CompensatedModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.B_prime = torch.nn.Parameter(torch.tensor(B_prime)).to(torch.float16)\n",
    "        self.A = torch.nn.Parameter(torch.tensor(A)).to(torch.float16)\n",
    "        # print(self.A.shape,self.B_prime.shape)\n",
    "    def forward(self, input_ids):\n",
    "        outputs = self.model(input_ids)\n",
    "        residual = (input_ids @ self.A.T) @ self.B_prime.T\n",
    "        torch.add(outputs, residual, out = outputs)\n",
    "    \n",
    "        return outputs\n",
    "    \n",
    "for i in range(32):\n",
    "    print(f\"Layer {i} done...\")\n",
    "    for j in range(8):\n",
    "        llmdevice = llm.model.layers[i].block_sparse_moe.experts[j].w3.device\n",
    "        Delta_W = llm_base.model.layers[i].block_sparse_moe.experts[j].w3.weight.to(llmdevice) - llm.model.layers[i].block_sparse_moe.experts[j].w3.dequantize()\n",
    "        Q_prime = profiling_mat[f\"model.layers.{i}.block_sparse_moe.experts.{j}.w3\"][f\"model.layers.{i}.block_sparse_moe.experts.{j}.w3\"].cuda().float()\n",
    "        Delta_W_prime =  Delta_W.to(torch.float32).to(llmdevice) @ Q_prime.to(torch.float32).to(llmdevice)\n",
    "        llm_base.model.layers[i].block_sparse_moe.experts[j].w3.cpu()\n",
    "        # 步骤5: 进行SVD分解并取前r个奇异值\n",
    "        rank = 256  # 设置 desired rank\n",
    "        U_prime, Sigma_prime, V_prime = torch.linalg.svd(Delta_W_prime, full_matrices=False)\n",
    "        U_prime = U_prime[:, :rank]\n",
    "        Sigma_prime = Sigma_prime[:rank]\n",
    "        V_prime = V_prime[:rank, :]\n",
    "\n",
    "        B_prime = U_prime @ torch.diag(Sigma_prime)\n",
    "        A_prime = V_prime\n",
    "\n",
    "        # 步骤6: 投影回原空间\n",
    "        A = A_prime.to(llmdevice) @ torch.linalg.inv(Q_prime).to(llmdevice)\n",
    "        llm.model.layers[i].block_sparse_moe.experts[j].w3 = CompensatedModel(llm.model.layers[i].block_sparse_moe.experts[j].w3, B_prime, A).to(llmdevice)\n",
    "        torch.save(B_prime, f\"./saved/B_prime_{i}_{j}.pt\")\n",
    "        torch.save(A, f\"./saved/A_{i}_{j}.pt\")\n",
    "\n",
    "del llm_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set profile_threshold to True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "datasets = torch.load('../saving/threshold/chess/datasets.pt')\n",
    "set_profile_mode(True)\n",
    "def get_batch(data, batch_size, block_size):\n",
    "    start_idxs = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in start_idxs])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in start_idxs])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "14\n",
      "16\n",
      "18\n",
      "20\n",
      "22\n",
      "24\n",
      "26\n",
      "28\n",
      "30\n",
      "32\n",
      "34\n",
      "36\n",
      "38\n",
      "40\n",
      "42\n",
      "44\n",
      "46\n",
      "48\n",
      "50\n",
      "52\n",
      "54\n",
      "56\n",
      "58\n",
      "60\n",
      "62\n",
      "64\n",
      "66\n",
      "68\n",
      "70\n",
      "72\n",
      "74\n",
      "76\n",
      "78\n",
      "80\n",
      "82\n",
      "84\n",
      "86\n",
      "88\n",
      "90\n",
      "92\n",
      "94\n",
      "96\n",
      "98\n",
      "100\n",
      "102\n",
      "104\n",
      "106\n",
      "108\n",
      "110\n",
      "112\n",
      "114\n",
      "116\n",
      "118\n",
      "120\n",
      "122\n",
      "124\n",
      "126\n"
     ]
    }
   ],
   "source": [
    "sparsity_level = 0.8\n",
    "# device = 'cuda:1'\n",
    "device_2 = 'cpu'\n",
    "avg_loss = 0.0\n",
    "n_batch = 64 * 2\n",
    "# accum_steps = 4 \n",
    "accum_steps = 2\n",
    "batch_size = 1\n",
    "block_size = 2048\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model = llm\n",
    "\n",
    "n_layers = len(model.model.layers)\n",
    "n_experts = len(model.model.layers[0].block_sparse_moe.experts)\n",
    "\n",
    "up_proj_states_thresholds = [torch.zeros([n_experts,]) for _ in range(n_layers)]\n",
    "gate_proj_states_mean_squares = [[torch.zeros(model.config.intermediate_size) for _ in range(n_experts)] for _ in range(n_layers)]\n",
    "\n",
    "up_states = [[torch.zeros([accum_steps * batch_size * block_size //2, model.config.intermediate_size]) for _ in range(n_experts)] for _ in range(n_layers)]\n",
    "gate_states = [[torch.zeros([accum_steps * batch_size * block_size //2, model.config.intermediate_size]) for _ in range(n_experts)] for _ in range(n_layers)]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step in range(n_batch // accum_steps):\n",
    "        print(step * accum_steps)\n",
    "        all_counts = [0 for _ in range(n_layers * n_experts)]\n",
    "        for batch_idx in range(accum_steps):\n",
    "            # print('batch_idx:', batch_idx)\n",
    "            inputs, labels = get_batch(datasets['validation'], batch_size, block_size)\n",
    "            inputs = inputs.cuda()\n",
    "            outputs = model(inputs, labels=inputs)\n",
    "            avg_loss = avg_loss + outputs.loss / n_batch\n",
    "\n",
    "            for layer_idx in range(n_layers):\n",
    "                for expert_idx in range(n_experts):\n",
    "                    counts = all_counts[layer_idx * n_experts + expert_idx]\n",
    "\n",
    "                    states = model.model.layers[layer_idx].block_sparse_moe.experts[expert_idx].up_proj_states.reshape(-1, model.config.intermediate_size)\n",
    "                    cur_counts = states.size(0)\n",
    "                    # print('counts and cur_counts:',counts, cur_counts)\n",
    "                    # print(states.size())\n",
    "                    # print(up_states[layer_idx][expert_idx][counts : counts+cur_counts, :].size())\n",
    "                    up_states[layer_idx][expert_idx][counts : counts+cur_counts, :] = states\n",
    "\n",
    "                    states = model.model.layers[layer_idx].block_sparse_moe.experts[expert_idx].gate_proj_states.reshape(-1, model.config.intermediate_size)\n",
    "                    gate_states[layer_idx][expert_idx][counts : counts+cur_counts, :] = states\n",
    "                    # counts += cur_counts\n",
    "                    all_counts[layer_idx * n_experts + expert_idx] += cur_counts\n",
    "\n",
    "        for layer_idx in range(n_layers):   \n",
    "            for expert_idx in range(n_experts):\n",
    "                # print('layer_idx:', layer_idx, 'expert_idx:', expert_idx)\n",
    "                useful_num = all_counts[layer_idx * n_experts + expert_idx]\n",
    "                topk_num = int(useful_num * model.config.intermediate_size * sparsity_level)\n",
    "                up_proj_states_thresholds[layer_idx][expert_idx] += up_states[layer_idx][expert_idx][0:useful_num,:].to(device_2).abs().flatten().kthvalue(topk_num).values.to('cpu')\n",
    "                gate_proj_states_mean_squares[layer_idx][expert_idx] += (torch.sum(gate_states[layer_idx][expert_idx][0:useful_num,:].to(dev\n",
    "                \n",
    "                \n",
    "                ice_2) ** 2, dim=0).to('cpu') / useful_num).to('cpu')\n",
    "\n",
    "for layer_idx in range(n_layers):\n",
    "    for expert_idx in range(n_experts):\n",
    "        gate_proj_states_mean_squares[layer_idx][expert_idx] /= n_batch // accum_steps\n",
    "        up_proj_states_thresholds[layer_idx][expert_idx] /= n_batch // accum_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "14\n",
      "16\n",
      "18\n",
      "20\n",
      "22\n",
      "24\n",
      "26\n",
      "28\n",
      "30\n",
      "32\n",
      "34\n",
      "36\n",
      "38\n",
      "40\n",
      "42\n",
      "44\n",
      "46\n",
      "48\n",
      "50\n",
      "52\n",
      "54\n",
      "56\n",
      "58\n",
      "60\n",
      "62\n",
      "64\n",
      "66\n",
      "68\n",
      "70\n",
      "72\n",
      "74\n",
      "76\n",
      "78\n",
      "80\n",
      "82\n",
      "84\n",
      "86\n",
      "88\n",
      "90\n",
      "92\n",
      "94\n",
      "96\n",
      "98\n",
      "100\n",
      "102\n",
      "104\n",
      "106\n",
      "108\n",
      "110\n",
      "112\n",
      "114\n",
      "116\n",
      "118\n",
      "120\n",
      "122\n",
      "124\n",
      "126\n"
     ]
    }
   ],
   "source": [
    "importance_thresholds = [torch.zeros([n_experts,]) for _ in range(n_layers)]\n",
    "up_proj_states_thresholds_2 = [[torch.zeros(model.config.intermediate_size) for _ in range(n_experts)] for _ in range(n_layers)]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step in range(n_batch // accum_steps):\n",
    "        print(step * accum_steps)\n",
    "        all_counts = [0 for _ in range(n_layers * n_experts)]\n",
    "        for batch_idx in range(accum_steps):\n",
    "            inputs, labels = get_batch(datasets['validation'], batch_size, block_size)\n",
    "            inputs = inputs.cuda()\n",
    "            outputs = model(inputs, labels=inputs)\n",
    "            avg_loss = avg_loss + outputs.loss / n_batch\n",
    "\n",
    "            for layer_idx in range(n_layers):\n",
    "                for expert_idx in range(n_experts):\n",
    "                    counts = all_counts[layer_idx * n_experts + expert_idx]\n",
    "                    states = model.model.layers[layer_idx].block_sparse_moe.experts[expert_idx].up_proj_states.reshape(-1, states.size(-1))\n",
    "                    cur_counts = states.size(0)\n",
    "                    up_states[layer_idx][expert_idx][counts:cur_counts+counts, :] = states\n",
    "                    # counts += cur_counts\n",
    "                    all_counts[layer_idx * n_experts + expert_idx] += cur_counts\n",
    "                \n",
    "        for layer_idx in range(n_layers):   \n",
    "            for expert_idx in range(n_experts):\n",
    "                useful_num = all_counts[layer_idx * n_experts + expert_idx]\n",
    "                importance_scores = up_states[layer_idx][expert_idx][:useful_num,:] ** 2 * gate_proj_states_mean_squares[layer_idx][expert_idx]\n",
    "                importance_thresholds[layer_idx][expert_idx] += importance_scores.to(device_2).flatten().kthvalue(int(importance_scores.numel() * sparsity_level)).values.to('cpu')\n",
    "\n",
    "for layer_idx in range(n_layers):\n",
    "    for expert_idx in range(n_experts):\n",
    "        importance_thresholds[layer_idx][expert_idx] /= n_batch // accum_steps\n",
    "        up_proj_states_thresholds_2[layer_idx][expert_idx] = (importance_thresholds[layer_idx][expert_idx].expand_as(up_proj_states_thresholds_2[layer_idx][expert_idx]) / gate_proj_states_mean_squares[layer_idx][expert_idx]) ** 0.5\n",
    "\n",
    "thresholds = {'up_proj_states_thresholds': up_proj_states_thresholds, 'up_proj_states_thresholds_2': up_proj_states_thresholds_2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save in: ./threshold/training_up\n"
     ]
    }
   ],
   "source": [
    "save_path = './threshold/training_up'\n",
    "\n",
    "sp = str(sparsity_level).replace('.', '_')\n",
    "print('save in:', save_path)\n",
    "torch.save(thresholds, f'{save_path}/thresholds_{sp}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:05<00:00,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 3.0378498029708862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# 计算评估损失\n",
    "total_loss = 0.0\n",
    "num_batches = 0\n",
    "\n",
    "for batch in tqdm(dataloader):\n",
    "    input_ids = batch['input_ids'].to(llm.device)\n",
    "    attention_mask = batch['attention_mask'].to(llm.device)\n",
    "    labels = batch['labels'].to(llm.device)\n",
    "    \n",
    "    # 禁用梯度计算\n",
    "    with torch.no_grad():\n",
    "        outputs = llm(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        if num_batches % 100 == 0:\n",
    "            print(f\"[{num_batches}], Eval Loss: {total_loss / (num_batches)}\")\n",
    "\n",
    "# 计算平均损失\n",
    "eval_loss = total_loss / num_batches\n",
    "print(f\"Eval Loss: {eval_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0 expert 0 ratio: 0.2167\n",
      "layer 0 expert 1 ratio: 0.1075\n",
      "layer 0 expert 2 ratio: 0.0871\n",
      "layer 0 expert 3 ratio: 0.2104\n",
      "layer 0 expert 4 ratio: 0.2017\n",
      "layer 0 expert 5 ratio: 0.2117\n",
      "layer 0 expert 6 ratio: 0.2164\n",
      "layer 0 expert 7 ratio: 0.2175\n",
      "layer 1 expert 0 ratio: 0.2209\n",
      "layer 1 expert 1 ratio: 0.2114\n",
      "layer 1 expert 2 ratio: 0.3353\n",
      "layer 1 expert 3 ratio: 0.2220\n",
      "layer 1 expert 4 ratio: 0.2407\n",
      "layer 1 expert 5 ratio: 0.3093\n",
      "layer 1 expert 6 ratio: 0.2343\n",
      "layer 1 expert 7 ratio: 0.2426\n",
      "layer 2 expert 0 ratio: 0.2640\n",
      "layer 2 expert 1 ratio: 0.2186\n",
      "layer 2 expert 2 ratio: 0.3159\n",
      "layer 2 expert 3 ratio: 0.3497\n",
      "layer 2 expert 4 ratio: 0.2149\n",
      "layer 2 expert 5 ratio: 0.2662\n",
      "layer 2 expert 6 ratio: 0.2368\n",
      "layer 2 expert 7 ratio: 0.2575\n",
      "layer 3 expert 0 ratio: 0.3555\n",
      "layer 3 expert 1 ratio: 0.2156\n",
      "layer 3 expert 2 ratio: 0.2297\n",
      "layer 3 expert 3 ratio: 0.2700\n",
      "layer 3 expert 4 ratio: 0.2890\n",
      "layer 3 expert 5 ratio: 0.2718\n",
      "layer 3 expert 6 ratio: 0.2421\n",
      "layer 3 expert 7 ratio: 0.3535\n",
      "layer 4 expert 0 ratio: 0.3492\n",
      "layer 4 expert 1 ratio: 0.2416\n",
      "layer 4 expert 2 ratio: 0.2716\n",
      "layer 4 expert 3 ratio: 0.3119\n",
      "layer 4 expert 4 ratio: 0.2632\n",
      "layer 4 expert 5 ratio: 0.3584\n",
      "layer 4 expert 6 ratio: 0.2183\n",
      "layer 4 expert 7 ratio: 0.3338\n",
      "layer 5 expert 0 ratio: 0.2305\n",
      "layer 5 expert 1 ratio: 0.2402\n",
      "layer 5 expert 2 ratio: 0.2377\n",
      "layer 5 expert 3 ratio: 0.2541\n",
      "layer 5 expert 4 ratio: 0.2931\n",
      "layer 5 expert 5 ratio: 0.2606\n",
      "layer 5 expert 6 ratio: 0.2186\n",
      "layer 5 expert 7 ratio: 0.2987\n",
      "layer 6 expert 0 ratio: 0.2579\n",
      "layer 6 expert 1 ratio: 0.2437\n",
      "layer 6 expert 2 ratio: 0.2908\n",
      "layer 6 expert 3 ratio: 0.2551\n",
      "layer 6 expert 4 ratio: 0.2546\n",
      "layer 6 expert 5 ratio: 0.2885\n",
      "layer 6 expert 6 ratio: 0.2213\n",
      "layer 6 expert 7 ratio: 0.2433\n",
      "layer 7 expert 0 ratio: 0.2440\n",
      "layer 7 expert 1 ratio: 0.2280\n",
      "layer 7 expert 2 ratio: 0.2376\n",
      "layer 7 expert 3 ratio: 0.2595\n",
      "layer 7 expert 4 ratio: 0.2468\n",
      "layer 7 expert 5 ratio: 0.2363\n",
      "layer 7 expert 6 ratio: 0.2458\n",
      "layer 7 expert 7 ratio: 0.2173\n",
      "layer 8 expert 0 ratio: 0.2298\n",
      "layer 8 expert 1 ratio: 0.2468\n",
      "layer 8 expert 2 ratio: 0.2369\n",
      "layer 8 expert 3 ratio: 0.2416\n",
      "layer 8 expert 4 ratio: 0.2393\n",
      "layer 8 expert 5 ratio: 0.2341\n",
      "layer 8 expert 6 ratio: 0.2378\n",
      "layer 8 expert 7 ratio: 0.2419\n",
      "layer 9 expert 0 ratio: 0.2343\n",
      "layer 9 expert 1 ratio: 0.2381\n",
      "layer 9 expert 2 ratio: 0.2315\n",
      "layer 9 expert 3 ratio: 0.2411\n",
      "layer 9 expert 4 ratio: 0.2284\n",
      "layer 9 expert 5 ratio: 0.2391\n",
      "layer 9 expert 6 ratio: 0.2319\n",
      "layer 9 expert 7 ratio: 0.2369\n",
      "layer 10 expert 0 ratio: 0.2271\n",
      "layer 10 expert 1 ratio: 0.2263\n",
      "layer 10 expert 2 ratio: 0.2302\n",
      "layer 10 expert 3 ratio: 0.2309\n",
      "layer 10 expert 4 ratio: 0.2269\n",
      "layer 10 expert 5 ratio: 0.2212\n",
      "layer 10 expert 6 ratio: 0.2255\n",
      "layer 10 expert 7 ratio: 0.2308\n",
      "layer 11 expert 0 ratio: 0.2250\n",
      "layer 11 expert 1 ratio: 0.2425\n",
      "layer 11 expert 2 ratio: 0.2303\n",
      "layer 11 expert 3 ratio: 0.2016\n",
      "layer 11 expert 4 ratio: 0.2332\n",
      "layer 11 expert 5 ratio: 0.2239\n",
      "layer 11 expert 6 ratio: 0.2303\n",
      "layer 11 expert 7 ratio: 0.2280\n",
      "layer 12 expert 0 ratio: 0.2178\n",
      "layer 12 expert 1 ratio: 0.2241\n",
      "layer 12 expert 2 ratio: 0.2268\n",
      "layer 12 expert 3 ratio: 0.2299\n",
      "layer 12 expert 4 ratio: 0.2261\n",
      "layer 12 expert 5 ratio: 0.2139\n",
      "layer 12 expert 6 ratio: 0.2301\n",
      "layer 12 expert 7 ratio: 0.2126\n",
      "layer 13 expert 0 ratio: 0.2233\n",
      "layer 13 expert 1 ratio: 0.2344\n",
      "layer 13 expert 2 ratio: 0.2324\n",
      "layer 13 expert 3 ratio: 0.2167\n",
      "layer 13 expert 4 ratio: 0.2211\n",
      "layer 13 expert 5 ratio: 0.2216\n",
      "layer 13 expert 6 ratio: 0.2221\n",
      "layer 13 expert 7 ratio: 0.2242\n",
      "layer 14 expert 0 ratio: 0.2204\n",
      "layer 14 expert 1 ratio: 0.2333\n",
      "layer 14 expert 2 ratio: 0.2187\n",
      "layer 14 expert 3 ratio: 0.2223\n",
      "layer 14 expert 4 ratio: 0.2266\n",
      "layer 14 expert 5 ratio: 0.2292\n",
      "layer 14 expert 6 ratio: 0.2242\n",
      "layer 14 expert 7 ratio: 0.2272\n",
      "layer 15 expert 0 ratio: 0.2323\n",
      "layer 15 expert 1 ratio: 0.2298\n",
      "layer 15 expert 2 ratio: 0.2195\n",
      "layer 15 expert 3 ratio: 0.2271\n",
      "layer 15 expert 4 ratio: 0.2193\n",
      "layer 15 expert 5 ratio: 0.2357\n",
      "layer 15 expert 6 ratio: 0.2274\n",
      "layer 15 expert 7 ratio: 0.2184\n",
      "layer 16 expert 0 ratio: 0.2153\n",
      "layer 16 expert 1 ratio: 0.2198\n",
      "layer 16 expert 2 ratio: 0.2185\n",
      "layer 16 expert 3 ratio: 0.2194\n",
      "layer 16 expert 4 ratio: 0.2257\n",
      "layer 16 expert 5 ratio: 0.2283\n",
      "layer 16 expert 6 ratio: 0.2221\n",
      "layer 16 expert 7 ratio: 0.2146\n",
      "layer 17 expert 0 ratio: 0.1951\n",
      "layer 17 expert 1 ratio: 0.2199\n",
      "layer 17 expert 2 ratio: 0.2179\n",
      "layer 17 expert 3 ratio: 0.2173\n",
      "layer 17 expert 4 ratio: 0.2267\n",
      "layer 17 expert 5 ratio: 0.2109\n",
      "layer 17 expert 6 ratio: 0.2225\n",
      "layer 17 expert 7 ratio: 0.2211\n",
      "layer 18 expert 0 ratio: 0.2289\n",
      "layer 18 expert 1 ratio: 0.2247\n",
      "layer 18 expert 2 ratio: 0.2228\n",
      "layer 18 expert 3 ratio: 0.2197\n",
      "layer 18 expert 4 ratio: 0.2201\n",
      "layer 18 expert 5 ratio: 0.2226\n",
      "layer 18 expert 6 ratio: 0.2081\n",
      "layer 18 expert 7 ratio: 0.2117\n",
      "layer 19 expert 0 ratio: 0.2000\n",
      "layer 19 expert 1 ratio: 0.2034\n",
      "layer 19 expert 2 ratio: 0.1949\n",
      "layer 19 expert 3 ratio: 0.1997\n",
      "layer 19 expert 4 ratio: 0.2208\n",
      "layer 19 expert 5 ratio: 0.2027\n",
      "layer 19 expert 6 ratio: 0.2111\n",
      "layer 19 expert 7 ratio: 0.2047\n",
      "layer 20 expert 0 ratio: 0.2178\n",
      "layer 20 expert 1 ratio: 0.1900\n",
      "layer 20 expert 2 ratio: 0.2077\n",
      "layer 20 expert 3 ratio: 0.1725\n",
      "layer 20 expert 4 ratio: 0.2186\n",
      "layer 20 expert 5 ratio: 0.2004\n",
      "layer 20 expert 6 ratio: 0.1913\n",
      "layer 20 expert 7 ratio: 0.2018\n",
      "layer 21 expert 0 ratio: 0.1862\n",
      "layer 21 expert 1 ratio: 0.1916\n",
      "layer 21 expert 2 ratio: 0.2224\n",
      "layer 21 expert 3 ratio: 0.2075\n",
      "layer 21 expert 4 ratio: 0.1779\n",
      "layer 21 expert 5 ratio: 0.1913\n",
      "layer 21 expert 6 ratio: 0.2235\n",
      "layer 21 expert 7 ratio: 0.1701\n",
      "layer 22 expert 0 ratio: 0.2049\n",
      "layer 22 expert 1 ratio: 0.2042\n",
      "layer 22 expert 2 ratio: 0.1815\n",
      "layer 22 expert 3 ratio: 0.2068\n",
      "layer 22 expert 4 ratio: 0.1681\n",
      "layer 22 expert 5 ratio: 0.2089\n",
      "layer 22 expert 6 ratio: 0.1860\n",
      "layer 22 expert 7 ratio: 0.2005\n",
      "layer 23 expert 0 ratio: 0.1930\n",
      "layer 23 expert 1 ratio: 0.1929\n",
      "layer 23 expert 2 ratio: 0.1583\n",
      "layer 23 expert 3 ratio: 0.1872\n",
      "layer 23 expert 4 ratio: 0.2176\n",
      "layer 23 expert 5 ratio: 0.1622\n",
      "layer 23 expert 6 ratio: 0.1987\n",
      "layer 23 expert 7 ratio: 0.1999\n",
      "layer 24 expert 0 ratio: 0.1848\n",
      "layer 24 expert 1 ratio: 0.1713\n",
      "layer 24 expert 2 ratio: 0.1695\n",
      "layer 24 expert 3 ratio: 0.1727\n",
      "layer 24 expert 4 ratio: 0.2012\n",
      "layer 24 expert 5 ratio: 0.1837\n",
      "layer 24 expert 6 ratio: 0.2015\n",
      "layer 24 expert 7 ratio: 0.1827\n",
      "layer 25 expert 0 ratio: 0.1668\n",
      "layer 25 expert 1 ratio: 0.1966\n",
      "layer 25 expert 2 ratio: 0.2055\n",
      "layer 25 expert 3 ratio: 0.2026\n",
      "layer 25 expert 4 ratio: 0.1772\n",
      "layer 25 expert 5 ratio: 0.2016\n",
      "layer 25 expert 6 ratio: 0.1646\n",
      "layer 25 expert 7 ratio: 0.1826\n",
      "layer 26 expert 0 ratio: 0.1745\n",
      "layer 26 expert 1 ratio: 0.2131\n",
      "layer 26 expert 2 ratio: 0.1526\n",
      "layer 26 expert 3 ratio: 0.2053\n",
      "layer 26 expert 4 ratio: 0.1692\n",
      "layer 26 expert 5 ratio: 0.1764\n",
      "layer 26 expert 6 ratio: 0.1643\n",
      "layer 26 expert 7 ratio: 0.2197\n",
      "layer 27 expert 0 ratio: 0.1578\n",
      "layer 27 expert 1 ratio: 0.1898\n",
      "layer 27 expert 2 ratio: 0.1826\n",
      "layer 27 expert 3 ratio: 0.1639\n",
      "layer 27 expert 4 ratio: 0.1610\n",
      "layer 27 expert 5 ratio: 0.1995\n",
      "layer 27 expert 6 ratio: 0.2002\n",
      "layer 27 expert 7 ratio: 0.2148\n",
      "layer 28 expert 0 ratio: 0.1564\n",
      "layer 28 expert 1 ratio: 0.1879\n",
      "layer 28 expert 2 ratio: 0.1810\n",
      "layer 28 expert 3 ratio: 0.1909\n",
      "layer 28 expert 4 ratio: 0.2137\n",
      "layer 28 expert 5 ratio: 0.1635\n",
      "layer 28 expert 6 ratio: 0.2119\n",
      "layer 28 expert 7 ratio: 0.1742\n",
      "layer 29 expert 0 ratio: 0.1979\n",
      "layer 29 expert 1 ratio: 0.1816\n",
      "layer 29 expert 2 ratio: 0.1984\n",
      "layer 29 expert 3 ratio: 0.2036\n",
      "layer 29 expert 4 ratio: 0.1918\n",
      "layer 29 expert 5 ratio: 0.1726\n",
      "layer 29 expert 6 ratio: 0.1919\n",
      "layer 29 expert 7 ratio: 0.1467\n",
      "layer 30 expert 0 ratio: 0.2143\n",
      "layer 30 expert 1 ratio: 0.2203\n",
      "layer 30 expert 2 ratio: 0.2172\n",
      "layer 30 expert 3 ratio: 0.2179\n",
      "layer 30 expert 4 ratio: 0.2201\n",
      "layer 30 expert 5 ratio: 0.1934\n",
      "layer 30 expert 6 ratio: 0.1919\n",
      "layer 30 expert 7 ratio: 0.1765\n",
      "layer 31 expert 0 ratio: 0.2272\n",
      "layer 31 expert 1 ratio: 0.2044\n",
      "layer 31 expert 2 ratio: 0.2166\n",
      "layer 31 expert 3 ratio: 0.2298\n",
      "layer 31 expert 4 ratio: 0.2461\n",
      "layer 31 expert 5 ratio: 0.2361\n",
      "layer 31 expert 6 ratio: 0.2506\n",
      "layer 31 expert 7 ratio: 0.2367\n"
     ]
    }
   ],
   "source": [
    "for layerid in range(32):\n",
    "    for expertid in range(8):\n",
    "        llm.model.layers[layerid].block_sparse_moe.experts[expertid].print_ratio()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_ENDPOINT\"]=\"https://hf-mirror.com\"\n",
    "\n",
    "\n",
    "import lm_eval\n",
    "from lm_eval.models.huggingface import HFLM\n",
    "from lm_eval import evaluator\n",
    "del dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-03:13:11:00,185 WARNING  [huggingface.py:121] `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "2025-01-03:13:11:00,251 WARNING  [huggingface.py:349] Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "2025-01-03:13:11:00,259 INFO     [evaluator.py:152] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2025-01-03:13:11:00,261 INFO     [evaluator.py:203] Using pre-initialized model\n",
      "Using the latest cached version of the module from /home/lz/.cache/huggingface/modules/datasets_modules/datasets/winogrande/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2 (last modified on Thu Jan  2 22:35:53 2025) since it couldn't be found locally at winogrande, or remotely on the Hugging Face Hub.\n",
      "2025-01-03:13:13:13,531 WARNING  [load.py:1407] Using the latest cached version of the module from /home/lz/.cache/huggingface/modules/datasets_modules/datasets/winogrande/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2 (last modified on Thu Jan  2 22:35:53 2025) since it couldn't be found locally at winogrande, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "def evaluate(task_name_list, model, tokenizer, num_fewshot, device):\n",
    "    hflm = HFLM(pretrained=llm, tokenizer=tokenizer)\n",
    "    results = evaluator.simple_evaluate(\n",
    "    model=hflm,\n",
    "    tasks=task_name_list,\n",
    "    num_fewshot=num_fewshot)\n",
    "    print(results['results'])\n",
    "\n",
    "\n",
    "\n",
    "# triviaqa\n",
    "task_list=['winogrande','sciq','openbookqa','arc_challenge','arc_easy']\n",
    "# 'boolq',\n",
    "# task_list=['truthfulqa_gen','triviaqa_gen']\n",
    "evaluate(task_list, llm, tokenizer, 0, \"cuda\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
