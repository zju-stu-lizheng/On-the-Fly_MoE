{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lz/miniconda3/envs/hqq/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:23<00:00,  1.24s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "# sys.path.append('/mnt/storage/zyx/llama3-8b')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6\"\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, MixtralForCausalLM\n",
    "import json\n",
    "\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "\n",
    "with open('./device_map.json', 'r') as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "llm = MixtralForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device_map,\n",
    "    use_cache=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    # attn_implementation=\"flash_attention_2\"\n",
    ") \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:11<00:00,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 3.1417097187042238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "def preprocess_data(batch):\n",
    "    # 使用 tokenizer 将文本数据转换为模型输入\n",
    "    inputs = tokenizer(batch['text'], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    inputs[\"labels\"] = inputs.input_ids.clone()\n",
    "    return inputs\n",
    "\n",
    "# 定义一个函数来选择特征并丢弃不需要的\n",
    "def select_features(example):\n",
    "    return {\n",
    "        'input_ids': example['input_ids'],\n",
    "        'attention_mask': example['attention_mask'],\n",
    "        'labels': example['labels']\n",
    "    }\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# # 加载 C4 数据集的验证集\n",
    "with open('../path.json', 'r') as file:\n",
    "    paths = json.load(file)\n",
    "    c4_path = paths.get('c4', '')\n",
    "c4 = load_dataset(c4_path)\n",
    "# 对数据集进行预处理\n",
    "c4_dataset = c4.map(preprocess_data, batched=True)\n",
    "# c4_dataset = c4_dataset.map(select_features, batched=True)\n",
    "c4_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "# c4_dataset\n",
    "top_four_thousand_data = c4_dataset['validation'].select(range(100))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "set_seed(42)\n",
    "\n",
    "# 定义数据加载器\n",
    "batch_size = 4\n",
    "# dataloader = DataLoader(c4_dataset['validation'], batch_size=batch_size)\n",
    "dataloader = DataLoader(top_four_thousand_data, batch_size=batch_size)\n",
    "\n",
    "# 计算评估损失\n",
    "total_loss = 0.0\n",
    "num_batches = 0\n",
    "\n",
    "for batch in tqdm(dataloader):\n",
    "    input_ids = batch['input_ids'].to(llm.device)\n",
    "    attention_mask = batch['attention_mask'].to(llm.device)\n",
    "    labels = batch['labels'].to(llm.device)\n",
    "    \n",
    "    # 禁用梯度计算\n",
    "    with torch.no_grad():\n",
    "        outputs = llm(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        if num_batches % 100 == 0:\n",
    "            print(f\"[{num_batches}], Eval Loss: {total_loss / (num_batches)}\")\n",
    "\n",
    "# 计算平均损失\n",
    "eval_loss = total_loss / num_batches\n",
    "print(f\"Eval Loss: {eval_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:13<00:00,  1.40it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# 加载预训练的 OPT-30B 模型和 tokenizerQ\n",
    "# model_name = \"/mnt/storage/zyx/Meta-Llama-3-8B\"\n",
    "\n",
    "llm_base = MixtralForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map='cpu',\n",
    "    use_cache=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    # attn_implementation=\"flash_attention_2\"\n",
    ") \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start obtaining the whitening matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:42<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Cholesky Decomposition...\n",
      "Lambda contains negative values. Clamping to zero.\n",
      "Lambda min: 2.000001133710074e-06, Lambda max: 34652.89016590064\n",
      "Lambda contains negative values. Clamping to zero.\n",
      "Lambda min: 1.9999711638830518e-06, Lambda max: 28471.76651950249\n",
      "Lambda contains negative values. Clamping to zero.\n",
      "Lambda min: 2.000000120742572e-06, Lambda max: 9294.48046434244\n",
      "Lambda contains negative values. Clamping to zero.\n",
      "Lambda min: 1.9999999134740025e-06, Lambda max: 7903.3157817122365\n",
      "Lambda contains negative values. Clamping to zero.\n",
      "Lambda min: 1.999999916268309e-06, Lambda max: 16116.83150997788\n",
      "Lambda contains negative values. Clamping to zero.\n",
      "Lambda min: 2.0000182049892386e-06, Lambda max: 221616.43419377087\n",
      "Lambda contains negative values. Clamping to zero.\n",
      "Lambda min: 2.021251831583815e-06, Lambda max: 43900592.940469876\n",
      "Lambda contains negative values. Clamping to zero.\n",
      "Lambda min: 1.9999673082059018e-06, Lambda max: 395779.1641855678\n",
      "Lambda contains negative values. Clamping to zero.\n",
      "Lambda min: 1.896669608880462e-06, Lambda max: 62817241.3009787\n",
      "Lambda contains negative values. Clamping to zero.\n",
      "Lambda min: 2.0001812095614184e-06, Lambda max: 394316.6069188431\n",
      "Lambda contains negative values. Clamping to zero.\n",
      "Lambda min: 1.9999587985680658e-06, Lambda max: 1232988.5135071636\n"
     ]
    }
   ],
   "source": [
    "def profle_svdllm(name, model, calib_loader, dev):\n",
    "    # model.to(dev)\n",
    "    if \"llama\" in name or \"mixtral\" in name or \"vicuna\" in name:\n",
    "        layers = model.model.layers\n",
    "    print(\"Start obtaining the whitening matrix...\")\n",
    "    def hook(module, input, output):\n",
    "        inp = input[0].detach().float()\n",
    "        if inp.dim() == 2:   # for opt\n",
    "            inp = inp.unsqueeze(0)\n",
    "        adds = torch.matmul(inp.transpose(1,2), inp)\n",
    "        adds_sum = torch.sum(adds, dim=0)\n",
    "        module.raw_scaling_diag_matrix += adds_sum\n",
    "        del inp, adds, adds_sum\n",
    "        torch.cuda.empty_cache()\n",
    "    for name, module in model.named_modules():\n",
    "        if \"w3\" in name:\n",
    "            # print(name)\n",
    "            module.raw_scaling_diag_matrix = 0\n",
    "            module.register_forward_hook(hook)\n",
    "            \n",
    "    for batch in tqdm(calib_loader):\n",
    "        inputs = batch['input_ids'].to(llm.device)\n",
    "        model(inputs)\n",
    "    for name, module in model.named_modules():\n",
    "        if \"w3\" in name:\n",
    "            module._forward_hooks.clear()\n",
    "            # print(module.raw_scaling_diag_matrix)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    profiling_mat = {}\n",
    "    print(\"Start Cholesky Decomposition...\")\n",
    "    \n",
    "    layer_profile = {}\n",
    "    for name, module in model.named_modules():\n",
    "        if \"w3\" in name:\n",
    "            covariance = module.raw_scaling_diag_matrix.double().to(dev)\n",
    "            if not torch.allclose(covariance, covariance.t(), atol=1e-6):\n",
    "                raise ValueError(\"Covariance matrix is not symmetric.\")\n",
    "                    # Perform eigen decomposition\n",
    "            Lambda, Q = torch.linalg.eigh(covariance, UPLO='U')\n",
    "            if torch.isnan(Lambda).any() or torch.isinf(Lambda).any():\n",
    "                raise ValueError(\"Lambda contains NaN or Inf values.\")\n",
    "\n",
    "            # 检查 Lambda 是否包含负值\n",
    "            if (Lambda < 0).any():\n",
    "                print(\"Lambda contains negative values. Clamping to zero.\")\n",
    "                eigenvalues = torch.linalg.eigvalsh(covariance)\n",
    "                covariance += (- eigenvalues[0] + 2e-6) * torch.eye(covariance.shape[0]).cuda()\n",
    "                Lambda, Q = torch.linalg.eigh(covariance, UPLO='U')\n",
    "                print(f\"Lambda min: {Lambda.min().item()}, Lambda max: {Lambda.max().item()}\")\n",
    "            # 现在进行平方根操作\n",
    "            Lambda_diag = torch.diag(torch.sqrt(Lambda))\n",
    "            # Sort eigenvalues and eigenvectors in descending order\n",
    "            indices = torch.argsort(Lambda, descending=True)\n",
    "            Lambda = Lambda[indices]\n",
    "            Q = Q[:, indices]\n",
    "\n",
    "            # Compute Q_prime = Q * sqrt(Lambda)\n",
    "            Lambda_diag = torch.diag(torch.sqrt(Lambda))\n",
    "            Q_prime = torch.matmul(Q, Lambda_diag)\n",
    "            layer_profile[name] = Q_prime.cpu()\n",
    "            profiling_mat[name] = layer_profile\n",
    "    return profiling_mat\n",
    "profiling_mat=profle_svdllm(\"mixtral\", llm, dataloader, \"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 354/354 [00:00<00:00, 39070.17it/s]\n",
      "100%|██████████| 929/929 [00:15<00:00, 59.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MixtralForCausalLM(\n",
       "  (model): MixtralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MixtralDecoderLayer(\n",
       "        (self_attn): MixtralSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MixtralRotaryEmbedding()\n",
       "        )\n",
       "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
       "          (gate): Linear(in_features=4096, out_features=8, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-7): 8 x MixtralBlockSparseTop2MLP(\n",
       "              (w1): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (w2): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "              (w3): HQQLinear(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): MixtralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MixtralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MixtralRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Quantize\n",
    "from hqq.core.quantize import *\n",
    "q4_config    = BaseQuantizeConfig(nbits=8, group_size=64) \n",
    "q3_config    = BaseQuantizeConfig(nbits=2, group_size=64)\n",
    "\n",
    "# quant_config = {'self_attn.q_proj':q4_config,\n",
    "#   'self_attn.k_proj':q4_config,\n",
    "#   'self_attn.v_proj':q4_config,\n",
    "#   'self_attn.o_proj':q4_config,\n",
    "\n",
    "#   'mlp.gate_proj':q3_config,\n",
    "#   'block_sparse_moe.experts[j].w3'  :q3_config,\n",
    "#   'mlp.down_proj':q3_config,\n",
    "# }\n",
    "quant_config = {\n",
    "  'block_sparse_moe.experts.w3'  :q3_config,\n",
    "}\n",
    "from hqq.models.hf.base import AutoHQQHFModel\n",
    "AutoHQQHFModel.quantize_model(llm, quant_config=quant_config, compute_dtype=torch.float16, device=device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4943/3829634443.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.B_prime = torch.nn.Parameter(torch.tensor(B_prime).float())\n",
      "/tmp/ipykernel_4943/3829634443.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.A = torch.nn.Parameter(torch.tensor(A).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n",
      "torch.Size([1024, 4096]) torch.Size([14336, 1024])\n"
     ]
    }
   ],
   "source": [
    "class CompensatedModel(torch.nn.Module):\n",
    "    def __init__(self, model, B_prime, A):\n",
    "        super(CompensatedModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.B_prime = torch.nn.Parameter(torch.tensor(B_prime).float())\n",
    "        self.A = torch.nn.Parameter(torch.tensor(A).float())\n",
    "        print(self.A.shape,self.B_prime.shape)\n",
    "    def forward(self, input_ids):\n",
    "        outputs = self.model(input_ids)\n",
    "        # 假设在特定层添加残差连接，根据实际模型结构进行修改\n",
    "        # print(self.B_prime.shape,self.A.shape,input_ids.shape)\n",
    "        residual = input_ids @ (self.B_prime @ self.A).T\n",
    "        outputs += residual\n",
    "    \n",
    "        return outputs\n",
    "    \n",
    "for i in range(32):\n",
    "    for j in range(8):\n",
    "        llmdevice = llm.model.layers[i].block_sparse_moe.experts[j].w3.device\n",
    "        Delta_W = llm_base.model.layers[i].block_sparse_moe.experts[j].w3.weight.to(llmdevice) - llm.model.layers[i].block_sparse_moe.experts[j].w3.dequantize()\n",
    "        Q_prime = profiling_mat[f\"model.layers.{i}.block_sparse_moe.experts.{j}.w3\"][f\"model.layers.{i}.block_sparse_moe.experts.{j}.w3\"].cuda().float()\n",
    "        Delta_W_prime =  Delta_W.to(torch.float32).to(llmdevice) @ Q_prime.to(torch.float32).to(llmdevice)\n",
    "        llm_base.model.layers[i].block_sparse_moe.experts[j].w3.cpu()\n",
    "        # 步骤5: 进行SVD分解并取前r个奇异值\n",
    "        rank = 1024  # 设置 desired rank\n",
    "        U_prime, Sigma_prime, V_prime = torch.linalg.svd(Delta_W_prime, full_matrices=False)\n",
    "        U_prime = U_prime[:, :rank]\n",
    "        Sigma_prime = Sigma_prime[:rank]\n",
    "        V_prime = V_prime[:rank, :]\n",
    "\n",
    "        B_prime = U_prime @ torch.diag(Sigma_prime)\n",
    "        A_prime = V_prime\n",
    "\n",
    "        # 步骤6: 投影回原空间\n",
    "        A = A_prime.to(llmdevice) @ torch.linalg.inv(Q_prime).to(llmdevice)\n",
    "        llm.model.layers[i].block_sparse_moe.experts[j].w3 = CompensatedModel(llm.model.layers[i].block_sparse_moe.experts[j].w3, B_prime, A).to(llmdevice)\n",
    "    # compensated_model = CompensatedModel(student.base, B_prime, A).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_ENDPOINT\"]=\"https://hf-mirror.com\"\n",
    "\n",
    "\n",
    "import lm_eval\n",
    "from lm_eval.models.huggingface import HFLM\n",
    "from lm_eval import evaluator\n",
    "del dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-03:13:11:00,185 WARNING  [huggingface.py:121] `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "2025-01-03:13:11:00,251 WARNING  [huggingface.py:349] Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "2025-01-03:13:11:00,259 INFO     [evaluator.py:152] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2025-01-03:13:11:00,261 INFO     [evaluator.py:203] Using pre-initialized model\n",
      "Using the latest cached version of the module from /home/lz/.cache/huggingface/modules/datasets_modules/datasets/winogrande/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2 (last modified on Thu Jan  2 22:35:53 2025) since it couldn't be found locally at winogrande, or remotely on the Hugging Face Hub.\n",
      "2025-01-03:13:13:13,531 WARNING  [load.py:1407] Using the latest cached version of the module from /home/lz/.cache/huggingface/modules/datasets_modules/datasets/winogrande/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2 (last modified on Thu Jan  2 22:35:53 2025) since it couldn't be found locally at winogrande, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "def evaluate(task_name_list, model, tokenizer, num_fewshot, device):\n",
    "    hflm = HFLM(pretrained=llm, tokenizer=tokenizer)\n",
    "    results = evaluator.simple_evaluate(\n",
    "    model=hflm,\n",
    "    tasks=task_name_list,\n",
    "    num_fewshot=num_fewshot)\n",
    "    print(results['results'])\n",
    "\n",
    "\n",
    "\n",
    "# triviaqa\n",
    "task_list=['winogrande','sciq','openbookqa','arc_challenge','arc_easy']\n",
    "# 'boolq',\n",
    "# task_list=['truthfulqa_gen','triviaqa_gen']\n",
    "evaluate(task_list, llm, tokenizer, 0, \"cuda\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
