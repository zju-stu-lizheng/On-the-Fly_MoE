{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lz/miniconda3/envs/hqq/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set profile_threshold to False\n",
      "using  torch.bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:33<00:00,  1.75s/it]\n",
      "100%|██████████| 354/354 [00:00<00:00, 11323.33it/s]\n",
      "100%|██████████| 929/929 [00:15<00:00, 60.49it/s]\n",
      "100%|██████████| 929/929 [00:16<00:00, 55.54it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6\"\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
    "from modeling_mixtral import MixtralForCausalLM, set_profile_mode\n",
    "import json\n",
    "from utils import get_model\n",
    "\n",
    "# # 加载 C4 数据集的验证集\n",
    "with open('../path.json', 'r') as file:\n",
    "    paths = json.load(file)\n",
    "    c4_path = paths.get('c4', '')\n",
    "    model_name = paths.get('mixtral','')\n",
    "\n",
    "with open('./device_map.json', 'r') as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "set_profile_mode(False)\n",
    "dtype = torch.bfloat16\n",
    "print('using ',dtype)\n",
    "llm, tokenizer = get_model(model_name, device_map, dtype=dtype)\n",
    "# %%\n",
    "#Quantize\n",
    "from hqq.core.quantize import *\n",
    "q4_config    = BaseQuantizeConfig(nbits=8, group_size=64) \n",
    "q3_config    = BaseQuantizeConfig(nbits=2, group_size=64)\n",
    "\n",
    "quant_config = {\n",
    "  'block_sparse_moe.experts.w3'  :q3_config,\n",
    "}\n",
    "from hqq.models.hf.base import AutoHQQHFModel\n",
    "AutoHQQHFModel.quantize_model(llm, quant_config=quant_config, compute_dtype=dtype, device=device_map)\n",
    "\n",
    "\n",
    "#First, quantize/load a quantized HQQ model the\n",
    "from hqq.core.peft import PeftUtils\n",
    "base_lora_params = {'lora_type':'default', 'r':128, 'lora_alpha':128, 'dropout':0.05, 'train_dtype':dtype}\n",
    "up_lora_params = {'lora_type':'default', 'r':256, 'lora_alpha':256, 'dropout':0.05, 'train_dtype':dtype}\n",
    "\n",
    "lora_params      = {'self_attn.q_proj': base_lora_params,\n",
    "                    'self_attn.k_proj': base_lora_params,\n",
    "                    'self_attn.v_proj': base_lora_params,\n",
    "                    'self_attn.o_proj': base_lora_params,\n",
    "                    'block_sparse_moe.experts.w1'   : base_lora_params,\n",
    "                    'block_sparse_moe.experts.w3'   : up_lora_params,\n",
    "                    'block_sparse_moe.experts.w2'   : base_lora_params}\n",
    "\n",
    "\n",
    "#Add LoRA to linear/HQQ modules\n",
    "PeftUtils.add_lora(llm, lora_params)\n",
    "### replace eora to the LoRA in w3\n",
    "for i in range(32):\n",
    "    for j in range(8):\n",
    "        llmdevice = llm.model.layers[i].block_sparse_moe.experts[j].w3.lora_A.device\n",
    "        a = torch.load('/home/lz/On-the-Fly_MoE_Inference/quantize/saved/eora/' + f'A_{i}_{j}.pt').to(dtype).to(llmdevice).transpose(0, 1)\n",
    "        b = torch.load('/home/lz/On-the-Fly_MoE_Inference/quantize/saved/eora/' + f'B_prime_{i}_{j}.pt').to(dtype).to(llmdevice).transpose(0, 1)\n",
    "        llm.model.layers[i].block_sparse_moe.experts[j].w3.lora_A = torch.nn.Parameter(a)\n",
    "        llm.model.layers[i].block_sparse_moe.experts[j].w3.lora_B = torch.nn.Parameter(b)\n",
    "\n",
    "        del a\n",
    "        del b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "2025-01-06:19:25:11,711 WARNING  [repocard.py:108] Repo card metadata block was not found. Setting CardData to empty.\n",
      "Map: 100%|██████████| 50018/50018 [00:23<00:00, 2104.63 examples/s]\n",
      "Map: 100%|██████████| 5558/5558 [00:03<00:00, 1816.16 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "import functools\n",
    "\n",
    "def preprocess_data(batch, tokenizer):\n",
    "    # 使用 tokenizer 将文本数据转换为模型输入\n",
    "    inputs = tokenizer(batch['text'], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    inputs[\"labels\"] = inputs.input_ids.clone()\n",
    "    return inputs\n",
    "\n",
    "\n",
    "openmath = load_dataset(\"/home/lz/web-math/\",data_files=\"/home/lz/web-math/openmath1.json\")\n",
    "c4 = load_dataset(c4_path)\n",
    "openmath_text = openmath['train']['text']  \n",
    "c4_text = c4['validation']['text']  \n",
    "combined_text = openmath_text + c4_text\n",
    "combined_dataset = Dataset.from_dict({\"text\": combined_text})\n",
    "\n",
    "test_num = 0.1\n",
    "seed = 42\n",
    "\n",
    "combined_train = combined_dataset.train_test_split(test_size=test_num, seed=seed)\n",
    "train_data = combined_train['train']\n",
    "test_data = combined_train['test']\n",
    "\n",
    "new_train_data = train_data.map(\n",
    "    functools.partial(\n",
    "    preprocess_data,\n",
    "    tokenizer=tokenizer\n",
    "), batched=True)\n",
    "new_test_data = test_data.map(\n",
    "    functools.partial(\n",
    "    preprocess_data,\n",
    "    tokenizer=tokenizer\n",
    "), batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lz/miniconda3/envs/hqq/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "2025-01-06:19:25:40,188 WARNING  [other.py:331] Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='37' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 37/750 01:30 < 30:41, 0.39 it/s, Epoch 0.10/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/lz/On-the-Fly_MoE_Inference/quantize/recover.ipynb Cell 3\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blza100/home/lz/On-the-Fly_MoE_Inference/quantize/recover.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39m# silence the warnings. re-enable for inference!\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blza100/home/lz/On-the-Fly_MoE_Inference/quantize/recover.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=72'>73</a>\u001b[0m llm\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_cache \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blza100/home/lz/On-the-Fly_MoE_Inference/quantize/recover.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/transformers/trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2164\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   2165\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   2166\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   2167\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   2168\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   2169\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/transformers/trainer.py:2524\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2517\u001b[0m context \u001b[39m=\u001b[39m (\n\u001b[1;32m   2518\u001b[0m     functools\u001b[39m.\u001b[39mpartial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mno_sync, model\u001b[39m=\u001b[39mmodel)\n\u001b[1;32m   2519\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(batch_samples) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   2520\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mdistributed_type \u001b[39m!=\u001b[39m DistributedType\u001b[39m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2521\u001b[0m     \u001b[39melse\u001b[39;00m contextlib\u001b[39m.\u001b[39mnullcontext\n\u001b[1;32m   2522\u001b[0m )\n\u001b[1;32m   2523\u001b[0m \u001b[39mwith\u001b[39;00m context():\n\u001b[0;32m-> 2524\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2526\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   2527\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2528\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2529\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2530\u001b[0m ):\n\u001b[1;32m   2531\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2532\u001b[0m     tr_loss \u001b[39m=\u001b[39m tr_loss \u001b[39m+\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/transformers/trainer.py:3687\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3685\u001b[0m         scaled_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m   3686\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 3687\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mbackward(loss, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   3688\u001b[0m     \u001b[39m# Finally we need to normalize the loss for reporting\u001b[39;00m\n\u001b[1;32m   3689\u001b[0m     \u001b[39mif\u001b[39;00m num_items_in_batch \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/accelerate/accelerator.py:2248\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2246\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2247\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2248\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    526\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    527\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[1;32m    268\u001b[0m     tensors,\n\u001b[1;32m    269\u001b[0m     grad_tensors_,\n\u001b[1;32m    270\u001b[0m     retain_graph,\n\u001b[1;32m    271\u001b[0m     create_graph,\n\u001b[1;32m    272\u001b[0m     inputs,\n\u001b[1;32m    273\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    274\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    275\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/hqq/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[39m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m         t_outputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    746\u001b[0m     )  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[39mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from hqq.core.peft import PeftUtils\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, AdamW\n",
    "from transformers import (\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "class CustomTrainer(transformers.Trainer):\n",
    "    def save_model(self, output_dir=None, _internal_call=False):\n",
    "        # 如果没有指定output_dir，则使用训练参数中的输出目录\n",
    "        if output_dir is None:\n",
    "            output_dir = self.args.output_dir #这里的args不是该脚本的输入，而是TrainerArgs\n",
    "\n",
    "        # 确保输出目录存在\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # 保存完整的模型参数\n",
    "        # torch.save(self.model.state_dict(), os.path.join(output_dir, 'pytorch_model.bin'))\n",
    "        \n",
    "        # PeftUtils.cast_lora_weights(self.model, dtype=torch.bfloat16)\n",
    "\n",
    "        #Save LoRA weights\n",
    "        PeftUtils.save_lora_weights(self.model, output_dir+'_lora_combine.pt')\n",
    "\n",
    "        # 保存配置文件和tokenizer\n",
    "        self.model.config.save_pretrained(output_dir)\n",
    "        if self.tokenizer is not None:\n",
    "            self.tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "model_save_path='./saved/training/less'\n",
    "learning_rate = 0.005\n",
    "micro_batch_size=8\n",
    "epochs=2\n",
    "save_steps = 50\n",
    "save_total_limit = 6\n",
    "sample_num = len(new_train_data)\n",
    "optimizer=AdamW(filter(lambda p : p.requires_grad, llm.parameters()),lr=learning_rate)\n",
    "linear_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(sample_num*epochs) // micro_batch_size)\n",
    "args = TrainingArguments(\n",
    "    output_dir=model_save_path,\n",
    "    num_train_epochs=epochs,\n",
    "    # max_steps=opt.max_steps,\n",
    "    # fp16=True,\n",
    "    bf16=True,\n",
    "    optim=\"adamw_torch\",# paged_adamw_8bit\n",
    "    learning_rate=learning_rate,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    per_device_train_batch_size=micro_batch_size,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=False,   ### 先设置成False\n",
    "    group_by_length=False,\n",
    "    logging_steps=100,\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_only_model=True,\n",
    "    save_steps=save_steps,\n",
    "    save_total_limit=save_total_limit,\n",
    "    disable_tqdm=False,\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=llm,\n",
    "    train_dataset=new_train_data.select(range(3000)),\n",
    "    eval_dataset=new_test_data.select(range(300)),\n",
    "    args=args,\n",
    "    optimizers=(optimizer, linear_scheduler),\n",
    "    data_collator=DataCollatorForSeq2Seq(\n",
    "    tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True),\n",
    ")\n",
    "\n",
    "# silence the warnings. re-enable for inference!\n",
    "llm.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lz/miniconda3/envs/hqq/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set profile_threshold to False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:24<00:00,  1.27s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6\"\n",
    "from modeling_mixtral import MixtralForCausalLM, set_profile_mode\n",
    "import json\n",
    "from utils import get_model\n",
    "\n",
    "# # 加载 C4 数据集的验证集\n",
    "with open('../path.json', 'r') as file:\n",
    "    paths = json.load(file)\n",
    "    c4_path = paths.get('c4', '')\n",
    "    model_name = paths.get('mixtral','')\n",
    "\n",
    "with open('./device_map.json', 'r') as f:\n",
    "    device_map = json.load(f)\n",
    "\n",
    "set_profile_mode(False)\n",
    "llm, tokenizer = get_model(model_name, device_map)\n",
    "# %%\n",
    "#Quantize\n",
    "from hqq.core.quantize import *\n",
    "q4_config    = BaseQuantizeConfig(nbits=8, group_size=64) \n",
    "q3_config    = BaseQuantizeConfig(nbits=2, group_size=64)\n",
    "\n",
    "quant_config = {\n",
    "  'block_sparse_moe.experts.w3'  :q3_config,\n",
    "}\n",
    "from hqq.models.hf.base import AutoHQQHFModel\n",
    "AutoHQQHFModel.quantize_model(llm, quant_config=quant_config, compute_dtype=torch.float16, device=device_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from datasets import load_dataset\n",
    "def preprocess_data(batch):\n",
    "    # 使用 tokenizer 将文本数据转换为模型输入\n",
    "    inputs = tokenizer(batch['text'], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    inputs[\"labels\"] = inputs.input_ids.clone()\n",
    "    return inputs\n",
    "\n",
    "# 定义一个函数来选择特征并丢弃不需要的\n",
    "def select_features(example):\n",
    "    return {\n",
    "        'input_ids': example['input_ids'],\n",
    "        'attention_mask': example['attention_mask'],\n",
    "        'labels': example['labels']\n",
    "    }\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "c4 = load_dataset(c4_path)\n",
    "# 对数据集进行预处理\n",
    "c4_dataset = c4.map(preprocess_data, batched=True)\n",
    "# c4_dataset = c4_dataset.map(select_features, batched=True)\n",
    "c4_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "# c4_dataset\n",
    "top_four_thousand_data = c4_dataset['validation'].select(range(400))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "set_seed(42)\n",
    "\n",
    "# 定义数据加载器\n",
    "batch_size = 8\n",
    "# dataloader = DataLoader(c4_dataset['validation'], batch_size=batch_size)\n",
    "dataloader = DataLoader(top_four_thousand_data, batch_size=batch_size)\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "llm_base = MixtralForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map='cpu',\n",
    "    use_cache=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    # attn_implementation=\"flash_attention_2\"\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eora恢复"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直接从文件中读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 done...\n",
      "Layer 1 done...\n",
      "Layer 2 done...\n",
      "Layer 3 done...\n",
      "Layer 4 done...\n",
      "Layer 5 done...\n",
      "Layer 6 done...\n",
      "Layer 7 done...\n",
      "Layer 8 done...\n",
      "Layer 9 done...\n",
      "Layer 10 done...\n",
      "Layer 11 done...\n",
      "Layer 12 done...\n",
      "Layer 13 done...\n",
      "Layer 14 done...\n",
      "Layer 15 done...\n",
      "Layer 16 done...\n",
      "Layer 17 done...\n",
      "Layer 18 done...\n",
      "Layer 19 done...\n",
      "Layer 20 done...\n",
      "Layer 21 done...\n",
      "Layer 22 done...\n",
      "Layer 23 done...\n",
      "Layer 24 done...\n",
      "Layer 25 done...\n",
      "Layer 26 done...\n",
      "Layer 27 done...\n",
      "Layer 28 done...\n",
      "Layer 29 done...\n",
      "Layer 30 done...\n",
      "Layer 31 done...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class CompensatedModel(torch.nn.Module):\n",
    "    def __init__(self, model, path, layerid, expertid):\n",
    "        super(CompensatedModel, self).__init__()\n",
    "        self.model = model\n",
    "        ### self.A and self.B_prime are initialized as the values loaded from the file\n",
    "        self.A = torch.load(path + f'A_{layerid}_{expertid}.pt').to(torch.float16)\n",
    "        self.B_prime = torch.load(path + f'B_prime_{layerid}_{expertid}.pt').to(torch.float16)\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        outputs = self.model(input_ids)\n",
    "        residual = (input_ids @ self.A.T) @ self.B_prime.T\n",
    "        torch.add(outputs, residual, out = outputs)\n",
    "    \n",
    "        return outputs\n",
    "\n",
    "for i in range(32):\n",
    "    print(f\"Layer {i} done...\")\n",
    "    for j in range(8):\n",
    "        llmdevice = llm.model.layers[i].block_sparse_moe.experts[j].w3.device\n",
    "        llm.model.layers[i].block_sparse_moe.experts[j].w3 = \\\n",
    "        CompensatedModel(llm.model.layers[i].block_sparse_moe.experts[j].w3, '/home/lz/On-the-Fly_MoE_Inference/quantize/saved/', layerid=i, expertid=j).to(llmdevice)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一次计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def profle_svdllm(name, model, calib_loader, dev):\n",
    "    # model.to(dev)\n",
    "    if \"llama\" in name or \"mixtral\" in name or \"vicuna\" in name:\n",
    "        layers = model.model.layers\n",
    "    print(\"Start obtaining the whitening matrix...\")\n",
    "    def hook(module, input, output):\n",
    "        inp = input[0].detach().float()\n",
    "        if inp.dim() == 2:   # for opt\n",
    "            inp = inp.unsqueeze(0)\n",
    "        adds = torch.matmul(inp.transpose(1,2), inp)\n",
    "        adds_sum = torch.sum(adds, dim=0)\n",
    "        module.raw_scaling_diag_matrix += adds_sum\n",
    "        del inp, adds, adds_sum\n",
    "        torch.cuda.empty_cache()\n",
    "    for name, module in model.named_modules():\n",
    "        if \"w3\" in name:\n",
    "            # print(name)\n",
    "            module.raw_scaling_diag_matrix = 0\n",
    "            module.register_forward_hook(hook)\n",
    "            \n",
    "    for batch in tqdm(calib_loader):\n",
    "        inputs = batch['input_ids'].to(llm.device)\n",
    "        model(inputs)\n",
    "    for name, module in model.named_modules():\n",
    "        if \"w3\" in name:\n",
    "            module._forward_hooks.clear()\n",
    "            # print(module.raw_scaling_diag_matrix)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    profiling_mat = {}\n",
    "    print(\"Start Cholesky Decomposition...\")\n",
    "    \n",
    "    layer_profile = {}\n",
    "    for name, module in model.named_modules():\n",
    "        if \"w3\" in name:\n",
    "            covariance = module.raw_scaling_diag_matrix.double().to(dev)\n",
    "            if not torch.allclose(covariance, covariance.t(), atol=1e-6):\n",
    "                raise ValueError(\"Covariance matrix is not symmetric.\")\n",
    "                    # Perform eigen decomposition\n",
    "            Lambda, Q = torch.linalg.eigh(covariance, UPLO='U')\n",
    "            if torch.isnan(Lambda).any() or torch.isinf(Lambda).any():\n",
    "                raise ValueError(\"Lambda contains NaN or Inf values.\")\n",
    "\n",
    "            # 检查 Lambda 是否包含负值\n",
    "            if (Lambda < 0).any():\n",
    "                print(\"Lambda contains negative values. Clamping to zero.\")\n",
    "                eigenvalues = torch.linalg.eigvalsh(covariance)\n",
    "                covariance += (- eigenvalues[0] + 2e-6) * torch.eye(covariance.shape[0]).cuda()\n",
    "                Lambda, Q = torch.linalg.eigh(covariance, UPLO='U')\n",
    "                print(f\"Lambda min: {Lambda.min().item()}, Lambda max: {Lambda.max().item()}\")\n",
    "            # 现在进行平方根操作\n",
    "            Lambda_diag = torch.diag(torch.sqrt(Lambda))\n",
    "            # Sort eigenvalues and eigenvectors in descending order\n",
    "            indices = torch.argsort(Lambda, descending=True)\n",
    "            Lambda = Lambda[indices]\n",
    "            Q = Q[:, indices]\n",
    "\n",
    "            # Compute Q_prime = Q * sqrt(Lambda)\n",
    "            Lambda_diag = torch.diag(torch.sqrt(Lambda))\n",
    "            Q_prime = torch.matmul(Q, Lambda_diag)\n",
    "            layer_profile[name] = Q_prime.cpu()\n",
    "            profiling_mat[name] = layer_profile\n",
    "    return profiling_mat\n",
    "\n",
    "profiling_mat=profle_svdllm(\"mixtral\", llm, dataloader, \"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompensatedModel(torch.nn.Module):\n",
    "    def __init__(self, model, B_prime, A):\n",
    "        super(CompensatedModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.B_prime = torch.nn.Parameter(torch.tensor(B_prime)).to(torch.float16)\n",
    "        self.A = torch.nn.Parameter(torch.tensor(A)).to(torch.float16)\n",
    "        # print(self.A.shape,self.B_prime.shape)\n",
    "    def forward(self, input_ids):\n",
    "        outputs = self.model(input_ids)\n",
    "        residual = (input_ids @ self.A.T) @ self.B_prime.T\n",
    "        torch.add(outputs, residual, out = outputs)\n",
    "    \n",
    "        return outputs\n",
    "    \n",
    "for i in range(32):\n",
    "    print(f\"Layer {i} done...\")\n",
    "    for j in range(8):\n",
    "        llmdevice = llm.model.layers[i].block_sparse_moe.experts[j].w3.device\n",
    "        Delta_W = llm_base.model.layers[i].block_sparse_moe.experts[j].w3.weight.to(llmdevice) - llm.model.layers[i].block_sparse_moe.experts[j].w3.dequantize()\n",
    "        Q_prime = profiling_mat[f\"model.layers.{i}.block_sparse_moe.experts.{j}.w3\"][f\"model.layers.{i}.block_sparse_moe.experts.{j}.w3\"].cuda().float()\n",
    "        Delta_W_prime =  Delta_W.to(torch.float32).to(llmdevice) @ Q_prime.to(torch.float32).to(llmdevice)\n",
    "        llm_base.model.layers[i].block_sparse_moe.experts[j].w3.cpu()\n",
    "        # 步骤5: 进行SVD分解并取前r个奇异值\n",
    "        rank = 256  # 设置 desired rank\n",
    "        U_prime, Sigma_prime, V_prime = torch.linalg.svd(Delta_W_prime, full_matrices=False)\n",
    "        U_prime = U_prime[:, :rank]\n",
    "        Sigma_prime = Sigma_prime[:rank]\n",
    "        V_prime = V_prime[:rank, :]\n",
    "\n",
    "        B_prime = U_prime @ torch.diag(Sigma_prime)\n",
    "        A_prime = V_prime\n",
    "\n",
    "        # 步骤6: 投影回原空间\n",
    "        A = A_prime.to(llmdevice) @ torch.linalg.inv(Q_prime).to(llmdevice)\n",
    "        llm.model.layers[i].block_sparse_moe.experts[j].w3 = CompensatedModel(llm.model.layers[i].block_sparse_moe.experts[j].w3, B_prime, A).to(llmdevice)\n",
    "        torch.save(B_prime, f\"./saved/B_prime_{i}_{j}.pt\")\n",
    "        torch.save(A, f\"./saved/A_{i}_{j}.pt\")\n",
    "\n",
    "del llm_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set profile_threshold to True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "datasets = torch.load('../saving/threshold/chess/datasets.pt')\n",
    "set_profile_mode(True)\n",
    "def get_batch(data, batch_size, block_size):\n",
    "    start_idxs = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in start_idxs])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in start_idxs])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "sparsity_level = 0.9\n",
    "# device = 'cuda:1'\n",
    "device_2 = 'cpu'\n",
    "avg_loss = 0.0\n",
    "n_batch = 2\n",
    "# accum_steps = 4 \n",
    "accum_steps = 2\n",
    "batch_size = 1\n",
    "block_size = 2048\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model = llm\n",
    "\n",
    "n_layers = len(model.model.layers)\n",
    "n_experts = len(model.model.layers[0].block_sparse_moe.experts)\n",
    "\n",
    "up_proj_states_thresholds = [torch.zeros([n_experts,]) for _ in range(n_layers)]\n",
    "gate_proj_states_mean_squares = [[torch.zeros(model.config.intermediate_size) for _ in range(n_experts)] for _ in range(n_layers)]\n",
    "\n",
    "up_states = [[torch.zeros([accum_steps * batch_size * block_size //2, model.config.intermediate_size]) for _ in range(n_experts)] for _ in range(n_layers)]\n",
    "gate_states = [[torch.zeros([accum_steps * batch_size * block_size //2, model.config.intermediate_size]) for _ in range(n_experts)] for _ in range(n_layers)]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step in range(n_batch // accum_steps):\n",
    "        print(step * accum_steps)\n",
    "        all_counts = [0 for _ in range(n_layers * n_experts)]\n",
    "        for batch_idx in range(accum_steps):\n",
    "            # print('batch_idx:', batch_idx)\n",
    "            inputs, labels = get_batch(datasets['validation'], batch_size, block_size)\n",
    "            inputs = inputs.cuda()\n",
    "            outputs = model(inputs, labels=inputs)\n",
    "            avg_loss = avg_loss + outputs.loss / n_batch\n",
    "\n",
    "            for layer_idx in range(n_layers):\n",
    "                for expert_idx in range(n_experts):\n",
    "                    counts = all_counts[layer_idx * n_experts + expert_idx]\n",
    "\n",
    "                    states = model.model.layers[layer_idx].block_sparse_moe.experts[expert_idx].up_proj_states.reshape(-1, model.config.intermediate_size)\n",
    "                    cur_counts = states.size(0)\n",
    "                    # print('counts and cur_counts:',counts, cur_counts)\n",
    "                    # print(states.size())\n",
    "                    # print(up_states[layer_idx][expert_idx][counts : counts+cur_counts, :].size())\n",
    "                    up_states[layer_idx][expert_idx][counts : counts+cur_counts, :] = states\n",
    "\n",
    "                    states = model.model.layers[layer_idx].block_sparse_moe.experts[expert_idx].gate_proj_states.reshape(-1, model.config.intermediate_size)\n",
    "                    gate_states[layer_idx][expert_idx][counts : counts+cur_counts, :] = states\n",
    "                    # counts += cur_counts\n",
    "                    all_counts[layer_idx * n_experts + expert_idx] += cur_counts\n",
    "\n",
    "        for layer_idx in range(n_layers):   \n",
    "            for expert_idx in range(n_experts):\n",
    "                # print('layer_idx:', layer_idx, 'expert_idx:', expert_idx)\n",
    "                useful_num = all_counts[layer_idx * n_experts + expert_idx]\n",
    "                topk_num = int(useful_num * model.config.intermediate_size * sparsity_level)\n",
    "                up_proj_states_thresholds[layer_idx][expert_idx] += up_states[layer_idx][expert_idx][0:useful_num,:].to(device_2).abs().flatten().kthvalue(topk_num).values.to('cpu')\n",
    "                gate_proj_states_mean_squares[layer_idx][expert_idx] += (torch.sum(gate_states[layer_idx][expert_idx][0:useful_num,:].to(device_2) ** 2, dim=0).to('cpu') / useful_num).to('cpu')\n",
    "\n",
    "for layer_idx in range(n_layers):\n",
    "    for expert_idx in range(n_experts):\n",
    "        gate_proj_states_mean_squares[layer_idx][expert_idx] /= n_batch // accum_steps\n",
    "        up_proj_states_thresholds[layer_idx][expert_idx] /= n_batch // accum_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "importance_thresholds = [torch.zeros([n_experts,]) for _ in range(n_layers)]\n",
    "up_proj_states_thresholds_2 = [[torch.zeros(model.config.intermediate_size) for _ in range(n_experts)] for _ in range(n_layers)]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step in range(n_batch // accum_steps):\n",
    "        print(step * accum_steps)\n",
    "        all_counts = [0 for _ in range(n_layers * n_experts)]\n",
    "        for batch_idx in range(accum_steps):\n",
    "            inputs, labels = get_batch(datasets['validation'], batch_size, block_size)\n",
    "            inputs = inputs.cuda()\n",
    "            outputs = model(inputs, labels=inputs)\n",
    "            avg_loss = avg_loss + outputs.loss / n_batch\n",
    "\n",
    "            for layer_idx in range(n_layers):\n",
    "                for expert_idx in range(n_experts):\n",
    "                    counts = all_counts[layer_idx * n_experts + expert_idx]\n",
    "                    states = model.model.layers[layer_idx].block_sparse_moe.experts[expert_idx].up_proj_states.reshape(-1, states.size(-1))\n",
    "                    cur_counts = states.size(0)\n",
    "                    up_states[layer_idx][expert_idx][counts:cur_counts+counts, :] = states\n",
    "                    # counts += cur_counts\n",
    "                    all_counts[layer_idx * n_experts + expert_idx] += cur_counts\n",
    "                \n",
    "        for layer_idx in range(n_layers):   \n",
    "            for expert_idx in range(n_experts):\n",
    "                useful_num = all_counts[layer_idx * n_experts + expert_idx]\n",
    "                importance_scores = up_states[layer_idx][expert_idx][:useful_num,:] ** 2 * gate_proj_states_mean_squares[layer_idx][expert_idx]\n",
    "                importance_thresholds[layer_idx][expert_idx] += importance_scores.to(device_2).flatten().kthvalue(int(importance_scores.numel() * sparsity_level)).values.to('cpu')\n",
    "\n",
    "for layer_idx in range(n_layers):\n",
    "    for expert_idx in range(n_experts):\n",
    "        importance_thresholds[layer_idx][expert_idx] /= n_batch // accum_steps\n",
    "        up_proj_states_thresholds_2[layer_idx][expert_idx] = (importance_thresholds[layer_idx][expert_idx].expand_as(up_proj_states_thresholds_2[layer_idx][expert_idx]) / gate_proj_states_mean_squares[layer_idx][expert_idx]) ** 0.5\n",
    "\n",
    "thresholds = {'up_proj_states_thresholds': up_proj_states_thresholds, 'up_proj_states_thresholds_2': up_proj_states_thresholds_2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save in: ./threshold/c4_mixtral_up\n"
     ]
    }
   ],
   "source": [
    "save_path = './threshold/c4_mixtral_up'\n",
    "\n",
    "sp = str(sparsity_level).replace('.', '_')\n",
    "print('save in:', save_path)\n",
    "torch.save(thresholds, f'{save_path}/thresholds_{sp}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:57<00:00,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 3.0307707452774046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 计算评估损失\n",
    "total_loss = 0.0\n",
    "num_batches = 0\n",
    "\n",
    "for batch in tqdm(dataloader):\n",
    "    input_ids = batch['input_ids'].to(llm.device)\n",
    "    attention_mask = batch['attention_mask'].to(llm.device)\n",
    "    labels = batch['labels'].to(llm.device)\n",
    "    \n",
    "    # 禁用梯度计算\n",
    "    with torch.no_grad():\n",
    "        outputs = llm(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        if num_batches % 100 == 0:\n",
    "            print(f\"[{num_batches}], Eval Loss: {total_loss / (num_batches)}\")\n",
    "\n",
    "# 计算平均损失\n",
    "eval_loss = total_loss / num_batches\n",
    "print(f\"Eval Loss: {eval_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0 expert 0 ratio: 0.2167\n",
      "layer 0 expert 1 ratio: 0.1075\n",
      "layer 0 expert 2 ratio: 0.0871\n",
      "layer 0 expert 3 ratio: 0.2104\n",
      "layer 0 expert 4 ratio: 0.2017\n",
      "layer 0 expert 5 ratio: 0.2117\n",
      "layer 0 expert 6 ratio: 0.2164\n",
      "layer 0 expert 7 ratio: 0.2175\n",
      "layer 1 expert 0 ratio: 0.2209\n",
      "layer 1 expert 1 ratio: 0.2114\n",
      "layer 1 expert 2 ratio: 0.3353\n",
      "layer 1 expert 3 ratio: 0.2220\n",
      "layer 1 expert 4 ratio: 0.2407\n",
      "layer 1 expert 5 ratio: 0.3093\n",
      "layer 1 expert 6 ratio: 0.2343\n",
      "layer 1 expert 7 ratio: 0.2426\n",
      "layer 2 expert 0 ratio: 0.2640\n",
      "layer 2 expert 1 ratio: 0.2186\n",
      "layer 2 expert 2 ratio: 0.3159\n",
      "layer 2 expert 3 ratio: 0.3497\n",
      "layer 2 expert 4 ratio: 0.2149\n",
      "layer 2 expert 5 ratio: 0.2662\n",
      "layer 2 expert 6 ratio: 0.2368\n",
      "layer 2 expert 7 ratio: 0.2575\n",
      "layer 3 expert 0 ratio: 0.3555\n",
      "layer 3 expert 1 ratio: 0.2156\n",
      "layer 3 expert 2 ratio: 0.2297\n",
      "layer 3 expert 3 ratio: 0.2700\n",
      "layer 3 expert 4 ratio: 0.2890\n",
      "layer 3 expert 5 ratio: 0.2718\n",
      "layer 3 expert 6 ratio: 0.2421\n",
      "layer 3 expert 7 ratio: 0.3535\n",
      "layer 4 expert 0 ratio: 0.3492\n",
      "layer 4 expert 1 ratio: 0.2416\n",
      "layer 4 expert 2 ratio: 0.2716\n",
      "layer 4 expert 3 ratio: 0.3119\n",
      "layer 4 expert 4 ratio: 0.2632\n",
      "layer 4 expert 5 ratio: 0.3584\n",
      "layer 4 expert 6 ratio: 0.2183\n",
      "layer 4 expert 7 ratio: 0.3338\n",
      "layer 5 expert 0 ratio: 0.2305\n",
      "layer 5 expert 1 ratio: 0.2402\n",
      "layer 5 expert 2 ratio: 0.2377\n",
      "layer 5 expert 3 ratio: 0.2541\n",
      "layer 5 expert 4 ratio: 0.2931\n",
      "layer 5 expert 5 ratio: 0.2606\n",
      "layer 5 expert 6 ratio: 0.2186\n",
      "layer 5 expert 7 ratio: 0.2987\n",
      "layer 6 expert 0 ratio: 0.2579\n",
      "layer 6 expert 1 ratio: 0.2437\n",
      "layer 6 expert 2 ratio: 0.2908\n",
      "layer 6 expert 3 ratio: 0.2551\n",
      "layer 6 expert 4 ratio: 0.2546\n",
      "layer 6 expert 5 ratio: 0.2885\n",
      "layer 6 expert 6 ratio: 0.2213\n",
      "layer 6 expert 7 ratio: 0.2433\n",
      "layer 7 expert 0 ratio: 0.2440\n",
      "layer 7 expert 1 ratio: 0.2280\n",
      "layer 7 expert 2 ratio: 0.2376\n",
      "layer 7 expert 3 ratio: 0.2595\n",
      "layer 7 expert 4 ratio: 0.2468\n",
      "layer 7 expert 5 ratio: 0.2363\n",
      "layer 7 expert 6 ratio: 0.2458\n",
      "layer 7 expert 7 ratio: 0.2173\n",
      "layer 8 expert 0 ratio: 0.2298\n",
      "layer 8 expert 1 ratio: 0.2468\n",
      "layer 8 expert 2 ratio: 0.2369\n",
      "layer 8 expert 3 ratio: 0.2416\n",
      "layer 8 expert 4 ratio: 0.2393\n",
      "layer 8 expert 5 ratio: 0.2341\n",
      "layer 8 expert 6 ratio: 0.2378\n",
      "layer 8 expert 7 ratio: 0.2419\n",
      "layer 9 expert 0 ratio: 0.2343\n",
      "layer 9 expert 1 ratio: 0.2381\n",
      "layer 9 expert 2 ratio: 0.2315\n",
      "layer 9 expert 3 ratio: 0.2411\n",
      "layer 9 expert 4 ratio: 0.2284\n",
      "layer 9 expert 5 ratio: 0.2391\n",
      "layer 9 expert 6 ratio: 0.2319\n",
      "layer 9 expert 7 ratio: 0.2369\n",
      "layer 10 expert 0 ratio: 0.2271\n",
      "layer 10 expert 1 ratio: 0.2263\n",
      "layer 10 expert 2 ratio: 0.2302\n",
      "layer 10 expert 3 ratio: 0.2309\n",
      "layer 10 expert 4 ratio: 0.2269\n",
      "layer 10 expert 5 ratio: 0.2212\n",
      "layer 10 expert 6 ratio: 0.2255\n",
      "layer 10 expert 7 ratio: 0.2308\n",
      "layer 11 expert 0 ratio: 0.2250\n",
      "layer 11 expert 1 ratio: 0.2425\n",
      "layer 11 expert 2 ratio: 0.2303\n",
      "layer 11 expert 3 ratio: 0.2016\n",
      "layer 11 expert 4 ratio: 0.2332\n",
      "layer 11 expert 5 ratio: 0.2239\n",
      "layer 11 expert 6 ratio: 0.2303\n",
      "layer 11 expert 7 ratio: 0.2280\n",
      "layer 12 expert 0 ratio: 0.2178\n",
      "layer 12 expert 1 ratio: 0.2241\n",
      "layer 12 expert 2 ratio: 0.2268\n",
      "layer 12 expert 3 ratio: 0.2299\n",
      "layer 12 expert 4 ratio: 0.2261\n",
      "layer 12 expert 5 ratio: 0.2139\n",
      "layer 12 expert 6 ratio: 0.2301\n",
      "layer 12 expert 7 ratio: 0.2126\n",
      "layer 13 expert 0 ratio: 0.2233\n",
      "layer 13 expert 1 ratio: 0.2344\n",
      "layer 13 expert 2 ratio: 0.2324\n",
      "layer 13 expert 3 ratio: 0.2167\n",
      "layer 13 expert 4 ratio: 0.2211\n",
      "layer 13 expert 5 ratio: 0.2216\n",
      "layer 13 expert 6 ratio: 0.2221\n",
      "layer 13 expert 7 ratio: 0.2242\n",
      "layer 14 expert 0 ratio: 0.2204\n",
      "layer 14 expert 1 ratio: 0.2333\n",
      "layer 14 expert 2 ratio: 0.2187\n",
      "layer 14 expert 3 ratio: 0.2223\n",
      "layer 14 expert 4 ratio: 0.2266\n",
      "layer 14 expert 5 ratio: 0.2292\n",
      "layer 14 expert 6 ratio: 0.2242\n",
      "layer 14 expert 7 ratio: 0.2272\n",
      "layer 15 expert 0 ratio: 0.2323\n",
      "layer 15 expert 1 ratio: 0.2298\n",
      "layer 15 expert 2 ratio: 0.2195\n",
      "layer 15 expert 3 ratio: 0.2271\n",
      "layer 15 expert 4 ratio: 0.2193\n",
      "layer 15 expert 5 ratio: 0.2357\n",
      "layer 15 expert 6 ratio: 0.2274\n",
      "layer 15 expert 7 ratio: 0.2184\n",
      "layer 16 expert 0 ratio: 0.2153\n",
      "layer 16 expert 1 ratio: 0.2198\n",
      "layer 16 expert 2 ratio: 0.2185\n",
      "layer 16 expert 3 ratio: 0.2194\n",
      "layer 16 expert 4 ratio: 0.2257\n",
      "layer 16 expert 5 ratio: 0.2283\n",
      "layer 16 expert 6 ratio: 0.2221\n",
      "layer 16 expert 7 ratio: 0.2146\n",
      "layer 17 expert 0 ratio: 0.1951\n",
      "layer 17 expert 1 ratio: 0.2199\n",
      "layer 17 expert 2 ratio: 0.2179\n",
      "layer 17 expert 3 ratio: 0.2173\n",
      "layer 17 expert 4 ratio: 0.2267\n",
      "layer 17 expert 5 ratio: 0.2109\n",
      "layer 17 expert 6 ratio: 0.2225\n",
      "layer 17 expert 7 ratio: 0.2211\n",
      "layer 18 expert 0 ratio: 0.2289\n",
      "layer 18 expert 1 ratio: 0.2247\n",
      "layer 18 expert 2 ratio: 0.2228\n",
      "layer 18 expert 3 ratio: 0.2197\n",
      "layer 18 expert 4 ratio: 0.2201\n",
      "layer 18 expert 5 ratio: 0.2226\n",
      "layer 18 expert 6 ratio: 0.2081\n",
      "layer 18 expert 7 ratio: 0.2117\n",
      "layer 19 expert 0 ratio: 0.2000\n",
      "layer 19 expert 1 ratio: 0.2034\n",
      "layer 19 expert 2 ratio: 0.1949\n",
      "layer 19 expert 3 ratio: 0.1997\n",
      "layer 19 expert 4 ratio: 0.2208\n",
      "layer 19 expert 5 ratio: 0.2027\n",
      "layer 19 expert 6 ratio: 0.2111\n",
      "layer 19 expert 7 ratio: 0.2047\n",
      "layer 20 expert 0 ratio: 0.2178\n",
      "layer 20 expert 1 ratio: 0.1900\n",
      "layer 20 expert 2 ratio: 0.2077\n",
      "layer 20 expert 3 ratio: 0.1725\n",
      "layer 20 expert 4 ratio: 0.2186\n",
      "layer 20 expert 5 ratio: 0.2004\n",
      "layer 20 expert 6 ratio: 0.1913\n",
      "layer 20 expert 7 ratio: 0.2018\n",
      "layer 21 expert 0 ratio: 0.1862\n",
      "layer 21 expert 1 ratio: 0.1916\n",
      "layer 21 expert 2 ratio: 0.2224\n",
      "layer 21 expert 3 ratio: 0.2075\n",
      "layer 21 expert 4 ratio: 0.1779\n",
      "layer 21 expert 5 ratio: 0.1913\n",
      "layer 21 expert 6 ratio: 0.2235\n",
      "layer 21 expert 7 ratio: 0.1701\n",
      "layer 22 expert 0 ratio: 0.2049\n",
      "layer 22 expert 1 ratio: 0.2042\n",
      "layer 22 expert 2 ratio: 0.1815\n",
      "layer 22 expert 3 ratio: 0.2068\n",
      "layer 22 expert 4 ratio: 0.1681\n",
      "layer 22 expert 5 ratio: 0.2089\n",
      "layer 22 expert 6 ratio: 0.1860\n",
      "layer 22 expert 7 ratio: 0.2005\n",
      "layer 23 expert 0 ratio: 0.1930\n",
      "layer 23 expert 1 ratio: 0.1929\n",
      "layer 23 expert 2 ratio: 0.1583\n",
      "layer 23 expert 3 ratio: 0.1872\n",
      "layer 23 expert 4 ratio: 0.2176\n",
      "layer 23 expert 5 ratio: 0.1622\n",
      "layer 23 expert 6 ratio: 0.1987\n",
      "layer 23 expert 7 ratio: 0.1999\n",
      "layer 24 expert 0 ratio: 0.1848\n",
      "layer 24 expert 1 ratio: 0.1713\n",
      "layer 24 expert 2 ratio: 0.1695\n",
      "layer 24 expert 3 ratio: 0.1727\n",
      "layer 24 expert 4 ratio: 0.2012\n",
      "layer 24 expert 5 ratio: 0.1837\n",
      "layer 24 expert 6 ratio: 0.2015\n",
      "layer 24 expert 7 ratio: 0.1827\n",
      "layer 25 expert 0 ratio: 0.1668\n",
      "layer 25 expert 1 ratio: 0.1966\n",
      "layer 25 expert 2 ratio: 0.2055\n",
      "layer 25 expert 3 ratio: 0.2026\n",
      "layer 25 expert 4 ratio: 0.1772\n",
      "layer 25 expert 5 ratio: 0.2016\n",
      "layer 25 expert 6 ratio: 0.1646\n",
      "layer 25 expert 7 ratio: 0.1826\n",
      "layer 26 expert 0 ratio: 0.1745\n",
      "layer 26 expert 1 ratio: 0.2131\n",
      "layer 26 expert 2 ratio: 0.1526\n",
      "layer 26 expert 3 ratio: 0.2053\n",
      "layer 26 expert 4 ratio: 0.1692\n",
      "layer 26 expert 5 ratio: 0.1764\n",
      "layer 26 expert 6 ratio: 0.1643\n",
      "layer 26 expert 7 ratio: 0.2197\n",
      "layer 27 expert 0 ratio: 0.1578\n",
      "layer 27 expert 1 ratio: 0.1898\n",
      "layer 27 expert 2 ratio: 0.1826\n",
      "layer 27 expert 3 ratio: 0.1639\n",
      "layer 27 expert 4 ratio: 0.1610\n",
      "layer 27 expert 5 ratio: 0.1995\n",
      "layer 27 expert 6 ratio: 0.2002\n",
      "layer 27 expert 7 ratio: 0.2148\n",
      "layer 28 expert 0 ratio: 0.1564\n",
      "layer 28 expert 1 ratio: 0.1879\n",
      "layer 28 expert 2 ratio: 0.1810\n",
      "layer 28 expert 3 ratio: 0.1909\n",
      "layer 28 expert 4 ratio: 0.2137\n",
      "layer 28 expert 5 ratio: 0.1635\n",
      "layer 28 expert 6 ratio: 0.2119\n",
      "layer 28 expert 7 ratio: 0.1742\n",
      "layer 29 expert 0 ratio: 0.1979\n",
      "layer 29 expert 1 ratio: 0.1816\n",
      "layer 29 expert 2 ratio: 0.1984\n",
      "layer 29 expert 3 ratio: 0.2036\n",
      "layer 29 expert 4 ratio: 0.1918\n",
      "layer 29 expert 5 ratio: 0.1726\n",
      "layer 29 expert 6 ratio: 0.1919\n",
      "layer 29 expert 7 ratio: 0.1467\n",
      "layer 30 expert 0 ratio: 0.2143\n",
      "layer 30 expert 1 ratio: 0.2203\n",
      "layer 30 expert 2 ratio: 0.2172\n",
      "layer 30 expert 3 ratio: 0.2179\n",
      "layer 30 expert 4 ratio: 0.2201\n",
      "layer 30 expert 5 ratio: 0.1934\n",
      "layer 30 expert 6 ratio: 0.1919\n",
      "layer 30 expert 7 ratio: 0.1765\n",
      "layer 31 expert 0 ratio: 0.2272\n",
      "layer 31 expert 1 ratio: 0.2044\n",
      "layer 31 expert 2 ratio: 0.2166\n",
      "layer 31 expert 3 ratio: 0.2298\n",
      "layer 31 expert 4 ratio: 0.2461\n",
      "layer 31 expert 5 ratio: 0.2361\n",
      "layer 31 expert 6 ratio: 0.2506\n",
      "layer 31 expert 7 ratio: 0.2367\n"
     ]
    }
   ],
   "source": [
    "for layerid in range(32):\n",
    "    for expertid in range(8):\n",
    "        llm.model.layers[layerid].block_sparse_moe.experts[expertid].print_ratio()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_ENDPOINT\"]=\"https://hf-mirror.com\"\n",
    "\n",
    "\n",
    "import lm_eval\n",
    "from lm_eval.models.huggingface import HFLM\n",
    "from lm_eval import evaluator\n",
    "del dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-03:13:11:00,185 WARNING  [huggingface.py:121] `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "2025-01-03:13:11:00,251 WARNING  [huggingface.py:349] Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "2025-01-03:13:11:00,259 INFO     [evaluator.py:152] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2025-01-03:13:11:00,261 INFO     [evaluator.py:203] Using pre-initialized model\n",
      "Using the latest cached version of the module from /home/lz/.cache/huggingface/modules/datasets_modules/datasets/winogrande/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2 (last modified on Thu Jan  2 22:35:53 2025) since it couldn't be found locally at winogrande, or remotely on the Hugging Face Hub.\n",
      "2025-01-03:13:13:13,531 WARNING  [load.py:1407] Using the latest cached version of the module from /home/lz/.cache/huggingface/modules/datasets_modules/datasets/winogrande/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2 (last modified on Thu Jan  2 22:35:53 2025) since it couldn't be found locally at winogrande, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "def evaluate(task_name_list, model, tokenizer, num_fewshot, device):\n",
    "    hflm = HFLM(pretrained=llm, tokenizer=tokenizer)\n",
    "    results = evaluator.simple_evaluate(\n",
    "    model=hflm,\n",
    "    tasks=task_name_list,\n",
    "    num_fewshot=num_fewshot)\n",
    "    print(results['results'])\n",
    "\n",
    "\n",
    "\n",
    "# triviaqa\n",
    "task_list=['winogrande','sciq','openbookqa','arc_challenge','arc_easy']\n",
    "# 'boolq',\n",
    "# task_list=['truthfulqa_gen','triviaqa_gen']\n",
    "evaluate(task_list, llm, tokenizer, 0, \"cuda\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
