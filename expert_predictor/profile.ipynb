{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model /home/bcds/venv/dilab/Mixtral-8x7B-v0.1\n",
      "Set profile_threshold to False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/On-the-Fly_MoE_Inference/expert_predictor/modeling_mixtral.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  up_th = torch.load(threshold_path, map_location='cuda')[\"up_proj_states_thresholds_2\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thresholds loaded from /home/bcds/On-the-Fly_MoE_Inference/saving/threshold/c4_mixtral_up/thresholds_0_8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:28<00:00,  1.48s/it]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from modeling_mixtral import MixtralForCausalLM, set_profile_mode, load_thresholds\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3,4\"\n",
    "\n",
    "def _load_model(model_name, threshold_path, device_map):\n",
    "    print(f\"Loading model {model_name}\")\n",
    "    ## 开启稀疏模式\n",
    "    set_profile_mode(False)\n",
    "    load_thresholds(f'{threshold_path}/thresholds_0_8.pt', use_average=True)\n",
    "\n",
    "    model = MixtralForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        use_cache=True,\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    return model, tokenizer\n",
    "\n",
    "model_name = 'mixtral'\n",
    "threshold_path_name = 'chess_up_threshold'\n",
    "with open('../path.json', 'r') as f:\n",
    "    path = json.load(f)\n",
    "    model_name = path['mixtral']\n",
    "    threshold_path = path[threshold_path_name]\n",
    "\n",
    "with open('../quantize/device_map_1.json', 'r') as f:\n",
    "    device_map = json.load(f)\n",
    "model, tokenizer = _load_model(model_name, threshold_path, device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 32):\n",
    "    print(\"layer \", i)\n",
    "    gate_router = model.model.layers[i].block_sparse_moe.gate\n",
    "    torch.save(gate_router.weight, f\"./router/{i}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1784045/954874683.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  datasets = torch.load('../saving/threshold/chess/datasets.pt')\n"
     ]
    }
   ],
   "source": [
    "datasets = torch.load('../saving/threshold/chess/datasets.pt')\n",
    "import torch\n",
    "import numpy as np\n",
    "def get_batch(data, batch_size, block_size):\n",
    "    start_idxs = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in start_idxs])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in start_idxs])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存激活值和专家路由的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "sparsity_level = 0.8\n",
    "# device = 'cuda:1'\n",
    "device_2 = 'cpu'\n",
    "avg_loss = 0.0\n",
    "n_batch = 64 * 20\n",
    "# n_batch = 2\n",
    "# accum_steps = 4 \n",
    "accum_steps = 64\n",
    "batch_size = 1\n",
    "block_size = 2048\n",
    "torch.manual_seed(42)\n",
    "n_layers = len(model.model.layers)\n",
    "n_experts = len(model.model.layers[0].block_sparse_moe.experts)\n",
    "split = 'train'\n",
    "\n",
    "with torch.no_grad():\n",
    "    # for step in range(n_batch // accum_steps):\n",
    "    for step in trange(n_batch // accum_steps):\n",
    "        # print(step * accum_steps)\n",
    "        for batch_idx in range(accum_steps):\n",
    "            # print('batch_idx:', batch_idx)\n",
    "            inputs, labels = get_batch(datasets[split], batch_size, block_size)\n",
    "            inputs = inputs.cuda()\n",
    "            outputs = model(inputs, labels=inputs)\n",
    "            avg_loss = avg_loss + outputs.loss / n_batch\n",
    "\n",
    "        for layer_idx in range(1, 32):\n",
    "            if layer_idx < 4:\n",
    "                d = list(zip(model.model.layers[layer_idx-1].block_sparse_moe.activations, model.model.layers[layer_idx].block_sparse_moe.gate_logits))\n",
    "                \n",
    "                torch.save(d,f'merge/a2ef_{layer_idx}_{step}.pth')\n",
    "                print(f'saving merge/a2ef_{layer_idx}_{step}.pth done')\n",
    "            model.model.layers[layer_idx-1].block_sparse_moe.activations.clear()\n",
    "            model.model.layers[layer_idx].block_sparse_moe.gate_logits.clear()\n",
    "            ### 清除缓存\n",
    "            torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 专家预测器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler, autocast # 用于混合精度训练\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3,4\"\n",
    "\n",
    "def top_k_position_accuracy_unordered(output, target, k=1):\n",
    "    \"\"\"Compute the accuracy based on the intersection of top-k values between output and target,\n",
    "       regardless of their order.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # 获取 output 和 target 中 top-k 最大值的索引\n",
    "        _, topk_pred_indices = output.topk(k, 1, True)\n",
    "        _, topk_target_indices = target.topk(k, 1, True)\n",
    "        # 初始化批次的正确计数\n",
    "        batch_size = output.size(0)\n",
    "        correct_counts = 0\n",
    "        \n",
    "        # 检查每个样本的预测top-k是否包含在真实的top-k中\n",
    "        for i in range(batch_size):\n",
    "            # 将预测和目标的top-k索引转换为集合\n",
    "            set_pred = set(topk_pred_indices[i].tolist())\n",
    "            set_target = set(topk_target_indices[i].tolist())\n",
    "            \n",
    "            # 计算交集\n",
    "            intersection = set_pred.intersection(set_target)\n",
    "            \n",
    "            # 计算正确的预测个数\n",
    "            correct_counts = correct_counts+len(intersection)\n",
    "        \n",
    "        # 计算平均正确率\n",
    "        return correct_counts,batch_size*k\n",
    "\n",
    "def eval_model(model, val_loader,):\n",
    "    # Example validation loop\n",
    "    model.eval()\n",
    "    total_topk_accuracy_1 = 0\n",
    "    total_topk_accuracy_2 = 0\n",
    "    cont=0\n",
    "    len1=0\n",
    "    len2=0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(\"cuda\"), targets.to(\"cuda\")\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "            # 计算 top-K 准确率（不考虑顺序）\n",
    "            topk_accuracy_1 = top_k_position_accuracy_unordered(outputs, targets, k=1)\n",
    "            topk_accuracy_2 = top_k_position_accuracy_unordered(outputs, targets, k=2)\n",
    "            total_topk_accuracy_1 += topk_accuracy_1[0]\n",
    "            total_topk_accuracy_2 += topk_accuracy_2[0]\n",
    "            len1+= topk_accuracy_1[1]\n",
    "            len2+= topk_accuracy_2[1]   \n",
    "        avg_topk_accuracy_1 = total_topk_accuracy_1 / len1\n",
    "        avg_topk_accuracy_2 = total_topk_accuracy_2 / len2\n",
    "        # print(len2)\n",
    "        print(f'Top-{1} Accuracy: {avg_topk_accuracy_1:.4f}', f'Top-{2} Accuracy (unordered): {avg_topk_accuracy_2:.4f}')\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, file_paths):\n",
    "        # 加载数据\n",
    "        self.data = []\n",
    "        \n",
    "        # 遍历文件路径列表，加载每个文件\n",
    "        for file_path in file_paths:\n",
    "            # 加载当前文件的数据\n",
    "            file_data = torch.load(file_path)\n",
    "            # 将当前文件的数据追加到总数据列表中\n",
    "            self.data.extend(file_data)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.data[idx]\n",
    "        return x.detach().clone(), y.detach().clone()\n",
    "\n",
    "class RouterModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, layer_id):\n",
    "        super(RouterModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, output_dim)\n",
    "        loaded_weights = torch.load(f\"./router/{i}.pt\")\n",
    "        with torch.no_grad():  # 禁用梯度计算\n",
    "            self.linear1.weight.copy_(loaded_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear1(x)\n",
    "\n",
    "def sparse_row(row, topk=2, use_abs=False):\n",
    "    \"\"\"\n",
    "    对每一行保留 topk 个最大值的索引为 1，其余为 0。\n",
    "    \n",
    "    参数:\n",
    "        row (torch.Tensor): 输入的一行数据。\n",
    "        topk (int): 保留的最大值的数量。\n",
    "        use_abs (bool): 是否使用绝对值进行排序。\n",
    "    \n",
    "    返回:\n",
    "        sparse_row (torch.Tensor): 稀疏化后的行。\n",
    "    \"\"\"\n",
    "    if use_abs:\n",
    "        row = torch.abs(row)  # 如果需要使用绝对值，先取绝对值\n",
    "    \n",
    "    # 找到 topk 个最大值的索引\n",
    "    topk_indices = torch.topk(row, topk).indices\n",
    "    \n",
    "    # 创建一个与 row 相同大小的零张量\n",
    "    sparse_row = torch.zeros_like(row)\n",
    "    \n",
    "    # 将 topk_indices 对应的值置为 1\n",
    "    sparse_row[topk_indices] = 1\n",
    "    \n",
    "    return sparse_row\n",
    "\n",
    "def generate_label(y, topk=2, use_abs=False):\n",
    "    \"\"\"\n",
    "    对输入的张量 y 的每一行进行稀疏化，保留 topk 个最大值的索引为 1，其余为 0。\n",
    "    \n",
    "    参数:\n",
    "        y (torch.Tensor): 输入的张量。\n",
    "        topk (int): 保留的最大值的数量。\n",
    "        use_abs (bool): 是否使用绝对值进行排序。\n",
    "    \n",
    "    返回:\n",
    "        sparse_tensor (torch.Tensor): 稀疏化后的张量。\n",
    "    \"\"\"\n",
    "    # 对每一行进行稀疏化\n",
    "    sparse_tensor = torch.stack([sparse_row(row, topk, use_abs) for row in y])\n",
    "    return sparse_tensor\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=25, writer=None):\n",
    "    scaler = GradScaler()  # 创建 GradScaler 对象\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 使用 autocast 来进行自动混合精度处理\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(inputs)\n",
    "                ### targets 按照大小编码成 0,1 \n",
    "                loss = criterion(outputs, generate_label(targets))\n",
    "\n",
    "            # 使用 GradScaler 来缩放损失，然后进行反向传播\n",
    "            # 注意：反向传播不包含在 autocast() 块中\n",
    "            scaler.scale(loss).backward()\n",
    "            writer.add_scalar('Loss/Train', loss.item(), epoch * len(train_loader) + batch_idx)\n",
    "            # 调用 scaler.step() 来更新模型权重，并调用 scaler.update() 准备下一步\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        if epoch % 2 == 0:\n",
    "            model.eval()\n",
    "            eval_model(model, val_loader,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3569024/2454517915.py:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  file_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1048576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3569024/2454517915.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_weights = torch.load(f\"./router/{i}.pt\")\n",
      "/tmp/ipykernel_3569024/2454517915.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy: 0.5306 Top-2 Accuracy (unordered): 0.6018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3569024/2454517915.py:135: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # 创建 GradScaler 对象\n",
      "/tmp/ipykernel_3569024/2454517915.py:144: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/tmp/ipykernel_3569024/2454517915.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy: 0.6168 Top-2 Accuracy (unordered): 0.7004\n",
      "Top-1 Accuracy: 0.6196 Top-2 Accuracy (unordered): 0.7022\n",
      "Top-1 Accuracy: 0.6157 Top-2 Accuracy (unordered): 0.7022\n",
      "Top-1 Accuracy: 0.6173 Top-2 Accuracy (unordered): 0.7024\n",
      "Top-1 Accuracy: 0.6173 Top-2 Accuracy (unordered): 0.7029\n",
      "layer  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3569024/2454517915.py:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  file_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1048576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3569024/2454517915.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_weights = torch.load(f\"./router/{i}.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy: 0.7208 Top-2 Accuracy (unordered): 0.7591\n",
      "Top-1 Accuracy: 0.7558 Top-2 Accuracy (unordered): 0.8121\n",
      "Top-1 Accuracy: 0.7560 Top-2 Accuracy (unordered): 0.8119\n",
      "Top-1 Accuracy: 0.7550 Top-2 Accuracy (unordered): 0.8124\n",
      "Top-1 Accuracy: 0.7544 Top-2 Accuracy (unordered): 0.8119\n",
      "Top-1 Accuracy: 0.7550 Top-2 Accuracy (unordered): 0.8125\n",
      "layer  3\n",
      "1048576\n",
      "Top-1 Accuracy: 0.7575 Top-2 Accuracy (unordered): 0.7682\n",
      "Top-1 Accuracy: 0.7749 Top-2 Accuracy (unordered): 0.8032\n",
      "Top-1 Accuracy: 0.7779 Top-2 Accuracy (unordered): 0.8035\n",
      "Top-1 Accuracy: 0.7760 Top-2 Accuracy (unordered): 0.8026\n",
      "Top-1 Accuracy: 0.7765 Top-2 Accuracy (unordered): 0.8033\n",
      "Top-1 Accuracy: 0.7770 Top-2 Accuracy (unordered): 0.8030\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 4):\n",
    "    print(\"layer \", i)\n",
    "    file_names = [f'merge/a2ef_{i}_{j}.pth' for j in range(10)]\n",
    "    dataset = CustomDataset(file_paths=file_names)\n",
    "    # 划分训练集和验证集\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=2048, shuffle=True)\n",
    "    print(len(train_dataset))\n",
    "    val_loader = DataLoader(val_dataset, batch_size=2048, shuffle=False)\n",
    "\n",
    "    model = RouterModel(4096, 8, i).cuda()\n",
    "    eval_model(model, val_loader)\n",
    "    # criterion = nn.MSELoss().to(\"cuda\")\n",
    "    # criterion = nn.CrossEntropyLoss().to(\"cuda\")\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    # criterion = nn.KLDivLoss(reduction='batchmean').to(\"cuda\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=5e-4) #lr=5e-5\n",
    "    writer = SummaryWriter('runs/predictor_multilayer')\n",
    "    train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10, writer=writer)\n",
    "    torch.save(model.state_dict(), f\"./training/{i}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重新训练router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleLinearModel(\n",
       "  (linear1): Linear(in_features=4096, out_features=128, bias=True)\n",
       "  (activation): SiLU()\n",
       "  (linear2): Linear(in_features=128, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.cuda.amp import GradScaler, autocast  # 用于混合精度训练\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "class SimpleLinearModel(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim,hidden_dim=32):\n",
    "        super(SimpleLinearModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.activation = nn.SiLU() # 添加 ReLU 激活函数\n",
    "        self.linear2 = nn.Linear(hidden_dim,output_dim)  # 添加一个 8x8 线性层\n",
    "        init.kaiming_normal_(self.linear1.weight, mode='fan_out', nonlinearity='relu')\n",
    "        init.kaiming_normal_(self.linear2.weight, mode='fan_out', nonlinearity='relu')\n",
    "        self.linear1.bias.data.fill_(0)\n",
    "        self.linear2.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x= self.linear1(x)\n",
    "        x= self.activation(x)\n",
    "        return self.linear2(x)\n",
    "        \n",
    "model=SimpleLinearModel(4096,8, hidden_dim=128)\n",
    "model.to(\"cuda\")  # 假设使用 GPU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
