{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lz/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "MixtralForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model path: /home/lz/Mixtral-8x7B-v0.1 \n",
      "dataset path: /home/lz/c4 \n",
      "save path: /home/lz/On-the-Fly_MoE_Inference/saving/threshold/c4_mixtral_up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:37<00:00,  1.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with sparsity of 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "# from modeling_llama_up import set_profile_mode\n",
    "import os\n",
    "import csv\n",
    "from utils import get_model, set_seed\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "### from path.json read paths of model and dataset\n",
    "model_name = \"mixtral\"\n",
    "dataset_name = \"c4\"\n",
    "with open('../path.json', 'r') as file:\n",
    "    paths = json.load(file)\n",
    "    model_path = paths.get(model_name, '')\n",
    "    dataset_path = paths.get(dataset_name, '')\n",
    "    save_path = paths.get('chess_up_threshold','')\n",
    "    print('model path:', model_path, '\\ndataset path:', dataset_path, '\\nsave path:', save_path)\n",
    "\n",
    "set_seed(42)\n",
    "# c4data = get_c4_data(model_path, dataset_path, sample_num = 400)\n",
    "model = get_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixtralForCausalLM(\n",
       "  (model): MixtralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MixtralDecoderLayer(\n",
       "        (self_attn): MixtralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MixtralRotaryEmbedding()\n",
       "        )\n",
       "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
       "          (gate): Linear(in_features=4096, out_features=8, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-7): 8 x MixtralBLockSparseTop2MLP(\n",
       "              (w1): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (w2): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "              (w3): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): MixtralRMSNorm()\n",
       "        (post_attention_layernorm): MixtralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MixtralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating validation split: 0 examples [00:00, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating validation split: 45576 examples [00:00, 121496.76 examples/s]\n",
      "tokenizing raw datasets (num_proc=64): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45576/45576 [00:02<00:00, 17946.73 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "raw_datasets = load_dataset(\"/home/lz/c4\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "def process(example):\n",
    "    ids = tokenizer.encode(example['text'])\n",
    "    out = {'ids': ids, 'len': len(ids)}\n",
    "    return out\n",
    "\n",
    "tokenized = raw_datasets.map(process, desc='tokenizing raw datasets', num_proc=64)\n",
    "import numpy as np\n",
    "datasets = dict()\n",
    "\n",
    "for split, dset in tokenized.items():\n",
    "    datasets[split] = []\n",
    "    length = np.sum(dset['len'])\n",
    "    datasets[split] = np.ndarray((length, ), np.uint32)\n",
    "    idx = 0\n",
    "    for row in dset:\n",
    "        datasets[split][idx:idx + row['len']] = row['ids']\n",
    "        idx += row['len']\n",
    "torch.save(datasets, 'datasets.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_56979/3427838030.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  datasets = torch.load('./threshold/chess/datasets.pt')\n"
     ]
    }
   ],
   "source": [
    "datasets = torch.load('./threshold/chess/datasets.pt')\n",
    "import torch\n",
    "import numpy as np\n",
    "def get_batch(data, batch_size, block_size):\n",
    "    start_idxs = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in start_idxs])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in start_idxs])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "14\n",
      "16\n",
      "18\n",
      "20\n",
      "22\n",
      "24\n",
      "26\n",
      "28\n",
      "30\n",
      "32\n",
      "34\n",
      "36\n",
      "38\n",
      "40\n",
      "42\n",
      "44\n",
      "46\n",
      "48\n",
      "50\n",
      "52\n",
      "54\n",
      "56\n",
      "58\n",
      "60\n",
      "62\n"
     ]
    }
   ],
   "source": [
    "sparsity_level = 0.7\n",
    "# device = 'cuda:1'\n",
    "device_2 = 'cpu'\n",
    "avg_loss = 0.0\n",
    "n_batch = 64\n",
    "# accum_steps = 4 \n",
    "accum_steps = 2\n",
    "batch_size = 1\n",
    "block_size = 2048\n",
    "torch.manual_seed(42)\n",
    "n_layers = len(model.model.layers)\n",
    "n_experts = len(model.model.layers[0].block_sparse_moe.experts)\n",
    "\n",
    "up_proj_states_thresholds = [torch.zeros([n_experts,]) for _ in range(n_layers)]\n",
    "gate_proj_states_mean_squares = [[torch.zeros(model.config.intermediate_size) for _ in range(n_experts)] for _ in range(n_layers)]\n",
    "\n",
    "up_states = [[torch.zeros([accum_steps * batch_size * block_size //2, model.config.intermediate_size]) for _ in range(n_experts)] for _ in range(n_layers)]\n",
    "gate_states = [[torch.zeros([accum_steps * batch_size * block_size //2, model.config.intermediate_size]) for _ in range(n_experts)] for _ in range(n_layers)]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step in range(n_batch // accum_steps):\n",
    "        print(step * accum_steps)\n",
    "        all_counts = [0 for _ in range(n_layers * n_experts)]\n",
    "        for batch_idx in range(accum_steps):\n",
    "            # print('batch_idx:', batch_idx)\n",
    "            inputs, labels = get_batch(datasets['validation'], batch_size, block_size)\n",
    "            inputs = inputs.cuda()\n",
    "            outputs = model(inputs, labels=inputs)\n",
    "            avg_loss = avg_loss + outputs.loss / n_batch\n",
    "\n",
    "            for layer_idx in range(n_layers):\n",
    "                for expert_idx in range(n_experts):\n",
    "                    counts = all_counts[layer_idx * n_experts + expert_idx]\n",
    "\n",
    "                    states = model.model.layers[layer_idx].block_sparse_moe.experts[expert_idx].up_proj_states.reshape(-1, model.config.intermediate_size)\n",
    "                    cur_counts = states.size(0)\n",
    "                    # print('counts and cur_counts:',counts, cur_counts)\n",
    "                    # print(states.size())\n",
    "                    # print(up_states[layer_idx][expert_idx][counts : counts+cur_counts, :].size())\n",
    "                    up_states[layer_idx][expert_idx][counts : counts+cur_counts, :] = states\n",
    "\n",
    "                    states = model.model.layers[layer_idx].block_sparse_moe.experts[expert_idx].gate_proj_states.reshape(-1, model.config.intermediate_size)\n",
    "                    gate_states[layer_idx][expert_idx][counts : counts+cur_counts, :] = states\n",
    "                    # counts += cur_counts\n",
    "                    all_counts[layer_idx * n_experts + expert_idx] += cur_counts\n",
    "\n",
    "        for layer_idx in range(n_layers):   \n",
    "            for expert_idx in range(n_experts):\n",
    "                # print('layer_idx:', layer_idx, 'expert_idx:', expert_idx)\n",
    "                useful_num = all_counts[layer_idx * n_experts + expert_idx]\n",
    "                topk_num = int(useful_num * model.config.intermediate_size * sparsity_level)\n",
    "                up_proj_states_thresholds[layer_idx][expert_idx] += up_states[layer_idx][expert_idx][0:useful_num,:].to(device_2).abs().flatten().kthvalue(topk_num).values.to('cpu')\n",
    "                gate_proj_states_mean_squares[layer_idx][expert_idx] += (torch.sum(gate_states[layer_idx][expert_idx][0:useful_num,:].to(device_2) ** 2, dim=0).to('cpu') / useful_num).to('cpu')\n",
    "\n",
    "for layer_idx in range(n_layers):\n",
    "    for expert_idx in range(n_experts):\n",
    "        gate_proj_states_mean_squares[layer_idx][expert_idx] /= n_batch // accum_steps\n",
    "        up_proj_states_thresholds[layer_idx][expert_idx] /= n_batch // accum_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0., 0.,  ..., 0., 0., 0.]), tensor(0.0481))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gate_proj_states_mean_squares[0][1],up_proj_states_thresholds[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "importance_thresholds = [torch.zeros([n_experts,]) for _ in range(n_layers)]\n",
    "up_proj_states_thresholds_2 = [[torch.zeros(model.config.intermediate_size) for _ in range(n_experts)] for _ in range(n_layers)]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step in range(n_batch // accum_steps):\n",
    "        print(step * accum_steps)\n",
    "        all_counts = [0 for _ in range(n_layers * n_experts)]\n",
    "        for batch_idx in range(accum_steps):\n",
    "            inputs, labels = get_batch(datasets['validation'], batch_size, block_size)\n",
    "            inputs = inputs.cuda()\n",
    "            outputs = model(inputs, labels=inputs)\n",
    "            avg_loss = avg_loss + outputs.loss / n_batch\n",
    "\n",
    "            for layer_idx in range(n_layers):\n",
    "                for expert_idx in range(n_experts):\n",
    "                    counts = all_counts[layer_idx * n_experts + expert_idx]\n",
    "                    states = model.model.layers[layer_idx].block_sparse_moe.experts[expert_idx].up_proj_states.reshape(-1, states.size(-1))\n",
    "                    cur_counts = states.size(0)\n",
    "                    up_states[layer_idx][expert_idx][counts:cur_counts+counts, :] = states\n",
    "                    # counts += cur_counts\n",
    "                    all_counts[layer_idx * n_experts + expert_idx] += cur_counts\n",
    "                \n",
    "        for layer_idx in range(n_layers):   \n",
    "            for expert_idx in range(n_experts):\n",
    "                useful_num = all_counts[layer_idx * n_experts + expert_idx]\n",
    "                importance_scores = up_states[layer_idx][expert_idx][:useful_num,:] ** 2 * gate_proj_states_mean_squares[layer_idx][expert_idx]\n",
    "                importance_thresholds[layer_idx][expert_idx] += importance_scores.to(device_2).flatten().kthvalue(int(importance_scores.numel() * sparsity_level)).values.to('cpu')\n",
    "\n",
    "for layer_idx in range(n_layers):\n",
    "    for expert_idx in range(n_experts):\n",
    "        importance_thresholds[layer_idx][expert_idx] /= n_batch // accum_steps\n",
    "        up_proj_states_thresholds_2[layer_idx][expert_idx] = (importance_thresholds[layer_idx][expert_idx].expand_as(up_proj_states_thresholds_2[layer_idx][expert_idx]) / gate_proj_states_mean_squares[layer_idx][expert_idx]) ** 0.5\n",
    "\n",
    "thresholds = {'up_proj_states_thresholds': up_proj_states_thresholds, 'up_proj_states_thresholds_2': up_proj_states_thresholds_2}\n",
    "\n",
    "torch.save(thresholds, f'{save_path}/thresholds.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[tensor([0.0828, 0.0886, 0.0805,  ..., 0.0463, 0.0734, 0.0805]), tensor([0.0914, 0.0872, 0.1076,  ..., 0.0616, 0.0878, 0.0748]), tensor([0.1119, 0.0712, 0.1166,  ..., 0.0903, 0.0871, 0.0887]), tensor([0.1036, 0.0800, 0.1252,  ..., 0.0806, 0.0852, 0.0863]), tensor([0.1034, 0.1043, 0.1129,  ..., 0.0677, 0.0950, 0.0903]), tensor([0.0984, 0.0832, 0.1072,  ..., 0.0709, 0.0874, 0.0822]), tensor([0.0671, 0.1130, 0.1085,  ..., 0.0673, 0.0583, 0.0829]), tensor([0.0929, 0.0765, 0.0864,  ..., 0.0673, 0.0767, 0.0761])], [tensor([0.1306, 0.0817, 0.1159,  ..., 0.0995, 0.1281, 0.1266]), tensor([0.1352, 0.1360, 0.1233,  ..., 0.1193, 0.1400, 0.1366]), tensor([0.1269, 0.1036, 0.1278,  ..., 0.1105, 0.1419, 0.1258]), tensor([0.1210, 0.1199, 0.1266,  ..., 0.1159, 0.1094, 0.1222]), tensor([0.1323, 0.1283, 0.1320,  ..., 0.1297, 0.1526, 0.1441]), tensor([0.1278, 0.1137, 0.1114,  ..., 0.1229, 0.1466, 0.1378]), tensor([0.1113, 0.0857, 0.1023,  ..., 0.0926, 0.1235, 0.1224]), tensor([0.1244, 0.1014, 0.1169,  ..., 0.1147, 0.1362, 0.1284])], [tensor([0.1639, 0.1715, 0.1338,  ..., 0.1645, 0.1519, 0.1732]), tensor([0.1690, 0.1850, 0.1427,  ..., 0.1847, 0.1553, 0.1996]), tensor([0.1743, 0.1864, 0.1308,  ..., 0.1797, 0.1694, 0.1909]), tensor([0.1667, 0.1786, 0.1323,  ..., 0.1714, 0.1711, 0.1831]), tensor([0.1802, 0.1873, 0.1224,  ..., 0.1781, 0.1655, 0.1828]), tensor([0.1873, 0.1856, 0.1409,  ..., 0.1784, 0.1793, 0.1816]), tensor([0.1865, 0.1891, 0.1262,  ..., 0.1859, 0.1721, 0.1914]), tensor([0.1342, 0.1617, 0.1214,  ..., 0.1700, 0.1236, 0.1623])], [tensor([0.1962, 0.2030, 0.2297,  ..., 0.2180, 0.1913, 0.2298]), tensor([0.2021, 0.2148, 0.2252,  ..., 0.2313, 0.1848, 0.2193]), tensor([0.2053, 0.2333, 0.2371,  ..., 0.2171, 0.1794, 0.2178]), tensor([0.1815, 0.2070, 0.2090,  ..., 0.1734, 0.1606, 0.2165]), tensor([0.2053, 0.2235, 0.2438,  ..., 0.2267, 0.1685, 0.2344]), tensor([0.1891, 0.1853, 0.2319,  ..., 0.2046, 0.1884, 0.2129]), tensor([0.2210, 0.2196, 0.2353,  ..., 0.2262, 0.1705, 0.2315]), tensor([0.2057, 0.2289, 0.2410,  ..., 0.2276, 0.1838, 0.2304])], [tensor([0.2697, 0.2389, 0.2349,  ..., 0.2322, 0.2968, 0.2362]), tensor([0.2243, 0.2268, 0.2361,  ..., 0.2184, 0.2889, 0.2307]), tensor([0.2555, 0.2530, 0.2392,  ..., 0.2439, 0.2918, 0.2448]), tensor([0.2454, 0.2334, 0.2442,  ..., 0.2356, 0.2719, 0.2454]), tensor([0.2708, 0.2337, 0.2364,  ..., 0.2135, 0.2715, 0.2487]), tensor([0.1862, 0.2193, 0.2423,  ..., 0.2159, 0.2815, 0.2395]), tensor([0.2354, 0.2147, 0.2382,  ..., 0.2190, 0.2862, 0.2478]), tensor([0.2632, 0.2262, 0.2260,  ..., 0.2422, 0.2774, 0.2356])], [tensor([0.3548, 0.2951, 0.3075,  ..., 0.3404, 0.3279, 0.3970]), tensor([0.3651, 0.3054, 0.2708,  ..., 0.3437, 0.3076, 0.4199]), tensor([0.3266, 0.2358, 0.2756,  ..., 0.3259, 0.3086, 0.3187]), tensor([0.3255, 0.2685, 0.2783,  ..., 0.3492, 0.2993, 0.2877]), tensor([0.3638, 0.3023, 0.2820,  ..., 0.3548, 0.3231, 0.4095]), tensor([0.3666, 0.3105, 0.2890,  ..., 0.3294, 0.3379, 0.3932]), tensor([0.3311, 0.2912, 0.2988,  ..., 0.3125, 0.3111, 0.3659]), tensor([0.3367, 0.2865, 0.2809,  ..., 0.3470, 0.3143, 0.3409])], [tensor([0.3788, 0.3308, 0.3215,  ..., 0.3459, 0.3562, 0.3058]), tensor([0.3737, 0.3370, 0.3010,  ..., 0.3267, 0.3636, 0.3301]), tensor([0.3565, 0.3436, 0.3303,  ..., 0.3498, 0.3365, 0.3401]), tensor([0.3433, 0.3311, 0.3519,  ..., 0.3515, 0.3468, 0.3573]), tensor([0.3722, 0.3642, 0.3061,  ..., 0.3483, 0.3514, 0.3325]), tensor([0.3416, 0.3350, 0.3455,  ..., 0.3199, 0.3488, 0.3352]), tensor([0.3143, 0.3287, 0.3320,  ..., 0.3320, 0.3240, 0.3386]), tensor([0.3683, 0.3410, 0.3282,  ..., 0.3452, 0.3475, 0.3422])], [tensor([0.4002, 0.3867, 0.3903,  ..., 0.4063, 0.4270, 0.3641]), tensor([0.3799, 0.3916, 0.3617,  ..., 0.3851, 0.4214, 0.3392]), tensor([0.3926, 0.3810, 0.3745,  ..., 0.3960, 0.4419, 0.3433]), tensor([0.4133, 0.3818, 0.3638,  ..., 0.4154, 0.4149, 0.3957]), tensor([0.4055, 0.4107, 0.3594,  ..., 0.4082, 0.4318, 0.3491]), tensor([0.3751, 0.3615, 0.3592,  ..., 0.3843, 0.4096, 0.3634]), tensor([0.3882, 0.3636, 0.3716,  ..., 0.3553, 0.4251, 0.3683]), tensor([0.3849, 0.3750, 0.3675,  ..., 0.4061, 0.4204, 0.3784])], [tensor([0.3773, 0.4129, 0.4036,  ..., 0.3977, 0.3877, 0.4386]), tensor([0.3343, 0.4096, 0.4139,  ..., 0.4019, 0.4192, 0.4381]), tensor([0.3627, 0.3717, 0.4152,  ..., 0.3840, 0.3820, 0.4146]), tensor([0.3821, 0.4131, 0.4114,  ..., 0.4338, 0.3906, 0.4359]), tensor([0.3711, 0.3887, 0.4244,  ..., 0.3610, 0.3585, 0.4079]), tensor([0.3488, 0.3764, 0.4285,  ..., 0.4151, 0.3359, 0.4150]), tensor([0.3966, 0.3847, 0.4474,  ..., 0.3735, 0.3822, 0.4384]), tensor([0.4101, 0.3990, 0.4315,  ..., 0.4363, 0.3899, 0.4138])], [tensor([0.4123, 0.4884, 0.4758,  ..., 0.3920, 0.4063, 0.4520]), tensor([0.3888, 0.5010, 0.4683,  ..., 0.4153, 0.5064, 0.4475]), tensor([0.4332, 0.4828, 0.4838,  ..., 0.3503, 0.3830, 0.2989]), tensor([0.4326, 0.4095, 0.4514,  ..., 0.4015, 0.4717, 0.4089]), tensor([0.4923, 0.5054, 0.4706,  ..., 0.3900, 0.4299, 0.4389]), tensor([0.4270, 0.4761, 0.4688,  ..., 0.3896, 0.4001, 0.4222]), tensor([0.4558, 0.4410, 0.4559,  ..., 0.3669, 0.3895, 0.4267]), tensor([0.3538, 0.4730, 0.4467,  ..., 0.2949, 0.4196, 0.4179])], [tensor([0.3922, 0.4674, 0.4554,  ..., 0.5024, 0.4551, 0.4830]), tensor([0.3256, 0.4577, 0.4903,  ..., 0.5166, 0.5480, 0.4732]), tensor([0.4428, 0.4696, 0.4792,  ..., 0.4915, 0.4500, 0.3717]), tensor([0.3205, 0.4505, 0.4809,  ..., 0.5120, 0.4951, 0.4728]), tensor([0.4139, 0.4462, 0.4388,  ..., 0.4715, 0.4804, 0.4261]), tensor([0.4509, 0.4382, 0.4227,  ..., 0.4789, 0.4978, 0.4132]), tensor([0.4368, 0.4778, 0.4474,  ..., 0.4439, 0.4845, 0.3794]), tensor([0.4578, 0.4564, 0.4851,  ..., 0.4645, 0.4858, 0.4510])], [tensor([0.4519, 0.4711, 0.4989,  ..., 0.5494, 0.5037, 0.5192]), tensor([0.4760, 0.5092, 0.5050,  ..., 0.5419, 0.4698, 0.4931]), tensor([0.5089, 0.5490, 0.5176,  ..., 0.4599, 0.5435, 0.5176]), tensor([0.5266, 0.5454, 0.6073,  ..., 0.6015, 0.5406, 0.5142]), tensor([0.4583, 0.5503, 0.4959,  ..., 0.5419, 0.5444, 0.5299]), tensor([0.5245, 0.5433, 0.5190,  ..., 0.6062, 0.5632, 0.5887]), tensor([0.4913, 0.4934, 0.5088,  ..., 0.5494, 0.4841, 0.4823]), tensor([0.5198, 0.4691, 0.5150,  ..., 0.5027, 0.4838, 0.5354])], [tensor([0.4475, 0.5622, 0.5451,  ..., 0.5776, 0.5242, 0.6235]), tensor([0.4801, 0.5002, 0.5238,  ..., 0.5450, 0.3680, 0.5681]), tensor([0.4064, 0.5345, 0.4443,  ..., 0.5130, 0.4862, 0.5932]), tensor([0.4385, 0.5542, 0.4859,  ..., 0.5790, 0.4212, 0.5607]), tensor([0.4138, 0.4996, 0.4330,  ..., 0.5880, 0.5086, 0.6150]), tensor([0.4153, 0.5200, 0.5121,  ..., 0.5697, 0.4784, 0.5708]), tensor([0.4175, 0.4916, 0.4725,  ..., 0.5316, 0.4562, 0.5438]), tensor([0.5425, 0.5899, 0.5225,  ..., 0.6154, 0.6030, 0.6297])], [tensor([0.4750, 0.5623, 0.5121,  ..., 0.5033, 0.3440, 0.5698]), tensor([0.4553, 0.5925, 0.5269,  ..., 0.4232, 0.2533, 0.5933]), tensor([0.4873, 0.6167, 0.5692,  ..., 0.4897, 0.3064, 0.5895]), tensor([0.5029, 0.5898, 0.5716,  ..., 0.6198, 0.5152, 0.5679]), tensor([0.5899, 0.6048, 0.5655,  ..., 0.6401, 0.5018, 0.5686]), tensor([0.4523, 0.5902, 0.6220,  ..., 0.5433, 0.3208, 0.5824]), tensor([0.5418, 0.5889, 0.5559,  ..., 0.4953, 0.2517, 0.5816]), tensor([0.5089, 0.6239, 0.6277,  ..., 0.4916, 0.2573, 0.6319])], [tensor([0.6667, 0.7065, 0.7084,  ..., 0.7065, 0.5778, 0.7511]), tensor([0.6169, 0.6001, 0.6643,  ..., 0.6057, 0.5000, 0.6021]), tensor([0.6656, 0.6449, 0.6785,  ..., 0.6558, 0.4644, 0.7235]), tensor([0.6389, 0.6458, 0.6008,  ..., 0.7034, 0.4476, 0.7519]), tensor([0.5784, 0.6237, 0.6697,  ..., 0.6399, 0.5850, 0.5749]), tensor([0.5618, 0.6463, 0.6007,  ..., 0.6310, 0.4549, 0.5482]), tensor([0.5714, 0.6065, 0.5677,  ..., 0.6115, 0.4712, 0.5848]), tensor([0.6044, 0.6745, 0.6525,  ..., 0.6201, 0.5005, 0.6406])], [tensor([0.5993, 0.6456, 0.4998,  ..., 0.6740, 0.5252, 0.7366]), tensor([0.4528, 0.6871, 0.4911,  ..., 0.6088, 0.5983, 0.7486]), tensor([0.6293, 0.7037, 0.5519,  ..., 0.6957, 0.5990, 0.7515]), tensor([0.6137, 0.5925, 0.5510,  ..., 0.6366, 0.5774, 0.7513]), tensor([0.5985, 0.6606, 0.5647,  ..., 0.6590, 0.6387, 0.7350]), tensor([0.5851, 0.6671, 0.5366,  ..., 0.5700, 0.5928, 0.7176]), tensor([0.6862, 0.6951, 0.4662,  ..., 0.6081, 0.6202, 0.7545]), tensor([0.4627, 0.7474, 0.3719,  ..., 0.6853, 0.5120, 0.8393])], [tensor([0.8090, 0.7478, 0.6820,  ..., 0.7377, 0.8393, 0.7997]), tensor([0.8414, 0.7505, 0.6931,  ..., 0.7898, 0.8409, 0.8226]), tensor([0.8253, 0.6038, 0.7105,  ..., 0.7337, 0.8538, 0.8063]), tensor([0.8141, 0.7317, 0.6675,  ..., 0.7582, 0.8233, 0.7730]), tensor([0.7480, 0.7444, 0.6519,  ..., 0.6367, 0.8249, 0.8104]), tensor([0.8604, 0.6955, 0.7422,  ..., 0.7727, 0.8362, 0.8090]), tensor([0.8173, 0.7782, 0.7061,  ..., 0.7329, 0.8424, 0.8178]), tensor([0.8008, 0.8317, 0.6449,  ..., 0.7451, 0.8215, 0.7916])], [tensor([1.0387, 0.6355, 0.5699,  ..., 0.6173, 0.9827, 0.7947]), tensor([0.9601, 0.6503, 0.5805,  ..., 0.5754, 0.9855, 0.8670]), tensor([0.9095, 0.6849, 0.6200,  ..., 0.5448, 0.9098, 0.8568]), tensor([0.7750, 0.6698, 0.7146,  ..., 0.7067, 0.8896, 0.8395]), tensor([0.9175, 0.4530, 0.7340,  ..., 0.6094, 0.9428, 0.8229]), tensor([0.8748, 0.7520, 0.5438,  ..., 0.5361, 0.8881, 0.8137]), tensor([0.8653, 0.7044, 0.7143,  ..., 0.7651, 0.8035, 0.8042]), tensor([0.8490, 0.6698, 0.6388,  ..., 0.6756, 0.8863, 0.8259])], [tensor([0.9320, 0.5422, 0.8279,  ..., 1.1076, 0.8281, 0.9969]), tensor([0.9495, 0.8084, 0.9896,  ..., 0.8966, 0.8693, 1.0838]), tensor([1.0029, 0.5628, 0.9456,  ..., 1.1208, 0.5634, 1.1540]), tensor([0.9385, 0.5925, 0.9724,  ..., 1.1719, 0.7545, 1.1093]), tensor([0.9046, 0.6263, 0.9384,  ..., 1.1004, 0.6420, 1.0680]), tensor([0.9228, 0.6505, 0.8305,  ..., 0.9780, 0.6987, 0.8361]), tensor([0.9723, 0.5339, 0.9328,  ..., 1.0522, 0.8058, 1.0390]), tensor([0.8961, 0.6999, 0.9297,  ..., 1.0278, 0.6935, 0.9001])], [tensor([1.1964, 1.0642, 1.2270,  ..., 0.7853, 1.1454, 0.9252]), tensor([0.9658, 1.1337, 1.2644,  ..., 0.9009, 1.0952, 0.9791]), tensor([0.7891, 1.1275, 1.1918,  ..., 0.7843, 1.0974, 0.8600]), tensor([0.9917, 1.2000, 1.2881,  ..., 0.8786, 1.1206, 0.8751]), tensor([1.0099, 1.1269, 1.2279,  ..., 0.8678, 1.1348, 0.9092]), tensor([1.2382, 1.0302, 1.1631,  ..., 0.8691, 0.8892, 0.9278]), tensor([0.9856, 1.1147, 1.2014,  ..., 0.8056, 1.1587, 0.9053]), tensor([1.1317, 0.9985, 1.0749,  ..., 0.7630, 1.1524, 0.9305])], [tensor([0.9543, 1.0388, 0.7210,  ..., 0.9832, 1.0012, 0.8936]), tensor([1.0136, 0.9784, 0.7662,  ..., 1.0025, 0.9434, 1.0011]), tensor([1.1005, 0.9531, 0.9066,  ..., 1.1208, 1.0069, 0.8826]), tensor([1.0285, 1.1514, 0.7790,  ..., 1.1028, 0.9570, 1.1230]), tensor([1.0003, 0.9442, 0.7150,  ..., 1.1637, 1.1140, 1.1749]), tensor([1.0321, 0.8926, 0.7432,  ..., 1.2116, 0.8597, 0.9013]), tensor([1.0096, 0.9931, 0.8444,  ..., 1.0037, 0.7917, 1.0213]), tensor([1.0456, 0.9786, 0.7946,  ..., 1.1664, 1.0871, 0.9953])], [tensor([1.0569, 1.0862, 1.4290,  ..., 0.9999, 1.1149, 1.2867]), tensor([1.0527, 1.0666, 1.3433,  ..., 0.9072, 0.9537, 1.2315]), tensor([1.0414, 0.9778, 1.2309,  ..., 0.9417, 0.9497, 1.1731]), tensor([1.0035, 0.9007, 1.2942,  ..., 0.9356, 0.9288, 1.2393]), tensor([1.0519, 1.0083, 1.4491,  ..., 1.0482, 1.1490, 1.3074]), tensor([1.0518, 1.1032, 1.2488,  ..., 0.9315, 0.8946, 1.1156]), tensor([1.1063, 0.9320, 1.3696,  ..., 1.0389, 1.1288, 1.2919]), tensor([1.1276, 0.9598, 1.3350,  ..., 1.0197, 0.9274, 1.2747])], [tensor([1.0203, 1.1230, 1.0108,  ..., 0.9377, 1.3794, 1.1509]), tensor([1.0608, 0.8973, 1.0726,  ..., 1.1101, 1.4299, 1.1721]), tensor([1.0843, 1.0848, 1.0066,  ..., 1.0512, 1.5088, 1.3008]), tensor([0.8232, 1.1981, 1.1221,  ..., 1.0162, 1.4296, 1.2304]), tensor([0.8996, 1.2039, 1.0362,  ..., 0.9191, 1.5124, 1.2859]), tensor([1.1001, 1.1270, 1.0857,  ..., 1.0346, 1.4999, 1.2137]), tensor([0.8967, 1.0754, 1.0030,  ..., 0.8362, 1.4552, 1.0686]), tensor([0.9649, 1.1579, 0.9709,  ..., 0.8930, 1.4237, 1.2697])], [tensor([1.2060, 1.0121, 1.3238,  ..., 1.1181, 1.0076, 1.1742]), tensor([1.1198, 1.2105, 1.3710,  ..., 1.1649, 1.1484, 1.2121]), tensor([1.1019, 0.9968, 1.3766,  ..., 1.2189, 1.0881, 1.2696]), tensor([1.1061, 1.0188, 1.3489,  ..., 1.2092, 1.0427, 1.1387]), tensor([1.1957, 1.0868, 1.4378,  ..., 1.1609, 1.1051, 1.1616]), tensor([1.1093, 1.1162, 1.2027,  ..., 1.2225, 1.0305, 1.0265]), tensor([1.0662, 1.0545, 1.2711,  ..., 1.3115, 1.0907, 1.1971]), tensor([1.2096, 0.8963, 1.4476,  ..., 1.1695, 1.0436, 1.2249])], [tensor([1.2360, 1.3963, 1.5807,  ..., 1.1036, 1.1843, 0.7481]), tensor([1.3378, 1.4287, 1.3943,  ..., 0.8639, 1.1263, 1.2463]), tensor([1.1400, 1.4241, 1.4966,  ..., 1.0926, 1.2052, 1.0593]), tensor([1.2508, 1.4030, 1.6041,  ..., 1.3318, 1.2493, 0.7456]), tensor([1.1277, 1.2821, 1.2902,  ..., 1.1756, 0.8501, 0.7023]), tensor([1.2055, 1.2570, 1.2342,  ..., 1.2368, 0.9694, 1.0624]), tensor([1.0665, 1.3574, 1.4863,  ..., 1.2199, 1.0135, 1.0427]), tensor([1.0965, 1.3381, 1.4795,  ..., 1.2871, 1.0419, 1.0350])], [tensor([1.1458, 1.3500, 1.2674,  ..., 1.1935, 0.9606, 1.1721]), tensor([1.0436, 1.3004, 1.0828,  ..., 1.1210, 0.9704, 1.1291]), tensor([1.2887, 1.4340, 1.1552,  ..., 1.1976, 0.9739, 1.1550]), tensor([1.2146, 1.3723, 1.2398,  ..., 1.1710, 1.0646, 1.1024]), tensor([1.1569, 1.5157, 1.3430,  ..., 1.3826, 1.0776, 1.2093]), tensor([1.0632, 1.4111, 1.2260,  ..., 1.1006, 0.9924, 1.0030]), tensor([1.1568, 1.1752, 1.3652,  ..., 1.2246, 1.0855, 1.0985]), tensor([1.1908, 1.2357, 1.4100,  ..., 1.2406, 0.9270, 1.2423])], [tensor([1.2826, 1.2542, 1.1302,  ..., 1.0101, 1.4348, 1.7387]), tensor([1.1341, 1.5342, 0.9856,  ..., 1.0360, 1.4642, 1.6837]), tensor([1.2440, 1.1832, 1.1254,  ..., 1.1197, 1.6149, 1.4116]), tensor([1.2018, 1.2200, 1.0499,  ..., 1.1970, 1.4787, 1.6150]), tensor([1.2556, 1.4253, 1.1190,  ..., 1.0650, 1.5156, 1.6313]), tensor([1.1409, 1.1723, 0.9607,  ..., 1.2122, 1.6324, 1.7968]), tensor([1.2412, 1.2761, 1.0943,  ..., 1.0767, 1.4049, 1.4785]), tensor([1.2710, 1.1962, 1.1646,  ..., 0.9477, 1.3989, 1.6122])], [tensor([1.3991, 1.3217, 1.2021,  ..., 1.3549, 1.1533, 0.8206]), tensor([1.4803, 1.2764, 1.3405,  ..., 1.3668, 1.1699, 0.9000]), tensor([1.3551, 1.3982, 1.1302,  ..., 1.4946, 1.6071, 0.6820]), tensor([1.5479, 1.2865, 1.0495,  ..., 1.5776, 0.8271, 1.0516]), tensor([1.6206, 1.2691, 0.9314,  ..., 1.4907, 0.8250, 1.0685]), tensor([1.5529, 1.2689, 0.9669,  ..., 1.3988, 1.1047, 0.8801]), tensor([1.7016, 1.2050, 0.9620,  ..., 1.2652, 1.0093, 1.0107]), tensor([1.3395, 1.2351, 1.0125,  ..., 1.4106, 1.2098, 1.2781])], [tensor([1.8954, 1.2735, 1.5124,  ..., 1.2865, 2.0288, 1.6368]), tensor([2.0078, 1.6380, 1.1413,  ..., 1.4587, 1.5156, 1.6123]), tensor([1.7886, 1.1843, 1.4319,  ..., 1.3342, 1.5894, 1.8313]), tensor([1.8421, 1.9004, 1.5347,  ..., 1.3414, 1.9153, 1.6302]), tensor([2.3863, 2.9017, 1.1402,  ..., 1.9720, 1.7669, 2.2057]), tensor([2.1021, 1.1721, 1.6531,  ..., 1.6164, 1.8711, 1.6149]), tensor([1.5841, 1.4693, 1.3233,  ..., 1.3155, 1.7011, 1.8367]), tensor([1.5687, 1.2021, 1.4617,  ..., 1.5615, 1.4214, 1.4781])], [tensor([1.9147, 1.7384, 2.5975,  ..., 2.0499, 1.2527, 1.6513]), tensor([1.5744, 1.0537, 3.6550,  ..., 1.9701, 2.1184, 1.6110]), tensor([1.0242, 1.6370, 2.7648,  ..., 1.6121, 1.3317, 1.2118]), tensor([1.4427, 1.6232, 3.4913,  ..., 2.0580, 1.3360, 1.9868]), tensor([1.2886, 0.9687, 2.9907,  ..., 1.6315, 1.3125, 1.7860]), tensor([1.1034, 0.8057, 3.2579,  ..., 1.7683, 2.0336, 1.5080]), tensor([1.6093, 1.5626, 2.7575,  ..., 1.5077, 1.3917, 2.0452]), tensor([1.4282, 1.2300, 3.5016,  ..., 2.1860, 1.9181, 1.3813])], [tensor([2.7957, 2.0615, 1.4699,  ..., 2.8010, 1.5469, 0.9476]), tensor([1.2753, 1.9842, 1.5168,  ..., 3.5985, 2.5626, 2.5658]), tensor([1.0019, 1.9214, 1.6884,  ..., 2.2325, 1.8918, 0.6210]), tensor([1.9379, 1.8144, 1.9720,  ..., 2.5789, 1.5979, 0.8140]), tensor([1.2920, 1.8307, 1.8135,  ..., 2.7493, 1.8423, 1.1888]), tensor([1.3004, 2.3299, 2.1300,  ..., 2.6711, 1.6483, 1.2008]), tensor([2.7339, 2.1339, 1.1975,  ..., 2.6906, 1.9179, 0.6390]), tensor([1.0844, 1.5942, 1.3104,  ..., 2.8132, 1.4576, 0.9005])], [tensor([0.8043, 1.4050, 0.7516,  ..., 1.2915, 1.4012, 1.7647]), tensor([2.1494, 2.5968, 2.1434,  ..., 1.7889, 3.0061, 2.6610]), tensor([1.9432, 2.8197, 3.0612,  ..., 2.1913, 1.0085, 3.3341]), tensor([1.8020, 1.7881, 1.8137,  ..., 2.9526, 2.3345, 1.9078]), tensor([1.5742, 1.3201, 0.8743,  ..., 1.2838, 1.9178, 1.6656]), tensor([1.8994, 2.2997, 3.0329,  ..., 1.7248, 2.7145, 2.3381]), tensor([1.6565, 1.6635, 2.2893,  ..., 1.7390, 2.2130, 2.0811]), tensor([2.2528, 2.3394, 1.8805,  ..., 2.1947, 3.1793, 2.1252])]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_61259/499095173.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  thresholds = torch.load(f'{save_path}/thresholds.pt')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "save_path = './threshold/c4_mixtral_up'\n",
    "thresholds = torch.load(f'{save_path}/thresholds.pt')\n",
    "print(thresholds[\"up_proj_states_thresholds_2\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
