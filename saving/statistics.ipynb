{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ÊµãËØï‰∏ìÂÆ∂logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set profile_threshold to False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/On-the-Fly_MoE_Inference/saving/modeling_mixtral.py:93: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  up_th = torch.load(threshold_path, map_location='cuda')[\"up_proj_states_thresholds\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thresholds loaded from /home/bcds/On-the-Fly_MoE_Inference/saving/threshold/c4_mixtral/thresholds_0_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:818: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n",
      "/home/bcds/On-the-Fly_MoE_Inference/saving/modeling_mixtral.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.up_threshold = torch.tensor(up_th[self.layeridx][self.expertidx])\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:12<00:00,  1.51it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3,4\"\n",
    "from transformers import MixtralForCausalLM as MixtralTeacher\n",
    "from modeling_mixtral import MixtralForCausalLM, load_thresholds, set_profile_mode\n",
    "from transformers import AutoTokenizer, MixtralConfig\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "import json\n",
    "import torch\n",
    "\n",
    "with open(\"../quantize/device_map.json\", \"r\") as f:\n",
    "\tsd = json.load(f)\n",
    "\n",
    "with open(\"../quantize/device_map_2.json\", \"r\") as f:\n",
    "\ttd = json.load(f)\n",
    "\n",
    "def prepare_model(model_name, is_eval=False, has_atten=False, sparsity=80):\n",
    "\tconfig = MixtralConfig(output_router_logits=True, use_cache=False, output_hidden_states=True)\n",
    "\tif is_eval:\n",
    "\t\t# set_teacher_sparsity(50,'c4')\n",
    "\t\tmodel = MixtralTeacher.from_pretrained(\n",
    "\t\t\tpretrained_model_name_or_path=model_name,\n",
    "\t\t\tconfig=config,\n",
    "\t\t\ttorch_dtype=torch.bfloat16,\n",
    "\t\t\tdevice_map=td,\n",
    "\t\t\t)\n",
    "\t\t### Âä†ËΩΩlora_pathÔºöbagelËÆ≠ÁªÉÁöÑ\n",
    "\t\tlora_path_teacher = '/home/bcds/On-the-Fly_MoE_Inference/quantize/saved/training/bagel0/checkpoint-1200'\n",
    "\t\t#### Âä†ËΩΩloraÊ®°ÂûãÂπ∂merge\n",
    "\t\t\n",
    "\t\tprint(f\"load lora model: {lora_path_teacher}\")\n",
    "\t\tmodel = PeftModel.from_pretrained(model, lora_path_teacher, adapter_name=f\"load_teacher\")\n",
    "\t\tmodel.set_adapter(f\"load_teacher\")\n",
    "\t\tmodel = model.merge_and_unload()\n",
    "\t\tmodel.eval()\n",
    "\telse:\n",
    "\t\tset_profile_mode(mode=False)\n",
    "\t\tload_thresholds(\"/home/bcds/On-the-Fly_MoE_Inference/saving/threshold/c4_mixtral/thresholds_0_5.pt\", use_average=False)\n",
    "\t\tmodel = MixtralForCausalLM.from_pretrained(\n",
    "\t\t\tpretrained_model_name_or_path=model_name,\n",
    "\t\t\tconfig=config,\n",
    "\t\t\ttorch_dtype=torch.bfloat16,\n",
    "\t\t\tdevice_map=sd,\n",
    "\t\t)\n",
    "\t\t# lora_path_student=\"/home/bcds/On-the-Fly_MoE_Inference/quantize/output/mixtral/90_10000_expert_atten/1\"\n",
    "\t\t# print(f\"load lora model: {lora_path_student}\")\n",
    "\t\t# model = PeftModel.from_pretrained(model, lora_path_student, adapter_name=f\"load_teacher\")\n",
    "\t\t# model.set_adapter(f\"load_teacher\")\n",
    "\t\t# model = model.merge_and_unload()\n",
    "\t\t# print(f\"set sparsity to {sparsity}\")\n",
    "\t\t# ### ÂåÖË£ÖloraÊ®°Âùó\n",
    "\t\t# rank = 32\n",
    "\t\t# target_modules = [\"w1\",\"w2\",\"w3\"]\n",
    "\t\t# if has_atten:\n",
    "\t\t# \ttarget_modules += [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",]\n",
    "\t\t# peft_config = LoraConfig(\n",
    "\t\t# \tlora_alpha=32,\n",
    "\t\t# \tlora_dropout=0.01,\n",
    "\t\t# \tr=rank,\n",
    "\t\t# \tbias=\"none\",\n",
    "\t\t# \ttarget_modules=target_modules,\n",
    "\t\t# \ttask_type=\"CAUSAL_LM\"\n",
    "\t\t# )\n",
    "\n",
    "\t\t# model = get_peft_model(model, peft_config) \n",
    "\t\t# for name,param in model.named_parameters():\n",
    "\t\t# \tif not any(nd in name for nd in [\"lora_A\",\"lora_B\"]):\n",
    "\t\t# \t\tparam.requires_grad = False\n",
    "\t\t# \telse:\n",
    "\t\t# \t\tparam.requires_grad = True\n",
    "\treturn model\n",
    "\n",
    "model_name = \"mixtral\"\n",
    "dataset_name = \"fineweb\"\n",
    "with open('../path.json', 'r') as file:\n",
    "    paths = json.load(file)\n",
    "    model_name = paths.get(model_name, '')\n",
    "    fineweb_path = paths.get(dataset_name,)\n",
    "\n",
    "sparsity = 0.5\n",
    "# teacher = prepare_model(model_name, is_eval=True)\n",
    "student = prepare_model(model_name, is_eval=False, has_atten=True, sparsity=sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Independent Jane\\nFor all the love, romance and scandal in Jane Austen‚Äôs books, what they are really about is freedom and independence. Independence of thought and the freedom to choose.\\nElizabeth‚Äôs refusal of Mr. Collins offer of marriage showed an independence seldom seen in heroines of the day. Her refusal of Mr. Darcy while triggered by anger showed a level of independence that left him shocked and stunned.\\nThe freedom she exhibited in finally accepting him in direct defiance of Lady Catherine and knowing her father would disapprove was unusual even for Austen. In her last book Anne Elliot is persuaded to refuse Captain Wentworth at Lady Russel‚Äôs insistence.\\nAlthough Jane played by the rules of the day, all of her writing is infused with how she wanted life to be. She ‚Äòscreams‚Äô her outrage at the limitations for women in Emma.\\nWhen accosted by Mrs. Elton, Jane Fairfax says,\\n‚ÄúExcuse me, ma‚Äôam, but this is by no means my intention; I make no inquiry myself, and should be sorry to have any made by my friends. When I am quite determined as to the time, I am not at all afraid of being long unemployed. There are places in town, offices, where inquiry would soon produce something ‚Äî offices for the sale, not quite of human flesh, but of human intellect.‚Äù\\n‚ÄúOh! my dear, human flesh! You quite shock me; if you mean a fling at the slave-trade, I assure you Mr. Suckling was always rather a friend to the abolition.‚Äù\\n‚ÄúI did not mean, I was not thinking of the slave-trade,‚Äù replied Jane; ‚Äúgoverness-trade, I assure you, was all that I had in view; widely different certainly, as to the guilt of those who carry it on; but as to the greater misery of the victims, I do not know where it lies.‚Äù\\nThat same sentiment is emphasized in Emma‚Äôs shock when Mrs. Weston tells her of Frank Churchill‚Äôs secret engagement to Jane.\\n‚ÄúGood God!‚Äù cried Emma, ‚ÄúJane actually on the point of going as governess! What could he mean by such horrible indelicacy? To suffer her to engage herself ‚Äî to suffer her even to think of such a measure!‚Äù\\nI find it interesting that at the moment of Austen‚Äôs birth or there about, John Adams left his farm in Massachusetts for the Continental Congress in Philadelphia. Doesn‚Äôt sound particularly interesting, I know but consider this.\\nJohn Adams left his home in mid-December 1775 to attend an unprecedented meeting of colonial representatives to consider severing ties with their mother country and her monarch; a decision that culminated in a document unlike any ever written. In the mother country, one day in that same cold December a baby girl was born at Steventon Rectory. Her cry was heard by only the people in the house but the years to come would see her pen create works unlike any the world had ever seen.\\nComparing Austen‚Äôs words with Thomas Jefferson‚Äôs may seem a trivialization but I believe that Austen‚Äôs impact on the world is no less important than Jefferson‚Äôs. The effect of Jane‚Äôs writing maybe more subtle than that of the Virginian but it is no less influential.\\nJefferson‚Äôs words instigated and promoted a revolution, a war of independence. Jane‚Äôs words had no such excessive consequence. Still in her own quiet, genteel yet powerful way she declared and promoted the same principles of freedom and self-regulated independence as our American forefathers. In all her novels Jane advocates independence of person and thought, the rights of all and acceptance of responsibility for those rights.\\nJane may not have incited military action as Jefferson did but even as an avowed royalist, I doubt not that Jane Austen firmly believed in his declaration of the right to life, liberty and the pursuit of happiness.', \"Taking Play Seriously\\nBy ROBIN MARANTZ HENIG\\nPublished: February 17, 2008\\nOn a drizzly Tuesday night in late January, 200 people came out to hear a psychiatrist talk rhapsodically about play -- not just the intense, joyous play of children, but play for all people, at all ages, at all times. (All species too; the lecture featured touching photos of a polar bear and a husky engaging playfully at a snowy outpost in northern Canada.) Stuart Brown, president of the National Institute for Play, was speaking at the New York Public Library's main branch on 42nd Street. He created the institute in 1996, after more than 20 years of psychiatric practice and research persuaded him of the dangerous long-term consequences of play deprivation. In a sold-out talk at the library, he and Krista Tippett, host of the public-radio program ''Speaking of Faith,'' discussed the biological and spiritual underpinnings of play. Brown called play part of the ''developmental sequencing of becoming a human primate. If you look at what produces learning and memory and well-being, play is as fundamental as any other aspect of life, including sleep and dreams.''\\nThe message seemed to resonate with audience members, who asked anxious questions about what seemed to be the loss of play in their children's lives. Their concern came, no doubt, from the recent deluge of eulogies to play . Educators fret that school officials are hacking away at recess to make room for an increasingly crammed curriculum. Psychologists complain that overscheduled kids have no time left for the real business of childhood: idle, creative, unstructured free play. Public health officials link insufficient playtime to a rise in childhood obesity. Parents bemoan the fact that kids don't play the way they themselves did -- or think they did. And everyone seems to worry that without the chance to play stickball or hopscotch out on the street, to play with dolls on the kitchen floor or climb trees in the woods, today's children are missing out on something essential.\\nThe success of ''The Dangerous Book for Boys'' -- which has been on the best-seller list for the last nine months -- and its step-by-step instructions for activities like folding paper airplanes is testament to the generalized longing for play's good old days. So were the questions after Stuart Brown's library talk; one woman asked how her children will learn trust, empathy and social skills when their most frequent playing is done online. Brown told her that while video games do have some play value, a true sense of ''interpersonal nuance'' can be achieved only by a child who is engaging all five senses by playing in the three-dimensional world.\\nThis is part of a larger conversation Americans are having about play. Parents bobble between a nostalgia-infused yearning for their children to play and fear that time spent playing is time lost to more practical pursuits. Alarming headlines about U.S. students falling behind other countries in science and math, combined with the ever-more-intense competition to get kids into college, make parents rush to sign up their children for piano lessons and test-prep courses instead of just leaving them to improvise on their own; playtime versus r?m?uilding.\\nDiscussions about play force us to reckon with our underlying ideas about childhood, sex differences, creativity and success. Do boys play differently than girls? Are children being damaged by staring at computer screens and video games? Are they missing something when fantasy play is populated with characters from Hollywood's imagination and not their own? Most of these issues are too vast to be addressed by a single field of study (let alone a magazine article). But the growing science of play does have much to add to the conversation. Armed with research grounded in evolutionary biology and experimental neuroscience, some scientists have shown themselves eager -- at times perhaps a little too eager -- to promote a scientific argument for play. They have spent the past few decades learning how and why play evolved in animals, generating insights that can inform our understanding of its evolution in humans too. They are studying, from an evolutionary perspective, to what extent play is a luxury that can be dispensed with when there are too many other competing claims on the growing brain, and to what extent it is central to how that brain grows in the first place.\\nScientists who study play, in animals and humans alike, are developing a consensus view that play is something more than a way for restless kids to work off steam; more than a way for chubby kids to burn off calories; more than a frivolous luxury. Play, in their view, is a central part of neurological growth and development -- one important way that children build complex, skilled, responsive, socially adept and cognitively flexible brains.\\nTheir work still leaves some questions unanswered, including questions about play's darker, more ambiguous side: is there really an evolutionary or developmental need for dangerous games, say, or for the meanness and hurt feelings that seem to attend so much child's play? Answering these and other questions could help us understand what might be lost if children play less.\"]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "fineweb = load_dataset(\"parquet\", data_files=fineweb_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "texts = fineweb[\"train\"][\"text\"][:2]\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "def expert_logits_loss(logits, teacher_logits, attention_mask, criterion=nn.KLDivLoss()):\n",
    "\texpert_loss = torch.tensor(0.0)\n",
    "\tfor layerid in range(2, 32):\n",
    "\t\ttensor1 = logits[layerid]\n",
    "\t\ttensor2 = teacher_logits[layerid]\n",
    "\n",
    "\t\t# print(tensor1.shape)\n",
    "\t\tmask = attention_mask.unsqueeze(-1).expand_as(tensor1)\n",
    "\t\t# print(mask.shape)\n",
    "\t\tmasked_tensor1 = tensor1 * mask\n",
    "\t\tmasked_tensor2 = tensor2 * mask\n",
    "\t\t\n",
    "\t\ttensor1 = F.softmax(tensor1, dim=0)\n",
    "\t\ttensor2 = F.softmax(tensor2, dim=0)\n",
    "\t\texpert_loss += criterion(tensor1.log(), tensor2)\n",
    "\n",
    "\treturn expert_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512]) tensor(512)\n",
      "expert_loss 0.0046350955963134766\n",
      "norm_loss tensor(1.8272)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/torch/nn/functional.py:3369: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512]) tensor(512)\n",
      "expert_loss 0.005122542381286621\n",
      "norm_loss tensor(1.8347)\n",
      "torch.Size([512]) tensor(152)\n",
      "expert_loss 0.0054895877838134766\n",
      "norm_loss tensor(7.4866)\n",
      "torch.Size([512]) tensor(512)\n",
      "expert_loss 0.004250049591064453\n",
      "norm_loss tensor(2.1639)\n",
      "torch.Size([512]) tensor(512)\n",
      "expert_loss 0.005034327507019043\n",
      "norm_loss tensor(1.8490)\n",
      "torch.Size([512]) tensor(512)\n",
      "expert_loss 0.0038340091705322266\n",
      "norm_loss tensor(2.6616)\n",
      "torch.Size([512]) tensor(512)\n",
      "expert_loss 0.003953456878662109\n",
      "norm_loss tensor(1.5148)\n",
      "torch.Size([512]) tensor(512)\n",
      "expert_loss 0.0032494664192199707\n",
      "norm_loss tensor(1.3836)\n",
      "torch.Size([512]) tensor(512)\n",
      "expert_loss 0.0036765336990356445\n",
      "norm_loss tensor(1.6332)\n",
      "torch.Size([512]) tensor(226)\n",
      "expert_loss 0.006219148635864258\n",
      "norm_loss tensor(5.4635)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "for text in texts:\n",
    "    inputs = tokenizer(text, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    inputs[\"labels\"] = inputs.input_ids.clone()\n",
    "    output = student(**inputs)\n",
    "\n",
    "    t_output = teacher(**inputs)\n",
    "\n",
    "    tensor1 = output[\"router_logits\"]\n",
    "    tensor2 = t_output[\"router_logits\"]\n",
    "    attention_mask = inputs[\"attention_mask\"].squeeze(0)\n",
    "    print(attention_mask.size(), attention_mask.sum())\n",
    "    dl = expert_logits_loss(tensor1, tensor2, attention_mask).detach()\n",
    "    print(\"expert_loss\", dl.item())\n",
    "    print(\"norm_loss\", output[\"loss\"].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "spar_model_logits = []\n",
    "for text in texts:\n",
    "    inputs = tokenizer(text, padding=\"max_length\", truncation=True, max_length=64, return_tensors=\"pt\")\n",
    "    inputs[\"labels\"] = inputs.input_ids.clone()\n",
    "    output = student(**inputs)\n",
    "\n",
    "    tensor1 = output[\"router_logits\"]\n",
    "    spar_model_logits.append(tensor1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(spar_model_logits, 'sparsity-2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2519875/1518376346.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  normal = torch.load(\"normal.pt\")\n"
     ]
    }
   ],
   "source": [
    "normal = torch.load(\"normal.pt\")\n",
    "sparsity = spar_model_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 64\n"
     ]
    }
   ],
   "source": [
    "print(len(normal[0]), len(normal[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_kl(logits, teacher_logits, attention_mask):\n",
    "\tteacher_probs = F.softmax(teacher_logits, dim=-1, dtype=torch.float32)\n",
    "\tinf_mask = torch.isinf(logits)\n",
    "\tstudent_logprobs = F.log_softmax(logits, dim=-1, dtype=torch.float32)\n",
    "\tprod_probs = torch.masked_fill(teacher_probs * student_logprobs, inf_mask, 0)\n",
    "\n",
    "\tx = torch.sum(prod_probs, dim=-1).view(-1)\n",
    "\tmask = (attention_mask != 0).int()  #### paddingÁöÑÈÉ®ÂàÜ, attention_mask==0\n",
    "\t# print(x.shape)\n",
    "\t# print(\"mask\", mask.shape) # mask torch.Size([6, 512])\n",
    "\tdistil_loss = -torch.sum(x * mask.view(-1), dim=0) / torch.sum(mask.view(-1), dim=0)\n",
    "\treturn distil_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n",
      "tensor(0.0285, grad_fn=<AddBackward0>)\n",
      "torch.Size([64])\n",
      "tensor(0.0199, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcds/.conda/envs/llm/lib/python3.9/site-packages/torch/nn/functional.py:3369: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "different_count = [0] * 32\n",
    "for textid in range(2):\n",
    "    for layerid in range(len(normal[0])):\n",
    "        # [seq_len, 8] -> Á¨¨‰∏Ä‰∏™Áª¥Â∫¶\n",
    "        for seq_len in range(len(normal[0][0])):\n",
    "            tensor1 = normal[textid][layerid][seq_len]\n",
    "            tensor2 = sparsity[textid][layerid][seq_len]\n",
    "\n",
    "        # print(normal[textid][layerid])\n",
    "            # # Âèñ top2 ÁöÑÂÄºÂíåÁ¥¢Âºï\n",
    "            # top2_values, top2_indices = torch.topk(tensor2, k=2)\n",
    "\n",
    "            _, top2_indices1 = torch.topk(tensor1, k=2)\n",
    "            # print(top2_indices1) # tensor([2, 3])\n",
    "            _, top2_indices2 = torch.topk(tensor2, k=2)\n",
    "            # print(top2_indices2) # tensor([3, 2])\n",
    "\n",
    "            # Â∞ÜÁ¥¢ÂºïËΩ¨Êç¢‰∏∫ÈõÜÂêà\n",
    "            set1 = set(top2_indices1.tolist())\n",
    "            set2 = set(top2_indices2.tolist())\n",
    "\n",
    "            # ËÆ°ÁÆóÁõ∏ÂêåÂÖÉÁ¥†ÁöÑ‰∏™Êï∞\n",
    "            same_count = len(set1.intersection(set2))\n",
    "\n",
    "            # ËÆ°ÁÆó‰∏çÂêåÁöÑ‰∏™Êï∞\n",
    "            different_count[layerid] += (2 - same_count)\n",
    "            # print(different_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0000000 \n",
      "0.0000000 \n",
      "0.0001059 \n",
      "0.0001240 \n",
      "0.0001507 \n",
      "0.0001850 \n",
      "0.0001907 \n",
      "0.0002174 \n",
      "0.0002422 \n",
      "0.0002594 \n",
      "0.0001516 \n",
      "0.0002861 \n",
      "0.0003090 \n",
      "0.0002213 \n",
      "0.0002518 \n",
      "0.0003891 \n",
      "0.0003433 \n",
      "0.0003681 \n",
      "0.0004158 \n",
      "0.0008316 \n",
      "0.0014877 \n",
      "0.0013885 \n",
      "0.0013580 \n",
      "0.0014343 \n",
      "0.0020599 \n",
      "0.0011292 \n",
      "0.0012360 \n",
      "0.0014038 \n",
      "0.0009155 \n",
      "0.0008392 \n",
      "0.0008888 \n",
      "0.0050049 \n"
     ]
    }
   ],
   "source": [
    "for i in range(32):\n",
    "    print('{:.7f} '.format(different_loss[i] / (2))) # 64 * 2 = 128 256 experts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0000 %\n",
      "0.0000 %\n",
      "0.7812 %\n",
      "2.7344 %\n",
      "1.5625 %\n",
      "1.5625 %\n",
      "3.1250 %\n",
      "2.7344 %\n",
      "4.2969 %\n",
      "4.2969 %\n",
      "1.1719 %\n",
      "1.9531 %\n",
      "4.2969 %\n",
      "2.7344 %\n",
      "4.2969 %\n",
      "2.7344 %\n",
      "5.0781 %\n",
      "3.9062 %\n",
      "2.7344 %\n",
      "0.7812 %\n",
      "5.0781 %\n",
      "3.9062 %\n",
      "7.0312 %\n",
      "3.9062 %\n",
      "4.2969 %\n",
      "6.6406 %\n",
      "7.4219 %\n",
      "5.0781 %\n",
      "2.7344 %\n",
      "5.0781 %\n",
      "5.8594 %\n",
      "5.0781 %\n"
     ]
    }
   ],
   "source": [
    "for i in range(32):\n",
    "    print('{:.4f} %'.format(different_count[i] / 256 * 100)) # 64 * 2 = 128 256 experts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.6875 %\n",
      "1.9531 %\n",
      "3.9062 %\n",
      "7.4219 %\n",
      "6.6406 %\n",
      "7.0312 %\n",
      "7.8125 %\n",
      "8.5938 %\n",
      "10.5469 %\n",
      "10.9375 %\n",
      "7.0312 %\n",
      "6.6406 %\n",
      "10.1562 %\n",
      "6.6406 %\n",
      "12.8906 %\n",
      "9.7656 %\n",
      "8.9844 %\n",
      "8.2031 %\n",
      "7.4219 %\n",
      "12.5000 %\n",
      "12.1094 %\n",
      "9.3750 %\n",
      "14.8438 %\n",
      "12.8906 %\n",
      "16.0156 %\n",
      "12.8906 %\n",
      "12.5000 %\n",
      "11.7188 %\n",
      "11.7188 %\n",
      "5.8594 %\n",
      "9.7656 %\n",
      "13.2812 %\n"
     ]
    }
   ],
   "source": [
    "for i in range(32):\n",
    "    print('{:.4f} %'.format(different_count[i] / 256 * 100)) # 64 * 2 = 128 256 experts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.8594 %\n",
      "2.7344 %\n",
      "3.9062 %\n",
      "7.8125 %\n",
      "6.2500 %\n",
      "8.2031 %\n",
      "8.5938 %\n",
      "7.8125 %\n",
      "8.5938 %\n",
      "11.3281 %\n",
      "7.4219 %\n",
      "5.8594 %\n",
      "7.8125 %\n",
      "7.8125 %\n",
      "10.9375 %\n",
      "9.7656 %\n",
      "8.2031 %\n",
      "6.2500 %\n",
      "7.8125 %\n",
      "12.1094 %\n",
      "12.1094 %\n",
      "10.1562 %\n",
      "16.4062 %\n",
      "10.5469 %\n",
      "13.6719 %\n",
      "13.6719 %\n",
      "12.8906 %\n",
      "13.6719 %\n",
      "11.7188 %\n",
      "7.0312 %\n",
      "6.2500 %\n",
      "13.2812 %\n"
     ]
    }
   ],
   "source": [
    "for i in range(32):\n",
    "    print('{:.4f} %'.format(different_count[i] / 256 * 100)) # 64 * 2 = 128 256 experts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0000 %\n",
      "0.0000 %\n",
      "4.2969 %\n",
      "5.8594 %\n",
      "4.2969 %\n",
      "4.2969 %\n",
      "8.9844 %\n",
      "5.0781 %\n",
      "10.5469 %\n",
      "9.7656 %\n",
      "8.2031 %\n",
      "7.4219 %\n",
      "10.1562 %\n",
      "7.0312 %\n",
      "9.7656 %\n",
      "11.3281 %\n",
      "8.9844 %\n",
      "6.2500 %\n",
      "5.8594 %\n",
      "13.2812 %\n",
      "14.8438 %\n",
      "10.1562 %\n",
      "17.9688 %\n",
      "8.5938 %\n",
      "11.7188 %\n",
      "13.2812 %\n",
      "14.0625 %\n",
      "12.1094 %\n",
      "12.1094 %\n",
      "8.5938 %\n",
      "13.6719 %\n",
      "16.7969 %\n"
     ]
    }
   ],
   "source": [
    "for i in range(32):\n",
    "    print('{:.4f} %'.format(different_count[i] / 256 * 100)) # 64 * 2 = 128 256 experts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lz/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "MixtralForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model path: /home/lz/Mixtral-8x7B-v0.1 \n",
      "dataset path: /home/lz/c4 \n",
      "save path: /home/lz/On-the-Fly_MoE_Inference/saving/threshold/c4_mixtral_up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:37<00:00,  1.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with sparsity of 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "# from modeling_llama_up import set_profile_mode\n",
    "import os\n",
    "import csv\n",
    "from utils import get_model, set_seed\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "### from path.json read paths of model and dataset\n",
    "model_name = \"mixtral\"\n",
    "dataset_name = \"c4\"\n",
    "with open('../path.json', 'r') as file:\n",
    "    paths = json.load(file)\n",
    "    model_path = paths.get(model_name, '')\n",
    "    dataset_path = paths.get(dataset_name, '')\n",
    "    save_path = paths.get('chess_up_threshold','')\n",
    "    print('model path:', model_path, '\\ndataset path:', dataset_path, '\\nsave path:', save_path)\n",
    "\n",
    "set_seed(42)\n",
    "# c4data = get_c4_data(model_path, dataset_path, sample_num = 400)\n",
    "model = get_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixtralForCausalLM(\n",
       "  (model): MixtralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MixtralDecoderLayer(\n",
       "        (self_attn): MixtralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MixtralRotaryEmbedding()\n",
       "        )\n",
       "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
       "          (gate): Linear(in_features=4096, out_features=8, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-7): 8 x MixtralBLockSparseTop2MLP(\n",
       "              (w1): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (w2): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "              (w3): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): MixtralRMSNorm()\n",
       "        (post_attention_layernorm): MixtralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MixtralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCannot read properties of undefined (reading 'path'). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import json\n",
    "with open(\"../path.json\",\"r\") as file:\n",
    "    fineweb = json.load(file)['fineweb']\n",
    "\n",
    "raw_datasets = load_dataset(\"parquet\", data_files = fineweb)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "def process(example):\n",
    "    ids = tokenizer.encode(example['text'])\n",
    "    out = {'ids': ids, 'len': len(ids)}\n",
    "    return out\n",
    "\n",
    "tokenized = raw_datasets.map(process, desc='tokenizing raw datasets', num_proc=64)\n",
    "import numpy as np\n",
    "datasets = dict()\n",
    "\n",
    "for split, dset in tokenized.items():\n",
    "    datasets[split] = []\n",
    "    length = np.sum(dset['len'])\n",
    "    datasets[split] = np.ndarray((length, ), np.uint32)\n",
    "    idx = 0\n",
    "    for row in dset:\n",
    "        datasets[split][idx:idx + row['len']] = row['ids']\n",
    "        idx += row['len']\n",
    "torch.save(datasets, 'datasets.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_56979/3427838030.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  datasets = torch.load('./threshold/chess/datasets.pt')\n"
     ]
    }
   ],
   "source": [
    "datasets = torch.load('./threshold/chess/datasets.pt')\n",
    "import torch\n",
    "import numpy as np\n",
    "def get_batch(data, batch_size, block_size):\n",
    "    start_idxs = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in start_idxs])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in start_idxs])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "14\n",
      "16\n",
      "18\n",
      "20\n",
      "22\n",
      "24\n",
      "26\n",
      "28\n",
      "30\n",
      "32\n",
      "34\n",
      "36\n",
      "38\n",
      "40\n",
      "42\n",
      "44\n",
      "46\n",
      "48\n",
      "50\n",
      "52\n",
      "54\n",
      "56\n",
      "58\n",
      "60\n",
      "62\n"
     ]
    }
   ],
   "source": [
    "sparsity_level = 0.7\n",
    "# device = 'cuda:1'\n",
    "device_2 = 'cpu'\n",
    "avg_loss = 0.0\n",
    "n_batch = 64\n",
    "# accum_steps = 4 \n",
    "accum_steps = 2\n",
    "batch_size = 1\n",
    "block_size = 2048\n",
    "torch.manual_seed(42)\n",
    "n_layers = len(model.model.layers)\n",
    "n_experts = len(model.model.layers[0].block_sparse_moe.experts)\n",
    "\n",
    "up_proj_states_thresholds = [torch.zeros([n_experts,]) for _ in range(n_layers)]\n",
    "gate_proj_states_mean_squares = [[torch.zeros(model.config.intermediate_size) for _ in range(n_experts)] for _ in range(n_layers)]\n",
    "\n",
    "up_states = [[torch.zeros([accum_steps * batch_size * block_size //2, model.config.intermediate_size]) for _ in range(n_experts)] for _ in range(n_layers)]\n",
    "gate_states = [[torch.zeros([accum_steps * batch_size * block_size //2, model.config.intermediate_size]) for _ in range(n_experts)] for _ in range(n_layers)]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step in range(n_batch // accum_steps):\n",
    "        print(step * accum_steps)\n",
    "        all_counts = [0 for _ in range(n_layers * n_experts)]\n",
    "        for batch_idx in range(accum_steps):\n",
    "            # print('batch_idx:', batch_idx)\n",
    "            inputs, labels = get_batch(datasets['validation'], batch_size, block_size)\n",
    "            inputs = inputs.cuda()\n",
    "            outputs = model(inputs, labels=inputs)\n",
    "            avg_loss = avg_loss + outputs.loss / n_batch\n",
    "\n",
    "            for layer_idx in range(n_layers):\n",
    "                for expert_idx in range(n_experts):\n",
    "                    counts = all_counts[layer_idx * n_experts + expert_idx]\n",
    "\n",
    "                    states = model.model.layers[layer_idx].block_sparse_moe.experts[expert_idx].up_proj_states.reshape(-1, model.config.intermediate_size)\n",
    "                    cur_counts = states.size(0)\n",
    "                    # print('counts and cur_counts:',counts, cur_counts)\n",
    "                    # print(states.size())\n",
    "                    # print(up_states[layer_idx][expert_idx][counts : counts+cur_counts, :].size())\n",
    "                    up_states[layer_idx][expert_idx][counts : counts+cur_counts, :] = states\n",
    "\n",
    "                    states = model.model.layers[layer_idx].block_sparse_moe.experts[expert_idx].gate_proj_states.reshape(-1, model.config.intermediate_size)\n",
    "                    gate_states[layer_idx][expert_idx][counts : counts+cur_counts, :] = states\n",
    "                    # counts += cur_counts\n",
    "                    all_counts[layer_idx * n_experts + expert_idx] += cur_counts\n",
    "\n",
    "        for layer_idx in range(n_layers):   \n",
    "            for expert_idx in range(n_experts):\n",
    "                # print('layer_idx:', layer_idx, 'expert_idx:', expert_idx)\n",
    "                useful_num = all_counts[layer_idx * n_experts + expert_idx]\n",
    "                topk_num = int(useful_num * model.config.intermediate_size * sparsity_level)\n",
    "                up_proj_states_thresholds[layer_idx][expert_idx] += up_states[layer_idx][expert_idx][0:useful_num,:].to(device_2).abs().flatten().kthvalue(topk_num).values.to('cpu')\n",
    "                gate_proj_states_mean_squares[layer_idx][expert_idx] += (torch.sum(gate_states[layer_idx][expert_idx][0:useful_num,:].to(device_2) ** 2, dim=0).to('cpu') / useful_num).to('cpu')\n",
    "\n",
    "for layer_idx in range(n_layers):\n",
    "    for expert_idx in range(n_experts):\n",
    "        gate_proj_states_mean_squares[layer_idx][expert_idx] /= n_batch // accum_steps\n",
    "        up_proj_states_thresholds[layer_idx][expert_idx] /= n_batch // accum_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0., 0.,  ..., 0., 0., 0.]), tensor(0.0481))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gate_proj_states_mean_squares[0][1],up_proj_states_thresholds[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "importance_thresholds = [torch.zeros([n_experts,]) for _ in range(n_layers)]\n",
    "up_proj_states_thresholds_2 = [[torch.zeros(model.config.intermediate_size) for _ in range(n_experts)] for _ in range(n_layers)]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step in range(n_batch // accum_steps):\n",
    "        print(step * accum_steps)\n",
    "        all_counts = [0 for _ in range(n_layers * n_experts)]\n",
    "        for batch_idx in range(accum_steps):\n",
    "            inputs, labels = get_batch(datasets['validation'], batch_size, block_size)\n",
    "            inputs = inputs.cuda()\n",
    "            outputs = model(inputs, labels=inputs)\n",
    "            avg_loss = avg_loss + outputs.loss / n_batch\n",
    "\n",
    "            for layer_idx in range(n_layers):\n",
    "                for expert_idx in range(n_experts):\n",
    "                    counts = all_counts[layer_idx * n_experts + expert_idx]\n",
    "                    states = model.model.layers[layer_idx].block_sparse_moe.experts[expert_idx].up_proj_states.reshape(-1, states.size(-1))\n",
    "                    cur_counts = states.size(0)\n",
    "                    up_states[layer_idx][expert_idx][counts:cur_counts+counts, :] = states\n",
    "                    # counts += cur_counts\n",
    "                    all_counts[layer_idx * n_experts + expert_idx] += cur_counts\n",
    "                \n",
    "        for layer_idx in range(n_layers):   \n",
    "            for expert_idx in range(n_experts):\n",
    "                useful_num = all_counts[layer_idx * n_experts + expert_idx]\n",
    "                importance_scores = up_states[layer_idx][expert_idx][:useful_num,:] ** 2 * gate_proj_states_mean_squares[layer_idx][expert_idx]\n",
    "                importance_thresholds[layer_idx][expert_idx] += importance_scores.to(device_2).flatten().kthvalue(int(importance_scores.numel() * sparsity_level)).values.to('cpu')\n",
    "\n",
    "for layer_idx in range(n_layers):\n",
    "    for expert_idx in range(n_experts):\n",
    "        importance_thresholds[layer_idx][expert_idx] /= n_batch // accum_steps\n",
    "        up_proj_states_thresholds_2[layer_idx][expert_idx] = (importance_thresholds[layer_idx][expert_idx].expand_as(up_proj_states_thresholds_2[layer_idx][expert_idx]) / gate_proj_states_mean_squares[layer_idx][expert_idx]) ** 0.5\n",
    "\n",
    "thresholds = {'up_proj_states_thresholds': up_proj_states_thresholds, 'up_proj_states_thresholds_2': up_proj_states_thresholds_2}\n",
    "\n",
    "torch.save(thresholds, f'{save_path}/thresholds.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[tensor([0.0828, 0.0886, 0.0805,  ..., 0.0463, 0.0734, 0.0805]), tensor([0.0914, 0.0872, 0.1076,  ..., 0.0616, 0.0878, 0.0748]), tensor([0.1119, 0.0712, 0.1166,  ..., 0.0903, 0.0871, 0.0887]), tensor([0.1036, 0.0800, 0.1252,  ..., 0.0806, 0.0852, 0.0863]), tensor([0.1034, 0.1043, 0.1129,  ..., 0.0677, 0.0950, 0.0903]), tensor([0.0984, 0.0832, 0.1072,  ..., 0.0709, 0.0874, 0.0822]), tensor([0.0671, 0.1130, 0.1085,  ..., 0.0673, 0.0583, 0.0829]), tensor([0.0929, 0.0765, 0.0864,  ..., 0.0673, 0.0767, 0.0761])], [tensor([0.1306, 0.0817, 0.1159,  ..., 0.0995, 0.1281, 0.1266]), tensor([0.1352, 0.1360, 0.1233,  ..., 0.1193, 0.1400, 0.1366]), tensor([0.1269, 0.1036, 0.1278,  ..., 0.1105, 0.1419, 0.1258]), tensor([0.1210, 0.1199, 0.1266,  ..., 0.1159, 0.1094, 0.1222]), tensor([0.1323, 0.1283, 0.1320,  ..., 0.1297, 0.1526, 0.1441]), tensor([0.1278, 0.1137, 0.1114,  ..., 0.1229, 0.1466, 0.1378]), tensor([0.1113, 0.0857, 0.1023,  ..., 0.0926, 0.1235, 0.1224]), tensor([0.1244, 0.1014, 0.1169,  ..., 0.1147, 0.1362, 0.1284])], [tensor([0.1639, 0.1715, 0.1338,  ..., 0.1645, 0.1519, 0.1732]), tensor([0.1690, 0.1850, 0.1427,  ..., 0.1847, 0.1553, 0.1996]), tensor([0.1743, 0.1864, 0.1308,  ..., 0.1797, 0.1694, 0.1909]), tensor([0.1667, 0.1786, 0.1323,  ..., 0.1714, 0.1711, 0.1831]), tensor([0.1802, 0.1873, 0.1224,  ..., 0.1781, 0.1655, 0.1828]), tensor([0.1873, 0.1856, 0.1409,  ..., 0.1784, 0.1793, 0.1816]), tensor([0.1865, 0.1891, 0.1262,  ..., 0.1859, 0.1721, 0.1914]), tensor([0.1342, 0.1617, 0.1214,  ..., 0.1700, 0.1236, 0.1623])], [tensor([0.1962, 0.2030, 0.2297,  ..., 0.2180, 0.1913, 0.2298]), tensor([0.2021, 0.2148, 0.2252,  ..., 0.2313, 0.1848, 0.2193]), tensor([0.2053, 0.2333, 0.2371,  ..., 0.2171, 0.1794, 0.2178]), tensor([0.1815, 0.2070, 0.2090,  ..., 0.1734, 0.1606, 0.2165]), tensor([0.2053, 0.2235, 0.2438,  ..., 0.2267, 0.1685, 0.2344]), tensor([0.1891, 0.1853, 0.2319,  ..., 0.2046, 0.1884, 0.2129]), tensor([0.2210, 0.2196, 0.2353,  ..., 0.2262, 0.1705, 0.2315]), tensor([0.2057, 0.2289, 0.2410,  ..., 0.2276, 0.1838, 0.2304])], [tensor([0.2697, 0.2389, 0.2349,  ..., 0.2322, 0.2968, 0.2362]), tensor([0.2243, 0.2268, 0.2361,  ..., 0.2184, 0.2889, 0.2307]), tensor([0.2555, 0.2530, 0.2392,  ..., 0.2439, 0.2918, 0.2448]), tensor([0.2454, 0.2334, 0.2442,  ..., 0.2356, 0.2719, 0.2454]), tensor([0.2708, 0.2337, 0.2364,  ..., 0.2135, 0.2715, 0.2487]), tensor([0.1862, 0.2193, 0.2423,  ..., 0.2159, 0.2815, 0.2395]), tensor([0.2354, 0.2147, 0.2382,  ..., 0.2190, 0.2862, 0.2478]), tensor([0.2632, 0.2262, 0.2260,  ..., 0.2422, 0.2774, 0.2356])], [tensor([0.3548, 0.2951, 0.3075,  ..., 0.3404, 0.3279, 0.3970]), tensor([0.3651, 0.3054, 0.2708,  ..., 0.3437, 0.3076, 0.4199]), tensor([0.3266, 0.2358, 0.2756,  ..., 0.3259, 0.3086, 0.3187]), tensor([0.3255, 0.2685, 0.2783,  ..., 0.3492, 0.2993, 0.2877]), tensor([0.3638, 0.3023, 0.2820,  ..., 0.3548, 0.3231, 0.4095]), tensor([0.3666, 0.3105, 0.2890,  ..., 0.3294, 0.3379, 0.3932]), tensor([0.3311, 0.2912, 0.2988,  ..., 0.3125, 0.3111, 0.3659]), tensor([0.3367, 0.2865, 0.2809,  ..., 0.3470, 0.3143, 0.3409])], [tensor([0.3788, 0.3308, 0.3215,  ..., 0.3459, 0.3562, 0.3058]), tensor([0.3737, 0.3370, 0.3010,  ..., 0.3267, 0.3636, 0.3301]), tensor([0.3565, 0.3436, 0.3303,  ..., 0.3498, 0.3365, 0.3401]), tensor([0.3433, 0.3311, 0.3519,  ..., 0.3515, 0.3468, 0.3573]), tensor([0.3722, 0.3642, 0.3061,  ..., 0.3483, 0.3514, 0.3325]), tensor([0.3416, 0.3350, 0.3455,  ..., 0.3199, 0.3488, 0.3352]), tensor([0.3143, 0.3287, 0.3320,  ..., 0.3320, 0.3240, 0.3386]), tensor([0.3683, 0.3410, 0.3282,  ..., 0.3452, 0.3475, 0.3422])], [tensor([0.4002, 0.3867, 0.3903,  ..., 0.4063, 0.4270, 0.3641]), tensor([0.3799, 0.3916, 0.3617,  ..., 0.3851, 0.4214, 0.3392]), tensor([0.3926, 0.3810, 0.3745,  ..., 0.3960, 0.4419, 0.3433]), tensor([0.4133, 0.3818, 0.3638,  ..., 0.4154, 0.4149, 0.3957]), tensor([0.4055, 0.4107, 0.3594,  ..., 0.4082, 0.4318, 0.3491]), tensor([0.3751, 0.3615, 0.3592,  ..., 0.3843, 0.4096, 0.3634]), tensor([0.3882, 0.3636, 0.3716,  ..., 0.3553, 0.4251, 0.3683]), tensor([0.3849, 0.3750, 0.3675,  ..., 0.4061, 0.4204, 0.3784])], [tensor([0.3773, 0.4129, 0.4036,  ..., 0.3977, 0.3877, 0.4386]), tensor([0.3343, 0.4096, 0.4139,  ..., 0.4019, 0.4192, 0.4381]), tensor([0.3627, 0.3717, 0.4152,  ..., 0.3840, 0.3820, 0.4146]), tensor([0.3821, 0.4131, 0.4114,  ..., 0.4338, 0.3906, 0.4359]), tensor([0.3711, 0.3887, 0.4244,  ..., 0.3610, 0.3585, 0.4079]), tensor([0.3488, 0.3764, 0.4285,  ..., 0.4151, 0.3359, 0.4150]), tensor([0.3966, 0.3847, 0.4474,  ..., 0.3735, 0.3822, 0.4384]), tensor([0.4101, 0.3990, 0.4315,  ..., 0.4363, 0.3899, 0.4138])], [tensor([0.4123, 0.4884, 0.4758,  ..., 0.3920, 0.4063, 0.4520]), tensor([0.3888, 0.5010, 0.4683,  ..., 0.4153, 0.5064, 0.4475]), tensor([0.4332, 0.4828, 0.4838,  ..., 0.3503, 0.3830, 0.2989]), tensor([0.4326, 0.4095, 0.4514,  ..., 0.4015, 0.4717, 0.4089]), tensor([0.4923, 0.5054, 0.4706,  ..., 0.3900, 0.4299, 0.4389]), tensor([0.4270, 0.4761, 0.4688,  ..., 0.3896, 0.4001, 0.4222]), tensor([0.4558, 0.4410, 0.4559,  ..., 0.3669, 0.3895, 0.4267]), tensor([0.3538, 0.4730, 0.4467,  ..., 0.2949, 0.4196, 0.4179])], [tensor([0.3922, 0.4674, 0.4554,  ..., 0.5024, 0.4551, 0.4830]), tensor([0.3256, 0.4577, 0.4903,  ..., 0.5166, 0.5480, 0.4732]), tensor([0.4428, 0.4696, 0.4792,  ..., 0.4915, 0.4500, 0.3717]), tensor([0.3205, 0.4505, 0.4809,  ..., 0.5120, 0.4951, 0.4728]), tensor([0.4139, 0.4462, 0.4388,  ..., 0.4715, 0.4804, 0.4261]), tensor([0.4509, 0.4382, 0.4227,  ..., 0.4789, 0.4978, 0.4132]), tensor([0.4368, 0.4778, 0.4474,  ..., 0.4439, 0.4845, 0.3794]), tensor([0.4578, 0.4564, 0.4851,  ..., 0.4645, 0.4858, 0.4510])], [tensor([0.4519, 0.4711, 0.4989,  ..., 0.5494, 0.5037, 0.5192]), tensor([0.4760, 0.5092, 0.5050,  ..., 0.5419, 0.4698, 0.4931]), tensor([0.5089, 0.5490, 0.5176,  ..., 0.4599, 0.5435, 0.5176]), tensor([0.5266, 0.5454, 0.6073,  ..., 0.6015, 0.5406, 0.5142]), tensor([0.4583, 0.5503, 0.4959,  ..., 0.5419, 0.5444, 0.5299]), tensor([0.5245, 0.5433, 0.5190,  ..., 0.6062, 0.5632, 0.5887]), tensor([0.4913, 0.4934, 0.5088,  ..., 0.5494, 0.4841, 0.4823]), tensor([0.5198, 0.4691, 0.5150,  ..., 0.5027, 0.4838, 0.5354])], [tensor([0.4475, 0.5622, 0.5451,  ..., 0.5776, 0.5242, 0.6235]), tensor([0.4801, 0.5002, 0.5238,  ..., 0.5450, 0.3680, 0.5681]), tensor([0.4064, 0.5345, 0.4443,  ..., 0.5130, 0.4862, 0.5932]), tensor([0.4385, 0.5542, 0.4859,  ..., 0.5790, 0.4212, 0.5607]), tensor([0.4138, 0.4996, 0.4330,  ..., 0.5880, 0.5086, 0.6150]), tensor([0.4153, 0.5200, 0.5121,  ..., 0.5697, 0.4784, 0.5708]), tensor([0.4175, 0.4916, 0.4725,  ..., 0.5316, 0.4562, 0.5438]), tensor([0.5425, 0.5899, 0.5225,  ..., 0.6154, 0.6030, 0.6297])], [tensor([0.4750, 0.5623, 0.5121,  ..., 0.5033, 0.3440, 0.5698]), tensor([0.4553, 0.5925, 0.5269,  ..., 0.4232, 0.2533, 0.5933]), tensor([0.4873, 0.6167, 0.5692,  ..., 0.4897, 0.3064, 0.5895]), tensor([0.5029, 0.5898, 0.5716,  ..., 0.6198, 0.5152, 0.5679]), tensor([0.5899, 0.6048, 0.5655,  ..., 0.6401, 0.5018, 0.5686]), tensor([0.4523, 0.5902, 0.6220,  ..., 0.5433, 0.3208, 0.5824]), tensor([0.5418, 0.5889, 0.5559,  ..., 0.4953, 0.2517, 0.5816]), tensor([0.5089, 0.6239, 0.6277,  ..., 0.4916, 0.2573, 0.6319])], [tensor([0.6667, 0.7065, 0.7084,  ..., 0.7065, 0.5778, 0.7511]), tensor([0.6169, 0.6001, 0.6643,  ..., 0.6057, 0.5000, 0.6021]), tensor([0.6656, 0.6449, 0.6785,  ..., 0.6558, 0.4644, 0.7235]), tensor([0.6389, 0.6458, 0.6008,  ..., 0.7034, 0.4476, 0.7519]), tensor([0.5784, 0.6237, 0.6697,  ..., 0.6399, 0.5850, 0.5749]), tensor([0.5618, 0.6463, 0.6007,  ..., 0.6310, 0.4549, 0.5482]), tensor([0.5714, 0.6065, 0.5677,  ..., 0.6115, 0.4712, 0.5848]), tensor([0.6044, 0.6745, 0.6525,  ..., 0.6201, 0.5005, 0.6406])], [tensor([0.5993, 0.6456, 0.4998,  ..., 0.6740, 0.5252, 0.7366]), tensor([0.4528, 0.6871, 0.4911,  ..., 0.6088, 0.5983, 0.7486]), tensor([0.6293, 0.7037, 0.5519,  ..., 0.6957, 0.5990, 0.7515]), tensor([0.6137, 0.5925, 0.5510,  ..., 0.6366, 0.5774, 0.7513]), tensor([0.5985, 0.6606, 0.5647,  ..., 0.6590, 0.6387, 0.7350]), tensor([0.5851, 0.6671, 0.5366,  ..., 0.5700, 0.5928, 0.7176]), tensor([0.6862, 0.6951, 0.4662,  ..., 0.6081, 0.6202, 0.7545]), tensor([0.4627, 0.7474, 0.3719,  ..., 0.6853, 0.5120, 0.8393])], [tensor([0.8090, 0.7478, 0.6820,  ..., 0.7377, 0.8393, 0.7997]), tensor([0.8414, 0.7505, 0.6931,  ..., 0.7898, 0.8409, 0.8226]), tensor([0.8253, 0.6038, 0.7105,  ..., 0.7337, 0.8538, 0.8063]), tensor([0.8141, 0.7317, 0.6675,  ..., 0.7582, 0.8233, 0.7730]), tensor([0.7480, 0.7444, 0.6519,  ..., 0.6367, 0.8249, 0.8104]), tensor([0.8604, 0.6955, 0.7422,  ..., 0.7727, 0.8362, 0.8090]), tensor([0.8173, 0.7782, 0.7061,  ..., 0.7329, 0.8424, 0.8178]), tensor([0.8008, 0.8317, 0.6449,  ..., 0.7451, 0.8215, 0.7916])], [tensor([1.0387, 0.6355, 0.5699,  ..., 0.6173, 0.9827, 0.7947]), tensor([0.9601, 0.6503, 0.5805,  ..., 0.5754, 0.9855, 0.8670]), tensor([0.9095, 0.6849, 0.6200,  ..., 0.5448, 0.9098, 0.8568]), tensor([0.7750, 0.6698, 0.7146,  ..., 0.7067, 0.8896, 0.8395]), tensor([0.9175, 0.4530, 0.7340,  ..., 0.6094, 0.9428, 0.8229]), tensor([0.8748, 0.7520, 0.5438,  ..., 0.5361, 0.8881, 0.8137]), tensor([0.8653, 0.7044, 0.7143,  ..., 0.7651, 0.8035, 0.8042]), tensor([0.8490, 0.6698, 0.6388,  ..., 0.6756, 0.8863, 0.8259])], [tensor([0.9320, 0.5422, 0.8279,  ..., 1.1076, 0.8281, 0.9969]), tensor([0.9495, 0.8084, 0.9896,  ..., 0.8966, 0.8693, 1.0838]), tensor([1.0029, 0.5628, 0.9456,  ..., 1.1208, 0.5634, 1.1540]), tensor([0.9385, 0.5925, 0.9724,  ..., 1.1719, 0.7545, 1.1093]), tensor([0.9046, 0.6263, 0.9384,  ..., 1.1004, 0.6420, 1.0680]), tensor([0.9228, 0.6505, 0.8305,  ..., 0.9780, 0.6987, 0.8361]), tensor([0.9723, 0.5339, 0.9328,  ..., 1.0522, 0.8058, 1.0390]), tensor([0.8961, 0.6999, 0.9297,  ..., 1.0278, 0.6935, 0.9001])], [tensor([1.1964, 1.0642, 1.2270,  ..., 0.7853, 1.1454, 0.9252]), tensor([0.9658, 1.1337, 1.2644,  ..., 0.9009, 1.0952, 0.9791]), tensor([0.7891, 1.1275, 1.1918,  ..., 0.7843, 1.0974, 0.8600]), tensor([0.9917, 1.2000, 1.2881,  ..., 0.8786, 1.1206, 0.8751]), tensor([1.0099, 1.1269, 1.2279,  ..., 0.8678, 1.1348, 0.9092]), tensor([1.2382, 1.0302, 1.1631,  ..., 0.8691, 0.8892, 0.9278]), tensor([0.9856, 1.1147, 1.2014,  ..., 0.8056, 1.1587, 0.9053]), tensor([1.1317, 0.9985, 1.0749,  ..., 0.7630, 1.1524, 0.9305])], [tensor([0.9543, 1.0388, 0.7210,  ..., 0.9832, 1.0012, 0.8936]), tensor([1.0136, 0.9784, 0.7662,  ..., 1.0025, 0.9434, 1.0011]), tensor([1.1005, 0.9531, 0.9066,  ..., 1.1208, 1.0069, 0.8826]), tensor([1.0285, 1.1514, 0.7790,  ..., 1.1028, 0.9570, 1.1230]), tensor([1.0003, 0.9442, 0.7150,  ..., 1.1637, 1.1140, 1.1749]), tensor([1.0321, 0.8926, 0.7432,  ..., 1.2116, 0.8597, 0.9013]), tensor([1.0096, 0.9931, 0.8444,  ..., 1.0037, 0.7917, 1.0213]), tensor([1.0456, 0.9786, 0.7946,  ..., 1.1664, 1.0871, 0.9953])], [tensor([1.0569, 1.0862, 1.4290,  ..., 0.9999, 1.1149, 1.2867]), tensor([1.0527, 1.0666, 1.3433,  ..., 0.9072, 0.9537, 1.2315]), tensor([1.0414, 0.9778, 1.2309,  ..., 0.9417, 0.9497, 1.1731]), tensor([1.0035, 0.9007, 1.2942,  ..., 0.9356, 0.9288, 1.2393]), tensor([1.0519, 1.0083, 1.4491,  ..., 1.0482, 1.1490, 1.3074]), tensor([1.0518, 1.1032, 1.2488,  ..., 0.9315, 0.8946, 1.1156]), tensor([1.1063, 0.9320, 1.3696,  ..., 1.0389, 1.1288, 1.2919]), tensor([1.1276, 0.9598, 1.3350,  ..., 1.0197, 0.9274, 1.2747])], [tensor([1.0203, 1.1230, 1.0108,  ..., 0.9377, 1.3794, 1.1509]), tensor([1.0608, 0.8973, 1.0726,  ..., 1.1101, 1.4299, 1.1721]), tensor([1.0843, 1.0848, 1.0066,  ..., 1.0512, 1.5088, 1.3008]), tensor([0.8232, 1.1981, 1.1221,  ..., 1.0162, 1.4296, 1.2304]), tensor([0.8996, 1.2039, 1.0362,  ..., 0.9191, 1.5124, 1.2859]), tensor([1.1001, 1.1270, 1.0857,  ..., 1.0346, 1.4999, 1.2137]), tensor([0.8967, 1.0754, 1.0030,  ..., 0.8362, 1.4552, 1.0686]), tensor([0.9649, 1.1579, 0.9709,  ..., 0.8930, 1.4237, 1.2697])], [tensor([1.2060, 1.0121, 1.3238,  ..., 1.1181, 1.0076, 1.1742]), tensor([1.1198, 1.2105, 1.3710,  ..., 1.1649, 1.1484, 1.2121]), tensor([1.1019, 0.9968, 1.3766,  ..., 1.2189, 1.0881, 1.2696]), tensor([1.1061, 1.0188, 1.3489,  ..., 1.2092, 1.0427, 1.1387]), tensor([1.1957, 1.0868, 1.4378,  ..., 1.1609, 1.1051, 1.1616]), tensor([1.1093, 1.1162, 1.2027,  ..., 1.2225, 1.0305, 1.0265]), tensor([1.0662, 1.0545, 1.2711,  ..., 1.3115, 1.0907, 1.1971]), tensor([1.2096, 0.8963, 1.4476,  ..., 1.1695, 1.0436, 1.2249])], [tensor([1.2360, 1.3963, 1.5807,  ..., 1.1036, 1.1843, 0.7481]), tensor([1.3378, 1.4287, 1.3943,  ..., 0.8639, 1.1263, 1.2463]), tensor([1.1400, 1.4241, 1.4966,  ..., 1.0926, 1.2052, 1.0593]), tensor([1.2508, 1.4030, 1.6041,  ..., 1.3318, 1.2493, 0.7456]), tensor([1.1277, 1.2821, 1.2902,  ..., 1.1756, 0.8501, 0.7023]), tensor([1.2055, 1.2570, 1.2342,  ..., 1.2368, 0.9694, 1.0624]), tensor([1.0665, 1.3574, 1.4863,  ..., 1.2199, 1.0135, 1.0427]), tensor([1.0965, 1.3381, 1.4795,  ..., 1.2871, 1.0419, 1.0350])], [tensor([1.1458, 1.3500, 1.2674,  ..., 1.1935, 0.9606, 1.1721]), tensor([1.0436, 1.3004, 1.0828,  ..., 1.1210, 0.9704, 1.1291]), tensor([1.2887, 1.4340, 1.1552,  ..., 1.1976, 0.9739, 1.1550]), tensor([1.2146, 1.3723, 1.2398,  ..., 1.1710, 1.0646, 1.1024]), tensor([1.1569, 1.5157, 1.3430,  ..., 1.3826, 1.0776, 1.2093]), tensor([1.0632, 1.4111, 1.2260,  ..., 1.1006, 0.9924, 1.0030]), tensor([1.1568, 1.1752, 1.3652,  ..., 1.2246, 1.0855, 1.0985]), tensor([1.1908, 1.2357, 1.4100,  ..., 1.2406, 0.9270, 1.2423])], [tensor([1.2826, 1.2542, 1.1302,  ..., 1.0101, 1.4348, 1.7387]), tensor([1.1341, 1.5342, 0.9856,  ..., 1.0360, 1.4642, 1.6837]), tensor([1.2440, 1.1832, 1.1254,  ..., 1.1197, 1.6149, 1.4116]), tensor([1.2018, 1.2200, 1.0499,  ..., 1.1970, 1.4787, 1.6150]), tensor([1.2556, 1.4253, 1.1190,  ..., 1.0650, 1.5156, 1.6313]), tensor([1.1409, 1.1723, 0.9607,  ..., 1.2122, 1.6324, 1.7968]), tensor([1.2412, 1.2761, 1.0943,  ..., 1.0767, 1.4049, 1.4785]), tensor([1.2710, 1.1962, 1.1646,  ..., 0.9477, 1.3989, 1.6122])], [tensor([1.3991, 1.3217, 1.2021,  ..., 1.3549, 1.1533, 0.8206]), tensor([1.4803, 1.2764, 1.3405,  ..., 1.3668, 1.1699, 0.9000]), tensor([1.3551, 1.3982, 1.1302,  ..., 1.4946, 1.6071, 0.6820]), tensor([1.5479, 1.2865, 1.0495,  ..., 1.5776, 0.8271, 1.0516]), tensor([1.6206, 1.2691, 0.9314,  ..., 1.4907, 0.8250, 1.0685]), tensor([1.5529, 1.2689, 0.9669,  ..., 1.3988, 1.1047, 0.8801]), tensor([1.7016, 1.2050, 0.9620,  ..., 1.2652, 1.0093, 1.0107]), tensor([1.3395, 1.2351, 1.0125,  ..., 1.4106, 1.2098, 1.2781])], [tensor([1.8954, 1.2735, 1.5124,  ..., 1.2865, 2.0288, 1.6368]), tensor([2.0078, 1.6380, 1.1413,  ..., 1.4587, 1.5156, 1.6123]), tensor([1.7886, 1.1843, 1.4319,  ..., 1.3342, 1.5894, 1.8313]), tensor([1.8421, 1.9004, 1.5347,  ..., 1.3414, 1.9153, 1.6302]), tensor([2.3863, 2.9017, 1.1402,  ..., 1.9720, 1.7669, 2.2057]), tensor([2.1021, 1.1721, 1.6531,  ..., 1.6164, 1.8711, 1.6149]), tensor([1.5841, 1.4693, 1.3233,  ..., 1.3155, 1.7011, 1.8367]), tensor([1.5687, 1.2021, 1.4617,  ..., 1.5615, 1.4214, 1.4781])], [tensor([1.9147, 1.7384, 2.5975,  ..., 2.0499, 1.2527, 1.6513]), tensor([1.5744, 1.0537, 3.6550,  ..., 1.9701, 2.1184, 1.6110]), tensor([1.0242, 1.6370, 2.7648,  ..., 1.6121, 1.3317, 1.2118]), tensor([1.4427, 1.6232, 3.4913,  ..., 2.0580, 1.3360, 1.9868]), tensor([1.2886, 0.9687, 2.9907,  ..., 1.6315, 1.3125, 1.7860]), tensor([1.1034, 0.8057, 3.2579,  ..., 1.7683, 2.0336, 1.5080]), tensor([1.6093, 1.5626, 2.7575,  ..., 1.5077, 1.3917, 2.0452]), tensor([1.4282, 1.2300, 3.5016,  ..., 2.1860, 1.9181, 1.3813])], [tensor([2.7957, 2.0615, 1.4699,  ..., 2.8010, 1.5469, 0.9476]), tensor([1.2753, 1.9842, 1.5168,  ..., 3.5985, 2.5626, 2.5658]), tensor([1.0019, 1.9214, 1.6884,  ..., 2.2325, 1.8918, 0.6210]), tensor([1.9379, 1.8144, 1.9720,  ..., 2.5789, 1.5979, 0.8140]), tensor([1.2920, 1.8307, 1.8135,  ..., 2.7493, 1.8423, 1.1888]), tensor([1.3004, 2.3299, 2.1300,  ..., 2.6711, 1.6483, 1.2008]), tensor([2.7339, 2.1339, 1.1975,  ..., 2.6906, 1.9179, 0.6390]), tensor([1.0844, 1.5942, 1.3104,  ..., 2.8132, 1.4576, 0.9005])], [tensor([0.8043, 1.4050, 0.7516,  ..., 1.2915, 1.4012, 1.7647]), tensor([2.1494, 2.5968, 2.1434,  ..., 1.7889, 3.0061, 2.6610]), tensor([1.9432, 2.8197, 3.0612,  ..., 2.1913, 1.0085, 3.3341]), tensor([1.8020, 1.7881, 1.8137,  ..., 2.9526, 2.3345, 1.9078]), tensor([1.5742, 1.3201, 0.8743,  ..., 1.2838, 1.9178, 1.6656]), tensor([1.8994, 2.2997, 3.0329,  ..., 1.7248, 2.7145, 2.3381]), tensor([1.6565, 1.6635, 2.2893,  ..., 1.7390, 2.2130, 2.0811]), tensor([2.2528, 2.3394, 1.8805,  ..., 2.1947, 3.1793, 2.1252])]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_61259/499095173.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  thresholds = torch.load(f'{save_path}/thresholds.pt')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "save_path = './threshold/c4_mixtral_up'\n",
    "thresholds = torch.load(f'{save_path}/thresholds.pt')\n",
    "print(thresholds[\"up_proj_states_thresholds_2\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
